"issue_key","issuetype","sprint_name","created","reporter","assignee","summary","description","storypoints","Tinprogress_proportion","Tassignee_proportion","Tsprint_proportion","isValid_inprogress","isValid_sprint","isValid_assignee","T1","T2","T3","T4","T5","T_sp_first","T_sp_second","T_sprint","T_sprint_plus_1hr","T_assignee","T_assignee_plus_1hr","T_inprogress","T_inprogress_plus_1hr","sp_estimator","T_infochange_start","T_sp","T_sp_year","T_sp_last","T_sp_after_sprint","last_summary","last_description","firststorypoints","sp_change","sp_sprint","sp_change_count","sp_diff_percent","count_comment_before","count_summ_before","info_change"
"XD-3765","Story","","2017-03-21T16:54:43.000+0000","mark.pollack","jvalkeal","Fix stream failover on YARN","See https://github.com/spring-projects/spring-xd/issues/1924",8.0,-1.0,0.0,-1.0,False,False,False,1490115283000.0,-1.0,-1.0,1490115283000.0,1490115323000.0,1490115283000.0,-1.0,-1.0,-1.0,1490115283000.0,1490115323000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1490115283000.0,-1.0,"Fix stream failover ","See https://github.com/spring-projects/spring-xd/issues/1924",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-3748","Bug","","2016-02-24T10:51:02.000+0000","dgarcia","","Unable to register the JMX bean MessageHistory from Spring Integration","If I try to use <int:message-history/> when developing a Spring XD module, it fails when try to export the JMX bean. I've seen that the naming strategy used is org.springframework.xd.dirt.module.jmx.ModuleObjectNamingStrategy

The stackTrace:

2016-02-24T10:40:39+0000 1.3.1.RELEASE ERROR DeploymentsPathChildrenCache-0 container.DeploymentListener - Exception deploying module
org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.history.MessageHistoryConfigurer@24902b5f] with key 'messageHistoryConfigurer'; nested exception is javax.management.MalformedObjectNameException: Key properties cannot be empty
	at org.springframework.integration.monitor.IntegrationMBeanExporter.registerBeanInstance(IntegrationMBeanExporter.java:375) ~[spring-integration-jmx-4.2.5.RELEASE.jar:na]
	at org.springframework.integration.monitor.IntegrationMBeanExporter.afterSingletonsInstantiated(IntegrationMBeanExporter.java:288) ~[spring-integration-jmx-4.2.5.RELEASE.jar:na]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:792) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:839) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:538) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:213) ~[spring-xd-module-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.server.container.DeploymentListener.deployModule(DeploymentListener.java:365) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.server.container.DeploymentListener.deployStreamModule(DeploymentListener.java:334) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.server.container.DeploymentListener.onChildAdded(DeploymentListener.java:181) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.server.container.DeploymentListener.childEvent(DeploymentListener.java:149) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509) [curator-recipes-2.6.0.jar:na]
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503) [curator-recipes-2.6.0.jar:na]
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92) [curator-framework-2.6.0.jar:na]
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [guava-16.0.1.jar:na]
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83) [curator-framework-2.6.0.jar:na]
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500) [curator-recipes-2.6.0.jar:na]
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35) [curator-recipes-2.6.0.jar:na]
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762) [curator-recipes-2.6.0.jar:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_72]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_72]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_72]
Caused by: javax.management.MalformedObjectNameException: Key properties cannot be empty
	at javax.management.ObjectName.construct(ObjectName.java:483) ~[na:1.8.0_72]
	at javax.management.ObjectName.<init>(ObjectName.java:1382) ~[na:1.8.0_72]
	at javax.management.ObjectName.getInstance(ObjectName.java:1273) ~[na:1.8.0_72]
	at org.springframework.jmx.support.ObjectNameManager.getInstance(ObjectNameManager.java:62) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
	at org.springframework.xd.dirt.module.jmx.ModuleObjectNamingStrategy.getObjectName(ModuleObjectNamingStrategy.java:50) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.jmx.export.MBeanExporter.getObjectName(MBeanExporter.java:751) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
",1.0,0.0177127764643487,0.0177125092227001,-1.0,True,False,True,1456311062000.0,1456775022000.0,1456778622000.0,1482503684000.0,1482504584000.0,1456311062000.0,-1.0,-1.0,-1.0,1456775015000.0,1456778615000.0,1456775022000.0,1456778622000.0,"dgarcia",-1.0,-1.0,0,1456311062000.0,-1.0,"Unable to register the JMX bean MessageHistory from Spring Integration","If I try to use <int:message-history/> when developing a Spring XD module, it fails when try to export the JMX bean. I've seen that the naming strategy used is org.springframework.xd.dirt.module.jmx.ModuleObjectNamingStrategy

The stackTrace:
{code}
2016-02-24T10:40:39+0000 1.3.1.RELEASE ERROR DeploymentsPathChildrenCache-0 container.DeploymentListener - Exception deploying module
org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.history.MessageHistoryConfigurer@24902b5f] with key 'messageHistoryConfigurer'; nested exception is javax.management.MalformedObjectNameException: Key properties cannot be empty
	at org.springframework.integration.monitor.IntegrationMBeanExporter.registerBeanInstance(IntegrationMBeanExporter.java:375) ~[spring-integration-jmx-4.2.5.RELEASE.jar:na]
	at org.springframework.integration.monitor.IntegrationMBeanExporter.afterSingletonsInstantiated(IntegrationMBeanExporter.java:288) ~[spring-integration-jmx-4.2.5.RELEASE.jar:na]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:792) ~[spring-beans-4.2.4.RELEASE.jar:4.2.4.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:839) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:538) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139) ~[spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:213) ~[spring-xd-module-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.server.container.DeploymentListener.deployModule(DeploymentListener.java:365) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.server.container.DeploymentListener.deployStreamModule(DeploymentListener.java:334) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.server.container.DeploymentListener.onChildAdded(DeploymentListener.java:181) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.xd.dirt.server.container.DeploymentListener.childEvent(DeploymentListener.java:149) [spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509) [curator-recipes-2.6.0.jar:na]
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503) [curator-recipes-2.6.0.jar:na]
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92) [curator-framework-2.6.0.jar:na]
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297) [guava-16.0.1.jar:na]
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83) [curator-framework-2.6.0.jar:na]
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500) [curator-recipes-2.6.0.jar:na]
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35) [curator-recipes-2.6.0.jar:na]
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762) [curator-recipes-2.6.0.jar:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_72]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_72]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_72]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_72]
Caused by: javax.management.MalformedObjectNameException: Key properties cannot be empty
	at javax.management.ObjectName.construct(ObjectName.java:483) ~[na:1.8.0_72]
	at javax.management.ObjectName.<init>(ObjectName.java:1382) ~[na:1.8.0_72]
	at javax.management.ObjectName.getInstance(ObjectName.java:1273) ~[na:1.8.0_72]
	at org.springframework.jmx.support.ObjectNameManager.getInstance(ObjectNameManager.java:62) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
	at org.springframework.xd.dirt.module.jmx.ModuleObjectNamingStrategy.getObjectName(ModuleObjectNamingStrategy.java:50) ~[spring-xd-dirt-1.3.1.RELEASE.jar:1.3.1.RELEASE]
	at org.springframework.jmx.export.MBeanExporter.getObjectName(MBeanExporter.java:751) ~[spring-context-4.2.4.RELEASE.jar:4.2.4.RELEASE]
{code}",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-3747","Improvement","","2016-02-23T18:31:37.000+0000","grussell","","Rabbit BusL Expose ChannelCacheSize on CachingConnectionFactory","http://stackoverflow.com/questions/35563064/processing-messages-through-namedchannels-with-prefetch-1/35584333#35584333",1.0,0.2303359846852089,0.2302641813652062,-1.0,True,False,True,1456252297000.0,1462299139000.0,1462302739000.0,1482503665000.0,1482504565000.0,1456252297000.0,-1.0,-1.0,-1.0,1462297254000.0,1462300854000.0,1462299139000.0,1462302739000.0,"grussell",-1.0,-1.0,0,1456252297000.0,-1.0,"Rabbit Bus: Expose ChannelCacheSize on CachingConnectionFactory","http://stackoverflow.com/questions/35563064/processing-messages-through-namedchannels-with-prefetch-1/35584333#35584333",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-3746","Improvement","","2016-02-23T18:18:48.000+0000","mark.pollack","","Update Spring Framework to 4.2.4","",1.0,0.6111111111111112,0.6111111111111112,-1.0,False,False,False,1456251528000.0,1456251539000.0,1456251546000.0,1456251528000.0,1456251546000.0,1456251528000.0,-1.0,-1.0,-1.0,1456251539000.0,1456251546000.0,1456251539000.0,1456251546000.0,"mark.pollack",-1.0,-1.0,0,1456251528000.0,-1.0,"Update Spring Framework to 4.2.4","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-3745","Improvement","","2016-02-23T18:04:09.000+0000","mark.pollack","","Update Spring-AMQP, RabbitMQ Client Versions","Update to amqp-client 3.6.0 and spring-amqp 1.5.4",1.0,0.40625,0.25,-1.0,False,False,False,1456250649000.0,1456250662000.0,1456250681000.0,1456250649000.0,1456250681000.0,1456250649000.0,-1.0,-1.0,-1.0,1456250657000.0,1456250681000.0,1456250662000.0,1456250681000.0,"mark.pollack",-1.0,-1.0,0,1456250649000.0,-1.0,"Update Spring-AMQP to 3.6, RabbitMQ Client to 1.5.4","Update to amqp-client 3.6.0 and spring-amqp 1.5.4",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-3744","Improvement","Sprint 68","2016-02-22T15:27:31.000+0000","grussell","grussell","Suppress DeliveryMode Header in RabbitMQ Source","Related to XD-2567 which fixed this problem, but only in the bus.

{quote}
2016-02-19T18:25:24-0500 1.2.1.RELEASE WARN SimpleAsyncTaskExecutor-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode]
{quote}",1.0,0.1133161512027491,0.0968213058419244,0.0968213058419244,True,True,True,1456154851000.0,1456156170000.0,1456159770000.0,1456165591000.0,1456166491000.0,1456154851000.0,-1.0,1456155978000.0,1456159578000.0,1456155978000.0,1456159578000.0,1456156170000.0,1456159770000.0,"grussell",1456159578000.0,1456154851000.0,46,1456154851000.0,-1.0,"Suppress DeliveryMode Header in RabbitMQ Source","Related to XD-2567 which fixed this problem, but only in the bus.

{quote}
2016-02-19T18:25:24-0500 1.2.1.RELEASE WARN SimpleAsyncTaskExecutor-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode]
{quote}",1.0,False,1.0,0,0.0,-1,1,False
"XD-3743","Bug","Sprint 68","2016-02-17T14:28:31.000+0000","grussell","Gary Russell","Update to Spring Integration 4.2.5 When Available (Fix Metrics)","See INT-3956",1.0,0.6199197074016018,0.61987341018671,0.0004199818779473,True,True,True,1455719311000.0,1455906771000.0,1455910371000.0,1456020805000.0,1456021705000.0,1455719311000.0,-1.0,1455719438000.0,1455723038000.0,1455906757000.0,1455910357000.0,1455906771000.0,1455910371000.0,"grussell",1455723038000.0,1455719311000.0,46,1455719311000.0,-1.0,"Update to Spring Integration 4.2.5 When Available (Fix Metrics)","See INT-3956",1.0,False,1.0,0,0.0,-1,0,False
"XD-3742","Improvement","Sprint 68","2016-02-16T14:59:06.000+0000","dturanski","dturanski","As a user I would like to encrypt secret information for SSL configuration","The following XD components have been identified to support SSL via an `sslProperties` property which points to the location of a properties file. The properties encryption extension for 1.3.1 does not currently apply to these:

  * Rabbit Message Bus
  * Rabbit Source
  * Rabbit Sink
  * Http Source (NettyHttpInboundChannelAdapter). 

This can be made to work in the case of Rabbit since the latest RabbitConnectionFactoryBean supports individual SSL properties settings as an alternative to the properties file. The http source may be extended to use the same approach.",3.0,0.3370220494175047,0.0,0.1867876459841521,True,True,True,1455634746000.0,1455728274000.0,1455731874000.0,1455911359000.0,1455912259000.0,1455634746000.0,-1.0,1455686582000.0,1455690182000.0,1455634746000.0,1455638346000.0,1455728274000.0,1455731874000.0,"dturanski",1455690182000.0,1455634746000.0,46,1455634746000.0,-1.0,"Enable in line SSL properties as an alternative to external properties files","The following XD components have been identified to support SSL via an `sslProperties` property which points to the location of a properties file. The properties encryption extension for 1.3.1 does not currently apply to these:

  * Rabbit Message Bus
  * Rabbit Source
  * Rabbit Sink
  * Http Source (NettyHttpInboundChannelAdapter). 

This can be made to work in the case of Rabbit since the latest RabbitConnectionFactoryBean supports individual SSL properties settings as an alternative to the properties file. The http source may be extended to use the same approach.",3.0,False,3.0,0,0.0,-1,0,True
"XD-3741","Bug","Sprint 68","2016-02-13T19:04:24.000+0000","stefano.massera","","[Flo] Stream creation/definitions doesn't show any component"," As a Flo for Spring XD user, I would like to be able to create a new stream using the graphicat UI. 

This flow should be shown in a graphical way also in definition tab.
!http://example.com/image.png!
Right now it doesn't happen due to a javascript error.

{code}
TypeError: this.node.getTransformToElement is not a function
    at Object.VElement.bbox (http://localhost:9393/admin-ui/lib/joint/src/vectorizer.js:323:36)
    at joint.dia.ElementView.joint.dia.CellView.extend.positionRelative (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2740:51)
    at null.<anonymous> (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2710:18)
    at http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1177:23
    at eval (eval at createIterator (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1:0), <anonymous>:10:9)
    at Function.forEach (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:3645:9)
    at joint.dia.ElementView.joint.dia.CellView.extend.update (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2700:11)
    at bound [as update] (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1005:21)
    at joint.dia.ElementView.joint.dia.CellView.extend.render (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2903:14)
    at joint.dia.Paper.Backbone.View.extend.addCell (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:5004:14)(anonymous function) @ :9393/admin-ui/lib/angular/angular.js:11500
:9393!attachment-name.jpg|thumbnail!
{code}
",1.0,-1.0,-1.0,0.0005875347611536,False,True,False,1455390264000.0,-1.0,-1.0,1456020816000.0,1456021716000.0,1455390264000.0,-1.0,1455390635000.0,1455394235000.0,-1.0,-1.0,-1.0,-1.0,"stefano.massera",1455394235000.0,1455390264000.0,46,1455390264000.0,-1.0,"[Flo] Stream creation/definitions doesn't show any component"," As a Flo for Spring XD user, I would like to be able to create a new stream using the graphicat UI. 

This flow should be shown in a graphical way also in definition tab.
!http://example.com/image.png!
Right now it doesn't happen due to a javascript error.

{code}
TypeError: this.node.getTransformToElement is not a function
    at Object.VElement.bbox (http://localhost:9393/admin-ui/lib/joint/src/vectorizer.js:323:36)
    at joint.dia.ElementView.joint.dia.CellView.extend.positionRelative (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2740:51)
    at null.<anonymous> (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2710:18)
    at http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1177:23
    at eval (eval at createIterator (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1:0), <anonymous>:10:9)
    at Function.forEach (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:3645:9)
    at joint.dia.ElementView.joint.dia.CellView.extend.update (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2700:11)
    at bound [as update] (http://localhost:9393/admin-ui/lib/lodash/lodash.compat.js:1005:21)
    at joint.dia.ElementView.joint.dia.CellView.extend.render (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:2903:14)
    at joint.dia.Paper.Backbone.View.extend.addCell (http://localhost:9393/admin-ui/lib/joint/dist/joint.all.clean.js:5004:14)(anonymous function) @ :9393/admin-ui/lib/angular/angular.js:11500
:9393!attachment-name.jpg|thumbnail!
{code}
",1.0,False,1.0,0,0.0,0,0,False
"XD-3740","Bug","Sprint 67","2016-02-11T17:45:54.000+0000","dgarcia","iperumal","Kafka message bus maxWait property is not set up","The maxWait property from server.yml in the message bus section for kafka is not propagated through the code, it is ignored.",1.0,0.3850721987967073,0.1070539849953087,0.1070539849953087,True,True,True,1455212754000.0,1455524257000.0,1455527857000.0,1456020801000.0,1456021701000.0,1455212754000.0,-1.0,1455299355000.0,1455302955000.0,1455299355000.0,1455302955000.0,1455524257000.0,1455527857000.0,"dgarcia",1455302955000.0,1455212754000.0,46,1455212754000.0,-1.0,"Kafka message bus maxWait property is not set up","The maxWait property from server.yml in the message bus section for kafka is not propagated through the code, it is ignored.",1.0,False,1.0,0,0.0,0,0,False
"XD-3739","Bug","Sprint 67","2016-02-05T16:01:49.000+0000","david_geary","grussell","Incorrect refresh period for groovy scripts","All modules that allow groovy implementations (filter, script, transform, router, tcpclient) allow automatic refresh of the script when it changes. In the XD documentation it is stated that this refresh occurs every minute eg for filter at http://docs.spring.io/spring-xd/docs/1.3.0.RELEASE/reference/html/#filter ""The script is checked for updates every 60 seconds, so it may be replaced in a running system. "" 

This set up can be seen in the spring xml for the modules - eg (again for filter)

{code:xml}
<filter input-channel=""to.script"" output-channel=""output"">
	<int-groovy:script location=""${script:filter.groovy}"" script-variable-generator=""variableGenerator"" refresh-check-delay=""60""/>
</filter>
{code}

However from the spring integration documentation http://docs.spring.io/spring-integration/docs/4.2.4.RELEASE/reference/html/messaging-endpoints-chapter.html#scripting-config
it specifies that the refresh-check-delay parameter is actually in milliseconds - ie the above XD configuration would recheck the script every 60 milliseconds which may be a performance concern as it will be checking the lastmodified time of the script file. 

Ideally this parameter would be configurable - in our case we would usually eliminate the refresh check altogether (set to -1) as our scripts will not change (or if they did a redeploy of the module would pick it up)

",5.0,0.0026753921355914,0.002287723507715,0.0014725026482712,True,True,True,1454688109000.0,1454689786000.0,1454693386000.0,1455314033000.0,1455314933000.0,1454688109000.0,-1.0,1454689032000.0,1454692632000.0,1454689543000.0,1454693143000.0,1454689786000.0,1454693386000.0,"david_geary",1454692632000.0,1454688109000.0,46,1454688109000.0,-1.0,"Incorrect refresh period for groovy scripts","All modules that allow groovy implementations (filter, script, transform, router, tcpclient) allow automatic refresh of the script when it changes. In the XD documentation it is stated that this refresh occurs every minute eg for filter at http://docs.spring.io/spring-xd/docs/1.3.0.RELEASE/reference/html/#filter ""The script is checked for updates every 60 seconds, so it may be replaced in a running system. "" 

This set up can be seen in the spring xml for the modules - eg (again for filter)

{code:xml}
<filter input-channel=""to.script"" output-channel=""output"">
	<int-groovy:script location=""${script:filter.groovy}"" script-variable-generator=""variableGenerator"" refresh-check-delay=""60""/>
</filter>
{code}

However from the spring integration documentation http://docs.spring.io/spring-integration/docs/4.2.4.RELEASE/reference/html/messaging-endpoints-chapter.html#scripting-config
it specifies that the refresh-check-delay parameter is actually in milliseconds - ie the above XD configuration would recheck the script every 60 milliseconds which may be a performance concern as it will be checking the lastmodified time of the script file. 

Ideally this parameter would be configurable - in our case we would usually eliminate the refresh check altogether (set to -1) as our scripts will not change (or if they did a redeploy of the module would pick it up)

",5.0,False,5.0,0,0.0,-1,2,False
"XD-3738","Improvement","Sprint 67","2016-02-03T09:40:05.000+0000","dturanski","dturanski","As a user I would like to encrypt secret information in XD configuration files","Spring XD keeps passwords in text files such sas servers.yml, properties files, and module configuration files. Some users have requested a way to store encrypted values rather than clear text.  XD should provide a ""hook"" for users to provide a custom component to detect encrypted property values and decrypt them during container, admin, and module initialization.",2.0,0.40521967226857,0.0,0.0,True,True,True,1454492405000.0,1454956465000.0,1454960065000.0,1455636711000.0,1455637611000.0,1454492405000.0,-1.0,1454492405000.0,1454496005000.0,1454492405000.0,1454496005000.0,1454956465000.0,1454960065000.0,"dturanski",1454496005000.0,1454492405000.0,46,1454492405000.0,-1.0,"Encrypt secret information in XD configuration files","Spring XD keeps passwords in text files such sas servers.yml, properties files, and module configuration files. Some users have requested a way to store encrypted values rather than clear text.  XD should provide a ""hook"" for users to provide a custom component to detect encrypted property values and decrypt them during container, admin, and module initialization.",2.0,False,2.0,0,0.0,-1,0,True
"XD-3737","Bug","Sprint 67","2016-02-02T17:52:11.000+0000","hillert","hillert","REST - Do not redirect after logout","In the following PR we removed the *RestLogoutSuccessHandler*. 

https://github.com/spring-projects/spring-xd/pull/1562

This is necessary, though, for REST calls and the Admin UI. Otherwise some weird UI behavior might occur due to the HTTP redirect.",1.0,0.0002771707265071,9.594371302169392e-05,0.0036245402697084,True,True,True,1454435531000.0,1454435557000.0,1454439157000.0,1454528436000.0,1454529336000.0,1454435531000.0,-1.0,1454435871000.0,1454439471000.0,1454435540000.0,1454439140000.0,1454435557000.0,1454439157000.0,"hillert",1454439471000.0,1454435531000.0,46,1454435531000.0,-1.0,"REST - Do not redirect after logout","In the following PR we removed the *RestLogoutSuccessHandler*. 

https://github.com/spring-projects/spring-xd/pull/1562

This is necessary, though, for REST calls and the Admin UI. Otherwise some weird UI behavior might occur due to the HTTP redirect.",1.0,False,1.0,0,0.0,0,0,False
"XD-3736","Improvement","Sprint 67","2016-02-01T19:00:42.000+0000","grussell","grussell","Rabbit Pub/Sub Consumers Should Support Concurrency","PubSub consumers can support concurrency since the threads are competing consumers on the queue.

",2.0,0.0016064983638672,0.0,0.0,True,True,True,1454353242000.0,1454354787000.0,1454358387000.0,1455314061000.0,1455314961000.0,1454353242000.0,-1.0,1454353242000.0,1454356842000.0,1454353242000.0,1454356842000.0,1454354787000.0,1454358387000.0,"grussell",1454356842000.0,1454353242000.0,46,1454353242000.0,-1.0,"Rabbit Pub/Sub Consumers Should Support Concurrency","PubSub consumers can support concurrency since the threads are competing consumers on the queue.

",2.0,False,2.0,0,0.0,-1,1,False
"XD-3734","Bug","Sprint 65","2016-01-15T19:55:40.000+0000","grussell","grussell","AutoBindDLQ Incompatible with Partitioned Streams (Producer Side).","See http://stackoverflow.com/questions/34817906/spring-xd-rabbitmq-partitioned-stream-deployment-in-failure",1.0,0.000754759578999,0.000754759578999,0.0001565549345085,True,True,True,1452887740000.0,1452889572000.0,1452893172000.0,1455314103000.0,1455315003000.0,1452887740000.0,-1.0,1452888120000.0,1452891720000.0,1452889572000.0,1452893172000.0,1452889572000.0,1452893172000.0,"grussell",1452891720000.0,1452887740000.0,46,1452887740000.0,-1.0,"AutoBindDLQ Incompatible with Partitioned Streams (Producer Side).","See http://stackoverflow.com/questions/34817906/spring-xd-rabbitmq-partitioned-stream-deployment-in-failure",1.0,False,1.0,0,0.0,-1,0,False
"XD-3733","Improvement","Sprint 65","2016-01-14T13:10:10.000+0000","dturanski","","Document redis pool properties in servers.yml","Add spring.redis.pool.*  properties to server.yml, commented out to show default values., e.g., 

   maxIdle: 8,
   minIdle: 0, 
   maxActive: 8,
   maxWait: -1
",1.0,0.618765698660578,0.618765698660578,0.0017600230099269,True,True,True,1452777010000.0,1454347459000.0,1454351059000.0,1455314145000.0,1455315045000.0,1452777010000.0,-1.0,1452781477000.0,1452785077000.0,1454347459000.0,1454351059000.0,1454347459000.0,1454351059000.0,"dturanski",1452785077000.0,1452777010000.0,46,1452777010000.0,-1.0,"Document redis pool properties in servers.yml","Add spring.redis.pool.*  properties to server.yml, commented out to show default values., e.g., 

   maxIdle: 8,
   minIdle: 0, 
   maxActive: 8,
   maxWait: -1
",1.0,False,1.0,0,0.0,-1,0,False
"XD-3731","Improvement","Sprint 65","2016-01-13T15:17:03.000+0000","grussell","grussell","Clean Up Compiler/Javadoc Warnings","{noformat}
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:96: warning: [rawtypes] found raw type: DomainRepository
			DomainRepository instanceRepository, String jobName,
			^
  missing type arguments for generic class DomainRepository<T,ID>
  where T,ID are type-variables:
    T extends Object declared in interface DomainRepository
    ID extends Serializable,Comparable<ID> declared in interface DomainRepository
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:116: warning: [rawtypes] found raw type: DomainRepository
			DomainRepository instanceRepository, String jobName,
			^
  missing type arguments for generic class DomainRepository<T,ID>
  where T,ID are type-variables:
    T extends Object declared in interface DomainRepository
    ID extends Serializable,Comparable<ID> declared in interface DomainRepository
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:127: warning: [unchecked] unchecked conversion
		this.instanceRepository = instanceRepository;
		                          ^
  required: DomainRepository<JobDefinition,String>
  found:    DomainRepository
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/ModuleException.java:23: warning: [serial] serializable class ModuleException has no definition of serialVersionUID
public class ModuleException extends RuntimeException {
       ^
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/support/ModuleDefinitionService.java:130: warning: [try] explicit call to close() on an auto-closeable resource
			target.close();
			      ^
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/StreamException.java:23: warning: [serial] serializable class StreamException has no definition of serialVersionUID
public class StreamException extends RuntimeException {
       ^
/Users/grussell/.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/1.0.0/512563fec38547cf446fbd221069799df1ab158d/kite-data-core-1.0.0.jar(org/kitesdk/data/DatasetDescriptor.class): warning: Cannot find annotation method 'value()' in type 'SuppressWarnings': class file for edu.umd.cs.findbugs.annotations.SuppressWarnings not found
/Users/grussell/.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/1.0.0/512563fec38547cf446fbd221069799df1ab158d/kite-data-core-1.0.0.jar(org/kitesdk/data/DatasetDescriptor.class): warning: Cannot find annotation method 'justification()' in type 'SuppressWarnings'
3 warnings
/Users/grussell/Development/spring-xd/spring-xd-tuple/src/test/java/org/springframework/xd/tuple/TupleJsonMarshallerTests.java:77: warning: [unchecked] unchecked cast
		List<Tuple> body = (List<Tuple>) tuple.getValue(""body"");
		                                               ^
  required: List<Tuple>
  found:    Object
:api
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/job/dao/XdJdbcSearchableJobExecutionDao.java:151: warning - @return tag has no arguments.
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/job/dao/XdJdbcSearchableJobExecutionDao.java:166: warning - @return tag has no arguments.
/Users/grussell/Development/spring-xd/spring-xd-rest-domain/src/main/java/org/springframework/xd/rest/domain/JobExecutionInfoResource.java:252: warning - @return tag cannot be used in method with void return type.
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer
{noformat}",1.0,2.6750208842701893e-06,0.0,0.104764135765717,True,True,True,1452698223000.0,1452698230000.0,1452701830000.0,1455314125000.0,1455315025000.0,1452698223000.0,-1.0,1452972370000.0,1452975970000.0,1452698223000.0,1452701823000.0,1452698230000.0,1452701830000.0,"grussell",1452975970000.0,1452698223000.0,46,1452698223000.0,-1.0,"Clean Up Compiler/Javadoc Warnings","{noformat}
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:96: warning: [rawtypes] found raw type: DomainRepository
			DomainRepository instanceRepository, String jobName,
			^
  missing type arguments for generic class DomainRepository<T,ID>
  where T,ID are type-variables:
    T extends Object declared in interface DomainRepository
    ID extends Serializable,Comparable<ID> declared in interface DomainRepository
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:116: warning: [rawtypes] found raw type: DomainRepository
			DomainRepository instanceRepository, String jobName,
			^
  missing type arguments for generic class DomainRepository<T,ID>
  where T,ID are type-variables:
    T extends Object declared in interface DomainRepository
    ID extends Serializable,Comparable<ID> declared in interface DomainRepository
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/batch/tasklet/JobLaunchingTasklet.java:127: warning: [unchecked] unchecked conversion
		this.instanceRepository = instanceRepository;
		                          ^
  required: DomainRepository<JobDefinition,String>
  found:    DomainRepository
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/ModuleException.java:23: warning: [serial] serializable class ModuleException has no definition of serialVersionUID
public class ModuleException extends RuntimeException {
       ^
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/support/ModuleDefinitionService.java:130: warning: [try] explicit call to close() on an auto-closeable resource
			target.close();
			      ^
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/StreamException.java:23: warning: [serial] serializable class StreamException has no definition of serialVersionUID
public class StreamException extends RuntimeException {
       ^
/Users/grussell/.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/1.0.0/512563fec38547cf446fbd221069799df1ab158d/kite-data-core-1.0.0.jar(org/kitesdk/data/DatasetDescriptor.class): warning: Cannot find annotation method 'value()' in type 'SuppressWarnings': class file for edu.umd.cs.findbugs.annotations.SuppressWarnings not found
/Users/grussell/.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/1.0.0/512563fec38547cf446fbd221069799df1ab158d/kite-data-core-1.0.0.jar(org/kitesdk/data/DatasetDescriptor.class): warning: Cannot find annotation method 'justification()' in type 'SuppressWarnings'
3 warnings
/Users/grussell/Development/spring-xd/spring-xd-tuple/src/test/java/org/springframework/xd/tuple/TupleJsonMarshallerTests.java:77: warning: [unchecked] unchecked cast
		List<Tuple> body = (List<Tuple>) tuple.getValue(""body"");
		                                               ^
  required: List<Tuple>
  found:    Object
:api
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/job/dao/XdJdbcSearchableJobExecutionDao.java:151: warning - @return tag has no arguments.
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/job/dao/XdJdbcSearchableJobExecutionDao.java:166: warning - @return tag has no arguments.
/Users/grussell/Development/spring-xd/spring-xd-rest-domain/src/main/java/org/springframework/xd/rest/domain/JobExecutionInfoResource.java:252: warning - @return tag cannot be used in method with void return type.
/Users/grussell/Development/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/admin/deployment/zk/ContainerListener.java:145: warning - Tag @link: reference not found: DepartedContainerDeployer
{noformat}",1.0,False,1.0,0,0.0,-1,0,False
"XD-3730","Bug","","2016-01-08T15:53:19.000+0000","virgiled","","NPE in spring-integration when using kafka as message bus when using aggrzgation module","as stated in https://jira.spring.io/browse/INT-3908 sprint-integration in springxd can't use kafka as message bus in most case. Could it spring-xd integrat this fix for us to use it?",3.0,-1.0,-1.0,-1.0,False,False,False,1452268399000.0,-1.0,-1.0,1456687301000.0,1456688201000.0,1452268399000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"virgiled",-1.0,-1.0,0,1452268399000.0,-1.0,"NPE in spring-integration when using kafka as message bus when using aggrzgation module","as stated in https://jira.spring.io/browse/INT-3908 sprint-integration in springxd can't use kafka as message bus in most case. Could it spring-xd integrat this fix for us to use it?",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-3725","Bug","Sprint 64","2015-12-17T18:11:37.000+0000","grussell","grussell","EmbeddedHeadersMessageConverter Buffer Overflow","See https://github.com/spring-projects/spring-xd/issues/1871",1.0,0.0018350673389927,0.0002824407991319,0.0007643454394587,True,True,True,1450375897000.0,1450377047000.0,1450380647000.0,1451001677000.0,1451002577000.0,1450375897000.0,-1.0,1450376376000.0,1450379976000.0,1450376074000.0,1450379674000.0,1450377047000.0,1450380647000.0,"grussell",1450379976000.0,1450375897000.0,45,1450375897000.0,-1.0,"EmbeddedHeadersMessageConverter Buffer Overflow","See https://github.com/spring-projects/spring-xd/issues/1871",1.0,False,1.0,0,0.0,-1,0,False
"XD-3721","Bug","Sprint 65","2015-12-14T18:40:55.000+0000","ali.iqbal@gmail.com","","XD Admin UI log out does not function properly","I am using XD 1.2.1.RELEASE. I have following environment variables 

XD_CONFIG_NAME = mycompany
And 
SPRING_PROFILE_ACTIVE= prod, admin

i have XD configuration file (mycompany-prod.yml) with following security configuration

# Config to enable security on administration endpoints (consider adding ssl)
spring:
  profiles: prod
security:
  basic:
    enabled: true # false to disable security settings (default)
    realm: SpringXD
xd:
  security:
    authentication:
      file:
        enabled: true 
        users:
          xdadmin: pwd, ROLE_ADMIN,ROLE_VIEW,ROLE_CREATE

I get a login screen, login works alright. When i logout - i still see all the tabs and contents in all the tabs. See the attached screenshot.
",1.0,0.978150350438572,0.9781489910518368,0.406031145815751,True,True,True,1450118455000.0,1454435771000.0,1454439371000.0,1454531310000.0,1454532210000.0,1450118455000.0,-1.0,1451910577000.0,1451914177000.0,1454435765000.0,1454439365000.0,1454435771000.0,1454439371000.0,"ali.iqbal@gmail.com",1451914177000.0,1450118455000.0,45,1450118455000.0,-1.0,"XD Admin UI log out does not function properly","I am using XD 1.2.1.RELEASE. I have following environment variables 

XD_CONFIG_NAME = mycompany
And 
SPRING_PROFILE_ACTIVE= prod, admin

i have XD configuration file (mycompany-prod.yml) with following security configuration

# Config to enable security on administration endpoints (consider adding ssl)
spring:
  profiles: prod
security:
  basic:
    enabled: true # false to disable security settings (default)
    realm: SpringXD
xd:
  security:
    authentication:
      file:
        enabled: true 
        users:
          xdadmin: pwd, ROLE_ADMIN,ROLE_VIEW,ROLE_CREATE

I get a login screen, login works alright. When i logout - i still see all the tabs and contents in all the tabs. See the attached screenshot.
",1.0,False,1.0,0,0.0,0,0,False
"XD-3718","Improvement","Sprint 64","2015-12-08T14:19:48.000+0000","mbogoevici","mbogoevici","Kafka message bus must accept partitioning properties for named queues","As a user, I want to be able to provide the partitioning logic for a named destination, so that I can control the ordering of outbound messages.",1.0,0.999203989656668,1.8009283785791575e-05,0.0548189091476656,True,True,True,1449584388000.0,1456242314000.0,1456245914000.0,1456246718000.0,1456247618000.0,1449584388000.0,-1.0,1449949659000.0,1449953259000.0,1449584508000.0,1449588108000.0,1456242314000.0,1456245914000.0,"mbogoevici",1449953259000.0,1449584388000.0,45,1449584388000.0,-1.0,"Kafka message bus must accept partitioning properties for named queues","As a user, I want to be able to provide the partitioning logic for a named destination, so that I can control the ordering of outbound messages.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3716","Improvement","Sprint 64","2015-12-02T23:15:01.000+0000","grussell","","Support Configuring the RabbitMessageBus MessagePropertiesConverter LongString Limit","http://stackoverflow.com/questions/34053997/passing-headerinformation-as-jsonobject-in-header-in-spring-xd",2.0,0.8442599719538045,0.8431053746200268,0.1369745239015159,True,True,True,1449098101000.0,1454346769000.0,1454350369000.0,1455314087000.0,1455314987000.0,1449098101000.0,-1.0,1449949656000.0,1449953256000.0,1454339591000.0,1454343191000.0,1454346769000.0,1454350369000.0,"grussell",1449953256000.0,1449098101000.0,45,1449098101000.0,-1.0,"Support Configuring the RabbitMessageBus MessagePropertiesConverter LongString Limit","http://stackoverflow.com/questions/34053997/passing-headerinformation-as-jsonobject-in-header-in-spring-xd",2.0,False,2.0,0,0.0,-1,0,False
"XD-3715","Story","Sprint 63","2015-11-30T15:11:45.000+0000","sabby","","Move K8s SPI to a separate repo","",5.0,0.8541482403709107,0.8541482403709107,8.450275760665656e-06,True,True,True,1448896305000.0,1449502781000.0,1449506381000.0,1449605441000.0,1449606341000.0,1448896305000.0,-1.0,1448896311000.0,1448899911000.0,1449502781000.0,1449506381000.0,1449502781000.0,1449506381000.0,"sabby",1448899911000.0,1448896305000.0,45,1448896305000.0,-1.0,"Move k8s SPI to a separate repo","As a developer, I'd like to move k8s SPI to it's own repo.",5.0,False,5.0,0,0.0,0,0,True
"XD-3714","Story","Sprint 63","2015-11-30T14:33:00.000+0000","sabby","","Upgrade XD Ambari release to 1.3 ","As a developer, I'd like to upgrade Spring XD's ambari plugin to 1.3 release.",3.0,0.6312452194348679,0.0406334179516456,0.0,True,True,True,1448893980000.0,1449007868000.0,1449011468000.0,1449073498000.0,1449074398000.0,1448893980000.0,-1.0,1448893980000.0,1448897580000.0,1448901311000.0,1448904911000.0,1449007868000.0,1449011468000.0,"sabby",1448897580000.0,1448893980000.0,45,1448893980000.0,-1.0,"Upgrade XD Ambari release to 1.3 ","As a developer, I'd like to upgrade Spring XD's ambari plugin to 1.3 release.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3709","Bug","Sprint 63","2015-11-21T16:59:25.000+0000","grussell","","Duplicate MBean Names With router Sink","For some reason, the Integration {{MBeanExporterHelper}} is not preventing the standard context {{MBeanExporter}} from exporting the {{AbstractMessageRouter}}. This should be suppressed (when an IMBE is present) because it's annotated {{@IntegrationManagedResource}}.

Causes {{InstanceAlreadyExistsException}}.

Workaround in the stack overflow answer.

http://stackoverflow.com/questions/33838502/error-deploying-more-than-one-stream-with-a-router-1-3-0

Could be an SI issue, but investigation needed. However, we should probably include the stream/job name in all MBeans for the stream (as is done for the integration exporter).",1.0,0.3485362466537005,0.3447938999393157,0.0012412294301983,True,True,True,1448125165000.0,1448909718000.0,1448913318000.0,1450375259000.0,1450376159000.0,1448125165000.0,1448143734000.0,1448127959000.0,1448131559000.0,1448901294000.0,1448904894000.0,1448909718000.0,1448913318000.0,"grussell",1448131559000.0,1448125165000.0,45,1448143734000.0,1448143734000.0,"Duplicate MBean Names With router Sink","For some reason, the Integration {{MBeanExporterHelper}} is not preventing the standard context {{MBeanExporter}} from exporting the {{AbstractMessageRouter}}. This should be suppressed (when an IMBE is present) because it's annotated {{@IntegrationManagedResource}}.

Causes {{InstanceAlreadyExistsException}}.

Workaround in the stack overflow answer.

http://stackoverflow.com/questions/33838502/error-deploying-more-than-one-stream-with-a-router-1-3-0

Could be an SI issue, but investigation needed. However, we should probably include the stream/job name in all MBeans for the stream (as is done for the integration exporter).",3.0,True,3.0,1,-66.66666666666666,0,4,False
"XD-3708","Story","Sprint 62","2015-11-16T19:16:45.000+0000","sabby","mminella","Document limitations with HSQL when using composed jobs","As a developer, I'd want to document the limitations of HSQL DB when using composed jobs. ",1.0,0.9768121104185218,0.0,0.0,True,True,True,1447701405000.0,1447865949000.0,1447869549000.0,1447868955000.0,1447869855000.0,1447701405000.0,-1.0,1447701405000.0,1447705005000.0,1447701405000.0,1447705005000.0,1447865949000.0,1447869549000.0,"sabby",1447705005000.0,1447701405000.0,45,1447701405000.0,-1.0,"Document limitations with HSQL when using composed jobs","As a developer, I'd want to document the limitations of HSQL DB when using composed jobs. ",1.0,False,1.0,0,0.0,-1,0,False
"XD-3705","Story","Sprint 61","2015-11-12T15:57:58.000+0000","sabby","grussell","Bump Boot and spring-cloud-build Versions","As a developer, I'd like to upgrade Boot and Spring Cloud Build revisions, so I can leverage the latest updates.",5.0,0.6486486486486487,0.0,0.0,False,False,False,1447343878000.0,1447343902000.0,1447343915000.0,1447343878000.0,1447343915000.0,1447343878000.0,-1.0,1447343878000.0,1447343915000.0,1447343878000.0,1447343915000.0,1447343902000.0,1447343915000.0,"sabby",1447343915000.0,1447343878000.0,45,1447343878000.0,-1.0,"Bump Boot and spring-cloud-build Versions","As a developer, I'd like to upgrade Boot and Spring Cloud Build revisions, so I can leverage the latest updates.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3702","Story","Sprint 62","2015-11-11T16:48:34.000+0000","mbogoevici","mbogoevici","Support partitioning for Kafka even if count == 1","As a developer, I want to be able to set a partitioning key for the Kafka bus even when there is a single downstream module, so that I can take advantage of the native Kafka partitioning and message ordering support.",3.0,0.8828290275549887,0.7436891204787667,0.7438269452387357,True,True,True,1447260514000.0,1447792166000.0,1447795766000.0,1447861828000.0,1447862728000.0,1447260514000.0,-1.0,1447708457000.0,1447712057000.0,1447708374000.0,1447711974000.0,1447792166000.0,1447795766000.0,"mbogoevici",1447712057000.0,1447260514000.0,45,1447260514000.0,-1.0,"Support partitioning for Kafka even if count == 1","As a developer, I want to be able to set a partitioning key for the Kafka bus even when there is a single downstream module, so that I can take advantage of the native Kafka partitioning and message ordering support.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3701","Story","Sprint 61","2015-11-11T15:44:36.000+0000","grussell","grussell","Improve Shell Connection Diagnostics","When a problem occurs connecting to admin, we just get {{Unable to contact Data Flow Admin}} even if the connection is successful and some problem occurs when interpreting the result.

The exception is eaten.

Log an error including the exception.

Currently investigating an NPE in DataFlowTemplate @ line 77.",1.0,0.2186996472586334,0.2185363410704176,0.2185363410704176,True,True,True,1447256676000.0,1447276764000.0,1447280364000.0,1447347628000.0,1447348528000.0,1447256676000.0,-1.0,1447276749000.0,1447280349000.0,1447276749000.0,1447280349000.0,1447276764000.0,1447280364000.0,"grussell",1447280349000.0,1447256676000.0,45,1447256676000.0,-1.0,"Improve Shell Connection Diagnostics","When a problem occurs connecting to admin, we just get {{Unable to contact Data Flow Admin}} even if the connection is successful and some problem occurs when interpreting the result.

The exception is eaten.

Log an error including the exception.

Currently investigating an NPE in DataFlowTemplate @ line 77.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3699","Improvement","Sprint 62","2015-11-09T22:58:04.000+0000","sabby","","Remove hardcoded buildpack commit reference","As a developer, I'd like to remove the hardcoded buildpack reference since the latest 1.6.2 ER release includes all the features required by Data Flow. ",1.0,0.8855520939428434,0.885147684380804,0.0,True,True,True,1447109884000.0,1447169007000.0,1447172607000.0,1447175748000.0,1447176648000.0,1447109884000.0,-1.0,1447109884000.0,1447113484000.0,1447168980000.0,1447172580000.0,1447169007000.0,1447172607000.0,"sabby",1447113484000.0,1447109884000.0,45,1447109884000.0,-1.0,"Remove hardcoded buildpack commit reference","As a developer, I'd like to remove the hardcoded buildpack reference since the latest 1.6.2 ER release includes all the features required by Data Flow. ",1.0,False,1.0,0,0.0,-1,0,False
"XD-3698","Bug","Sprint 61","2015-11-09T20:58:13.000+0000","sabby","hillert","Execution list page includes child job in pagination","As a user, I created a composed job with over 10 child jobs in the workflow; I expected to see 'a' job in the execution list page without any pagination, but instead I noticed empty pagination to skip to next page.",1.0,0.4822522364064443,0.0,0.0,True,True,True,1447102693000.0,1447258976000.0,1447262576000.0,1447425862000.0,1447426762000.0,1447102693000.0,-1.0,1447102693000.0,1447106293000.0,1447102693000.0,1447106293000.0,1447258976000.0,1447262576000.0,"sabby",1447106293000.0,1447102693000.0,45,1447102693000.0,-1.0,"Execution list page includes child jobs in pagination scope","As a user, I created a composed job with over 10 child jobs in the workflow; I expected to see 'a' job in the execution list page without any pagination, but instead I noticed empty pagination to skip to next page.",1.0,False,1.0,0,0.0,-1,0,True
"XD-3697","Bug","Sprint 61","2015-11-09T16:32:05.000+0000","dgarcia","grussell","Output modules cannot use minPartitionCount when sending to named channels","If the output module is connected to a named channel, cannot be set up the property minPartitionCount, it is giving an exception.

Streams:
stream create f --definition ""queue:foo > transform --expression=payload+'-foo' | log"" 
stream create b --definition ""queue:bar > transform --expression=payload+'-bar' | log""
stream deploy --name f --properties ""module.transform.count=2""
stream deploy --name b --properties ""module.transform.count=2""

stream create r --definition ""time | router --expression=payload.contains('10')?'queue:foo':'queue:bar'""
stream deploy --name r --properties ""module.router.producer.minPartitionCount=20""

The error is:
Caused by: java.lang.IllegalArgumentException: KafkaMessageBus does not support producer property: minPartitionCount for queue:bar.
at org.springframework.xd.dirt.integration.bus.MessageBusSupport.validateProperties(MessageBusSupport.java:781) ~[spring-xd-messagebus-spi-1.2.1.RELEASE.jar:1.2.1.RELEASE]",1.0,0.0223069051698163,0.0201235213032763,0.0201235213032763,True,True,True,1447086725000.0,1447102050000.0,1447105650000.0,1447772832000.0,1447773732000.0,1447086725000.0,-1.0,1447100550000.0,1447104150000.0,1447100550000.0,1447104150000.0,1447102050000.0,1447105650000.0,"dgarcia",1447104150000.0,1447100550000.0,45,1447100550000.0,-1.0,"Output modules cannot use minPartitionCount when sending to named channels","If the output module is connected to a named channel, cannot be set up the property minPartitionCount, it is giving an exception.

Streams:
stream create f --definition ""queue:foo > transform --expression=payload+'-foo' | log"" 
stream create b --definition ""queue:bar > transform --expression=payload+'-bar' | log""
stream deploy --name f --properties ""module.transform.count=2""
stream deploy --name b --properties ""module.transform.count=2""

stream create r --definition ""time | router --expression=payload.contains('10')?'queue:foo':'queue:bar'""
stream deploy --name r --properties ""module.router.producer.minPartitionCount=20""

The error is:
Caused by: java.lang.IllegalArgumentException: KafkaMessageBus does not support producer property: minPartitionCount for queue:bar.
at org.springframework.xd.dirt.integration.bus.MessageBusSupport.validateProperties(MessageBusSupport.java:781) ~[spring-xd-messagebus-spi-1.2.1.RELEASE.jar:1.2.1.RELEASE]",1.0,False,1.0,0,0.0,-1,0,False
"XD-3696","Story","Sprint 61","2015-11-09T16:25:19.000+0000","sabby","grussell","Upgrade to SI 4.2.2.GA","As a developer, I'd like to upgrade to SI 4.2.2.GA release, so I can leverage the latest improvements.",1.0,0.8798300811304807,0.0,0.0,True,True,True,1447086319000.0,1447695245000.0,1447698845000.0,1447777514000.0,1447778414000.0,1447086319000.0,-1.0,1447086319000.0,1447089919000.0,1447086319000.0,1447089919000.0,1447695245000.0,1447698845000.0,"sabby",1447089919000.0,1447086319000.0,45,1447086319000.0,-1.0,"Upgrade to SI 4.2.2.GA","As a developer, I'd like to upgrade to SI 4.2.2.GA release, so I can leverage the latest improvements.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3695","Story","Sprint 61","2015-11-09T16:24:29.000+0000","sabby","thomas.risberg","Upgrade to SHDP 2.3.0.GA","As a developer, I'd like to upgrade to 2.3.0 GA release, so I can leverage the latest improvements.",1.0,0.8047873706964012,0.0,0.0,True,True,True,1447086269000.0,1447358803000.0,1447362403000.0,1447424010000.0,1447424910000.0,1447086269000.0,-1.0,1447086269000.0,1447089869000.0,1447086269000.0,1447089869000.0,1447358803000.0,1447362403000.0,"sabby",1447089869000.0,1447086269000.0,45,1447086269000.0,-1.0,"Upgrade to SHDP 2.2.1.GA","As a developer, I'd like to upgrade to 2.2.1 GA release, so I can leverage the latest improvements without breaking backwards compatibility. SHDP 2.3.0 uses Boot 1.3 and HDP and CDH versions that drop older Hive support. To avoid breaking changes we should instead use SHDP 2.2.1 that has backported any improvements that we need as well as move Spring and Hadoop versions to more recent ones.",1.0,False,1.0,0,0.0,-1,0,True
"XD-3693","Story","Sprint 62","2015-11-06T16:15:53.000+0000","grussell","grussell","Add Timestamp to XD Message History","I don't recall why [this commit | https://github.com/garyrussell/spring-xd/commit/ba15a1390f7e448dbc723ee76a45c2e239e0994e] was not applied to master but having the timestamp for each step in the history will be useful.

See [this github issue | https://github.com/spring-projects/spring-xd-modules/issues/24#issuecomment-154436643].
",1.0,0.0038130976879312,0.0038130976879312,0.0,True,True,True,1446826553000.0,1446826868000.0,1446830468000.0,1446908263000.0,1446909163000.0,1446826553000.0,-1.0,1446826553000.0,1446830153000.0,1446826868000.0,1446830468000.0,1446826868000.0,1446830468000.0,"grussell",1446830153000.0,1446826553000.0,45,1446826553000.0,-1.0,"Add Timestamp to XD Message History","I don't recall why [this commit | https://github.com/garyrussell/spring-xd/commit/ba15a1390f7e448dbc723ee76a45c2e239e0994e] was not applied to master but having the timestamp for each step in the history will be useful.

See [this github issue | https://github.com/spring-projects/spring-xd-modules/issues/24#issuecomment-154436643].
",1.0,False,1.0,0,0.0,-1,0,False
"XD-3692","Story","Sprint 61","2015-11-06T16:09:22.000+0000","sabby","jvalkeal","Optimize YARN deployer","As a developer, I'd like to optimize YARN deployer, so I can deploy stream and the modules part of the definition rapidly.",5.0,3.2265870775187545e-06,0.0,0.0,True,True,True,1446826162000.0,1446826167000.0,1446829767000.0,1448374887000.0,1448375787000.0,1446826162000.0,-1.0,1446826162000.0,1446829762000.0,1446826162000.0,1446829762000.0,1446826167000.0,1446829767000.0,"sabby",1446829762000.0,1446826162000.0,45,1446826162000.0,-1.0,"Optimize YARN deployer","As a developer, I'd like to optimize YARN deployer, so I can deploy stream and the modules part of the definition rapidly.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3691","Bug","Sprint 61","2015-11-06T02:14:48.000+0000","grenfro","hillert","Composed Job's  definition does not appear in column on UI","If using the definition <aaa || bbb> where the definition starts with a ""<"" and ends with a "">"" the definition for the composed job does not appear on the definition page.",2.0,0.9098656312573188,0.0,0.0001668618383978,True,True,True,1446776088000.0,1447686707000.0,1447690307000.0,1447776016000.0,1447776916000.0,1446776088000.0,-1.0,1446776255000.0,1446779855000.0,1446776088000.0,1446779688000.0,1447686707000.0,1447690307000.0,"grenfro",1446779855000.0,1446776088000.0,45,1446776088000.0,-1.0,"Ensure Job definitions are escaped in UI","If using the definition <aaa || bbb> where the definition starts with a ""<"" and ends with a "">"" the definition for the composed job does not appear on the definition page.",2.0,False,2.0,0,0.0,-1,0,True
"XD-3690","Story","Sprint 61","2015-11-05T21:23:45.000+0000","thomas.risberg","","Improve ""Server Configuration - Database Configuration"" section","Make it more clear what drivers need to be copied where. See - https://github.com/spring-projects/spring-xd/issues/1653",1.0,0.0852918969391812,0.0852918969391812,0.0013885600167985,True,True,True,1446758625000.0,1446854079000.0,1446857679000.0,1447876870000.0,1447877770000.0,1446758625000.0,-1.0,1446760179000.0,1446763779000.0,1446854079000.0,1446857679000.0,1446854079000.0,1446857679000.0,"thomas.risberg",1446763779000.0,1446758625000.0,45,1446758625000.0,-1.0,"Improve ""Server Configuration - Database Configuration"" section","Make it more clear what drivers need to be copied where. See - https://github.com/spring-projects/spring-xd/issues/1653",1.0,False,1.0,0,0.0,-1,0,False
"XD-3689","Story","Sprint 61","2015-11-05T14:03:33.000+0000","grenfro","grenfro","Update default configs to support Composed Jobs","Users want the ability to use Composed Jobs (specifically parallel Jobs) without having to update the configurations for the hsqldb and the Isolation Level for spring batch.  These should be set by default.
",3.0,0.0068938238119455,0.0,2.0705769339436064,True,True,True,1446732213000.0,1446732335000.0,1446735935000.0,1446749010000.0,1446749910000.0,1446732213000.0,-1.0,1446768856000.0,1446749910000.0,1446732213000.0,1446735813000.0,1446732335000.0,1446735935000.0,"grenfro",1446749910000.0,1446732213000.0,45,1446732213000.0,-1.0,"Update default configs to support Composed Jobs","Users want the ability to use Composed Jobs (specifically parallel Jobs) without having to update the configurations for the hsqldb and the Isolation Level for spring batch.  These should be set by default.
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3685","Bug","Sprint 61","2015-11-02T20:54:40.000+0000","grenfro","hillert","Job Definitions page fails to display definitions if page ","In this scenario we created 30 jobs that can be used for a composed job.  
if the composed job uses jobs in its composition that are not present on the first page of the of the result set the following exception is thrown.  

{noformat}
2015-11-02T14:47:17-0500 1.3.0.SNAP ERROR qtp1587928736-26 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.IllegalStateException: Not all instances were looked at: fff
	at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:244) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:209) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.rest.JobsController.list(JobsController.java:128) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]
	at sun.reflect.GeneratedMethodAccessor133.invoke(Unknown Source) ~[na:na]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735) [javax.servlet-3.0.0.v201112011016.jar:na]
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848) [javax.servlet-3.0.0.v201112011016.jar:na]
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:207) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:176) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557) [jetty-security-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.Server.handle(Server.java:370) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
{noformat}",3.0,0.0063209418118907,0.0019747668678003,0.0020254019156926,True,True,True,1446497680000.0,1446499178000.0,1446502778000.0,1446733770000.0,1446734670000.0,1446497680000.0,-1.0,1446498160000.0,1446501760000.0,1446498148000.0,1446501748000.0,1446499178000.0,1446502778000.0,"grenfro",1446501760000.0,1446497680000.0,45,1446497680000.0,-1.0,"Job Definitions page fails to display definitions if page ","In this scenario we created 30 jobs that can be used for a composed job.  
if the composed job uses jobs in its composition that are not present on the first page of the of the result set the following exception is thrown.  

{noformat}
2015-11-02T14:47:17-0500 1.3.0.SNAP ERROR qtp1587928736-26 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.IllegalStateException: Not all instances were looked at: fff
	at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:244) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:209) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.rest.JobsController.list(JobsController.java:128) ~[spring-xd-dirt-1.3.0.BUILD-SNAPSHOT.jar:1.3.0.BUILD-SNAPSHOT]
	at sun.reflect.GeneratedMethodAccessor133.invoke(Unknown Source) ~[na:na]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:861) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735) [javax.servlet-3.0.0.v201112011016.jar:na]
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848) [javax.servlet-3.0.0.v201112011016.jar:na]
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:207) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:176) [spring-security-web-4.0.2.RELEASE.jar:4.0.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90) [spring-boot-actuator-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.2.RELEASE.jar:4.2.2.RELEASE]
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557) [jetty-security-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428) [jetty-servlet-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.Server.handle(Server.java:370) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235) [jetty-http-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82) [jetty-server-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52) [jetty-io-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543) [jetty-util-8.1.14.v20131031.jar:8.1.14.v20131031]
	at java.lang.Thread.run(Thread.java:745) [na:1.7.0_67]
{noformat}",3.0,False,3.0,0,0.0,0,0,False
"XD-3684","Bug","Sprint 61","2015-11-02T19:30:30.000+0000","sabby","mminella","Job composition fails for large transitions","As a user, I'm trying to create a composed job with >20 steps/transitions using Rabbit as the message bus and it doesn't complete successfully.",3.0,0.9116768288561116,0.0029098215316393,0.0,True,True,True,1446492630000.0,1447369272000.0,1447372872000.0,1447453301000.0,1447454201000.0,1446492630000.0,-1.0,1446492630000.0,1446496230000.0,1446495428000.0,1446499028000.0,1447369272000.0,1447372872000.0,"sabby",1446496230000.0,1446492630000.0,45,1446492630000.0,-1.0,"Job composition fails for large transitions","As a user, I'm trying to create a composed job with >20 steps/transitions using Rabbit as the message bus and it doesn't complete successfully.",3.0,False,3.0,0,0.0,0,0,False
"XD-3683","Bug","Sprint 61","2015-11-02T19:27:53.000+0000","sabby","grenfro","Fix composed job error message","As a user, I'm trying to compose a job just with one definition; however, I'm getting the following error message, which could be misinterpreted.

{code}
xd:>job create salsa --definition timestampfile
Successfully created job 'salsa'
xd:>job create foo --definition ""salsa || salsa""
Successfully created job 'foo'
xd:>job create foo222 --definition ""salsa""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'salsa' and type 'job'
{code}",1.0,0.1124400927064561,0.0,0.0,True,True,True,1446492473000.0,1446579702000.0,1446583302000.0,1447267355000.0,1447268255000.0,1446492473000.0,-1.0,1446492473000.0,1446496073000.0,1446492473000.0,1446496073000.0,1446579702000.0,1446583302000.0,"sabby",1446496073000.0,1446492473000.0,45,1446492473000.0,-1.0,"Fix composed job error message","As a user, I'm trying to compose a job just with one definition; however, I'm getting the following error message, which could be misinterpreted.

{code}
xd:>job create salsa --definition timestampfile
Successfully created job 'salsa'
xd:>job create foo --definition ""salsa || salsa""
Successfully created job 'foo'
xd:>job create foo222 --definition ""salsa""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'salsa' and type 'job'
{code}",1.0,False,1.0,0,0.0,0,0,False
"XD-3673","Bug","Sprint 61","2015-11-02T16:59:40.000+0000","sabby","","Multiple module instances produces duplicate messages ","As a follow-up from [XD-3613|https://jira.spring.io/browse/XD-3629], we would want to fix this experience for Kafka message bus.",5.0,0.6755517346782821,0.6755486779764633,0.0,True,True,True,1446483580000.0,1447367607000.0,1447371207000.0,1447791280000.0,1447792180000.0,1446483580000.0,-1.0,1446483580000.0,1446487180000.0,1447367603000.0,1447371203000.0,1447367607000.0,1447371207000.0,"sabby",1446487180000.0,1446483580000.0,45,1446483580000.0,-1.0,"Multiple module instances produces duplicate messages ","As a follow-up from [XD-3613|https://jira.spring.io/browse/XD-3629], we would want to fix this experience for Kafka message bus.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3672","Story","Sprint 61","2015-11-02T15:34:40.000+0000","sabby","","Move Mesos SPI to master","",2.0,0.0295207222044099,0.0295207222044099,0.0,True,True,True,1446478480000.0,1446570830000.0,1446574430000.0,1449605891000.0,1449606791000.0,1446478480000.0,1446480619000.0,1446478480000.0,1446482080000.0,1446570830000.0,1446574430000.0,1446570830000.0,1446574430000.0,"sabby",1446482080000.0,1446480619000.0,45,1447686623000.0,1447686623000.0,"Move Mesos SPI to a separate repo","As a developer, I'd like to submit a PR for existing work on Mesos SPI. ",1.0,True,1.0,1,100.0,0,0,True
"XD-3670","Story","Sprint 61","2015-10-30T23:49:41.000+0000","sabby","","Spike: Revisit the core design and document gaps","As a developer, I'd like to revisit the existing design and identify known limitations and/or the gaps. ",5.0,0.7726435725230713,0.7726435725230713,0.0,True,True,True,1446248981000.0,1447183838000.0,1447187438000.0,1447458027000.0,1447458927000.0,1446248981000.0,-1.0,1446248981000.0,1446252581000.0,1447183838000.0,1447187438000.0,1447183838000.0,1447187438000.0,"sabby",1446252581000.0,1446248981000.0,45,1446248981000.0,-1.0,"Spike: Revisit the core design and document gaps","As a developer, I'd like to revisit the existing design and identify known limitations and/or the gaps. ",5.0,False,5.0,0,0.0,-1,0,False
"XD-3669","Story","Sprint 61","2015-10-30T23:17:51.000+0000","sabby","","Add Flo screenshots to Batch DSL section","As a user, I'd like Flo Graphs as screenshots while referring to the batch DSL, so it will be easy for me to relate to concepts. ",1.0,0.994984253030668,0.994984253030668,0.0,True,True,True,1446247071000.0,1447862415000.0,1447866015000.0,1447869658000.0,1447870558000.0,1446247071000.0,-1.0,1446247071000.0,1446250671000.0,1447862415000.0,1447866015000.0,1447862415000.0,1447866015000.0,"sabby",1446250671000.0,1446247071000.0,45,1446247071000.0,-1.0,"Add Flo screenshots to Batch DSL section","As a user, I'd like Flo Graphs as screenshots while referring to the batch DSL, so it will be easy for me to relate to concepts. ",1.0,False,1.0,0,0.0,-1,0,False
"XD-3654","Story","Sprint 60","2015-10-29T14:47:35.000+0000","sabby","grenfro","Documentation: Flo for XD Batch","As a user, I'd like to refer to 'job orchestration' documentation, so I can use it as guideline for building batch workflows.  ",3.0,0.0002086375964948,0.0002086375964948,0.0,True,True,True,1446130055000.0,1446130078000.0,1446133678000.0,1446239394000.0,1446240294000.0,1446130055000.0,-1.0,1446130055000.0,1446133655000.0,1446130078000.0,1446133678000.0,1446130078000.0,1446133678000.0,"sabby",1446133655000.0,1446130055000.0,45,1446130055000.0,-1.0,"Documentation: Flo for XD Batch","As a user, I'd like to refer to 'job orchestration' documentation, so I can use it as guideline for building batch workflows.  ",3.0,False,3.0,0,0.0,-1,0,False
"XD-3652","Bug","Sprint 61","2015-10-28T18:46:20.000+0000","mbogoevici","","The shell processor module cannot be stopped while blocked in receive()","Both lifecycle and send/receive methods are synchronized, so if the shell command processor is blocked reading from the script's input - e.g. when no proper terminator is sent by the script, the stop() method can't acquire the object lock and proceed stopping the instance, and therefore the module. ",5.0,0.0944652612278036,0.0944652612278036,0.000162027039865,True,True,True,1446057980000.0,1446220060000.0,1446223660000.0,1447772843000.0,1447773743000.0,1446057980000.0,-1.0,1446058258000.0,1446061858000.0,1446220060000.0,1446223660000.0,1446220060000.0,1446223660000.0,"mbogoevici",1446061858000.0,1446057980000.0,45,1446057980000.0,-1.0,"The shell processor module cannot be stopped while blocked in receive()","Both lifecycle and send/receive methods are synchronized, so if the shell command processor is blocked reading from the script's input - e.g. when no proper terminator is sent by the script, the stop() method can't acquire the object lock and proceed stopping the instance, and therefore the module. ",5.0,False,5.0,0,0.0,-1,0,False
"XD-3651","Improvement","Sprint 61","2015-10-28T17:47:22.000+0000","thomas.risberg","thomas.risberg","The JsonStringToTupleConverter converts all values to String","As a module developer I would like the JsonStringToTupleConverter in the Spring Cloud Streams project to maintain the types provided in the JSON string and not convert everything to a String representation.",1.0,0.999929945676056,3.1842874519968666e-05,0.0004394316683755,True,True,True,1446054442000.0,1446211452000.0,1446211463000.0,1446210563000.0,1446211463000.0,1446054442000.0,-1.0,1446054511000.0,1446058111000.0,1446054447000.0,1446058047000.0,1446211452000.0,1446211463000.0,"thomas.risberg",1446058111000.0,1446054442000.0,45,1446054442000.0,-1.0,"The JsonStringToTupleConverter converts all values to String","As a module developer I would like the JsonStringToTupleConverter in the Spring Cloud Streams project to maintain the types provided in the JSON string and not convert everything to a String representation.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3645","Bug","Sprint 61","2015-10-27T02:57:41.000+0000","martindam","","Tuple unable to serialize objects with nested arrays of objects","Serializing a tuple object with that have a nested array which contains objects (as a tuple) fails to serialize. The error is:

{noformat}
Caused by: com.fasterxml.jackson.databind.JsonMappingException: No serializer found for class org.springframework.xd.tuple.DefaultTupleConversionService and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) ) (through reference chain: java.util.ArrayList[0]->org.springframework.xd.tuple.DefaultTuple[""values""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.xd.tuple.DefaultTuple[""conversionService""])
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:59) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:26) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:505) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:639) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:505) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:639) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:2881) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:2338) ~[jackson-databind-2.4.5.jar:2.4.5]
	at org.springframework.xd.tuple.TupleToJsonStringConverter.convert(TupleToJsonStringConverter.java:37) ~[spring-xd-tuple-1.3.0.M1.jar:1.3.0.M1]
{noformat}
when the input string (read from a Kafka topic in my case) looks something like:

{noformat}
{
    ""body"": [
        {
            ""dataType"": ""har"",
            ""har"": {
                ""log"": {
                    ""browser"": {
                        ""name"": ""Google Chrome"",
                        ""version"": ""44.0.2403.155""
                    },
                    ""creator"": {
                        ""name"": ""My extension"",
                        ""version"": ""0.23.6""
                    },
                    ""pages"": [
                        {
                            ""_requestTimings"": {
                                ""blocked"": -1,
                                ""connect"": -1,
                                ""dns"": -1,
                                ""receive"": 11,
                                ""send"": -1,
                                ""ssl"": -1,
                                ""wait"": 244
                            },
                            ""_requestUrl"": ""https://google.com""
                        },
                        {
                            ""_requestTimings"": {
                                ""blocked"": -1,
                                ""connect"": -1,
                                ""dns"": -1,
                                ""receive"": 11,
                                ""send"": -1,
                                ""ssl"": -1,
                                ""wait"": 244
                            },
                            ""_requestUrl"": ""https://google.com""
                        }
                    ],
                    ""version"": ""1.2""
                }
            },
            ""testId"": 1
        }
    ],
    ""bodyType"": ""models.MultiMessage"",
    ""headers"": {
        ""appInstance"": ""localhost/127.0.0.1:8080"",
        ""clientIp"": ""0:0:0:0:0:0:0:1"",
        ""host"": ""localhost:8080"",
        ""requestId"": ""27acf948-33ff-491c-8be7-1beb4b8c95d9"",
        ""requestMethod"": ""POST"",
        ""requestUrl"": ""http://localhost:8080/har"",
        ""timestamp"": 1445914510549,
        ""userPrincipal"": ""235""
    }
}
{noformat}
If the inner array (the Pages array) is just an object, it works, when it is an array, it fails. 

The stream used:
kafka --topic=agent_mixed --outputType=application/x-xd-tuple | splitter --expression=payload.body | log",2.0,0.7569798199236569,0.7569798199236569,0.0864515377069242,True,True,True,1445914661000.0,1447083699000.0,1447087299000.0,1447458106000.0,1447459006000.0,1445914661000.0,-1.0,1446048172000.0,1446051772000.0,1447083699000.0,1447087299000.0,1447083699000.0,1447087299000.0,"martindam",1446051772000.0,1445914661000.0,45,1445914661000.0,-1.0,"Tuple unable to serialize objects with nested arrays of objects","Serializing a tuple object with that have a nested array which contains objects (as a tuple) fails to serialize. The error is:

{noformat}
Caused by: com.fasterxml.jackson.databind.JsonMappingException: No serializer found for class org.springframework.xd.tuple.DefaultTupleConversionService and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) ) (through reference chain: java.util.ArrayList[0]->org.springframework.xd.tuple.DefaultTuple[""values""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.xd.tuple.DefaultTuple[""conversionService""])
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.failForEmpty(UnknownSerializer.java:59) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.UnknownSerializer.serialize(UnknownSerializer.java:26) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:505) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:639) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:505) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:639) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:152) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:100) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:21) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serialize(AsArraySerializerBase.java:183) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:1902) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.core.base.GeneratorBase.writeObject(GeneratorBase.java:280) ~[jackson-core-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.POJONode.serialize(POJONode.java:111) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.node.ObjectNode.serialize(ObjectNode.java:264) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:44) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.std.SerializableSerializer.serialize(SerializableSerializer.java:29) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:128) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper._configAndWriteValue(ObjectMapper.java:2881) ~[jackson-databind-2.4.5.jar:2.4.5]
	at com.fasterxml.jackson.databind.ObjectMapper.writeValueAsString(ObjectMapper.java:2338) ~[jackson-databind-2.4.5.jar:2.4.5]
	at org.springframework.xd.tuple.TupleToJsonStringConverter.convert(TupleToJsonStringConverter.java:37) ~[spring-xd-tuple-1.3.0.M1.jar:1.3.0.M1]
{noformat}
when the input string (read from a Kafka topic in my case) looks something like:

{noformat}
{
    ""body"": [
        {
            ""dataType"": ""har"",
            ""har"": {
                ""log"": {
                    ""browser"": {
                        ""name"": ""Google Chrome"",
                        ""version"": ""44.0.2403.155""
                    },
                    ""creator"": {
                        ""name"": ""My extension"",
                        ""version"": ""0.23.6""
                    },
                    ""pages"": [
                        {
                            ""_requestTimings"": {
                                ""blocked"": -1,
                                ""connect"": -1,
                                ""dns"": -1,
                                ""receive"": 11,
                                ""send"": -1,
                                ""ssl"": -1,
                                ""wait"": 244
                            },
                            ""_requestUrl"": ""https://google.com""
                        },
                        {
                            ""_requestTimings"": {
                                ""blocked"": -1,
                                ""connect"": -1,
                                ""dns"": -1,
                                ""receive"": 11,
                                ""send"": -1,
                                ""ssl"": -1,
                                ""wait"": 244
                            },
                            ""_requestUrl"": ""https://google.com""
                        }
                    ],
                    ""version"": ""1.2""
                }
            },
            ""testId"": 1
        }
    ],
    ""bodyType"": ""models.MultiMessage"",
    ""headers"": {
        ""appInstance"": ""localhost/127.0.0.1:8080"",
        ""clientIp"": ""0:0:0:0:0:0:0:1"",
        ""host"": ""localhost:8080"",
        ""requestId"": ""27acf948-33ff-491c-8be7-1beb4b8c95d9"",
        ""requestMethod"": ""POST"",
        ""requestUrl"": ""http://localhost:8080/har"",
        ""timestamp"": 1445914510549,
        ""userPrincipal"": ""235""
    }
}
{noformat}
If the inner array (the Pages array) is just an object, it works, when it is an array, it fails. 

The stream used:
kafka --topic=agent_mixed --outputType=application/x-xd-tuple | splitter --expression=payload.body | log",2.0,False,2.0,0,0.0,0,0,False
"XD-3644","Story","Sprint 60","2015-10-26T18:54:06.000+0000","sabby","aclement","Add test coverage for batch DSL and XML generation variants","As a developer, I'd like to enhance test coverage to capture DSL and XML generation variants. ",5.0,9.537232295187936e-05,0.0,0.0,True,True,True,1445885646000.0,1445885655000.0,1445889255000.0,1445979113000.0,1445980013000.0,1445885646000.0,-1.0,1445885646000.0,1445889246000.0,1445885646000.0,1445889246000.0,1445885655000.0,1445889255000.0,"sabby",1445889246000.0,1445885646000.0,45,1445885646000.0,-1.0,"Add test coverage for batch DSL and XML generation variants","As a developer, I'd like to enhance test coverage to capture DSL and XML generation variants. ",5.0,False,5.0,0,0.0,-1,0,False
"XD-3641","Story","Sprint 60","2015-10-23T14:14:04.000+0000","sabby","grussell","Job composition improvements","As a developer, I'd like to review and refactor {{JobLaunchingTasklet}}, so I can improve performance characteristics. ",3.0,0.0003763256927813,0.0,0.0,True,True,True,1445609644000.0,1445609655000.0,1445613255000.0,1445637974000.0,1445638874000.0,1445609644000.0,1445609758000.0,1445609644000.0,1445613244000.0,1445609644000.0,1445613244000.0,1445609655000.0,1445613255000.0,"sabby",1445613244000.0,1445609758000.0,45,1445609758000.0,-1.0,"Job composition improvements","As a developer, I'd like to review and refactor {{JobLaunchingTasklet}}, so I can improve performance characteristics. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-3637","Story","Sprint 60","2015-10-21T21:54:37.000+0000","sabby","grussell","Upgrade to SI 4.2.1","As a developer, I'd like to upgrade to SI 4.2.1 release, so I can take advantage of the latest improvements.",1.0,0.8517720991830797,1.9737140759366752e-05,0.0,True,True,True,1445464477000.0,1445896035000.0,1445899635000.0,1445970236000.0,1445971136000.0,1445464477000.0,-1.0,1445464477000.0,1445468077000.0,1445464487000.0,1445468087000.0,1445896035000.0,1445899635000.0,"sabby",1445468077000.0,1445464477000.0,45,1445464477000.0,-1.0,"Upgrade to SI 4.2.1","As a developer, I'd like to upgrade to SI 4.2.1 release, so I can take advantage of the latest improvements.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3636","Story","Sprint 60","2015-10-21T21:53:01.000+0000","sabby","aclement","Add support for global ""options"" in DSL","As a Flo user, I'd like to have {{timeout}} and {{pollInterval}} as global options at the DSL level, so I can override the defaults at will. ",1.0,1.166102186830301e-05,1.166102186830301e-05,0.0,True,True,True,1445464381000.0,1445464390000.0,1445467990000.0,1446235283000.0,1446236183000.0,1445464381000.0,-1.0,1445464381000.0,1445467981000.0,1445464390000.0,1445467990000.0,1445464390000.0,1445467990000.0,"sabby",1445467981000.0,1445464381000.0,45,1445464381000.0,-1.0,"Add support for global ""options"" in DSL","As a Flo user, I'd like to have {{timeout}} and {{pollInterval}} as global options at the DSL level, so I can override the defaults at will. ",1.0,False,1.0,0,0.0,0,0,False
"XD-3633","Story","Sprint 60","2015-10-19T22:31:37.000+0000","sabby","eric.bottard","Add SFTP source to default registry","As a user, I'd like to use SFTP source module, so I can create streaming pipeline with it. However, I cannot see SFTP as OOTB module listed on: {{module list}} and as well as the module bits are not available in [maven repo|http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/module/]. ",1.0,-1.0,0.0,0.0,False,True,True,1445293897000.0,-1.0,-1.0,1445329989000.0,1445330889000.0,1445293897000.0,-1.0,1445293897000.0,1445297497000.0,1445293897000.0,1445297497000.0,-1.0,-1.0,"sabby",1445297497000.0,1445293897000.0,45,1445293897000.0,-1.0,"Add SFTP source to default registry","As a user, I'd like to use SFTP source module, so I can create streaming pipeline with it. However, I cannot see SFTP as OOTB module listed on: {{module list}} and as well as the module bits are not available in [maven repo|http://repo.spring.io/libs-snapshot/org/springframework/cloud/stream/module/]. ",1.0,False,1.0,0,0.0,0,0,False
"XD-3629","Bug","Sprint 60","2015-10-19T16:50:50.000+0000","sabby","","Add support for namenode HA via Ambari plugin","As a user, I'd like to enable HA on {{namenode}} without having to enable custom configuration. 

More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].",3.0,-1.0,0.0625796453128733,0.0,False,True,True,1445273450000.0,-1.0,-1.0,1446477894000.0,1446478794000.0,1445273450000.0,-1.0,1445273450000.0,1445277050000.0,1445348880000.0,1445352480000.0,-1.0,-1.0,"sabby",1445277050000.0,1445273450000.0,45,1445273450000.0,-1.0,"Turning on HA via Ambari plugin requires custom configuration","As a user, I'd like to enable HA on {{namenode}} without having to enable custom configuration. 

More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].",3.0,False,3.0,0,0.0,0,0,True
"XD-3627","Story","Sprint 60","2015-10-19T16:02:05.000+0000","sabby","","Get rid of XDRuntimeException","As a developer, I'd like to get rid off {{XDRuntimeException}} from XD.",1.0,0.2169642149100704,0.2169642149100704,0.0,True,True,True,1445270525000.0,1445440155000.0,1445443755000.0,1446051459000.0,1446052359000.0,1445270525000.0,-1.0,1445270525000.0,1445274125000.0,1445440155000.0,1445443755000.0,1445440155000.0,1445443755000.0,"sabby",1445274125000.0,1445270525000.0,45,1445270525000.0,-1.0,"Get rid of XDRuntimeException","As a developer, I'd like to get rid off {{XDRuntimeException}} from XD.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3622","Story","Sprint 60","2015-10-18T14:48:21.000+0000","sabby","","Port File as s-c-s source","As a developer, I'd like to port {{file}} module from XD to s-c-s repo, so I can use it as {{source}} module to build streaming pipeline.",3.0,0.1089598063197399,0.1089598063197399,0.0,True,True,True,1445179701000.0,1445283755000.0,1445287355000.0,1446133777000.0,1446134677000.0,1445179701000.0,-1.0,1445179701000.0,1445183301000.0,1445283755000.0,1445287355000.0,1445283755000.0,1445287355000.0,"sabby",1445183301000.0,1445179701000.0,45,1445179701000.0,-1.0,"Port File as s-c-s source","As a developer, I'd like to port {{file}} module from XD to s-c-s repo, so I can use it as {{source}} module to build streaming pipeline.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3621","Bug","Sprint 61","2015-10-16T16:44:02.000+0000","mbogoevici","mbogoevici","Add support for custom headers with the Kafka bus","Currently, the Kafka Message Bus does not have the ability to configure a set of custom headers to persist in the `embeddedHeaders` mode (only the message-bus specific) message headers are persisted. ",3.0,0.9915569958737648,0.0035988543938015,0.9915373419359348,True,True,True,1445013842000.0,1446830071000.0,1446833671000.0,1446844636000.0,1446845536000.0,1445013842000.0,-1.0,1446830035000.0,1446833635000.0,1445020434000.0,1445024034000.0,1446830071000.0,1446833671000.0,"mbogoevici",1446833635000.0,1445013842000.0,45,1445013842000.0,-1.0,"Add support for custom headers with the Kafka bus","Currently, the Kafka Message Bus does not have the ability to configure a set of custom headers to persist in the `embeddedHeaders` mode (only the message-bus specific) message headers are persisted. ",3.0,False,3.0,0,0.0,-1,1,False
"XD-3619","Story","Sprint 60","2015-10-14T23:26:03.000+0000","sabby","","Study YARN SPI gaps","As a s-c-d user, I'd like to deploy data flow on YARN, so I can reuse the existing Hadoop cluster and leverage data flow features to build streaming or batch pipelines.",2.0,0.9999846833278024,0.9272552888104416,0.0,True,True,True,1444865163000.0,1446236197000.0,1446236218000.0,1446235318000.0,1446236218000.0,1444865163000.0,1445264884000.0,1444865163000.0,1444868763000.0,1446136481000.0,1446140081000.0,1446236197000.0,1446236218000.0,"sabby",1444868763000.0,1444865163000.0,45,1445264884000.0,1445264884000.0,"Study YARN SPI gaps","As a s-c-d user, I'd like to deploy data flow on YARN, so I can reuse the existing Hadoop cluster and leverage data flow features to build streaming or batch pipelines.",3.0,True,3.0,1,-33.33333333333333,-1,0,False
"XD-3617","Story","Sprint 59","2015-10-14T14:51:41.000+0000","thomas.risberg","thomas.risberg","Update build to use SHDP 2.3.0.RC1","",2.0,0.4918746701781901,0.4918746701781901,0.8728305739269386,True,True,True,1444834301000.0,1444934034000.0,1444937634000.0,1445036162000.0,1445037062000.0,1444834301000.0,-1.0,1445011277000.0,1445014877000.0,1444934034000.0,1444937634000.0,1444934034000.0,1444937634000.0,"thomas.risberg",1445014877000.0,1444834301000.0,45,1444834301000.0,-1.0,"Update build to use SHDP 2.3.0.RC1","",2.0,False,2.0,0,0.0,-1,0,False
"XD-3616","Story","Sprint 60","2015-10-14T11:12:09.000+0000","eric.bottard","","Add standardized way to pass props from Deployers/Admin to ModuleLauncher","There is a need to customize the ModuleLauncher behavior (itself, NOT pass options to modules that are launched, which is already supported) for example to set the location of the maven repository.

",3.0,0.3882042614957293,0.1937185421265014,0.1644381906305982,True,True,True,1444821129000.0,1445870607000.0,1445874207000.0,1447523646000.0,1447524546000.0,1444821129000.0,-1.0,1445265674000.0,1445269274000.0,1445344831000.0,1445348431000.0,1445870607000.0,1445874207000.0,"eric.bottard",1445269274000.0,1444821129000.0,45,1444821129000.0,-1.0,"Add standardized way to pass props from Deployers/Admin to ModuleLauncher","There is a need to customize the ModuleLauncher behavior (itself, NOT pass options to modules that are launched, which is already supported) for example to set the location of the maven repository.

",3.0,False,3.0,0,0.0,0,0,False
"XD-3610","Bug","Sprint 60","2015-10-13T01:04:44.000+0000","mbogoevici","","Kafka source and sink headers shouldn't interfere with bus functionality","The Kafka sink should not make use of the message headers sent by the Kafka receivers in the Kafka bus. 

Similarly, the headers received from the Kafka source should not be propagated when sending to the Kakfa bus. 

https://github.com/spring-projects/spring-xd/issues/1804",1.0,0.2890191703754231,0.2890191703754231,0.1810425832173083,True,True,True,1444698284000.0,1445615182000.0,1445618782000.0,1447869831000.0,1447870731000.0,1444698284000.0,1446478261000.0,1445272632000.0,1445276232000.0,1445615182000.0,1445618782000.0,1445615182000.0,1445618782000.0,"mbogoevici",1445276232000.0,1444698284000.0,45,1446478261000.0,1446478261000.0,"Kafka source and sink headers shouldn't interfere with bus functionality","The Kafka sink should not make use of the message headers sent by the Kafka receivers in the Kafka bus. 

Similarly, the headers received from the Kafka source should not be propagated when sending to the Kakfa bus. 

https://github.com/spring-projects/spring-xd/issues/1804",3.0,True,3.0,1,-66.66666666666666,-1,2,False
"XD-3602","Story","Sprint 59","2015-10-07T19:58:48.000+0000","sabby","grussell","Port Log as s-c-s sink","As a developer, I'd like to port {{Log}} module from XD to s-c-s repo, so I can use it as {{sink}} modules to build streaming pipeline.",2.0,0.0009526644834772,0.0009526644834772,0.0,True,True,True,1444247928000.0,1444248072000.0,1444251672000.0,1444398183000.0,1444399083000.0,1444247928000.0,-1.0,1444247928000.0,1444251528000.0,1444248072000.0,1444251672000.0,1444248072000.0,1444251672000.0,"sabby",1444251528000.0,1444247928000.0,45,1444247928000.0,-1.0,"Port Log as s-c-s sink","As a developer, I'd like to port {{Log}} module from XD to s-c-s repo, so I can use it as {{sink}} modules to build streaming pipeline.",2.0,False,2.0,0,0.0,0,1,False
"XD-3601","Story","Sprint 60","2015-10-07T17:50:58.000+0000","eric.bottard","","JMX MBean name clash when using labels","We need to make sure that JMX MBean names are unique, even in the case of labeled modules.

The following stream fails for example: ""http | filter | filter2: filter | log""

A good candidate could be stream name (group) + module label.",3.0,0.8827954461497128,0.8827954461497128,3.93166439640351e-05,True,True,True,1444240258000.0,1445318025000.0,1445321625000.0,1445460215000.0,1445461115000.0,1444240258000.0,-1.0,1444240306000.0,1444243906000.0,1445318025000.0,1445321625000.0,1445318025000.0,1445321625000.0,"eric.bottard",1444243906000.0,1444240258000.0,45,1444240258000.0,-1.0,"JMX MBean name clash when using labels with s-c-d deployment","We need to make sure that JMX MBean names are unique, even in the case of labeled modules.

The following stream fails for example: ""http | filter | filter2: filter | log""

A good candidate could be stream name (group) + module label.",3.0,False,3.0,0,0.0,0,0,True
"XD-3598","Bug","Sprint 59","2015-10-07T15:02:03.000+0000","grussell","grussell","Set Bean Name in ConsumerEndpointFactoryBean","{{LocalMessageBus}} and {{CompositeModule}}.",1.0,0.0066844842223739,0.0,0.0,True,True,True,1444230123000.0,1444237036000.0,1444240636000.0,1445263409000.0,1445264309000.0,1444230123000.0,-1.0,1444230123000.0,1444233723000.0,1444230123000.0,1444233723000.0,1444237036000.0,1444240636000.0,"grussell",1444233723000.0,1444230123000.0,45,1444230123000.0,-1.0,"Set Bean Name in ConsumerEndpointFactoryBean","{{LocalMessageBus}} and {{CompositeModule}}.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3597","Story","Sprint 59","2015-10-06T15:46:36.000+0000","mbogoevici","mbogoevici","Separate Lifecycle of Input and Output adapter endpoints","Described in https://github.com/spring-cloud/spring-cloud-stream/issues/144

As a developer, I want Input enpoints to be started after all the beans in the context, so that received messages can be delivered to components. 
",3.0,0.4684607732895078,4.324885390537151e-05,0.0630027679266499,True,True,True,1444146396000.0,1444233050000.0,1444236650000.0,1444330472000.0,1444331372000.0,1444146396000.0,-1.0,1444158050000.0,1444161650000.0,1444146404000.0,1444150004000.0,1444233050000.0,1444236650000.0,"mbogoevici",1444161650000.0,1444146396000.0,45,1444146396000.0,-1.0,"Separate Lifecycle of Input and Output adapter endpoints","Described in https://github.com/spring-cloud/spring-cloud-stream/issues/144

As a developer, I want Input enpoints to be started after all the beans in the context, so that received messages can be delivered to components. 
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3596","Story","Sprint 59","2015-10-06T14:58:53.000+0000","sabby","mark.fisher","Prevent streams with duplicate name","As a s-c-d user, I should be prevented from creating streams with duplicate name. I'd expect streams to have unique names all the time. ",1.0,0.0008601739462869,0.0,0.0,True,True,True,1444143533000.0,1444143542000.0,1444147142000.0,1444153096000.0,1444153996000.0,1444143533000.0,1444143659000.0,1444143533000.0,1444147133000.0,1444143533000.0,1444147133000.0,1444143542000.0,1444147142000.0,"sabby",1444147133000.0,1444143659000.0,45,1444143659000.0,-1.0,"Prevent streams with duplicate name","As a s-c-d user, I should be prevented from creating streams with duplicate name. I'd expect streams to have unique names all the time. ",1.0,False,1.0,0,0.0,0,1,False
"XD-3595","Story","Sprint 59","2015-10-06T14:42:48.000+0000","sabby","mark.fisher","Add test coverage for StreamController","As a s-c-d developer, I'd like to add test coverage for {{StreamController}}, so I can verify API contracts at build time. ",3.0,0.0013108450581141,0.0,0.0,True,True,True,1444142568000.0,1444142583000.0,1444146183000.0,1444153111000.0,1444154011000.0,1444142568000.0,-1.0,1444142568000.0,1444146168000.0,1444142568000.0,1444146168000.0,1444142583000.0,1444146183000.0,"sabby",1444146168000.0,1444142568000.0,45,1444142568000.0,-1.0,"Add test coverage for StreamController","As a s-c-d developer, I'd like to add test coverage for {{StreamController}}, so I can verify API contracts at build time. ",3.0,False,3.0,0,0.0,0,0,False
"XD-3594","Story","Sprint 59","2015-10-05T22:45:31.000+0000","sabby","mark.fisher","Add support for named channels","As an s-c-d user, I'd like to have the option to use _named channels_, so I can create streaming pipelines without source or sink modules. ",3.0,3.470173855710171e-05,0.0,0.0,True,True,True,1444085131000.0,1444085136000.0,1444088736000.0,1444228316000.0,1444229216000.0,1444085131000.0,-1.0,1444085131000.0,1444088731000.0,1444085131000.0,1444088731000.0,1444085136000.0,1444088736000.0,"sabby",1444088731000.0,1444085131000.0,45,1444085131000.0,-1.0,"Add support for named channels","As an s-c-d user, I'd like to have the option to use _named channels_, so I can create streaming pipelines without source or sink modules. ",3.0,False,3.0,0,0.0,0,0,False
"XD-3593","Story","Sprint 59","2015-10-05T22:38:13.000+0000","sabby","","Add support to register artifacts as libraries","As a SCDF user, I want to be able to register artifacts as libraries, so that I can reference them in include and exclude statements.",2.0,0.487934598808749,0.4879313318854329,0.0,True,True,True,1444084693000.0,1444831473000.0,1444835073000.0,1445614285000.0,1445615185000.0,1444084693000.0,-1.0,1444084693000.0,1444088293000.0,1444831468000.0,1444835068000.0,1444831473000.0,1444835073000.0,"sabby",1444088293000.0,1444084693000.0,45,1444084693000.0,-1.0,"Add support to register artifacts as libraries","As a SCDF user, I want to be able to register artifacts as libraries, so that I can reference them in include and exclude statements.",2.0,False,2.0,0,0.0,0,0,False
"XD-3589","Story","Sprint 59","2015-10-05T17:40:45.000+0000","grenfro","grenfro","Create Composed Job Module ","h2. Narrative
As an XD developer, I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository. 
While the user uses the composed job as if it is a normal job including seeing only the DSL.  In the background the JobFactory will deploy the composed job module.  
* When the user destroys the job the module will be deleted from the file module repository.
* When the user creates the job a module will be created in the file Module repository.
h2. Back story
For the composed job story, we need to create a ""real"" job module to be expressed in XML, so that we can take advantage of the job execution tasklet in XD-3556, so that each job can be executed as a step in the composed job.",8.0,0.1892352695483315,0.0,0.0002398188165196,True,True,True,1444066845000.0,1444326451000.0,1444330051000.0,1445437814000.0,1445438714000.0,1444066845000.0,-1.0,1444067174000.0,1444070774000.0,1444066845000.0,1444070445000.0,1444326451000.0,1444330051000.0,"grenfro",1444070774000.0,1444066845000.0,45,1444066845000.0,-1.0,"Create Composed Job Module ","h2. Narrative
As an XD developer, I need to be able to create a composed job module as XML from the DSL an store it in the Module File repository. 
While the user uses the composed job as if it is a normal job including seeing only the DSL.  In the background the JobFactory will deploy the composed job module.  
* When the user destroys the job the module will be deleted from the file module repository.
* When the user creates the job a module will be created in the file Module repository.
h2. Back story
For the composed job story, we need to create a ""real"" job module to be expressed in XML, so that we can take advantage of the job execution tasklet in XD-3556, so that each job can be executed as a step in the composed job.",8.0,False,8.0,0,0.0,-1,1,False
"XD-3582","Story","Sprint 59","2015-10-05T14:52:18.000+0000","sabby","","Add support for tab completion in shell","",8.0,0.1287066973578098,0.1287066973578098,0.0,True,True,True,1444056738000.0,1444231975000.0,1444235575000.0,1445417360000.0,1445418260000.0,1444056738000.0,-1.0,1444056738000.0,1444060338000.0,1444231975000.0,1444235575000.0,1444231975000.0,1444235575000.0,"sabby",1444060338000.0,1444056738000.0,45,1444056738000.0,-1.0,"Add support for tab completion in shell","As an s-c-d user, I'd like to have tab completion on shell, so I can interact with the modules and its available options.",8.0,False,8.0,0,0.0,-1,0,True
"XD-3581","Story","Sprint 59","2015-10-05T14:40:18.000+0000","sabby","","Add support for SpEL property accessors in spring-cloud-stream","",3.0,0.3584103286124624,0.3584021130441658,0.0,True,True,True,1444056018000.0,1444405024000.0,1444408624000.0,1445028879000.0,1445029779000.0,1444056018000.0,-1.0,1444056018000.0,1444059618000.0,1444405016000.0,1444408616000.0,1444405024000.0,1444408624000.0,"sabby",1444059618000.0,1444056018000.0,45,1444056018000.0,-1.0,"Add support for Tuple and JSON SpEL property accessors in spring-cloud-stream","As a spring-cloud-stream user, I'd like to build stream definitions using dot-delimited syntax for resolving properties for Tuple and JSON.",3.0,False,3.0,0,0.0,0,1,True
"XD-3576","Story","Sprint 59","2015-10-02T22:48:56.000+0000","sabby","","Add support to retrieve job details ","As an XD user, I'd like to click to go the detail page of the job page whether or not the selected entity is singular or part of a composed job.",5.0,0.8037271318060962,0.8037271318060962,0.0,True,True,True,1443826136000.0,1445548557000.0,1445552157000.0,1445968278000.0,1445969178000.0,1443826136000.0,-1.0,1443826136000.0,1443829736000.0,1445548557000.0,1445552157000.0,1445548557000.0,1445552157000.0,"sabby",1443829736000.0,1443826136000.0,45,1443826136000.0,-1.0,"Add support to retrieve job details ","As an XD user, I'd like to click to go the detail page of the job page whether or not the selected entity is singular or part of a composed job.",5.0,False,5.0,0,0.0,0,0,False
"XD-3575","Story","Sprint 59","2015-10-02T22:45:11.000+0000","sabby","","Add visual representation of job workflow in executions list page","As an XD user, I'd like to be able to visually differentiate between job-composition workflow and single job.",5.0,0.8037976987037712,0.8037976987037712,0.0,True,True,True,1443825911000.0,1445548562000.0,1445552162000.0,1445968151000.0,1445969051000.0,1443825911000.0,-1.0,1443825911000.0,1443829511000.0,1445548562000.0,1445552162000.0,1445548562000.0,1445552162000.0,"sabby",1443829511000.0,1443825911000.0,45,1443825911000.0,-1.0,"Add visual representation of job workflow in executions list page","As an XD user, I'd like to be able to visually differentiate between job-composition workflow and single job.",5.0,False,5.0,0,0.0,0,0,False
"XD-3574","Story","Sprint 59","2015-10-02T22:43:06.000+0000","sabby","","Include job-composition graph in REST endpoint","As an XD user, I'd like to have a REST endpoint that returns job composition graph, so I can use it to build visual representation of parent-child relationship. 
",3.0,0.8444522614425067,0.8444522614425067,0.0,True,True,True,1443825786000.0,1445635850000.0,1445639450000.0,1445968363000.0,1445969263000.0,1443825786000.0,1444057444000.0,1443825786000.0,1443829386000.0,1445635850000.0,1445639450000.0,1445635850000.0,1445639450000.0,"sabby",1443829386000.0,1443825786000.0,45,1444057444000.0,1444057444000.0,"Include job-composition graph in REST endpoint","As an XD user, I'd like to have a REST endpoint that returns job composition graph, so I can use it to build visual representation of parent-child relationship. 
",2.0,True,2.0,1,50.0,0,0,False
"XD-3573","Story","Sprint 59","2015-10-02T22:42:00.000+0000","sabby","","Include job-composition flag in REST endpoint","As an XD user, I'd like to have a REST endpoint that returns job composition {{flag}}, so I can use it to differentiate visual representation between parent-child relationship and standalone jobs.
",2.0,0.431780783315301,0.431780783315301,0.0,True,True,True,1443825720000.0,1444751189000.0,1444754789000.0,1445968197000.0,1445969097000.0,1443825720000.0,-1.0,1443825720000.0,1443829320000.0,1444751189000.0,1444754789000.0,1444751189000.0,1444754789000.0,"sabby",1443829320000.0,1443825720000.0,45,1443825720000.0,-1.0,"Include job-composition flag in REST endpoint","As an XD user, I'd like to have a REST endpoint that returns job composition {{flag}}, so I can use it to differentiate visual representation between parent-child relationship and standalone jobs.
",2.0,False,2.0,0,0.0,0,2,False
"XD-3571","Story","Sprint 59","2015-10-02T17:55:14.000+0000","sabby","","Port Cassandra as s-c-s sink","As a Spring XD developer, I'd like to move {{cassandra}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.",5.0,0.197735795211537,0.1977324334187602,0.0,True,True,True,1443808514000.0,1444337881000.0,1444341481000.0,1446484757000.0,1446485657000.0,1443808514000.0,1444057801000.0,1443808514000.0,1443812114000.0,1444337872000.0,1444341472000.0,1444337881000.0,1444341481000.0,"sabby",1443812114000.0,1443808514000.0,45,1444057801000.0,1444057801000.0,"Port Cassandra as s-c-s sink","As a Spring XD developer, I'd like to move {{cassandra}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.",2.0,True,2.0,1,150.0,-1,0,False
"XD-3570","Story","Sprint 59","2015-10-02T16:16:37.000+0000","pharris","pharris","Readme has conflicting CF information","In https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/README.md#running-on-cloud-foundry the section starting 'Now we can configure the app' needs to be revised - the information is both out of date and, even if up-to-date, misleading (it includes some values as if they are universal, when they are really just examples).",1.0,0.1756511227728207,0.0,0.0002876165463333,True,True,True,1443802597000.0,1444230096000.0,1444233696000.0,1446235493000.0,1446236393000.0,1443802597000.0,-1.0,1443803297000.0,1443806897000.0,1443802597000.0,1443806197000.0,1444230096000.0,1444233696000.0,"pharris",1443806897000.0,1443802597000.0,45,1443802597000.0,-1.0,"Readme has conflicting CF information","In https://github.com/spring-cloud/spring-cloud-dataflow/blob/master/README.md#running-on-cloud-foundry the section starting 'Now we can configure the app' needs to be revised - the information is both out of date and, even if up-to-date, misleading (it includes some values as if they are universal, when they are really just examples).",1.0,False,1.0,0,0.0,-1,0,False
"XD-3569","Story","Sprint 59","2015-10-02T15:05:53.000+0000","thomas.risberg","","ResourceModuleRegistry doesn't support HA namenode for hdfs custom module location","As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA. 

We had an issue filed in the `spring-xd-ambari` project:

""It seems like custom module doesn't pickup namenode HA? and still use NameNodeProxies.createNonHAProxy?""

see: https://github.com/spring-projects/spring-xd-ambari/issues/14",3.0,0.8314522941538349,0.8704750726490452,0.0005714914149022,True,True,True,1443798353000.0,1445529662000.0,1445533262000.0,1445879724000.0,1445880624000.0,1443798353000.0,-1.0,1443799543000.0,1443803143000.0,1445610918000.0,1445614518000.0,1445529662000.0,1445533262000.0,"thomas.risberg",1443803143000.0,1443798353000.0,45,1443798353000.0,-1.0,"ResourceModuleRegistry doesn't support HA namenode for hdfs custom module location","As an XD module developer I would like to use HDFS for my custom module location even when my namenode is configured for HA. 

We had an issue filed in the `spring-xd-ambari` project:

""It seems like custom module doesn't pickup namenode HA? and still use NameNodeProxies.createNonHAProxy?""

see: https://github.com/spring-projects/spring-xd-ambari/issues/14",3.0,False,3.0,0,0.0,-1,2,False
"XD-3568","Bug","Sprint 58","2015-10-02T03:02:25.000+0000","thomas.risberg","","AdminServer fails on HDP 2.3","Submitting XD on YARN for HDP 2.3 fails due to some Solr issue in Boot - https://github.com/spring-projects/spring-boot/issues/2795

The xd-admin sysout is:

{code}
Started : AdminServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki

02:51:36,624  ERROR main boot.SpringApplication - Application startup failed
java.lang.IllegalStateException: Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer
	at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:58)
	at org.springframework.context.annotation.ConditionEvaluator.shouldSkip(ConditionEvaluator.java:102)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForBeanMethod(ConfigurationClassBeanDefinitionReader.java:178)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:140)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:333)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:273)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:98)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:673)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:519)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79)
Caused by: java.lang.IllegalArgumentException: @ConditionalOnMissingBean annotations must specify at least one bean (type, name or annotation)
	at org.springframework.util.Assert.isTrue(Assert.java:68)
	at org.springframework.boot.autoconfigure.condition.OnBeanCondition$BeanSearchSpec.<init>(OnBeanCondition.java:223)
	at org.springframework.boot.autoconfigure.condition.OnBeanCondition.getMatchOutcome(OnBeanCondition.java:92)
	at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:45)
	... 17 more
02:51:36,628   WARN main annotation.AnnotationConfigApplicationContext - Exception thrown from LifecycleProcessor on context close
java.lang.IllegalStateException: LifecycleProcessor not initialized - call 'refresh' before invoking lifecycle methods via the context: org.springframework.context.annotation.AnnotationConfigApplicationContext@1cf1df22: startup date [Fri Oct 02 02:51:31 UTC 2015]; root of context hierarchy
	at org.springframework.context.support.AbstractApplicationContext.getLifecycleProcessor(AbstractApplicationContext.java:414)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:966)
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:925)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:342)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79)
02:51:36,642  ERROR main admin.AdminServerApplication - Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer
{code}
",3.0,0.1421195745706123,0.942052612310417,0.12932477634918,True,True,True,1443754945000.0,1443800364000.0,1443803964000.0,1444073628000.0,1444074528000.0,1443754945000.0,-1.0,1443796275000.0,1443799875000.0,1444056009000.0,1444059609000.0,1443800364000.0,1443803964000.0,"thomas.risberg",1443799875000.0,1443754945000.0,45,1443754945000.0,-1.0,"AdminServer fails on HDP 2.3","Submitting XD on YARN for HDP 2.3 fails due to some Solr issue in Boot - https://github.com/spring-projects/spring-boot/issues/2795

The xd-admin sysout is:

{code}
Started : AdminServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki

02:51:36,624  ERROR main boot.SpringApplication - Application startup failed
java.lang.IllegalStateException: Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer
	at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:58)
	at org.springframework.context.annotation.ConditionEvaluator.shouldSkip(ConditionEvaluator.java:102)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForBeanMethod(ConfigurationClassBeanDefinitionReader.java:178)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:140)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:333)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:273)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:98)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:673)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:519)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79)
Caused by: java.lang.IllegalArgumentException: @ConditionalOnMissingBean annotations must specify at least one bean (type, name or annotation)
	at org.springframework.util.Assert.isTrue(Assert.java:68)
	at org.springframework.boot.autoconfigure.condition.OnBeanCondition$BeanSearchSpec.<init>(OnBeanCondition.java:223)
	at org.springframework.boot.autoconfigure.condition.OnBeanCondition.getMatchOutcome(OnBeanCondition.java:92)
	at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:45)
	... 17 more
02:51:36,628   WARN main annotation.AnnotationConfigApplicationContext - Exception thrown from LifecycleProcessor on context close
java.lang.IllegalStateException: LifecycleProcessor not initialized - call 'refresh' before invoking lifecycle methods via the context: org.springframework.context.annotation.AnnotationConfigApplicationContext@1cf1df22: startup date [Fri Oct 02 02:51:31 UTC 2015]; root of context hierarchy
	at org.springframework.context.support.AbstractApplicationContext.getLifecycleProcessor(AbstractApplicationContext.java:414)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:966)
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:925)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:342)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95)
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79)
02:51:36,642  ERROR main admin.AdminServerApplication - Error processing condition on org.springframework.boot.autoconfigure.solr.SolrAutoConfiguration.solrServer
{code}
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3567","Bug","Sprint 58","2015-10-01T21:44:39.000+0000","thomas.risberg","thomas.risberg","Fix classpath and servlet container issues","Several issues with 1.3.0.M1 staged version

- we now use Tomcat instead of Jetty which prevent s xd-admin from starting on YARN

- we now have Guava 18.0 on classpath instead of 16.0.1

- xd-yarn push doesn't work, hadoop client for 2.7.1 needs Servlet API 

- updating Hadoop to 2.7.1 instead of 2.6.0
  -- this causes Curator to also update to 2.7.1 which throws exception on startup
",3.0,0.0088097527685578,0.0088097527685578,0.0085460805906256,True,True,True,1443735879000.0,1443736447000.0,1443740047000.0,1443799453000.0,1443800353000.0,1443735879000.0,-1.0,1443736430000.0,1443740030000.0,1443736447000.0,1443740047000.0,1443736447000.0,1443740047000.0,"thomas.risberg",1443740030000.0,1443735879000.0,45,1443735879000.0,-1.0,"Fix classpath and servlet container issues","Several issues with 1.3.0.M1 staged version

- we now use Tomcat instead of Jetty which prevent s xd-admin from starting on YARN

- we now have Guava 18.0 on classpath instead of 16.0.1

- xd-yarn push doesn't work, hadoop client for 2.7.1 needs Servlet API 

- updating Hadoop to 2.7.1 instead of 2.6.0
  -- this causes Curator to also update to 2.7.1 which throws exception on startup
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3566","Story","Sprint 58","2015-10-01T13:39:32.000+0000","grenfro","grenfro","TwitterStream test must use unique name to prevent test collision","XD Developer does not want the the twitter stream acceptance tests to interfere with other tests.",3.0,0.0506679195720189,0.0506679195720189,0.0346186291364563,True,True,True,1443706772000.0,1443711517000.0,1443715117000.0,1443799521000.0,1443800421000.0,1443706772000.0,-1.0,1443710014000.0,1443713614000.0,1443711517000.0,1443715117000.0,1443711517000.0,1443715117000.0,"grenfro",1443713614000.0,1443706772000.0,45,1443706772000.0,-1.0,"TwitterStream test must use unique name to prevent test collision","XD Developer does not want the the twitter stream acceptance tests to interfere with other tests.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3560","Story","Sprint 58","2015-09-30T08:09:36.000+0000","eric.bottard","","Better printing of array default valuesin documentation","When a default value is an array, the current behavior (using toString()) not only produces useless results (like `[Ljava.lang.String;@2638011`) but also constantly changing results.",5.0,0.9966318234610918,0.9966318234610918,0.5307975222609369,True,True,True,1443600576000.0,1443652062000.0,1443652236000.0,1443651336000.0,1443652236000.0,1443600576000.0,-1.0,1443627997000.0,1443631597000.0,1443652062000.0,1443652236000.0,1443652062000.0,1443652236000.0,"eric.bottard",1443631597000.0,1443600576000.0,45,1443600576000.0,-1.0,"Better printing of array default valuesin documentation","When a default value is an array, the current behavior (using toString()) not only produces useless results (like `[Ljava.lang.String;@2638011`) but also constantly changing results.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3556","Story","Sprint 59","2015-09-29T22:02:52.000+0000","sabby","","Develop tasklet to run each step of the Job graph","As an XD developer, I'd like to create a {{tasklet}} to run each step of the job graph, so I can invoke the right REST endpoint at each step to orchestrate composition. ",5.0,0.0411599510512544,0.04079153435965,0.0,True,True,True,1443564172000.0,1443624278000.0,1443627878000.0,1445023575000.0,1445024475000.0,1443564172000.0,-1.0,1443564172000.0,1443567772000.0,1443623740000.0,1443627340000.0,1443624278000.0,1443627878000.0,"sabby",1443567772000.0,1443564172000.0,45,1443564172000.0,-1.0,"Develop tasklet to execute a Job","h2. Narrative
As the system, I would like a way to launch a previously deployed job module from another job module.

h2.  Back story
For the composed job story, we will have a driver job that consists of each step that represents the execution of a job.  This story is the creation of a {{Tasklet}} that will launch the child job, and upon it's completion, set the results of the driver's step to that of the slave job's results.
",5.0,False,5.0,0,0.0,-1,0,True
"XD-3552","Story","Sprint 59","2015-09-29T20:59:49.000+0000","mark.pollack","","Improve automated documentation generation process for modules to handle array arguments","For example the generated value for the cassandra sink results in 

{{-$$entityBasePackages$$:: $$the base packages to scan for entities annotated with Table annotations$$ ($$String;$$, default: `[Ljava.lang.String;@2638011`)}}

where the default value changes each time the build is run.
",3.0,-1.0,-1.0,0.8432341258878636,False,True,False,1443560389000.0,-1.0,-1.0,1444862617000.0,1444863517000.0,1443560389000.0,-1.0,1444659231000.0,1444662831000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1444662831000.0,1443560389000.0,45,1443560389000.0,-1.0,"Improve automated documentation generation process for modules to handle array arguments","For example the generated value for the cassandra sink results in 

{{-$$entityBasePackages$$:: $$the base packages to scan for entities annotated with Table annotations$$ ($$String;$$, default: `[Ljava.lang.String;@2638011`)}}

where the default value changes each time the build is run.
",3.0,False,3.0,0,0.0,0,0,False
"XD-3508","Bug","Sprint 58","2015-09-25T21:51:55.000+0000","sabby","David Turanski","Refactor to replace codec implementation with SI library","As a XD developer, I'd like to refactor and replace {{codec}} code from XD with SI library, so I don't have to maintain duplicate code.",3.0,0.2023156282455424,0.7839933278508179,0.0,True,True,True,1443217915000.0,1443461222000.0,1443464822000.0,1444419626000.0,1444420526000.0,1443217915000.0,-1.0,1443217915000.0,1443221515000.0,1444160754000.0,1444164354000.0,1443461222000.0,1443464822000.0,"sabby",1443221515000.0,1443217915000.0,45,1443217915000.0,-1.0,"Refactor to replace codec implementation with SI library","As a XD developer, I'd like to refactor and replace {{codec}} code from XD with SI library, so I don't have to maintain duplicate code.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3506","Bug","Sprint 58","2015-09-25T16:55:49.000+0000","hillert","hillert","UI - Container List - Module Properties - Escape Passwords","",1.0,0.0014955844649131,0.0,0.0104038078055265,True,True,True,1443200149000.0,1443200653000.0,1443204253000.0,1443536241000.0,1443537141000.0,1443200149000.0,-1.0,1443203655000.0,1443207255000.0,1443200149000.0,1443203749000.0,1443200653000.0,1443204253000.0,"hillert",1443207255000.0,1443200149000.0,45,1443200149000.0,-1.0,"UI - Container List - Module Properties - Escape Passwords","",1.0,False,1.0,0,0.0,-1,0,False
"XD-3505","Bug","Sprint 58","2015-09-25T16:37:33.000+0000","sabby","","Admin app crashes with SSL certification errors","As a s-c-d user, I'm unable to push admin app to CF due to SSL certification errors while bootstrapping. 

Consider adding [CF trusted certificate|https://github.com/pivotal-cf/cloudfoundry-certificate-truster] as a CF SPI dependency.",3.0,0.6141976207817431,0.6141976207817431,0.0,True,True,True,1443199053000.0,1443452041000.0,1443455641000.0,1443610053000.0,1443610953000.0,1443199053000.0,-1.0,1443199053000.0,1443202653000.0,1443452041000.0,1443455641000.0,1443452041000.0,1443455641000.0,"sabby",1443202653000.0,1443199053000.0,45,1443199053000.0,-1.0,"Admin app crashes with SSL certification errors","As a s-c-d user, I'm unable to push admin app to CF due to SSL certification errors while bootstrapping. 

Consider adding [CF trusted certificate|https://github.com/pivotal-cf/cloudfoundry-certificate-truster] as a CF SPI dependency.

Adding CF trusted certificate as dependency doesn't help either:

{code}
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:387)
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:292)
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.validator.Validator.validate(Validator.java:260)
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324)
> Fri Sep 25 2015 12:55:32 GMT-0400 (EDT) [App/0] ERR at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229)
> Fri Sep 25 2015 12:55:32 GMT-...
{code}",3.0,False,3.0,0,0.0,0,1,True
"XD-3503","Story","Sprint 58","2015-09-24T13:40:46.000+0000","hillert","hillert","Document the setting of the CORS allow_origin property","We do set a default value in: xd/lib/spring-xd-dirt-1.2.1.RELEASE.jar/application.yml

{code}
...
xd:
  data:
    home: file:${XD_HOME}/data
  config:
    home: file:${XD_HOME}/config
  module:
    home: file:${XD_HOME}/modules
  customModule:
    home: file:${XD_HOME}/custom-modules
  ui:
    home: file:${XD_HOME}/spring-xd-ui/dist/
    allow_origin: http://localhost:9889
...
{code}

We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using **servers.yml**",2.0,0.6570400260815851,0.6570400260815851,0.6574261021757533,True,True,True,1443102046000.0,1443561543000.0,1443565143000.0,1443800490000.0,1443801390000.0,1443102046000.0,-1.0,1443561813000.0,1443565413000.0,1443561543000.0,1443565143000.0,1443561543000.0,1443565143000.0,"hillert",1443565413000.0,1443102046000.0,45,1443102046000.0,-1.0,"Document the setting of the CORS allow_origin property","We do set a default value in: xd/lib/spring-xd-dirt-1.2.1.RELEASE.jar/application.yml

{code}
...
xd:
  data:
    home: file:${XD_HOME}/data
  config:
    home: file:${XD_HOME}/config
  module:
    home: file:${XD_HOME}/modules
  customModule:
    home: file:${XD_HOME}/custom-modules
  ui:
    home: file:${XD_HOME}/spring-xd-ui/dist/
    allow_origin: http://localhost:9889
...
{code}

We need to document how this property can be used to allow for external services to use the XD Rest API and how you can customize it using **servers.yml**",2.0,False,2.0,0,0.0,-1,1,False
"XD-3502","Story","Sprint 58","2015-09-22T13:16:13.000+0000","thomas.risberg","thomas.risberg","Upgrade SCSM hdfs sink to SHDP 2.3.0.M3","",1.0,0.0147417683436403,0.0142737756978104,0.9991308708006016,True,True,True,1442927773000.0,1442928214000.0,1442931814000.0,1442956788000.0,1442957688000.0,1442927773000.0,-1.0,1442957662000.0,1442957688000.0,1442928200000.0,1442931800000.0,1442928214000.0,1442931814000.0,"thomas.risberg",1442957688000.0,1442927773000.0,45,1442927773000.0,-1.0,"Upgrade SCSM hdfs sink to SHDP 2.3.0.M3","",1.0,False,1.0,0,0.0,-1,1,False
"XD-3501","Story","Sprint 58","2015-09-21T17:48:25.000+0000","sabby","","Admin UI container shutdown not working","As a user, I'm not able to shutdown {{container}} from Admin UI with the following stream definition deployed.

{code}
stream create swagataTestIssue --definition ""jdbc --query='select employee_id, employee_name, employer from EMPLOYEE' --url='jdbc:oracle:thin:@//localhost:1521/orcl'  --username=springxd --password=xdpwd --driverClassName=oracle.jdbc.OracleDriver --testOnBorrow=false | hdfs --inputType=application/json "" --deploy 
{code}

More details [here|https://issuetracker.springsource.com/browse/VESC-504].",2.0,0.3982417051986543,0.3982417051986543,0.0,True,True,True,1442857705000.0,1443128319000.0,1443131919000.0,1443536327000.0,1443537227000.0,1442857705000.0,-1.0,1442857705000.0,1442861305000.0,1443128319000.0,1443131919000.0,1443128319000.0,1443131919000.0,"sabby",1442861305000.0,1442857705000.0,45,1442857705000.0,-1.0,"Admin UI container shutdown not working","As a user, I'm not able to shutdown {{container}} from Admin UI with the following stream definition deployed.

{code}
stream create swagataTestIssue --definition ""jdbc --query='select employee_id, employee_name, employer from EMPLOYEE' --url='jdbc:oracle:thin:@//localhost:1521/orcl'  --username=springxd --password=xdpwd --driverClassName=oracle.jdbc.OracleDriver --testOnBorrow=false | hdfs --inputType=application/json "" --deploy 
{code}

More details [here|https://issuetracker.springsource.com/browse/VESC-504].",2.0,False,2.0,0,0.0,0,2,False
"XD-3493","Story","Sprint 58","2015-09-21T15:59:52.000+0000","sabby","","Update SI, SBoot, and AMQP dependencies","As a XD developer, I'd like to upgrade to SI 4.2, Spring Boot 1.3, and AMQP 1.5 dependencies, so I can take advantage of the latest improvements. ",3.0,0.1044931719718912,0.0919025864711445,0.0,True,True,True,1442851192000.0,1442914939000.0,1442918539000.0,1443460351000.0,1443461251000.0,1442851192000.0,-1.0,1442851192000.0,1442854792000.0,1442907258000.0,1442910858000.0,1442914939000.0,1442918539000.0,"sabby",1442854792000.0,1442851192000.0,45,1442851192000.0,-1.0,"Update SI, Spring, and AMQP dependencies","As a XD developer, I'd like to upgrade to SI 4.2, Spring 4.2.1, and AMQP 1.5 dependencies, so I can take advantage of the latest improvements. ",3.0,False,3.0,0,0.0,0,0,True
"XD-3492","Story","Sprint 58","2015-09-21T15:57:46.000+0000","sabby","","Move header enricher to XD proper","",1.0,0.8356451300858827,0.0787146466133369,0.0,True,True,True,1442851066000.0,1443447130000.0,1443450730000.0,1443563464000.0,1443564364000.0,1442851066000.0,-1.0,1442851066000.0,1442854666000.0,1442907213000.0,1442910813000.0,1443447130000.0,1443450730000.0,"sabby",1442854666000.0,1442851066000.0,45,1442851066000.0,-1.0,"Move header-enricher to XD proper","As a XD developer, I'd like to move header-enricher from modules repo to XD proper. ",1.0,False,1.0,0,0.0,-1,0,True
"XD-3491","Story","Sprint 58","2015-09-21T15:57:15.000+0000","sabby","","Move Cassandra sink to XD proper","",3.0,0.2424058430648974,0.2424058430648974,0.0,True,True,True,1442851035000.0,1443015519000.0,1443019119000.0,1443528683000.0,1443529583000.0,1442851035000.0,-1.0,1442851035000.0,1442854635000.0,1443015519000.0,1443019119000.0,1443015519000.0,1443019119000.0,"sabby",1442854635000.0,1442851035000.0,45,1442851035000.0,-1.0,"Move Cassandra sink to XD proper","",3.0,False,3.0,0,0.0,-1,0,False
"XD-3488","Story","Sprint 58","2015-09-21T14:55:33.000+0000","sabby","","Refactor CF SPI with CF java-client library","",8.0,0.1049250311006351,0.0066234531526222,0.0,True,True,True,1442847333000.0,1442927459000.0,1442931059000.0,1443610083000.0,1443610983000.0,1442847333000.0,1442847366000.0,1442847333000.0,1442850933000.0,1442852391000.0,1442855991000.0,1442927459000.0,1442931059000.0,"sabby",1442850933000.0,1442847366000.0,45,1442847366000.0,-1.0,"Refactor CF SPI with CF java-client library","As a s-c-d developer, I'd like to refactor CC SPI deployer with CF java-client, so I can improve the overall design and performance. ",8.0,False,8.0,0,0.0,0,0,True
"XD-3486","Story","Sprint 58","2015-09-21T02:37:02.000+0000","sabby","","Add support for different binder-types for module channels","As a s-c-d developer, I'd like to add support for having different binder types for module's channels, so I can plug {{rabbit}}, {{redis}}, or {{kafka}} as the source or sink to read and write respectively.",8.0,0.3848251004132041,0.2106548125408089,0.0,True,True,True,1442803022000.0,1443196132000.0,1443199732000.0,1443823651000.0,1443824551000.0,1442803022000.0,1442848545000.0,1442803022000.0,1442806622000.0,1443018212000.0,1443021812000.0,1443196132000.0,1443199732000.0,"sabby",1442806622000.0,1442803022000.0,45,1442848545000.0,1442848545000.0,"Spike: Study support for different binder-types for module channels","As a s-c-d developer, I'd like to add support for having different binder types for module's channels, so I can plug {{rabbit}}, {{redis}}, or {{kafka}} as the source or sink to read and write respectively.",5.0,True,5.0,1,60.0,-1,1,True
"XD-3482","Story","Sprint 58","2015-09-21T01:57:10.000+0000","sabby","","Port JDBC as s-c-s sink","As a s-c-s-m developer, I'd like to move {{jdbc}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.",5.0,0.0329274853278857,0.0127645576140769,0.0,True,True,True,1442800630000.0,1442927451000.0,1442931051000.0,1446651254000.0,1446652154000.0,1442800630000.0,-1.0,1442800630000.0,1442804230000.0,1442849793000.0,1442853393000.0,1442927451000.0,1442931051000.0,"sabby",1442804230000.0,1442800630000.0,45,1442800630000.0,-1.0,"Port JDBC as s-c-s sink","As a s-c-s-m developer, I'd like to move {{jdbc}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.


See also XD-2250",5.0,False,5.0,0,0.0,-1,0,True
"XD-3469","Improvement","Sprint 58","2015-09-10T20:42:50.000+0000","thomas.risberg","Thomas Risberg","The new SCSM twitterstream module should produce same json as old XD source","The new SCSM twitterstream module uses a different format than XD 1.x source module. It should match what Twitter uses so existing processors etc. will continue to work.",3.0,0.0029621467461757,0.0733601244490536,0.0007065076484314,True,True,True,1441917770000.0,1441918227000.0,1441921827000.0,1442071150000.0,1442072050000.0,1441917770000.0,-1.0,1441917879000.0,1441921479000.0,1441929088000.0,1441932688000.0,1441918227000.0,1441921827000.0,"thomas.risberg",1441921479000.0,1441917770000.0,45,1441917770000.0,-1.0,"The new SCSM twitterstream module should produce same json as old XD source","The new SCSM twitterstream module uses a different format than XD 1.x source module. It should match what Twitter uses so existing processors etc. will continue to work.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3468","Bug","Sprint 57","2015-09-09T18:35:27.000+0000","thomas.risberg","thomas.risberg","Unable to set --closeTimeout on SCSM hdfs sink module","Creating a stream like this:

  stream create --name myhdfsstream1 --definition ""time | hdfs --closeTimeout=5000"" --deploy

causes:

java.lang.IllegalArgumentException: Task executor must be set
        at org.springframework.util.Assert.notNull(Assert.java:115) ~[spring-core-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.data.hadoop.store.support.PollingTaskSupport.init(PollingTaskSupport.java:105) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.data.hadoop.store.support.StoreObjectSupport.onInit(StoreObjectSupport.java:97) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.data.hadoop.store.support.OutputStoreObjectSupport.onInit(OutputStoreObjectSupport.java:81) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.data.hadoop.store.support.LifecycleObjectSupport.afterPropertiesSet(LifecycleObjectSupport.java:67) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.cloud.stream.module.hdfs.sink.DataStoreWriterFactoryBean.afterPropertiesSet(DataStoreWriterFactoryBean.java:175) ~[hdfs-sink-1.0.0.BUILD-SNAPSHOT-exec.jar!/:1.0.0.BUILD-SNAPSHOT]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1637) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]        ... 35 common frames omittedWrapped by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataStoreWriter' defined in class path resource [org/springframework/cloud/stream/module/hdfs/sink/HdfsSinkConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Task executor must be set
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1578) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:305) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:301) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBea",2.0,0.9310147377605692,0.8835430612550955,1.289451798933864,True,True,True,1441823727000.0,1441892016000.0,1441895616000.0,1441896176000.0,1441897076000.0,1441823727000.0,-1.0,1441918307000.0,1441897076000.0,1441888534000.0,1441892134000.0,1441892016000.0,1441895616000.0,"thomas.risberg",1441897076000.0,1441823727000.0,45,1441823727000.0,-1.0,"Unable to set --closeTimeout on SCSM hdfs sink module","Creating a stream like this:

  stream create --name myhdfsstream1 --definition ""time | hdfs --closeTimeout=5000"" --deploy

causes:

java.lang.IllegalArgumentException: Task executor must be set
        at org.springframework.util.Assert.notNull(Assert.java:115) ~[spring-core-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.data.hadoop.store.support.PollingTaskSupport.init(PollingTaskSupport.java:105) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.data.hadoop.store.support.StoreObjectSupport.onInit(StoreObjectSupport.java:97) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.data.hadoop.store.support.OutputStoreObjectSupport.onInit(OutputStoreObjectSupport.java:81) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.data.hadoop.store.support.LifecycleObjectSupport.afterPropertiesSet(LifecycleObjectSupport.java:67) ~[spring-data-hadoop-store-2.3.0.M2.jar!/:2.3.0.M2]
        at org.springframework.cloud.stream.module.hdfs.sink.DataStoreWriterFactoryBean.afterPropertiesSet(DataStoreWriterFactoryBean.java:175) ~[hdfs-sink-1.0.0.BUILD-SNAPSHOT-exec.jar!/:1.0.0.BUILD-SNAPSHOT]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1637) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]        ... 35 common frames omittedWrapped by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'dataStoreWriter' defined in class path resource [org/springframework/cloud/stream/module/hdfs/sink/HdfsSinkConfiguration.class]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Task executor must be set
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1578) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:545) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:482) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:305) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:301) ~[spring-beans-4.2.1.RELEASE.jar!/:4.2.1.RELEASE]
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBea",2.0,False,2.0,0,0.0,-1,0,False
"XD-3466","Story","Sprint 57","2015-09-09T17:45:55.000+0000","sabby","mark.fisher","Add hdfs sink to module registry","As a s-c-d developer, I'd like to add _hdfs_ sink to module registry, so I can use this module to build streaming pipeline and write to Hadoop.",1.0,0.0501999111505997,0.0501999111505997,0.0,True,True,True,1441820755000.0,1441820868000.0,1441823006000.0,1441822106000.0,1441823006000.0,1441820755000.0,-1.0,1441820755000.0,1441823006000.0,1441820868000.0,1441823006000.0,1441820868000.0,1441823006000.0,"sabby",1441823006000.0,1441820755000.0,45,1441820755000.0,-1.0,"Add hdfs sink to module registry","As a s-c-d developer, I'd like to add _hdfs_ sink to module registry, so I can use this module to build streaming pipeline and write to Hadoop.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3464","Bug","Sprint 57","2015-09-08T15:05:39.000+0000","pharris","pharris","Stream Destroy fails if stream deploy failed","(From Eric)

Deploying using the following stream fails (probably because of issues around quoting):

`stream create foo --definition time | filter expression=payload.contains(0) | log --deploy`

When you try to destroy the stream the destroy fails, which shouldn't happen whether the stream was valid or not.",2.0,0.720984405233576,0.0,0.0002793006541969,True,True,True,1441724739000.0,1441975134000.0,1441978734000.0,1442071135000.0,1442072035000.0,1441724739000.0,-1.0,1441724836000.0,1441728436000.0,1441724739000.0,1441728339000.0,1441975134000.0,1441978734000.0,"pharris",1441728436000.0,1441724739000.0,45,1441724739000.0,-1.0,"Stream Destroy fails if stream deploy failed","(From Eric)

Deploying using the following stream fails (probably because of issues around quoting):

`stream create foo --definition ""time | filter --expression=payload.contains('0') | log"" --deploy`

When you try to destroy the stream the destroy fails, which shouldn't happen whether the stream was valid or not.",2.0,False,2.0,0,0.0,-1,0,True
"XD-3463","Story","Sprint 57","2015-09-07T16:10:23.000+0000","sabby","eric.bottard","Complete 'Running on Cloud Foundry' section in README","As a s-c-d developer, I'd like to document [Running on Cloud Foundry|https://github.com/spring-cloud/spring-cloud-dataflow#running-on-cloud-foundry] section in README, so it can be publicly available as deployment guideline.",2.0,0.3136819408755988,0.0,0.0,True,True,True,1441642223000.0,1441719619000.0,1441723219000.0,1441888057000.0,1441888957000.0,1441642223000.0,-1.0,1441642223000.0,1441645823000.0,1441642223000.0,1441645823000.0,1441719619000.0,1441723219000.0,"sabby",1441645823000.0,1441642223000.0,45,1441642223000.0,-1.0,"Complete 'Running on Cloud Foundry' section in README","As a s-c-d developer, I'd like to document [Running on Cloud Foundry|https://github.com/spring-cloud/spring-cloud-dataflow#running-on-cloud-foundry] section in README, so it can be publicly available as deployment guideline.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3462","Story","Sprint 57","2015-09-07T15:14:53.000+0000","sabby","hillert","Create a new banner for spring-cloud-data-flow","As a s-c-d user, I'd like to create a new banner, so I can embed and display the banner when the shell server boots-up. 

Perhaps use this [banner generator|http://www.patorjk.com/software/taag/#p=display&f=Standard&t=Spring%20Cloud%20Data%20Flow]?",1.0,0.8042370924540501,0.0,0.0,True,True,True,1441638893000.0,1441987192000.0,1441990792000.0,1442071073000.0,1442071973000.0,1441638893000.0,-1.0,1441638893000.0,1441642493000.0,1441638893000.0,1441642493000.0,1441987192000.0,1441990792000.0,"sabby",1441642493000.0,1441638893000.0,45,1441638893000.0,-1.0,"Create a new banner for spring-cloud-data-flow","As a s-c-d user, I'd like to create a new banner, so I can embed and display the banner when the shell server boots-up. 

Perhaps use this [banner generator|http://patorjk.com/software/taag/#p=display&f=Standard&t=Spring%20Cloud%0AData%20Flow%20%20%3E%3E%3E%3E%3E%20]?",1.0,False,1.0,0,0.0,-1,2,True
"XD-3461","Story","Sprint 57","2015-09-03T12:38:29.000+0000","jvalkeal","jvalkeal","Remove hardcoded yarn app version jars","We currently use fixed paths like `spring-cloud-data-yarn/spring-cloud-data-yarn-appmaster/target/spring-cloud-data-yarn-appmaster-1.0.0.BUILD-SNAPSHOT.jar` in yml files. Need to make define version during a build and allow to override location of those files.",1.0,0.4316340310600945,0.0,0.2446404456448345,True,True,True,1441283909000.0,1441294137000.0,1441297737000.0,1441306705000.0,1441307605000.0,1441283909000.0,-1.0,1441289706000.0,1441293306000.0,1441283909000.0,1441287509000.0,1441294137000.0,1441297737000.0,"jvalkeal",1441293306000.0,1441283909000.0,45,1441283909000.0,-1.0,"Remove hardcoded yarn app version jars","We currently use fixed paths like `spring-cloud-data-yarn/spring-cloud-data-yarn-appmaster/target/spring-cloud-data-yarn-appmaster-1.0.0.BUILD-SNAPSHOT.jar` in yml files. Need to make define version during a build and allow to override location of those files.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3458","Story","Sprint 57","2015-09-02T16:05:09.000+0000","grenfro","grenfro","Create CI Build for SCTM","",3.0,0.0021737535923611,0.0,0.0022358708648584,True,True,True,1441209909000.0,1441216103000.0,1441219703000.0,1444058458000.0,1444059358000.0,1441209909000.0,-1.0,1441216280000.0,1441219880000.0,1441209909000.0,1441213509000.0,1441216103000.0,1441219703000.0,"grenfro",1441219880000.0,1441209909000.0,45,1441209909000.0,-1.0,"Create CI Build for SCTM","",3.0,False,3.0,0,0.0,0,0,False
"XD-3456","Story","Sprint 57","2015-09-02T16:02:56.000+0000","grenfro","grenfro","Create infrastructure for Spring cloud task modules","Create Parent pom file for build
Create .settings file
Migrate Timestamp task from SCSM to SCTM.
",3.0,0.0022033813753849,0.0,0.0022898022647893,True,True,True,1441209776000.0,1441216048000.0,1441219648000.0,1444055410000.0,1444056310000.0,1441209776000.0,-1.0,1441216294000.0,1441219894000.0,1441209776000.0,1441213376000.0,1441216048000.0,1441219648000.0,"grenfro",1441219894000.0,1441209776000.0,45,1441209776000.0,-1.0,"Create infrastructure for Spring cloud task modules","Create Parent pom file for build
Create .settings file
Migrate Timestamp task from SCSM to SCTM.
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3454","Story","Sprint 57","2015-09-02T13:49:14.000+0000","iperumal","iperumal","Add RxJava processor module","As a module author, I would like to apply RxJava processor module with spring cloud stream. ",3.0,0.0565974800133423,0.0,0.0034647126548091,True,True,True,1441201754000.0,1441207014000.0,1441210614000.0,1441293791000.0,1441294691000.0,1441201754000.0,-1.0,1441202076000.0,1441205676000.0,1441201754000.0,1441205354000.0,1441207014000.0,1441210614000.0,"iperumal",1441205676000.0,1441201754000.0,45,1441201754000.0,-1.0,"Add RxJava processor module","As a module author, I would like to apply RxJava processor module with spring cloud stream. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-3451","Story","Sprint 58","2015-09-02T10:51:05.000+0000","zteve","","Correctly report status of module instances","Currently only the STARTED application (and application instance) status is recognised. This issue will look at the other possible states and report them as module instance states.

This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.",2.0,0.1962862979998687,0.1962862979998687,0.0187442311331502,True,True,True,1441191065000.0,1441292715000.0,1441296315000.0,1441708031000.0,1441708931000.0,1441191065000.0,-1.0,1441200772000.0,1441204372000.0,1441292715000.0,1441296315000.0,1441292715000.0,1441296315000.0,"zteve",1441204372000.0,1441191065000.0,45,1441191065000.0,-1.0,"Correctly report state of module instances","Currently only the STARTED application (and application instance) status is recognised. This issue will look at the other possible states and report them as module instance states.

This would be trivial if we knew what the possible states might be and how we should interpret them for module instance state.",2.0,False,2.0,0,0.0,0,0,True
"XD-3450","Story","Sprint 58","2015-09-02T10:45:14.000+0000","zteve","","Deploy multiple instances of a module","Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting.

It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.",2.0,0.1981335067571609,0.1981335067571609,0.019409831510003,True,True,True,1441190714000.0,1441293385000.0,1441296985000.0,1441708005000.0,1441708905000.0,1441190714000.0,-1.0,1441200772000.0,1441204372000.0,1441293385000.0,1441296985000.0,1441293385000.0,1441296985000.0,"zteve",1441204372000.0,1441190714000.0,45,1441190714000.0,-1.0,"Deploy multiple instances of a module","Currently we deploy a single instance, and ignore the ModuleDeploymentRequest instances setting.

It is easy to change this in the ModuleDeployer, but there is no guarantee this will work in the ModuleLauncher, so we hold off until that can be verified.",2.0,False,2.0,0,0.0,0,0,False
"XD-3447","Story","Sprint 57","2015-09-02T00:24:32.000+0000","sabby","","Document s-c-d architecture and deployment variants","As a s-c-d developer, I'd like to produce ref. documentation for s-c-d architecture, so I could define 1.x and 2.x deployment differences. ",8.0,0.9997060840599588,0.9997060840599588,0.0,True,True,True,1441153472000.0,1442071832000.0,1442072102000.0,1442071202000.0,1442072102000.0,1441153472000.0,-1.0,1441153472000.0,1441157072000.0,1442071832000.0,1442072102000.0,1442071832000.0,1442072102000.0,"sabby",1441157072000.0,1441153472000.0,45,1441153472000.0,-1.0,"Document s-c-d architecture and deployment variants","As a s-c-d developer, I'd like to produce ref. documentation for s-c-d architecture, so I could define 1.x and 2.x deployment differences. ",8.0,False,8.0,0,0.0,-1,0,False
"XD-3446","Story","Sprint 58","2015-09-02T00:07:53.000+0000","sroy","","The tooltip for source displays incorrect information when using HDFS as sink","Deploying a Stream with HDFS sink and JDBC as source displays incorrect information on the tooltip for the JDBC source. The issue occurs when there are more than 1 containers deployed and the source is deployed on one container and the sink is deployed on another container. I have checked the REST endpoints and they seem to show the correct information. (http://localhost:9393/runtime/containers.json)

In my test case, say if there are 2 containers and source and sink are deployed on the same container, the tooltip's show correct information. The Stream I used for testing purposes is as follows -
{noformat}
 stream create swagataTestIssue --definition ""jdbc --query='select employee_id, employee_name, employer from EMPLOYEE' --url='jdbc:oracle:thin:@//localhost:1521/orcl'  --username=springxd --password=xdpwd --driverClassName=oracle.jdbc.OracleDriver --testOnBorrow=false | hdfs --inputType=application/json "" --deploy 
{noformat}

I will attach the screenshots. This issue has also been reported when using Gemfire as source and HDFS as sink.

Thanks,
Swagata",1.0,0.8848640153251365,0.8848640153251365,2.3230733903558503e-05,True,True,True,1441152473000.0,1443133165000.0,1443136765000.0,1443389987000.0,1443390887000.0,1441152473000.0,-1.0,1441152525000.0,1441156125000.0,1443133165000.0,1443136765000.0,1443133165000.0,1443136765000.0,"sroy",1441156125000.0,1441152473000.0,45,1441152473000.0,-1.0,"The tooltip for source displays incorrect information when using HDFS as sink","Deploying a Stream with HDFS sink and JDBC as source displays incorrect information on the tooltip for the JDBC source. The issue occurs when there are more than 1 containers deployed and the source is deployed on one container and the sink is deployed on another container. I have checked the REST endpoints and they seem to show the correct information. (http://localhost:9393/runtime/containers.json)

In my test case, say if there are 2 containers and source and sink are deployed on the same container, the tooltip's show correct information. The Stream I used for testing purposes is as follows -
{noformat}
 stream create swagataTestIssue --definition ""jdbc --query='select employee_id, employee_name, employer from EMPLOYEE' --url='jdbc:oracle:thin:@//localhost:1521/orcl'  --username=springxd --password=xdpwd --driverClassName=oracle.jdbc.OracleDriver --testOnBorrow=false | hdfs --inputType=application/json "" --deploy 
{noformat}

I will attach the screenshots. This issue has also been reported when using Gemfire as source and HDFS as sink.

Thanks,
Swagata",1.0,False,1.0,0,0.0,-1,0,False
"XD-3445","Story","Sprint 56","2015-08-31T22:48:09.000+0000","sabby","mbogoevici","Fix Kafka Binder for s-c-s modules","As a s-c-s developer, I'd like to fix the {{Kafka}} binder, so I can create messaging microservices apps and successfully bind them to an operational Kafka broker. ",3.0,0.0001538067162266,0.0,0.0,True,True,True,1441061289000.0,1441061298000.0,1441064898000.0,1441118904000.0,1441119804000.0,1441061289000.0,-1.0,1441061289000.0,1441064889000.0,1441061289000.0,1441064889000.0,1441061298000.0,1441064898000.0,"sabby",1441064889000.0,1441061289000.0,45,1441061289000.0,-1.0,"Fix Kafka Binder for s-c-s modules","As a s-c-s developer, I'd like to fix the {{Kafka}} binder, so I can create messaging microservices apps and successfully bind them to an operational Kafka broker. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-3444","Story","Sprint 56","2015-08-31T22:05:56.000+0000","sabby","hillert","Create gh_pages for s-c-d and s-c-s-m repos","As a s-c-d developer, I'd like to setup {{gh_pages}} branch for s-c-d and s-c-s-m repos, so I can start pushing documentation with PR commits.",5.0,0.162928489097311,1.9736945983926232e-05,0.0,True,True,True,1441058756000.0,1441223856000.0,1441227456000.0,1442071184000.0,1442072084000.0,1441058756000.0,-1.0,1441058756000.0,1441062356000.0,1441058776000.0,1441062376000.0,1441223856000.0,1441227456000.0,"sabby",1441062356000.0,1441058756000.0,45,1441058756000.0,-1.0,"Create gh_pages for s-c-d and s-c-s-m repos","As a s-c-d developer, I'd like to setup {{gh_pages}} branch for s-c-d and s-c-s-m repos, so I can start pushing documentation with PR commits.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3436","Story","Sprint 56","2015-08-31T14:51:07.000+0000","grenfro","grenfro","Create a spring cloud stream timestamp task module","Create a timestamp job that will be used a a sample for users to create their own spring boot based jobs.  
",3.0,0.0805369127516778,0.0,0.3691275167785235,False,False,False,1441032667000.0,1441032679000.0,1441032816000.0,1441032667000.0,1441032816000.0,1441032667000.0,-1.0,1441032722000.0,1441032816000.0,1441032667000.0,1441032816000.0,1441032679000.0,1441032816000.0,"grenfro",1441032816000.0,1441032667000.0,45,1441032667000.0,-1.0,"Create a spring cloud stream timestamp task module","Create a timestamp job that will be used a a sample for users to create their own spring boot based jobs.  
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3432","Story","Sprint 56","2015-08-25T18:42:18.000+0000","iperumal","iperumal","Update documentation for module launcher","The s-c-s-module-launcher document requires update for running it on standalone, docker, lattice.
Also, the docker-compose yml requires fix so that modules in there are bound together.",1.0,0.0003337245966535,0.0,0.0038320789891597,True,True,True,1440528138000.0,1440528225000.0,1440531825000.0,1440787932000.0,1440788832000.0,1440528138000.0,-1.0,1440529137000.0,1440532737000.0,1440528138000.0,1440531738000.0,1440528225000.0,1440531825000.0,"iperumal",1440532737000.0,1440528138000.0,45,1440528138000.0,-1.0,"Update documentation for module launcher","The s-c-s-module-launcher document requires update for running it on standalone, docker, lattice.
Also, the docker-compose yml requires fix so that modules in there are bound together.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3430","Story","Sprint 56","2015-08-25T09:00:53.000+0000","sabby","mark.fisher","Add support for ""deployment properties""","As a s-c-d developer, I'd like to provide optional key-value pairs as deployment properties, so I could leverage them at the runtime to instruct how the modules will be deployed. 

_The scope of this story is to specifically support {{count}} to represent {{N}} instances of modules that share the same environment variables._",8.0,0.2049107202300901,0.0,0.0,True,True,True,1440493253000.0,1440508642000.0,1440512242000.0,1440567454000.0,1440568354000.0,1440493253000.0,-1.0,1440493253000.0,1440496853000.0,1440493253000.0,1440496853000.0,1440508642000.0,1440512242000.0,"sabby",1440496853000.0,1440493253000.0,45,1440493253000.0,-1.0,"Add support for ""deployment properties""","As a s-c-d developer, I'd like to provide optional key-value pairs as deployment properties, so I could leverage them at the runtime to instruct how the modules will be deployed. 

_The scope of this story is to specifically support {{count}} to represent {{N}} instances of modules that share the same environment variables._",8.0,False,8.0,0,0.0,-1,0,False
"XD-3425","Story","Sprint 56","2015-08-24T10:49:14.000+0000","sabby","pperalta","Support shell commands to interact with module registry","As a s-c-d developer, I'd like to have {{module info}}, {{module list}}, {{module register}}, and {{module unregister}} commands, so I can interact with {{ModuleRegistry}}.",8.0,0.2351830045076258,0.0007637882195232,0.0,True,True,True,1440413354000.0,1440579321000.0,1440582921000.0,1441118147000.0,1441119047000.0,1440413354000.0,-1.0,1440413354000.0,1440416954000.0,1440413893000.0,1440417493000.0,1440579321000.0,1440582921000.0,"sabby",1440416954000.0,1440413354000.0,45,1440413354000.0,-1.0,"Support shell commands to interact with module registry","As a s-c-d developer, I'd like to have {{module info}}, {{module list}}, {{module register}}, and {{module unregister}} commands, so I can interact with {{ModuleRegistry}}.",8.0,False,8.0,0,0.0,-1,0,False
"XD-3424","Bug","Sprint 56","2015-08-21T17:18:13.000+0000","iperumal","iperumal","Fix Cloud connector dependencies and service resolution","This JIRA addresses couple of issues:
1) When the modules are deployed into cloud environment there is an issue where local configuration beans collide with cloud service beans. We witnessed an issue where there are two `RedisConnectionFactory` beans registered in the same application context.
We need to have a control the way in which the auto configuration gets invoked and service beans are created.
2) We need to align the cloud connector dependencies into a common place so that we don't have to specify them at various places like (SCS, SCS-Binder, SCS-modules) etc.,
It is a good idea to have these dependencies specified in SCS-modules so that it get used subsequently by SCS when the module is assembled at runtime.",3.0,0.0001752911406059,0.0,0.9944311353022872,True,True,True,1440177493000.0,1440177532000.0,1440181132000.0,1440399080000.0,1440399980000.0,1440177493000.0,-1.0,1440398741000.0,1440399980000.0,1440177493000.0,1440181093000.0,1440177532000.0,1440181132000.0,"iperumal",1440399980000.0,1440177591000.0,45,1440177591000.0,-1.0,"Fix Cloud connector dependencies and service resolution","This JIRA addresses couple of issues:
1) When the modules are deployed into cloud environment there is an issue where local configuration beans collide with cloud service beans. We witnessed an issue where there are two `RedisConnectionFactory` beans registered in the same application context.
We need to have a control the way in which the auto configuration gets invoked and service beans are created.
2) We need to align the cloud connector dependencies into a common place so that we don't have to specify them at various places like (SCS, SCS-Binder, SCS-modules) etc.,
It is a good idea to have these dependencies specified in SCS-modules so that it get used subsequently by SCS when the module is assembled at runtime.",3.0,False,3.0,0,0.0,0,0,False
"XD-3423","Story","Sprint 56","2015-08-21T08:02:37.000+0000","mminella","Michael Minella","Update Shell to support tasks","h2. Narrative
As a user, I need to be able to deploy a task (boot jar) via the CLI.

h2.  Back story
Since the concept of jobs as an explicit primitive within Spring XD is going away in spring-cloud-data, the shell needs to be updated to reflect that.",5.0,0.6752405777662489,0.4420398969991838,0.0006991148762788,True,True,True,1440144157000.0,1440571063000.0,1440574663000.0,1440775485000.0,1440776385000.0,1440144157000.0,-1.0,1440144599000.0,1440148199000.0,1440423627000.0,1440427227000.0,1440571063000.0,1440574663000.0,"mminella",1440148199000.0,1440144599000.0,45,1440144599000.0,-1.0,"Update Shell to support tasks","h2. Narrative
As a user, I need to be able to deploy a task (boot jar) via the CLI.

h2.  Back story
Since the concept of jobs as an explicit primitive within Spring XD is going away in spring-cloud-data, the shell needs to be updated to reflect that.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3419","Story","Sprint 57","2015-08-20T09:22:32.000+0000","sabby","David Turanski","Add registry to lookup module coordinates by name","As a s-c-d developer, I'd like to create {{ModuleRegistry}} implementation, so I can use this infrastructure to lookup module coordinates by name.",8.0,0.2184719132369299,0.2176434464219503,0.0,True,True,True,1440062552000.0,1440137972000.0,1440141572000.0,1440406868000.0,1440407768000.0,1440062552000.0,-1.0,1440062552000.0,1440066152000.0,1440137686000.0,1440141286000.0,1440137972000.0,1440141572000.0,"sabby",1440066152000.0,1440062552000.0,45,1440062552000.0,-1.0,"Add registry to lookup module coordinates by name","As a s-c-d developer, I'd like to create {{ModuleRegistry}} implementation, so I can use this infrastructure to lookup module coordinates by name.",8.0,False,8.0,0,0.0,-1,0,False
"XD-3417","Story","Sprint 56","2015-08-19T11:40:25.000+0000","iperumal","iperumal","Add SmartLifecycle to ChannelBindingAdapter","Make ChannelBindingAdapter implement SmartLifecycle so that it gets started with the highest precedence and before any other message producing bean.",1.0,0.0158371040723981,0.0,0.4502262443438914,False,False,False,1439984425000.0,1439984432000.0,1439984867000.0,1439984425000.0,1439984867000.0,1439984425000.0,-1.0,1439984624000.0,1439984867000.0,1439984425000.0,1439984867000.0,1439984432000.0,1439984867000.0,"iperumal",1439984867000.0,1439984570000.0,45,1439984570000.0,-1.0,"Add SmartLifecycle to ChannelBindingAdapter","Make ChannelBindingAdapter implement SmartLifecycle so that it gets started with the highest precedence and before any other message producing bean.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3416","Story","Sprint 56","2015-08-19T11:03:26.000+0000","sabby","","Create foundation to support s-c-s 'processor' modules","As a s-c-d developer, I'd like to create foundation to support _processor_ as OOTB modules, so I can use the processor modules from {{s-c-s-m}} repo to build streaming pipeline.",5.0,0.1529285592223245,0.1529285592223245,0.0,True,True,True,1439982206000.0,1440060063000.0,1440063663000.0,1440490413000.0,1440491313000.0,1439982206000.0,-1.0,1439982206000.0,1439985806000.0,1440060063000.0,1440063663000.0,1440060063000.0,1440063663000.0,"sabby",1439985806000.0,1439982206000.0,45,1439982206000.0,-1.0,"Create foundation to support s-c-s 'processor' modules","As a s-c-d developer, I'd like to create foundation to support _processor_ as OOTB modules, so I can use the processor modules from {{s-c-s-m}} repo to build streaming pipeline.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3412","Story","Sprint 57","2015-08-19T08:44:19.000+0000","sabby","","Port SFTP as s-c-s source module","As a Spring XD developer, I'd like to port SFTP module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline.
",5.0,0.8250562614965861,0.8250562614965861,0.0,True,True,True,1439973859000.0,1444068968000.0,1444072568000.0,1444936389000.0,1444937289000.0,1439973859000.0,1444057580000.0,1439973859000.0,1439977459000.0,1444068968000.0,1444072568000.0,1444068968000.0,1444072568000.0,"sabby",1439977459000.0,1439973859000.0,45,1444057580000.0,1444057580000.0,"Port SFTP as s-c-s source module","As a Spring XD developer, I'd like to port SFTP module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline.
",3.0,True,3.0,1,66.66666666666666,-1,0,False
"XD-3404","Story","Sprint 56","2015-08-18T09:53:31.000+0000","sabby","","Refactor YARN deployer to deploy asycnhrounously","As a s-c-d developer, I'd like to make the deployer work asynchronously, so I can use the shell to return quickly and also queue deploy operations within YARN as tasks.",2.0,0.9570976623138444,0.9570976623138444,0.0,True,True,True,1439891611000.0,1441206712000.0,1441210312000.0,1441264762000.0,1441265662000.0,1439891611000.0,-1.0,1439891611000.0,1439895211000.0,1441206712000.0,1441210312000.0,1441206712000.0,1441210312000.0,"sabby",1439895211000.0,1439891611000.0,45,1439891611000.0,-1.0,"Refactor YARN deployer to deploy asycnhrounously","As a s-c-d developer, I'd like to make the deployer work asynchronously, so I can use the shell to return quickly and also queue deploy operations within YARN as tasks.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3402","Story","Sprint 56","2015-08-18T09:46:34.000+0000","sabby","","Add support to start Apps in YARN automatically by type","As an s-c-d developer, I'd like to add support to negotiate with the ResourceManager REST-APIs to deploy modules by groups. so I can build instrumentation to start the App instances automatically. Perhaps also take into account of the App specifics such as  {{appType=CLOUDDATA}} and {{appName=spring-cloud-data-yarn-app}}.
",3.0,0.5003976104087348,0.1068724394508101,0.0,True,True,True,1439891194000.0,1440578971000.0,1440582571000.0,1441264755000.0,1441265655000.0,1439891194000.0,1439972061000.0,1439891194000.0,1439894794000.0,1440038086000.0,1440041686000.0,1440578971000.0,1440582571000.0,"sabby",1439894794000.0,1439891194000.0,45,1439972061000.0,1439972061000.0,"Add support to start Apps in YARN automatically by type","As an s-c-d developer, I'd like to add support to negotiate with the ResourceManager REST-APIs to deploy modules by groups. so I can build instrumentation to start the App instances automatically. Perhaps also take into account of the App specifics such as  {{appType=CLOUDDATA}} and {{appName=spring-cloud-data-yarn-app}}.
",2.0,True,2.0,1,50.0,-1,0,False
"XD-3401","Story","Sprint 56","2015-08-18T09:42:17.000+0000","sabby","","Add support for deploying YARN app into HDFS","As a s-c-d developer, I'd like to add support to deploy YARN App into HDFS automatically, so I can have the {{xd-admin}} orchestrate overall deployment by leveraging the manifest to deploy where and with what assets.",3.0,0.5004837320546319,0.1070349916492571,0.0,True,True,True,1439890937000.0,1440578966000.0,1440582566000.0,1441264765000.0,1441265665000.0,1439890937000.0,1439972058000.0,1439890937000.0,1439894537000.0,1440038081000.0,1440041681000.0,1440578966000.0,1440582566000.0,"sabby",1439894537000.0,1439890937000.0,45,1439972058000.0,1439972058000.0,"Add support for deploying YARN app into HDFS","As a s-c-d developer, I'd like to add support to deploy YARN App into HDFS automatically, so I can have the {{xd-admin}} orchestrate overall deployment by leveraging the manifest to deploy where and with what assets.",2.0,True,2.0,1,50.0,-1,0,False
"XD-3400","Story","Sprint 56","2015-08-18T09:38:37.000+0000","sabby","","Add support for passing parameters to YARN container","As a s-c-d user, I'd like to have the option to support passing definition parameters into YARN container, so I can effectively use those _params_ within the module running inside the container.",3.0,0.1071696115181585,0.1071696115181585,0.0,True,True,True,1439890717000.0,1440038069000.0,1440041669000.0,1441264759000.0,1441265659000.0,1439890717000.0,1439972027000.0,1439890717000.0,1439894317000.0,1440038069000.0,1440041669000.0,1440038069000.0,1440041669000.0,"sabby",1439894317000.0,1439890717000.0,45,1439972027000.0,1439972027000.0,"Add support for passing parameters to YARN container","As a s-c-d user, I'd like to have the option to support passing definition parameters into YARN container, so I can effectively use those _params_ within the module running inside the container.",2.0,True,2.0,1,50.0,-1,0,False
"XD-3399","Story","Sprint 55","2015-08-18T08:24:47.000+0000","grenfro","grenfro","Create CI Builds for SCD and Receptor Client","Build SCS and SCD projects upon change in github repo.
Push docker image for SCD-Admin to docker hub",5.0,0.3445378151260504,0.3235294117647059,0.5966386554621849,False,False,False,1439886287000.0,1439886369000.0,1439886525000.0,1439886287000.0,1439886525000.0,1439886287000.0,-1.0,1439886429000.0,1439886525000.0,1439886364000.0,1439886525000.0,1439886369000.0,1439886525000.0,"grenfro",1439886525000.0,1439886287000.0,45,1439886287000.0,-1.0,"Create CI Builds for SCD and Receptor Client","Build SCS and SCD projects upon change in github repo.
Push docker image for SCD-Admin to docker hub",5.0,False,5.0,0,0.0,-1,0,False
"XD-3398","Story","Sprint 55","2015-08-17T14:30:51.000+0000","iperumal","iperumal","Create auto configuration/properties for Local Binder","As a s-c-s developer, I'd like to create auto configuration for {{singlenode}} binder configuration/properties, so I can automatically configure the Spring application based on the dependencies.",1.0,5.368440179058128e-05,0.0,0.2910437899535836,True,True,True,1439821851000.0,1439821864000.0,1439825464000.0,1440063107000.0,1440064007000.0,1439821851000.0,-1.0,1439892329000.0,1439895929000.0,1439821851000.0,1439825451000.0,1439821864000.0,1439825464000.0,"iperumal",1439895929000.0,1439821851000.0,45,1439821851000.0,-1.0,"Create auto configuration/properties for Local Binder","As a s-c-s developer, I'd like to create auto configuration for {{singlenode}} binder configuration/properties, so I can automatically configure the Spring application based on the dependencies.",1.0,False,1.0,0,0.0,-1,1,False
"XD-3397","Story","Sprint 55","2015-08-17T07:59:09.000+0000","iperumal","iperumal","Port admin web UI to Spring Cloud Data admin","As a user I should be able to use the existing admin UI client for spring-cloud-data admin with the appropriate server configurations.",2.0,0.0198807157057654,0.0,0.8349900596421471,False,False,False,1439798349000.0,1439798359000.0,1439798852000.0,1439798349000.0,1439798852000.0,1439798349000.0,-1.0,1439798769000.0,1439798852000.0,1439798349000.0,1439798852000.0,1439798359000.0,1439798852000.0,"iperumal",1439798852000.0,1439798349000.0,45,1439798349000.0,-1.0,"Port admin web UI to Spring Cloud Data admin","As a user I should be able to use the existing admin UI client for spring-cloud-data admin with the appropriate server configurations.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3396","Story","Sprint 55","2015-08-17T07:55:25.000+0000","iperumal","iperumal","Add cloud connector dependencies for spring-cloud-data admin","Spring-cloud-data admin requires lattice connector and `spring-cloud-spring-service-connector` dependencies so that the admin controllers get access to any services while running on lattice.

One example is, CounterContoller using `redis` service for MetricRepository.",3.0,0.065625,0.0,0.734375,False,False,False,1439798125000.0,1439798146000.0,1439798445000.0,1439798125000.0,1439798445000.0,1439798125000.0,-1.0,1439798360000.0,1439798445000.0,1439798125000.0,1439798445000.0,1439798146000.0,1439798445000.0,"iperumal",1439798445000.0,1439798125000.0,45,1439798125000.0,-1.0,"Add cloud connector dependencies for spring-cloud-data admin","Spring-cloud-data admin requires lattice connector and `spring-cloud-spring-service-connector` dependencies so that the admin controllers get access to any services while running on lattice.

One example is, CounterContoller using `redis` service for MetricRepository.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3395","Story","Sprint 55","2015-08-17T07:48:57.000+0000","iperumal","iperumal","Module Launcher properties improvments","Improve Spring Cloud Stream module launcher/resolver properties:

1) Support comma separated remoteRepositories
2) Classify/group the properties",3.0,2.101035810654653e-05,0.0,0.0002071021013359,True,True,True,1439797737000.0,1439797758000.0,1439801358000.0,1440796344000.0,1440797244000.0,1439797737000.0,-1.0,1439797944000.0,1439801544000.0,1439797737000.0,1439801337000.0,1439797758000.0,1439801358000.0,"iperumal",1439801544000.0,1439797737000.0,45,1439797737000.0,-1.0,"Module Launcher properties improvments","Improve Spring Cloud Stream module launcher/resolver properties:

1) Support comma separated remoteRepositories
2) Classify/group the properties",3.0,False,3.0,0,0.0,0,0,False
"XD-3394","Story","Sprint 56","2015-08-14T11:11:42.000+0000","grussell","","Known Hosts Configuration for SFTP Source","Spring Integration 4.2 changed the default SFTP session factory to *not* accept keys from unknown hosts by default. This is more secure.

You either have to provide a pre-populated {{known_hosts}} file or set {{allowUnknownKeys}} to true.

If you do both, the keys will be automatically added to the known hosts file.

When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior.

Add properties to the SFTP source to allow configuration of these properties at the stream level.
",1.0,0.1549025633398439,0.8534463903342716,0.0021456552122361,True,True,True,1439550702000.0,1440400565000.0,1440404165000.0,1445036238000.0,1445037138000.0,1439550702000.0,-1.0,1439562474000.0,1439566074000.0,1444233081000.0,1444236681000.0,1440400565000.0,1440404165000.0,"grussell",1439566074000.0,1439550702000.0,45,1439550702000.0,-1.0,"Known Hosts Configuration for SFTP Source","Spring Integration 4.2 changed the default SFTP session factory to *not* accept keys from unknown hosts by default. This is more secure.

You either have to provide a pre-populated {{known_hosts}} file or set {{allowUnknownKeys}} to true.

If you do both, the keys will be automatically added to the known hosts file.

When updating XD to 4.2.0.RC1 I simply set the boolean to true, to retain the previous behavior.

Add properties to the SFTP source to allow configuration of these properties at the stream level.
",1.0,False,1.0,0,0.0,0,0,False
"XD-3393","Story","Sprint 55","2015-08-14T07:03:31.000+0000","sabby","mark.fisher","Upgrade receptor-client to comply with latest Receptor APIs","As a s-c-d developer, I'd like to upgrade {{receptor-client}} to comply with latest {{Receptor}} API changes, so I can sync-up and take advantage of the recent improvements. ",3.0,0.188953488372093,0.188953488372093,0.0,False,False,False,1439535811000.0,1439535876000.0,1439536155000.0,1439535811000.0,1439536155000.0,1439535811000.0,-1.0,1439535811000.0,1439536155000.0,1439535876000.0,1439536155000.0,1439535876000.0,1439536155000.0,"sabby",1439536155000.0,1439535811000.0,45,1439535811000.0,-1.0,"Upgrade receptor-client to comply with latest Receptor APIs","As a s-c-d developer, I'd like to upgrade {{receptor-client}} to comply with latest {{Receptor}} API changes, so I can sync-up and take advantage of the recent improvements. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-3387","Story","Sprint 56","2015-08-11T14:20:45.000+0000","svennela","","Hide the passwords in custom modules from being displayed.","Hi,
Passwords are visibly when using custom modules.

Attached is example custom module code and xd-shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE.
 
Compile with Maven (mvn clean install) and run xd-shell script (xd-shell --cmdfile ./runme.cmd).
",2.0,0.913868191758438,0.429528920808403,0.0397000044030664,True,True,True,1439302845000.0,1443038793000.0,1443042393000.0,1443390005000.0,1443390905000.0,1439302845000.0,-1.0,1439465141000.0,1439468741000.0,1441058785000.0,1441062385000.0,1443038793000.0,1443042393000.0,"svennela",1439468741000.0,1439302845000.0,45,1439302845000.0,-1.0,"Hide the passwords in custom modules from being displayed.","Hi,
Passwords are visibly when using custom modules.

Attached is example custom module code and xd-shell script to reproduce the problem using stream module of type processor on Spring XD 1.2.1.RELEASE.
 
Compile with Maven (mvn clean install) and run xd-shell script (xd-shell --cmdfile ./runme.cmd).
",2.0,False,2.0,0,0.0,-1,0,False
"XD-3385","Bug","","2015-08-10T08:44:26.000+0000","thomas.risberg","","Can't build and run singlenode spring-cloud-data-rest app on Ubuntu","Building and then running spring-cloud-data-rest app on Ubuntu fails when trying to create the first stream. The configuration ends up with a CloudFoundryConfig instead of LocalConfig for the moduleDeployer.

Env:
Ubuntu 15.04
java version ""1.8.0_51""
Java(TM) SE Runtime Environment (build 1.8.0_51-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)

Error:
{code}
2015-08-10 11:43:47.199 ERROR 11062 --- [nio-9393-exec-1] o.s.c.d.r.c.RestControllerAdvice         : Caught exception while handling a request
java.lang.UnsupportedOperationException: null
	at org.springframework.cloud.data.module.deployer.cloudfoundry.CloudFoundryModuleDeployer.deploy(CloudFoundryModuleDeployer.java:30) ~[spring-cloud-data-module-deployer-cloudfoundry-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]
	at org.springframework.cloud.data.rest.controller.StreamController.deployStream(StreamController.java:213) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]
	at org.springframework.cloud.data.rest.controller.StreamController.save(StreamController.java:140) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_51]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_51]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_51]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_51]
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:111) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:648) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat-embed-websocket-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:235) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:85) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:69) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:518) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1091) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:668) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1521) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1478) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
2015-08-10 11:43:47.284  WARN 11062 --- [nio-9393-exec-1] .m.m.a.ExceptionHandlerExceptionResolver : Handler execution resulted in exception: null
{code}",3.0,-1.0,0.9992933218918784,-1.0,False,False,True,1439196266000.0,-1.0,-1.0,1439274610000.0,1439275510000.0,1439196266000.0,-1.0,-1.0,-1.0,1439275454000.0,1439275510000.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1439196266000.0,-1.0,"Can't build and run singlenode spring-cloud-data-rest app on Ubuntu","Building and then running spring-cloud-data-rest app on Ubuntu fails when trying to create the first stream. The configuration ends up with a CloudFoundryConfig instead of LocalConfig for the moduleDeployer.

Env:
Ubuntu 15.04
java version ""1.8.0_51""
Java(TM) SE Runtime Environment (build 1.8.0_51-b16)
Java HotSpot(TM) 64-Bit Server VM (build 25.51-b03, mixed mode)

Error:
{code}
2015-08-10 11:43:47.199 ERROR 11062 --- [nio-9393-exec-1] o.s.c.d.r.c.RestControllerAdvice         : Caught exception while handling a request
java.lang.UnsupportedOperationException: null
	at org.springframework.cloud.data.module.deployer.cloudfoundry.CloudFoundryModuleDeployer.deploy(CloudFoundryModuleDeployer.java:30) ~[spring-cloud-data-module-deployer-cloudfoundry-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]
	at org.springframework.cloud.data.rest.controller.StreamController.deployStream(StreamController.java:213) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]
	at org.springframework.cloud.data.rest.controller.StreamController.save(StreamController.java:140) ~[spring-cloud-data-rest-1.0.0.BUILD-SNAPSHOT.jar!/:1.0.0.BUILD-SNAPSHOT]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_51]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[na:1.8.0_51]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.8.0_51]
	at java.lang.reflect.Method.invoke(Method.java:497) ~[na:1.8.0_51]
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137) ~[spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:111) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandlerMethod(RequestMappingHandlerAdapter.java:806) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:729) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893) ~[spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:970) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:872) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:648) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:846) [spring-webmvc-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:729) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:291) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.tomcat.websocket.server.WsFilter.doFilter(WsFilter.java:52) [tomcat-embed-websocket-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:235) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:85) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.springframework.boot.actuate.autoconfigure.MetricsFilter.doFilterInternal(MetricsFilter.java:69) [spring-boot-actuator-1.3.0.BUILD-SNAPSHOT.jar!/:1.3.0.BUILD-SNAPSHOT]
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107) [spring-web-4.2.0.RELEASE.jar!/:4.2.0.RELEASE]
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:239) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:219) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:106) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:502) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:142) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:79) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:88) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:518) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1091) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:668) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1521) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1478) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_51]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_51]
	at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) [tomcat-embed-core-8.0.23.jar!/:8.0.23]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_51]
2015-08-10 11:43:47.284  WARN 11062 --- [nio-9393-exec-1] .m.m.a.ExceptionHandlerExceptionResolver : Handler execution resulted in exception: null
{code}",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-3384","Story","Sprint 55","2015-08-10T08:19:08.000+0000","sabby","jvalkeal","Spike: Investigate distributed deployment of s-c-s modules via YARN SPI","As an s-c-d developer, I'd like to investigate the distributed deployment of s-c-s modules on YARN, so I can experiment the implementation of YARN SPI and derive the strategy for {{YARNModuleDeployer}}.",8.0,8.014305535380655e-05,4.3410821649978544e-05,0.0,True,True,True,1439194748000.0,1439194796000.0,1439198396000.0,1439792777000.0,1439793677000.0,1439194748000.0,-1.0,1439194748000.0,1439198348000.0,1439194774000.0,1439198374000.0,1439194796000.0,1439198396000.0,"sabby",1439198348000.0,1439194748000.0,45,1439194748000.0,-1.0,"Spike: Investigate distributed deployment of s-c-s modules via YARN SPI","As an s-c-d developer, I'd like to investigate the distributed deployment of s-c-s modules on YARN, so I can experiment the implementation of YARN SPI and derive the strategy for {{YARNModuleDeployer}}.",8.0,False,8.0,0,0.0,-1,0,False
"XD-3381","Story","Sprint 55","2015-08-10T03:48:19.000+0000","eric.bottard","","Provide test infrastructure for module authors","As a module author, I want to be able to test my code in ""next to real world"" conditions (ie Integration Testing, but not really):
- I want all my module wiring to be testable
- I want all my module configuration (@ConfigurationProperties) to be in effect, and I want to be able to test various combination of props
- I want to be able to send data to my module and assert what is coming at the other end
- I want an idiomatic way of asserting the above (eg integration with Hamcrest, etc)
- I DONT want to have to send data to an actual bus (redis, rabbit, etc)",5.0,0.1560525545915404,0.2780206009108414,0.1466646872093581,True,True,True,1439178499000.0,1439285716000.0,1439289316000.0,1439864656000.0,1439865556000.0,1439178499000.0,-1.0,1439279266000.0,1439282866000.0,1439369515000.0,1439373115000.0,1439285716000.0,1439289316000.0,"eric.bottard",1439282866000.0,1439178499000.0,45,1439178499000.0,-1.0,"Provide test infrastructure for module authors","As a module author, I want to be able to test my code in ""next to real world"" conditions (ie Integration Testing, but not really):
- I want all my module wiring to be testable
- I want all my module configuration (@ConfigurationProperties) to be in effect, and I want to be able to test various combination of props
- I want to be able to send data to my module and assert what is coming at the other end
- I want an idiomatic way of asserting the above (eg integration with Hamcrest, etc)
- I DONT want to have to send data to an actual bus (redis, rabbit, etc)",5.0,False,5.0,0,0.0,-1,0,False
"XD-3380","Story","Sprint 55","2015-08-10T03:43:52.000+0000","eric.bottard","eric.bottard","Refactor Binder @Configuration to use AutoConfiguration","Currently, @EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don't trigger based on the presence of another jar (which itself has the meat of the configuration).
This is typically what boot AutoConfiguration is for.

Moreover, adding a new binding (eg Kafka or a stub for module testing) would require to crack open @EnableModule",5.0,0.967891490095406,0.0,0.967795700984712,True,True,True,1439178232000.0,1439279276000.0,1439282628000.0,1439281728000.0,1439282628000.0,1439178232000.0,-1.0,1439279266000.0,1439282628000.0,1439178232000.0,1439181832000.0,1439279276000.0,1439282628000.0,"eric.bottard",1439282628000.0,1439178232000.0,45,1439178232000.0,-1.0,"Refactor Binder @Configuration to use AutoConfiguration","Currently, @EnableModule hardcodes references to both the redis and rabbit configuration classes, which trigger or don't trigger based on the presence of another jar (which itself has the meat of the configuration).
This is typically what boot AutoConfiguration is for.

Moreover, adding a new binding (eg Kafka or a stub for module testing) would require to crack open @EnableModule",5.0,False,5.0,0,0.0,-1,0,False
"XD-3378","Story","Sprint 57","2015-08-09T18:43:05.000+0000","sabby","","Upgrade HDP/PHD distrubutions","As a Spring XD user, I'd like to use the latest releases of {{HDP}}/{{PHD}} distros, so I can leverage the latest features to create pipelines involving {{HDFS}}.",5.0,0.7978809075630974,0.7978809075630974,0.0,True,True,True,1439145785000.0,1442863168000.0,1442866768000.0,1443803955000.0,1443804855000.0,1439145785000.0,1442850346000.0,1439145785000.0,1439149385000.0,1442863168000.0,1442866768000.0,1442863168000.0,1442866768000.0,"sabby",1439149385000.0,1439145785000.0,45,1442850346000.0,1442850346000.0,"Upgrade HDP/PHD distrubutions","As a Spring XD user, I'd like to use the latest releases of {{HDP}}/{{PHD}} distros, so I can leverage the latest features to create pipelines involving {{HDFS}}.",8.0,True,8.0,1,-37.5,-1,0,False
"XD-3377","Story","Sprint 56","2015-08-07T20:10:47.000+0000","mminella","","Refactor Task parsing ","Currently the DSL parsing for tasks is a copy and paste of what it is for streams (minus the ability to parse multiple modules).  This results in a lot of duplication.  This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.",8.0,0.2983995520468628,0.2650024305149615,0.0532042801676132,True,True,True,1438978247000.0,1439220722000.0,1439224322000.0,1439789932000.0,1439790832000.0,1438978247000.0,1439196522000.0,1439021480000.0,1439025080000.0,1439193584000.0,1439197184000.0,1439220722000.0,1439224322000.0,"mminella",1439025080000.0,1438978247000.0,45,1439196522000.0,1439196522000.0,"Refactor Task parsing ","Currently the DSL parsing for tasks is a copy and paste of what it is for streams (minus the ability to parse multiple modules).  This results in a lot of duplication.  This should be refactored to remove duplication and remove explicit references to either streams or tasks in common code.",5.0,True,5.0,1,60.0,-1,0,False
"XD-3376","Story","Sprint 56","2015-08-07T16:52:25.000+0000","sabby","","Port Gemfire as s-c-s module","As a Spring XD developer, I'd like to move {{gemfire}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.",2.0,0.2079001580318746,0.2079001580318746,0.0096564723909945,True,True,True,1438966345000.0,1440153768000.0,1440157368000.0,1444676951000.0,1444677851000.0,1438966345000.0,1444063664000.0,1439021498000.0,1439025098000.0,1440153768000.0,1440157368000.0,1440153768000.0,1440157368000.0,"sabby",1439025098000.0,1438966345000.0,45,1444063687000.0,-1.0,"Port gemfire-server sink as s-c-s module","As a Spring XD developer, I'd like to move {{gemfire}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.",2.0,False,2.0,2,0.0,-1,0,True
"XD-3374","Story","Sprint 56","2015-08-07T16:51:23.000+0000","sabby","","Port Redis as s-c-s sink","As a Spring XD developer, I'd like to move {{redis}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.",2.0,0.9992579407130828,0.3690821903056289,0.0499668335696762,True,True,True,1438966283000.0,1440070496000.0,1440071316000.0,1440070416000.0,1440071316000.0,1438966283000.0,-1.0,1439021498000.0,1439025098000.0,1439374131000.0,1439377731000.0,1440070496000.0,1440071316000.0,"sabby",1439025098000.0,1438966283000.0,45,1438966283000.0,-1.0,"Port Redis as s-c-s sink","As a Spring XD developer, I'd like to move {{redis}} module from XD to s-c-s repo, so I can use it as {{sink}} to build streaming pipeline.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3373","Bug","Sprint 56","2015-08-07T16:13:19.000+0000","thomas.risberg","","First deploy/launch of Pig job that includes yarn-site.xml file fails","Deploying and launching a Pig job that contains a yarn-site.xml config file fails on the first deploy after XD starts up. This happens consistently.

The error is: 
  Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster

which indicates that the yarn-site.xml file never made it to the classpath.

Un-deploying and re-deploying the job seems to fix the problem.",5.0,0.8327154206086986,0.8327154206086986,0.0004962626667277,True,True,True,1438963999000.0,1440400344000.0,1440403944000.0,1440687992000.0,1440688892000.0,1438963999000.0,-1.0,1438964855000.0,1438968455000.0,1440400344000.0,1440403944000.0,1440400344000.0,1440403944000.0,"thomas.risberg",1438968455000.0,1438963999000.0,45,1438963999000.0,-1.0,"First deploy/launch of Pig job that includes yarn-site.xml file fails","Deploying and launching a Pig job that contains a yarn-site.xml config file fails on the first deploy after XD starts up. This happens consistently.

The error is: 
  Error: Could not find or load main class org.apache.hadoop.mapreduce.v2.app.MRAppMaster

which indicates that the yarn-site.xml file never made it to the classpath.

Un-deploying and re-deploying the job seems to fix the problem.",5.0,False,5.0,0,0.0,0,0,False
"XD-3370","Story","Sprint 56","2015-08-06T15:08:47.000+0000","sabby","","Port FTP/SFTP as s-c-s sink module","As a Spring XD developer, I'd like to port {{FTP/SFTP}} module from XD to s-c-s repo, so I can use it as {{sink}} modules to build streaming pipeline.",5.0,0.8157658660226267,0.8157658660226267,0.0,True,True,True,1438873727000.0,1444144294000.0,1444147894000.0,1445333709000.0,1445334609000.0,1438873727000.0,1444057599000.0,1438873727000.0,1438877327000.0,1444144294000.0,1444147894000.0,1444144294000.0,1444147894000.0,"sabby",1438877327000.0,1438873727000.0,45,1444057599000.0,1444057599000.0,"Port FTP as s-c-s sink module","As a Spring XD developer, I'd like to port {{FTP}} module from XD to s-c-s repo, so I can use it as {{sink}} modules to build streaming pipeline.",2.0,True,2.0,1,150.0,0,0,True
"XD-3369","Story","Sprint 56","2015-08-06T15:08:09.000+0000","sabby","","Port File as s-c-s module","As a Spring XD developer, I'd like to port {{file}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.",3.0,0.975871925315991,0.9754951164113592,0.0,True,True,True,1438873689000.0,1443711496000.0,1443715096000.0,1443830209000.0,1443831109000.0,1438873689000.0,1443711503000.0,1438873689000.0,1438877289000.0,1443709628000.0,1443713228000.0,1443711496000.0,1443715096000.0,"sabby",1438877289000.0,1438873689000.0,45,1443711503000.0,1443711503000.0,"Port File as s-c-s sink","As a Spring XD developer, I'd like to port {{file}} module from XD to s-c-s repo, so I can use it as {{sink}} module to build streaming pipeline.",2.0,True,2.0,1,50.0,-1,0,True
"XD-3367","Story","Sprint 56","2015-08-06T15:07:06.000+0000","sabby","","Port Transform as s-c-s module","As a Spring XD developer, I'd like to port {{transform}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.",2.0,0.8605741907158034,0.4201570339999681,0.0,True,True,True,1438873626000.0,1439898748000.0,1439902348000.0,1440063933000.0,1440064833000.0,1438873626000.0,-1.0,1438873626000.0,1438877226000.0,1439374120000.0,1439377720000.0,1439898748000.0,1439902348000.0,"sabby",1438877226000.0,1438873626000.0,45,1438873626000.0,-1.0,"Port Transform as s-c-s module","As a Spring XD developer, I'd like to port {{transform}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.",2.0,False,2.0,0,0.0,0,0,False
"XD-3366","Story","Sprint 56","2015-08-06T15:06:25.000+0000","sabby","","Port Filter as s-c-s module","As a Spring XD developer, I'd like to port {{filter}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.",2.0,0.8605925782905975,0.4201748605438893,0.0,True,True,True,1438873585000.0,1439898753000.0,1439902353000.0,1440063920000.0,1440064820000.0,1438873585000.0,-1.0,1438873585000.0,1438877185000.0,1439374112000.0,1439377712000.0,1439898753000.0,1439902353000.0,"sabby",1438877185000.0,1438873585000.0,45,1438873585000.0,-1.0,"Port Filter as s-c-s module","As a Spring XD developer, I'd like to port {{filter}} module from XD to s-c-s repo, so I can use it as {{processor}} module to build streaming pipeline.",2.0,False,2.0,0,0.0,0,0,False
"XD-3364","Story","Sprint 56","2015-08-06T15:04:29.000+0000","sabby","","Port Twitterstream as s-c-s module","As a Spring XD developer, I'd like to move {{twitterstream}} module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline.
",3.0,0.3092463428745848,0.30924308877002,0.0,True,True,True,1438873469000.0,1439823796000.0,1439827396000.0,1441945611000.0,1441946511000.0,1438873469000.0,1439970506000.0,1438873469000.0,1438877069000.0,1439823786000.0,1439827386000.0,1439823796000.0,1439827396000.0,"sabby",1438877069000.0,1438873469000.0,45,1439970506000.0,1439970506000.0,"Port Twitterstream as s-c-s module","As a Spring XD developer, I'd like to move {{twitterstream}} module from XD to s-c-s repo, so I can use it as source modules to build streaming pipeline.
",2.0,True,2.0,1,50.0,0,0,False
"XD-3362","Story","Sprint 56","2015-08-06T14:59:09.000+0000","sabby","","Port HTTP as s-c-s module","As a Spring XD developer, I'd like to port {{http}} module from XD to s-c-s repo, so I can use it as {{source}} module in streaming pipeline.
",2.0,0.7573695104207225,0.5505969629280488,0.0,True,True,True,1438873149000.0,1439797083000.0,1439800683000.0,1440092174000.0,1440093074000.0,1438873149000.0,-1.0,1438873149000.0,1438876749000.0,1439544836000.0,1439548436000.0,1439797083000.0,1439800683000.0,"sabby",1438876749000.0,1438873149000.0,45,1438873149000.0,-1.0,"Port HTTP as s-c-s module","As a Spring XD developer, I'd like to port {{http}} module from XD to s-c-s repo, so I can use it as {{source}} module in streaming pipeline.
",2.0,False,2.0,0,0.0,-1,1,False
"XD-3359","Story","Sprint 56","2015-08-06T12:54:07.000+0000","grenfro","grenfro","Standardize Spring Cloud Data configuration","User can configure spring cloud data via  via Spring Cloud Config, data-admin.yml or Spring Cloud Connector
* Add bootstrap.yml to spring cloud data
* create a default data-admin.yml and configure spring data to look for this vs application.yml.
* Spring Cloud Data will have Spring Cloud Config enabled by default
** User has the ability to disable it via the bootstrap.yml",5.0,0.4546655931750834,0.1424355700289449,0.4529382327358145,True,True,True,1438865647000.0,1439975884000.0,1439979484000.0,1441306623000.0,1441307523000.0,1438865647000.0,-1.0,1439971666000.0,1439975266000.0,1439213457000.0,1439217057000.0,1439975884000.0,1439979484000.0,"grenfro",1439975266000.0,1438865647000.0,45,1438865647000.0,-1.0,"Standardize Spring Cloud Data configuration","User can configure spring cloud data via  via Spring Cloud Config, data-admin.yml or Spring Cloud Connector
* Add bootstrap.yml to spring cloud data
* create a default data-admin.yml and configure spring data to look for this vs application.yml.
* Spring Cloud Data will have Spring Cloud Config enabled by default
** User has the ability to disable it via the bootstrap.yml",5.0,False,5.0,0,0.0,-1,0,False
"XD-3358","Bug","Sprint 57","2015-08-06T11:27:36.000+0000","tcostasouza","","Admin UI deploys job with wrong module count","When deploying a job through admin UI with a count of 0 the module is actually deployed with count 1.

More info here: [http://stackoverflow.com/questions/31858631/how-to-define-named-channel-consumer-module-deployment-properties]",2.0,0.9592137837847802,0.9592137837847802,7.097062000670995e-05,True,True,True,1438860456000.0,1443023275000.0,1443026875000.0,1443199380000.0,1443200280000.0,1438860456000.0,1442850528000.0,1438860764000.0,1438864364000.0,1443023275000.0,1443026875000.0,1443023275000.0,1443026875000.0,"tcostasouza",1438864364000.0,1438860456000.0,45,1442850528000.0,1442850528000.0,"Admin UI deploys job with wrong module count","When deploying a job through admin UI with a count of 0 the module is actually deployed with count 1.

More info here: [http://stackoverflow.com/questions/31858631/how-to-define-named-channel-consumer-module-deployment-properties]",1.0,True,1.0,1,100.0,-1,0,False
"XD-3354","Story","Sprint 55","2015-08-05T12:15:13.000+0000","iperumal","","Move shell integration tests to spring-cloud-data shell ","This could focus only on the subset (Stream operations)",3.0,0.2302547148162942,0.2302547148162942,0.0410048075969664,True,True,True,1438776913000.0,1438939465000.0,1438943065000.0,1439481979000.0,1439482879000.0,1438776913000.0,-1.0,1438805861000.0,1438809461000.0,1438939465000.0,1438943065000.0,1438939465000.0,1438943065000.0,"iperumal",1438809461000.0,1438776913000.0,45,1438776913000.0,-1.0,"Move shell integration tests to spring-cloud-data shell ","This could focus only on the subset (Stream operations)",3.0,False,3.0,0,0.0,-1,0,False
"XD-3353","Story","Sprint 55","2015-08-05T12:14:17.000+0000","iperumal","iperumal","Add shell as a rest client to the spring-cloud-data REST API","As a user I would like to have shell interface to the spring-cloud-data rest API. The scope for this JIRA could be limited to stream commands.",5.0,0.1890759920293242,0.0,0.1783807719747103,True,True,True,1438776857000.0,1438807600000.0,1438811200000.0,1438938553000.0,1438939453000.0,1438776857000.0,-1.0,1438805861000.0,1438809461000.0,1438776857000.0,1438780457000.0,1438807600000.0,1438811200000.0,"iperumal",1438809461000.0,1438776857000.0,45,1438776857000.0,-1.0,"Add shell as a rest client to the spring-cloud-data REST API","As a user I would like to have shell interface to the spring-cloud-data rest API. The scope for this JIRA could be limited to stream commands.",5.0,False,5.0,0,0.0,0,1,False
"XD-3352","Story","Sprint 55","2015-08-05T10:54:27.000+0000","dturanski","dturanski","[SCS] - Replace Binder XML config with @Configuration","Create Confguration and ConfigurationProperties. Configuration must support replacing the default Kryo Codec implementation with something else.",3.0,0.9998406285994392,0.9998261402902974,0.9996957455080204,True,True,True,1438772067000.0,1439186128000.0,1439186194000.0,1439185294000.0,1439186194000.0,1438772067000.0,-1.0,1439186068000.0,1439186194000.0,1439186122000.0,1439186194000.0,1439186128000.0,1439186194000.0,"dturanski",1439186194000.0,1438772067000.0,45,1438772067000.0,-1.0,"[SCS] - Replace Binder XML config with @Configuration","Create Confguration and ConfigurationProperties. Configuration must support replacing the default Kryo Codec implementation with something else.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3351","Story","Sprint 55","2015-08-05T10:52:02.000+0000","dturanski","dturanski","[SCS]- Replace codec impl in spring-cloud-stream-codec with SI-Codec","",2.0,0.999804473454254,0.999789990006421,0.9996837780556456,True,True,True,1438771922000.0,1439186107000.0,1439186188000.0,1439185288000.0,1439186188000.0,1438771922000.0,-1.0,1439186057000.0,1439186188000.0,1439186101000.0,1439186188000.0,1439186107000.0,1439186188000.0,"dturanski",1439186188000.0,1438771922000.0,45,1438771922000.0,-1.0,"[SCS]- Replace codec impl in spring-cloud-stream-codec with SI-Codec","",2.0,False,2.0,0,0.0,-1,0,False
"XD-3350","Story","Sprint 55","2015-08-05T10:39:34.000+0000","sabby","","Add support to expose counter metrics for dahsboarding","As a s-c-d developer, I'd like to add support to expose counter (metrics) endpoints, so I can consume to feed the dashboards to demonstrate {{firehose | counter}} pipe.",3.0,0.6905966409000551,0.6905966409000551,0.0,True,True,True,1438771174000.0,1439285683000.0,1439289283000.0,1439515295000.0,1439516195000.0,1438771174000.0,-1.0,1438771174000.0,1438774774000.0,1439285683000.0,1439289283000.0,1439285683000.0,1439289283000.0,"sabby",1438774774000.0,1438771174000.0,45,1438771174000.0,-1.0,"Add support to expose counter metrics for dashboarding","As a s-c-d developer, I'd like to add support to expose counter (metrics) endpoints, so I can consume to feed the dashboards to demonstrate {{firehose | counter}} pipe.",3.0,False,3.0,0,0.0,0,0,True
"XD-3349","Story","Sprint 55","2015-08-05T10:28:20.000+0000","sabby","","Design the foundation to port XD modules to s-c-s","As an s-c-s developer, I'd like to brainstorm and design the foundation to port XD modules as s-c-s modules, so I can use it as the base and start migrating the modules.",5.0,0.3541965675515528,0.0849602046404788,0.0,True,True,True,1438770500000.0,1439196698000.0,1439200298000.0,1439972881000.0,1439973781000.0,1438770500000.0,-1.0,1438770500000.0,1438774100000.0,1438872731000.0,1438876331000.0,1439196698000.0,1439200298000.0,"sabby",1438774100000.0,1438770500000.0,45,1438770500000.0,-1.0,"Design the foundation to port XD modules to s-c-s","As an s-c-s developer, I'd like to brainstorm and design the foundation to port XD modules as s-c-s modules, so I can use it as the base and start migrating the modules.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3347","Story","Sprint 55","2015-08-05T09:18:22.000+0000","hillert","hillert","Run all shell integration tests also with enabled security","Apply the same strategy for the Module Command Tests also to all other Shell integration tests.",5.0,0.1646816608828124,0.0001362815576982,0.0025397041716758,True,True,True,1438766302000.0,1438935477000.0,1438939077000.0,1439792687000.0,1439793587000.0,1438766302000.0,-1.0,1438768911000.0,1438772511000.0,1438766442000.0,1438770042000.0,1438935477000.0,1438939077000.0,"hillert",1438772511000.0,1438766302000.0,45,1438766302000.0,-1.0,"Run all shell integration tests also with enabled security","Apply the same strategy for the Module Command Tests also to all other Shell integration tests.",5.0,False,5.0,0,0.0,-1,1,False
"XD-3346","Bug","Sprint 55","2015-08-05T07:24:58.000+0000","sabby","","Accessing step progress via REST fails with 403","As a XD user, I'm trying to access URI (- GET /jobs/executions//steps//progress => hasRole('ROLE_VIEW')), but it fails with 403 forbidden error for the role with view access. More details [here|https://issuetracker.springsource.com/browse/VESC-475].

Another URL with the same error: http://<HOST>:9393/streams/definitions.json?page=0&size=10",1.0,0.5968483605962874,0.4176749043822197,0.0,True,True,True,1438759498000.0,1439376678000.0,1439380278000.0,1439792663000.0,1439793563000.0,1438759498000.0,-1.0,1438759498000.0,1438763098000.0,1439191401000.0,1439195001000.0,1439376678000.0,1439380278000.0,"sabby",1438763098000.0,1438759498000.0,45,1438759498000.0,-1.0,"Accessing step progress via REST fails with 403","As a XD user, I'm trying to access URI (- GET /jobs/executions//steps//progress => hasRole('ROLE_VIEW')), but it fails with 403 forbidden error for the role with view access. More details [here|https://issuetracker.springsource.com/browse/VESC-475].

Another URL with the same error: http://<HOST>:9393/streams/definitions.json?page=0&size=10",1.0,False,1.0,0,0.0,0,1,False
"XD-3344","Story","Sprint 55","2015-08-04T17:27:51.000+0000","sabby","","Implement undeploy operation for singlenode SPI ","As a s-c-d developer, I'd like to implement _undeploy_ operation for {{singlenode}} (single JVM), so I can use this target to undeploy a running stream. More details in [this PR|https://github.com/spring-cloud/spring-cloud-data/pull/19].",8.0,0.970152553416163,0.0860652901660338,0.0,True,True,True,1438709271000.0,1439788784000.0,1439792384000.0,1439821096000.0,1439821996000.0,1438709271000.0,1438766504000.0,1438709271000.0,1438712871000.0,1438805038000.0,1438808638000.0,1439788784000.0,1439792384000.0,"sabby",1438712871000.0,1438709271000.0,45,1438766504000.0,1438766504000.0,"Implement undeploy operation for singlenode SPI ","As a s-c-d developer, I'd like to implement _undeploy_ operation for {{singlenode}} (single JVM), so I can use this target to undeploy a running stream. More details in [this PR|https://github.com/spring-cloud/spring-cloud-data/pull/19].

*Note:* Its a prerequisite to determine consistent _undeploy_ strategy for both {{jobs}} and {{streams}}. ",2.0,True,2.0,1,300.0,-1,1,True
"XD-3341","Story","Sprint 55","2015-08-04T14:15:31.000+0000","sabby","","Publish s-c-d image to DockerHub","As a s-c-d developer, I'd like to publish the s-c-d image to DockerHub, so I can incrementally push the latest commits to the remote location.",1.0,0.9754531383910868,0.9753635979891784,0.0,True,True,True,1438697731000.0,1438773989000.0,1438775908000.0,1438775008000.0,1438775908000.0,1438697731000.0,-1.0,1438697731000.0,1438701331000.0,1438773982000.0,1438775908000.0,1438773989000.0,1438775908000.0,"sabby",1438701331000.0,1438697731000.0,45,1438697731000.0,-1.0,"Publish s-c-d image to DockerHub","As a s-c-d developer, I'd like to publish the s-c-d image to DockerHub, so I can incrementally push the latest commits to the remote location.",1.0,False,1.0,0,0.0,0,0,False
"XD-3338","Bug","Sprint 56","2015-08-04T13:58:43.000+0000","sabby","","Include multiple libraries via BOM template","As a s-c-d developer, I'd like to add as many jars via a bom (e.g. hadoop distro deps), so I don't have to explicitly worry about individual but related libraries. ",2.0,0.5348075437575517,0.5348075437575517,0.0114914951029643,True,True,True,1438696723000.0,1441975658000.0,1441979258000.0,1444826879000.0,1444827779000.0,1438696723000.0,-1.0,1438767178000.0,1438770778000.0,1441975658000.0,1441979258000.0,1441975658000.0,1441979258000.0,"sabby",1438770778000.0,1438696723000.0,45,1438696723000.0,-1.0,"Do not include optional dependencies automatically via 'includes'","As a s-c-d developer, I'd like the 'includes' feature of the module launcher not to include optional dependencies, so that I can have better control over what gets added to the class path.",2.0,False,2.0,0,0.0,-1,0,True
"XD-3335","Bug","Sprint 54","2015-08-04T12:55:47.000+0000","mbogoevici","mbogoevici","Kafka Source must set autoStartup=false on KafkaMessageDrivenChannelAdapter","If the value is not set, the source may start before being bound to the bus, throwing a ""Dispatcher has no subscribers"" error",3.0,0.0012665985699693,0.0012665985699693,0.0015584415584415,True,True,True,1438692947000.0,1438693164000.0,1438696764000.0,1438863372000.0,1438864272000.0,1438692947000.0,-1.0,1438693214000.0,1438696814000.0,1438693164000.0,1438696764000.0,1438693164000.0,1438696764000.0,"mbogoevici",1438696814000.0,1438692947000.0,45,1438692947000.0,-1.0,"Kafka Source must set autoStartup=false on KafkaMessageDrivenChannelAdapter","If the value is not set, the source may start before being bound to the bus, throwing a ""Dispatcher has no subscribers"" error",3.0,False,3.0,0,0.0,-1,0,False
"XD-3334","Story","Sprint 56","2015-08-04T09:09:40.000+0000","pharris","","Refactor to use RestOperations","The current implementation makes use of cf-java-client, which is relatively heavy for our needs. It should be removed in favour of a bespoke RestOperations wrapper. See https://github.com/Zteve/test-cc-oauth for sample code.",5.0,0.0924511844169815,0.0924511844169815,0.0068493544708127,True,True,True,1438679380000.0,1438936513000.0,1438940113000.0,1441459764000.0,1441460664000.0,1438679380000.0,1438827268000.0,1438698430000.0,1438702030000.0,1438936513000.0,1438940113000.0,1438936513000.0,1438940113000.0,"pharris",1438702030000.0,1438679380000.0,45,1438827268000.0,1438827268000.0,"Refactor to use RestOperations","The current implementation makes use of cf-java-client, which is relatively heavy for our needs. It should be removed in favour of a bespoke RestOperations wrapper. See https://github.com/Zteve/test-cc-oauth for sample code.",3.0,True,3.0,1,66.66666666666666,0,0,False
"XD-3332","Story","Sprint 56","2015-08-04T08:57:26.000+0000","pharris","","Obtain username and password credentials for CloudFoundry","As part of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry. These will need to be supplied from the new XD Admin app at runtime.",2.0,-1.0,-1.0,0.0065145893345661,False,True,False,1438678646000.0,-1.0,-1.0,1441714622000.0,1441715522000.0,1438678646000.0,-1.0,1438698430000.0,1438702030000.0,-1.0,-1.0,-1.0,-1.0,"pharris",1438702030000.0,1438678646000.0,45,1438678646000.0,-1.0,"Obtain username and password credentials for CloudFoundry","As part of moving to a bespoke RestOperations application we will need credentials to access CloudFoundry. These will need to be supplied from the new XD Admin app at runtime.",2.0,False,2.0,0,0.0,0,2,False
"XD-3331","Story","Sprint 56","2015-08-04T08:56:28.000+0000","pharris","","Add real ModuleRunner application","The current ModuleRunner is test app used for validation. This should be replaced by a real app.",2.0,0.9090952877322884,0.9090952877322884,0.0071320225728766,True,True,True,1438678588000.0,1441207782000.0,1441211382000.0,1441459788000.0,1441460688000.0,1438678588000.0,-1.0,1438698430000.0,1438702030000.0,1441207782000.0,1441211382000.0,1441207782000.0,1441211382000.0,"pharris",1438702030000.0,1438678588000.0,45,1438678588000.0,-1.0,"Add real ModuleRunner application","The current ModuleRunner is test app used for validation. This should be replaced by a real app.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3330","Story","Sprint 55","2015-08-04T08:54:49.000+0000","pharris","","Implement undeploy operation for CC SPI","Currently undeploy is a no-op.",2.0,0.909101169109515,0.909101169109515,0.0022065248294247,True,True,True,1438678489000.0,1441207793000.0,1441211393000.0,1441459792000.0,1441460692000.0,1438678489000.0,-1.0,1438684628000.0,1438688228000.0,1441207793000.0,1441211393000.0,1441207793000.0,1441211393000.0,"pharris",1438688228000.0,1438678489000.0,45,1438678489000.0,-1.0,"Implement undeploy operation for CC SPI","Currently undeploy is a no-op.",2.0,False,2.0,0,0.0,-1,1,False
"XD-3329","Story","Sprint 55","2015-08-04T08:53:24.000+0000","pharris","","Return full ModuleInstanceStatus information","Currently there is no ModuleInstanceStatus returned. This issue will fill in the details.",1.0,0.9025984925242616,0.9025984925242616,0.002234149346558,True,True,True,1438678404000.0,1441189674000.0,1441193274000.0,1441459771000.0,1441460671000.0,1438678404000.0,1439971308000.0,1438684620000.0,1438688220000.0,1441189674000.0,1441193274000.0,1441189674000.0,1441193274000.0,"pharris",1438688220000.0,1438678404000.0,45,1439971308000.0,1439971308000.0,"Return full ModuleInstanceStatus information","Currently there is no ModuleInstanceStatus returned. This issue will fill in the details.",2.0,True,2.0,1,-50.0,0,0,False
"XD-3328","Story","Sprint 55","2015-08-04T08:52:41.000+0000","pharris","","Return full ModuleStatus information","Remove all stubs and check all required information is returned accurately.",1.0,0.902586120761352,0.902586120761352,0.0022470504766895,True,True,True,1438678361000.0,1441189639000.0,1441193239000.0,1441459775000.0,1441460675000.0,1438678361000.0,1439971305000.0,1438684613000.0,1438688213000.0,1441189639000.0,1441193239000.0,1441189639000.0,1441193239000.0,"pharris",1438688213000.0,1438678361000.0,45,1439971305000.0,1439971305000.0,"Return full ModuleStatus information","Remove all stubs and check all required information is returned accurately.",2.0,True,2.0,1,-50.0,0,0,False
"XD-3326","Story","Sprint 55","2015-08-04T08:49:07.000+0000","pharris","","Add parameter information to application definition","A ModuleDefinition contains parameters, which need to be passed to the CloudFoundry application. Currently these are put directly into the application's environment. This issue will verify they are correctly named.",1.0,0.9091058072288116,0.9091058072288116,0.0022871255388976,True,True,True,1438678147000.0,1441207763000.0,1441211363000.0,1441459779000.0,1441460679000.0,1438678147000.0,-1.0,1438684511000.0,1438688111000.0,1441207763000.0,1441211363000.0,1441207763000.0,1441211363000.0,"pharris",1438688111000.0,1438678147000.0,45,1438678147000.0,-1.0,"Add parameter information to application definition","A ModuleDefinition contains parameters, which need to be passed to the CloudFoundry application. Currently these are put directly into the application's environment. This issue will verify they are correctly named.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3325","Story","Sprint 55","2015-08-04T08:43:58.000+0000","pharris","","Add binding information to application definition","ModuleDefinition contains bindings that need to be passed to the ModuleRunner app. It appears these can be included in the application's environment.",2.0,0.9091174679150296,0.9091174679150296,0.0023935936065429,True,True,True,1438677838000.0,1441207771000.0,1441211371000.0,1441459783000.0,1441460683000.0,1438677838000.0,-1.0,1438684499000.0,1438688099000.0,1441207771000.0,1441211371000.0,1441207771000.0,1441211371000.0,"pharris",1438688099000.0,1438677838000.0,45,1438677838000.0,-1.0,"Add binding information to application definition","ModuleDefinition contains bindings that need to be passed to the ModuleRunner app. It appears these can be included in the application's environment.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3322","Story","Sprint 54","2015-08-03T15:05:25.000+0000","sabby","mark.pollack","Create CI infrastructure for s-c-s-m repo","As a s-c-s developer, I'd like to setup CI infrastructure for {{spring-cloud-stream-modules}} (s-c-s-m) repo, so I can build the project continuously on every commits.
",3.0,0.8829787234042553,0.7836879432624113,0.0,False,False,False,1438614325000.0,1438614574000.0,1438614607000.0,1438614325000.0,1438614607000.0,1438614325000.0,-1.0,1438614325000.0,1438614607000.0,1438614546000.0,1438614607000.0,1438614574000.0,1438614607000.0,"sabby",1438614607000.0,1438614325000.0,45,1438614325000.0,-1.0,"Create CI infrastructure for s-c-s-m repo","As a s-c-s developer, I'd like to setup CI infrastructure for {{spring-cloud-stream-modules}} (s-c-s-m) repo, so I can build the project continuously on every commits.
",3.0,False,3.0,0,0.0,0,0,False
"XD-3321","Bug","Sprint 55","2015-08-03T14:57:35.000+0000","sabby","","xd.customModule.home property is broke","As a Spring XD user, I have a development {{singlenode}} installed on windows. Post 1.2 upgrade, the custom modules no longer show up by just placing them in the {{xd.customModule.home}} directory. Instead I have to use the 'module upload' command to install the modules. More details in [SO thread|http://stackoverflow.com/questions/31792220/spring-xd-1-2-0-custom-module-deployment].",3.0,0.3471541266546816,0.3471541266546816,0.0,True,True,True,1438613855000.0,1438933828000.0,1438937428000.0,1439534658000.0,1439535558000.0,1438613855000.0,-1.0,1438613855000.0,1438617455000.0,1438933828000.0,1438937428000.0,1438933828000.0,1438937428000.0,"sabby",1438617455000.0,1438613855000.0,45,1438613855000.0,-1.0,"Make requirement for MD5 hash files configurable for the custom module registry ","Post 1.2 upgrade, the custom modules no longer show up by just copying the jars to the {{xd.customModule.home}} directory. Instead I have to use the 'module upload' command to install the modules. This is because an MD5 file is required. More details in [SO thread|http://stackoverflow.com/questions/31792220/spring-xd-1-2-0-custom-module-deployment]. ",3.0,False,3.0,0,0.0,0,1,True
"XD-3320","Story","Sprint 54","2015-08-03T10:45:45.000+0000","sabby","pperalta","Migrate StreamController from XD 1.0","As a s-c-d user, I'd like to add REST support for stream commands, so I can maneuver streaming pipeline backed by StreamController.",5.0,0.0006229866893575,0.0006229866893575,0.0,True,True,True,1438598745000.0,1438598868000.0,1438602468000.0,1438795281000.0,1438796181000.0,1438598745000.0,-1.0,1438598745000.0,1438602345000.0,1438598868000.0,1438602468000.0,1438598868000.0,1438602468000.0,"sabby",1438602345000.0,1438598745000.0,45,1438598745000.0,-1.0,"Migrate StreamController from XD 1.0","As a s-c-d user, I'd like to add REST support for stream commands, so I can maneuver streaming pipeline backed by StreamController.",5.0,False,5.0,0,0.0,0,1,False
"XD-3319","Story","Sprint 55","2015-08-03T10:30:39.000+0000","sabby","","Spike: Investigate right approach to move HDFS module to s-c-s","As a s-c-s developer, I'd like to investigate the right approach to port {{HDFS}} module from XD, so I can decide better handling of HDFS dependencies, which needs loaded and available in root CP at the runtime. ",5.0,0.2400277495201386,0.2400277495201386,0.0,True,True,True,1438597839000.0,1439369401000.0,1439373001000.0,1441811409000.0,1441812309000.0,1438597839000.0,-1.0,1438597839000.0,1438601439000.0,1439369401000.0,1439373001000.0,1439369401000.0,1439373001000.0,"sabby",1438601439000.0,1438597839000.0,45,1438597839000.0,-1.0,"Add PHD HDFS as s-c-s module","As a s-c-s developer, I'd like to investigate the right approach to port {{PHD}} as the provider to support {{HDFS}} module from XD, so I can decide better handling of HDFS dependencies, which needs loaded and available in root CP at the runtime. ",5.0,False,5.0,0,0.0,0,0,True
"XD-3318","Story","Sprint 54","2015-08-03T09:37:20.000+0000","sabby","mbogoevici","Bootify ModuleLauncher","As a s-c-s developer, I'd like to _bootify_ {{ModuleLauncher}}, so I can use Spring Boot's support for property, setting, as well as adding options and new functionality in the future, such as CP augmentation.
",5.0,0.0008650632413657,0.0008650632413657,0.0,True,True,True,1438594640000.0,1438594706000.0,1438598306000.0,1438670035000.0,1438670935000.0,1438594640000.0,-1.0,1438594640000.0,1438598240000.0,1438594706000.0,1438598306000.0,1438594706000.0,1438598306000.0,"sabby",1438598240000.0,1438594640000.0,45,1438594640000.0,-1.0,"Bootify ModuleLauncher","As a s-c-s developer, I'd like to _bootify_ {{ModuleLauncher}}, so I can use Spring Boot's support for property, setting, as well as adding options and new functionality in the future, such as CP augmentation.
",5.0,False,5.0,0,0.0,-1,2,False
"XD-3317","Story","Sprint 55","2015-08-03T09:30:04.000+0000","sabby","","Spike: Investigate right approach to move JDBC module to s-c-s","As a s-c-s developer, I'd like to investigate the right approach to port {{JDBC}} module from XD, so I can decide better handling of JDBC drivers, which needs loaded and available in root CP at the runtime. ",8.0,0.3374127940140511,0.3373997486073389,0.0,True,True,True,1438594204000.0,1439913293000.0,1439916893000.0,1442502726000.0,1442503626000.0,1438594204000.0,1438767059000.0,1438594204000.0,1438597804000.0,1439913242000.0,1439916842000.0,1439913293000.0,1439916893000.0,"sabby",1438597804000.0,1438594204000.0,45,1438767059000.0,1438767059000.0,"Add support to resolve and add JARs to Boot loader","As a s-c-d developer, I'd like to resolve and then add module dependent JAR's to Boot loader, so I have an approach to handle external libraries (ex: database drivers) required by OOTB modules.
",5.0,True,5.0,1,60.0,-1,0,True
"XD-3316","Story","Sprint 54","2015-08-03T09:24:19.000+0000","sabby","mark.pollack","Create CI infrastructure for s-c-d repo","As a s-c-d developer, I'd like to setup CI infrastructure for [s-c-d repo|https://github.com/spring-cloud/spring-cloud-data], so I can build the project continuously on every commits. ",5.0,6.513748662355185e-05,6.513748662355185e-05,0.0,True,True,True,1438593859000.0,1438593866000.0,1438597466000.0,1438700424000.0,1438701324000.0,1438593859000.0,-1.0,1438593859000.0,1438597459000.0,1438593866000.0,1438597466000.0,1438593866000.0,1438597466000.0,"sabby",1438597459000.0,1438593859000.0,45,1438593859000.0,-1.0,"Create CI infrastructure for s-c-d repo","As a s-c-d developer, I'd like to setup CI infrastructure for [s-c-d repo|https://github.com/spring-cloud/spring-cloud-data], so I can build the project continuously on every commits. ",5.0,False,5.0,0,0.0,-1,0,False
"XD-3315","Story","Sprint 55","2015-08-03T08:29:24.000+0000","sabby","","Port redis counter from XD as a s-c-s sink","As a s-c-s developer, I'd like to adapt redis {{counter}} sink from XD to s-c-s, so I can build streaming pipes using s-c-s modules with simple counters to feed dashboards. ",3.0,0.8025846733486744,0.9064798688778376,0.0,True,True,True,1438590564000.0,1439279042000.0,1439282642000.0,1439447490000.0,1439448390000.0,1438590564000.0,1438596765000.0,1438590564000.0,1438594164000.0,1439368166000.0,1439371766000.0,1439279042000.0,1439282642000.0,"sabby",1438594164000.0,1438590564000.0,45,1438596765000.0,1438596765000.0,"Port Redis counter as s-c-s sink","As a s-c-s developer, I'd like to adapt redis {{counter}} from XD to s-c-s, so I can build streaming pipes using s-c-s modules with simple counters to feed dashboards. ",5.0,True,5.0,1,-40.0,0,0,True
"XD-3314","Story","Sprint 54","2015-08-03T08:14:28.000+0000","sabby","","Validate stream commands from shell","As a s-c-d developer, I'd like to invoke REST APIs via shell, so I can validate {{StreamController}} operations.",8.0,0.846478397839784,0.96749228494278,0.0,True,True,True,1438589668000.0,1438694997000.0,1438698597000.0,1438713200000.0,1438714100000.0,1438589668000.0,1438709676000.0,1438589668000.0,1438593268000.0,1438710055000.0,1438713655000.0,1438694997000.0,1438698597000.0,"sabby",1438593268000.0,1438589668000.0,45,1438710055000.0,1438709676000.0,"Validate stream commands from shell","As a s-c-d developer, I'd like to invoke REST APIs via shell, so I can validate {{StreamController}} operations.",3.0,True,3.0,2,166.66666666666669,-1,0,False
"XD-3313","Story","Sprint 54","2015-08-03T08:12:26.000+0000","sabby","Mark Fisher","Add in-memory stream definition repository","As a spring-cloud-data developer, I'd like to use an in-memory stream definition repository, so I don't have to spin up a store; obviously, this will not persist between application executions, but it will be useful for a simplified development experience.
",5.0,0.0035363628464679,0.1090450951429316,0.0,True,True,True,1438589546000.0,1438589953000.0,1438593553000.0,1438703736000.0,1438704636000.0,1438589546000.0,1438589717000.0,1438589546000.0,1438593146000.0,1438602096000.0,1438605696000.0,1438589953000.0,1438593553000.0,"sabby",1438593146000.0,1438589717000.0,45,1438589717000.0,-1.0,"Add in-memory stream definition repository","As a spring-cloud-data developer, I'd like to use an in-memory stream definition repository, so I don't have to spin up a store; obviously, this will not persist between application executions, but it will be useful for a simplified development experience.
",5.0,False,5.0,0,0.0,0,4,False
"XD-3312","Story","Sprint 54","2015-08-03T08:08:58.000+0000","sabby","mark.fisher","Move spring-cloud-stream-modules to spring-cloud repo","As a s-c-s developer, I'd like to move {{spring-cloud-stream-modules}} from s-c-s to s-c repo, so I can cleanup s-c-s project and at the same time make these modules visible outside of s-c-s.",3.0,0.2920989624900239,0.2920989624900239,0.0,True,True,True,1438589338000.0,1438589704000.0,1438590591000.0,1438589691000.0,1438590591000.0,1438589338000.0,-1.0,1438589338000.0,1438590591000.0,1438589704000.0,1438590591000.0,1438589704000.0,1438590591000.0,"sabby",1438590591000.0,1438589338000.0,45,1438589338000.0,-1.0,"Move spring-cloud-stream-modules to spring-cloud repo","As a s-c-s developer, I'd like to move {{spring-cloud-stream-modules}} from s-c-s to s-c repo, so I can cleanup s-c-s project and at the same time make these modules visible outside of s-c-s.",3.0,False,3.0,0,0.0,0,0,False
"XD-3311","Story","Sprint 54","2015-08-03T08:06:54.000+0000","sabby","mark.fisher","Create ModuleRegistry stubs","As a s-c-d developer, I'd like to create {[ModuleRegistry}} stubs, so I can create mock streams by interacting with the registry APIs.",3.0,0.0408180441003198,0.0408180441003198,0.0,True,True,True,1438589214000.0,1438589699000.0,1438593299000.0,1438600196000.0,1438601096000.0,1438589214000.0,-1.0,1438589214000.0,1438592814000.0,1438589699000.0,1438593299000.0,1438589699000.0,1438593299000.0,"sabby",1438592814000.0,1438589214000.0,45,1438589214000.0,-1.0,"Create ModuleRegistry stubs","As a s-c-d developer, I'd like to create {[ModuleRegistry}} stubs, so I can create mock streams by interacting with the registry APIs.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3310","Story","Sprint 54","2015-08-03T08:04:56.000+0000","sabby","mark.fisher","Add REST support for spring-cloud-data","As a s-c-d user, I'd like to use the REST-APIs to interact with the {{xd-admin}} and likewise perform CRUD operations to maneuver streaming and batch pipelines. ",5.0,0.0591854419410745,0.0591854419410745,0.0,True,True,True,1438589096000.0,1438589779000.0,1438593379000.0,1438599736000.0,1438600636000.0,1438589096000.0,-1.0,1438589096000.0,1438592696000.0,1438589779000.0,1438593379000.0,1438589779000.0,1438593379000.0,"sabby",1438592696000.0,1438589096000.0,45,1438589096000.0,-1.0,"Add REST support for spring-cloud-data","As a s-c-d developer, I'd like to establish the foundation to expose REST-APIs to interact with the {{xd-admin}} and likewise perform CRUD operations to maneuver streaming and batch pipelines. ",5.0,False,5.0,0,0.0,-1,0,True
"XD-3309","Story","Sprint 54","2015-08-03T06:33:50.000+0000","sabby","mbogoevici","Add direct binding option for s-c-s modules","As a s-c-s user, I'd like to have the option to direct bind _modules_, so I don't have to use messaging middleware and I can eliminate latency between them. This is important for high throughput and low latency use cases. 

",5.0,2.271468500599857e-06,2.271468500599857e-06,0.0,True,True,True,1438583630000.0,1438583636000.0,1438587236000.0,1441224193000.0,1441225093000.0,1438583630000.0,1439971067000.0,1438583630000.0,1438587230000.0,1438583636000.0,1438587236000.0,1438583636000.0,1438587236000.0,"sabby",1438587230000.0,1438583630000.0,45,1439971067000.0,1439971067000.0,"Add direct binding option for s-c-s modules","As a s-c-s user, I'd like to have the option to direct bind _modules_, so I don't have to use messaging middleware and I can eliminate latency between them. This is important for high throughput and low latency use cases. 

",8.0,True,8.0,1,-37.5,-1,0,False
"XD-3308","Bug","Sprint 54","2015-07-30T12:00:44.000+0000","hillert","hillert","With Security - Unable to upload module","Once security is enabled, one cannot upload modules using the shell any longer.",2.0,0.0003916879689948,9.276820318298012e-05,0.0225323657953327,True,True,True,1438257644000.0,1438257682000.0,1438261282000.0,1438353760000.0,1438354660000.0,1438257644000.0,-1.0,1438259830000.0,1438263430000.0,1438257653000.0,1438261253000.0,1438257682000.0,1438261282000.0,"hillert",1438263430000.0,1438257644000.0,45,1438257644000.0,-1.0,"With Security - Unable to upload module","Once security is enabled, one cannot upload modules using the shell any longer.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3303","Improvement","Sprint 59","2015-07-28T10:25:18.000+0000","mark.pollack","","Add documentation on upgrading from 1.1 to 1.2/1.3","",3.0,0.9063017157654432,0.8822809813584561,0.4988895857514571,True,True,True,1438079118000.0,1448036964000.0,1448040564000.0,1449065559000.0,1449066459000.0,1438079118000.0,-1.0,1443560588000.0,1443564188000.0,1447773040000.0,1447776640000.0,1448036964000.0,1448040564000.0,"mark.pollack",1443564188000.0,1438079118000.0,45,1438079118000.0,-1.0,"Update 1.3 installation instructions","As a user, I'd like to refer to documentation while migrating to 1.3 release.",3.0,False,3.0,0,0.0,0,0,True
"XD-3300","Story","Sprint 54","2015-07-24T13:35:06.000+0000","mminella","grenfro","Spike: Determine best way to centrally configure the job repository for batch jobs.","h2. Narrative
As a developer, I need to be able to run batch jobs that use the centrally configured job repository to store job state.

h2. Back story
The XD containers each used a {{BatchConfigurer}} implementation ({{RuntimeBatchConfigurer}}) to add a consistent configuration for the job repository.  This functionality needs to be replicated in some way in just a regular Spring Boot application.",5.0,0.3985609686605803,0.2907951616047462,0.2935448075043385,True,True,True,1437744906000.0,1438329634000.0,1438333234000.0,1439211104000.0,1439212004000.0,1437744906000.0,-1.0,1438175565000.0,1438179165000.0,1438171531000.0,1438175131000.0,1438329634000.0,1438333234000.0,"mminella",1438179165000.0,1437744906000.0,45,1437744906000.0,-1.0,"Spike: Determine best way to centrally configure the job repository for batch jobs.","h2. Narrative
As a developer, I need to be able to run batch jobs that use the centrally configured job repository to store job state.

h2. Back story
The XD containers each used a {{BatchConfigurer}} implementation ({{RuntimeBatchConfigurer}}) to add a consistent configuration for the job repository.  This functionality needs to be replicated in some way in just a regular Spring Boot application.",5.0,False,5.0,0,0.0,0,0,False
"XD-3298","Story","Sprint 54","2015-07-24T12:49:33.000+0000","mminella","mminella","Create TaskLauncher","h2. Narrative
As Spring XD, I will be able to launch Spring Boot jar files as Diego Tasks.

h2. Back story
The {{TaskLauncher}} will be responsible for listening for launch requests, looking up the definition in the {{TaskDescriptorRepository}}, and launching it.  The first implementation of this would be a Receptor based implementation.

*See:* https://docs.google.com/document/d/1q964adRCA-kJke_i0GBToJHLXJJTV_7TaTpQT0ymsbc/edit",5.0,0.0015938698767688,0.001591323177582,0.0016553544714216,True,True,True,1437742173000.0,1437746554000.0,1437750154000.0,1440489929000.0,1440490829000.0,1437742173000.0,1439971259000.0,1437746723000.0,1437750323000.0,1437746547000.0,1437750147000.0,1437746554000.0,1437750154000.0,"mminella",1437750323000.0,1437742173000.0,45,1439971259000.0,1439971259000.0,"Create basic TaskLauncher","h2. Narrative
As Spring XD, I will be able to launch Spring Boot jar files as Diego Tasks.

h2. Back story
The {{TaskLauncher}} will be responsible for listening for launch requests, looking up the definition in the {{TaskDescriptorRepository}}, and launching it.  The first implementation of this would be a Receptor based implementation. The scope here is to produce a _basic_ version of {{TaskLauncher}} and incrementally evolve into comprehensive launch capabilities.

*See:* https://docs.google.com/document/d/1q964adRCA-kJke_i0GBToJHLXJJTV_7TaTpQT0ymsbc/edit",8.0,True,8.0,1,-37.5,-1,1,True
"XD-3296","Story","Sprint 56","2015-07-24T11:36:56.000+0000","mminella","","Spike: Design a tasks repository","h2. Narrative
As a developer, I'd like to be able to run a boot jar as a task on CF and obtain the result reliably.

h2. Back story
Currently Lattice/Diego's tasks implementation provides the ability to run things as short lived tasks.  However, obtaining the result of said task can be an issue.  There are two ways to do so:

# Poll for the result.
# Register a callback URL to be called once the task completes.

Since a task is only available for a short time after its completion before it is deleted, polling can run the risk of missing the result completely.  When you consider the fact that the provided GUIDs that identify tasks can be re-used polling becomes a precarious option.

Registering a callback URL would be a better option, however there are no good guarantees that the message will be delivered.  The service will try to execute the callback until it's successful or the task is cleaned up.  ""Successful"" is defined in this case as anything other than a 502 or a 503 return code.

In order for Spring XD to be able to support Diego tasks, a more durable option for maintaining the result of tasks will need to be developed.

*Note:* The outcome of this spike may be feature requests for the CF/Diego team.",8.0,0.5858444313961405,0.5858422700206412,0.3102398766575048,True,True,True,1437737816000.0,1439364126000.0,1439367726000.0,1440512926000.0,1440513826000.0,1437737816000.0,-1.0,1438599045000.0,1438602645000.0,1439364120000.0,1439367720000.0,1439364126000.0,1439367726000.0,"mminella",1438602645000.0,1437737816000.0,45,1437737816000.0,-1.0,"Spike: Design a tasks repository","h2. Narrative
As a developer, I'd like to be able to run a boot jar as a task on CF and obtain the result reliably.

h2. Back story
Currently Lattice/Diego's tasks implementation provides the ability to run things as short lived tasks.  However, obtaining the result of said task can be an issue.  There are two ways to do so:

# Poll for the result.
# Register a callback URL to be called once the task completes.

Since a task is only available for a short time after its completion before it is deleted, polling can run the risk of missing the result completely.  When you consider the fact that the provided GUIDs that identify tasks can be re-used polling becomes a precarious option.

Registering a callback URL would be a better option, however there are no good guarantees that the message will be delivered.  The service will try to execute the callback until it's successful or the task is cleaned up.  ""Successful"" is defined in this case as anything other than a 502 or a 503 return code.

In order for Spring XD to be able to support Diego tasks, a more durable option for maintaining the result of tasks will need to be developed.

*Note:* The outcome of this spike may be feature requests for the CF/Diego team.",8.0,False,8.0,0,0.0,0,0,False
"XD-3288","Story","Sprint 54","2015-07-22T12:54:27.000+0000","sabby","","Add CI workflow to build, bundle and upload module-launcher image to DockerHub","As a s-c-s developer, I'd like to setup a CI workflow to build, bundle and upload the {{module-launcher}} image to DockerHub, so I don't have to worry about having a local-private docker registry for development/testing.

It could be nice to have the image uploaded to existing [spring-cloud|https://registry.hub.docker.com/repos/springcloud/] DockerHub location. ",5.0,0.6610878110470816,0.2067080496859426,0.0,True,True,True,1437569667000.0,1438072023000.0,1438075623000.0,1438328660000.0,1438329560000.0,1437569667000.0,-1.0,1437569667000.0,1437573267000.0,1437726743000.0,1437730343000.0,1438072023000.0,1438075623000.0,"sabby",1437573267000.0,1437569667000.0,45,1437569667000.0,-1.0,"Add CI workflow to build, bundle and upload module-launcher image to DockerHub","As a s-c-s developer, I'd like to setup a CI workflow to build, bundle and upload the {{module-launcher}} image to DockerHub, so I don't have to worry about having a local-private docker registry for development/testing.

It could be nice to have the image uploaded to existing [spring-cloud|https://registry.hub.docker.com/repos/springcloud/] DockerHub location. ",5.0,False,5.0,0,0.0,0,2,False
"XD-3287","Bug","Sprint 54","2015-07-22T09:09:26.000+0000","sabby","jvalkeal","Add HA support for NameNode when installed using Ambari","As a user, I'm trying to setup HA cluster using Ambari installed Spring XD; however, I'm running into issues with the overrides. More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].",5.0,0.0002364068271479,0.0,0.0,True,True,True,1437556166000.0,1437556435000.0,1437560035000.0,1438693135000.0,1438694035000.0,1437556166000.0,-1.0,1437556166000.0,1437559766000.0,1437556166000.0,1437559766000.0,1437556435000.0,1437560035000.0,"sabby",1437559766000.0,1437556166000.0,45,1437556166000.0,-1.0,"Add HA support for NameNode when installed using Ambari","As a user, I'm trying to setup HA cluster using Ambari installed Spring XD; however, I'm running into issues with the overrides. More details [here|https://github.com/spring-projects/spring-xd-ambari/issues/6].",5.0,False,5.0,0,0.0,-1,0,False
"XD-3283","Story","Sprint 54","2015-07-22T08:22:07.000+0000","sabby","","Migrate popular modules from XD to s-c-s repo","As a Spring XD developer, I'd like to move popular OOTB modules from XD to s-c-s repo, so I can use the compliant modules to build demos for SpringOne.",1.0,0.2365801700950451,0.2083880865916737,0.0,True,True,True,1437553327000.0,1438148703000.0,1438152303000.0,1440069020000.0,1440069920000.0,1437553327000.0,1438872970000.0,1437553327000.0,1437556927000.0,1438077755000.0,1438081355000.0,1438148703000.0,1438152303000.0,"sabby",1437556927000.0,1437553327000.0,45,1439970705000.0,1438872970000.0,"Port FTP as s-c-s source module","As a Spring XD developer, I'd like to port {{FTP}} modules from XD to s-c-s repo, so I can use them as {{source}} modules to build streaming pipeline.",8.0,True,8.0,2,-87.5,-1,0,True
"XD-3282","Story","Sprint 54","2015-07-22T08:17:23.000+0000","sabby","","Create CI infrastructure for s-c-s","",3.0,0.5802743266660793,0.5802508297353658,0.0,True,True,True,1437553043000.0,1437651826000.0,1437655426000.0,1437722378000.0,1437723278000.0,1437553043000.0,-1.0,1437553043000.0,1437556643000.0,1437651822000.0,1437655422000.0,1437651826000.0,1437655426000.0,"sabby",1437556643000.0,1437553043000.0,45,1437553043000.0,-1.0,"Create CI infrastructure for s-c-s","As a s-c-s developer, I'd like to setup CI builds for s-c-s builds, so I can incrementally build and test code commits automatically.",3.0,False,3.0,0,0.0,0,0,True
"XD-3271","Story","Sprint 54","2015-07-21T11:45:49.000+0000","sabby","","Add automatic wiring of profile-driven SPI implementations","As a Spring XD user, I'd like to make SPI implementation profile aware, so I can run {{java -jar admin}} or {{cf push}} admin or {{ltc create admin}} and the corresponding implementation gets wired-in automatically.",3.0,0.9913774775521896,0.9913774775521896,0.0,True,True,True,1437479149000.0,1438794122000.0,1438797722000.0,1438804659000.0,1438805559000.0,1437479149000.0,1438766562000.0,1437479149000.0,1437482749000.0,1438794122000.0,1438797722000.0,1438794122000.0,1438797722000.0,"sabby",1437482749000.0,1437479149000.0,45,1438766562000.0,1438766562000.0,"Add automatic wiring of profile-driven SPI implementations","As a Spring XD user, I'd like to make SPI implementation profile aware, so I can run {{java -jar admin}} or {{cf push}} admin or {{ltc create admin}} and the corresponding implementation gets wired-in automatically.",5.0,True,5.0,1,-40.0,0,0,False
"XD-3270","Story","Sprint 54","2015-07-21T11:41:42.000+0000","sabby","","Create module registry abstraction","As a Spring XD developer, I'd like to create initial version of the new module registry abstraction, so we could leverage the foundation to make progress and test the respective SPI ({{receptor}} or {{cloudcontroller}}) implementations.",5.0,0.0706694243277299,0.0706694243277299,0.0,True,True,True,1437478902000.0,1437557251000.0,1437560851000.0,1438586671000.0,1438587571000.0,1437478902000.0,-1.0,1437478902000.0,1437482502000.0,1437557251000.0,1437560851000.0,1437557251000.0,1437560851000.0,"sabby",1437482502000.0,1437478902000.0,45,1437478902000.0,-1.0,"Create module registry abstraction","As a Spring XD developer, I'd like to create initial version of the new module registry abstraction, so we could leverage the foundation to make progress and test the respective SPI ({{receptor}} or {{cloudcontroller}}) implementations.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3269","Story","Sprint 54","2015-07-21T11:30:01.000+0000","sabby","","Find a permanent home for SPI","As a Spring XD developer, I'd like to have a permanent location of SPI implementations, so I could use the common repo every time I contribute or enhance the test coverage.
",3.0,-1.0,0.9996827411167512,0.0,False,True,True,1437478201000.0,-1.0,-1.0,1437552949000.0,1437553849000.0,1437478201000.0,-1.0,1437478201000.0,1437481801000.0,1437553825000.0,1437553849000.0,-1.0,-1.0,"sabby",1437481801000.0,1437478201000.0,45,1437478201000.0,-1.0,"Find a permanent home for SPI","As a Spring XD developer, I'd like to have a permanent location of SPI implementations, so I could use the common repo every time I contribute or enhance the test coverage.
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3267","Story","Sprint 54","2015-07-21T11:18:55.000+0000","sabby","","Add support for Receptor SPI to query module status","As a Spring XD on CF user, I'd like to use Receptor implementation of Admin SPI every time I deploy Spring XD modules, so I can leverage the SPI to query for module status and health metrics.

*Possible APIs:*
{code}

ModuleStatus getStatus(ModuleDescriptor descriptor);

Collection<ModuleDescriptor> listModules();

Map<ModuleDescriptor.Key, ModuleStatus>

{code}",5.0,0.9683353389136954,0.9471430397545698,0.0,True,True,True,1437477535000.0,1438795635000.0,1438799235000.0,1438837837000.0,1438838737000.0,1437477535000.0,1437551289000.0,1437477535000.0,1437481135000.0,1438766788000.0,1438770388000.0,1438795635000.0,1438799235000.0,"sabby",1437481135000.0,1437477535000.0,45,1437551289000.0,1437551289000.0,"Add support for Receptor SPI to query module status","As a Spring XD on CF user, I'd like to use Receptor implementation of Admin SPI every time I deploy Spring XD modules, so I can leverage the SPI to query for module status and health metrics.

*Possible APIs:*
{code}

ModuleStatus getStatus(ModuleDescriptor descriptor);

Collection<ModuleDescriptor> listModules();

Map<ModuleDescriptor.Key, ModuleStatus>

{code}",8.0,True,8.0,1,-37.5,0,0,False
"XD-3266","Bug","Sprint 54","2015-07-20T01:47:57.000+0000","kdowbecki","hillert","No pagination for Jobs / Deployments page in Admin UI","After successfully deploying 12 jobs the Jobs / Deployments page still shows only 10 results.

It looks like {{http://localhost:9393/jobs/configurations.json?page=0&size=10}} always returns {{content.page.totalPages}} of 1 regardless of the {{size}} parameter.",2.0,0.3283702659036997,0.3283486521097765,0.3242150140219488,True,True,True,1437356877000.0,1437478418000.0,1437482018000.0,1437726111000.0,1437727011000.0,1437356877000.0,-1.0,1437476880000.0,1437480480000.0,1437478410000.0,1437482010000.0,1437478418000.0,1437482018000.0,"kdowbecki",1437480480000.0,1437356877000.0,45,1437356877000.0,-1.0,"No pagination for Jobs / Deployments page in Admin UI","After successfully deploying 12 jobs the Jobs / Deployments page still shows only 10 results.

It looks like {{http://localhost:9393/jobs/configurations.json?page=0&size=10}} always returns {{content.page.totalPages}} of 1 regardless of the {{size}} parameter.",2.0,False,2.0,0,0.0,0,1,False
"XD-3265","Story","Sprint 54","2015-07-19T18:01:11.000+0000","sabby","","Spike: Investigate distributed implementation of CloudController Admin SPI","As a Spring XD user, I'd like to use {{CloudController}} based implementation of XD Admin SPI (based on ModuleLauncher), so I can run data pipeline use-cases running on CF.
",8.0,0.5470049359562256,0.1670754201745617,0.0,True,True,True,1437328871000.0,1438072033000.0,1438075633000.0,1438686573000.0,1438687473000.0,1437328871000.0,-1.0,1437328871000.0,1437332471000.0,1437555860000.0,1437559460000.0,1438072033000.0,1438075633000.0,"sabby",1437332471000.0,1437328871000.0,45,1437328871000.0,-1.0,"Spike: Investigate distributed implementation of CloudController Admin SPI","As a Spring XD user, I'd like to use {{CloudController}} based implementation of XD Admin SPI (based on ModuleLauncher), so I can run data pipeline use-cases running on CF.

Relevant repos: [spring-cloud-data|https://github.com/spring-cloud/spring-cloud-data/tree/master/spring-cloud-data-module-deployers] | [spring-cloud-stream|https://github.com/spring-cloud/spring-cloud-stream]

Please refer to XD-3194 or XD-3229 as sample spike-deliverables (_google doc_) that were completed in the last sprint. ",8.0,False,8.0,0,0.0,0,0,True
"XD-3263","Bug","Sprint 53","2015-07-16T12:27:39.000+0000","svennela","hillert","Pagination for containers, it is limited to only 20","Hi ,

Customer has 48 containers, but it only shows 20 containers. We need pagination to browse all containers.",2.0,0.4853859255039361,0.4853859255039361,0.4852590005992036,True,True,True,1437049659000.0,1437378540000.0,1437382140000.0,1437726325000.0,1437727225000.0,1437049659000.0,-1.0,1437378454000.0,1437382054000.0,1437378540000.0,1437382140000.0,1437378540000.0,1437382140000.0,"svennela",1437382054000.0,1437049659000.0,45,1437049659000.0,-1.0,"Pagination for containers, it is limited to only 20","Hi ,

Customer has 48 containers, but it only shows 20 containers. We need pagination to browse all containers.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3261","Story","Sprint 53","2015-07-16T08:35:36.000+0000","thomas.risberg","thomas.risberg","Update Groovy to 2.4.4","There is a vulnerability in Groovy that is fixed in 2.4.4:

CVE-2015-3253: Remote execution of untrusted code

See:

http://groovy-lang.org/security.html

http://mail-archives.apache.org/mod_mbox/incubator-groovy-users/201507.mbox/%3CCADQzvmmYC7RbZnsQ8O63XN4HCMYh9RGRdMiuWupVt=u=pjH8+g@mail.gmail.com%3E


",1.0,0.0044865718799368,0.004402317008952,0.0047393364928909,True,True,True,1437035736000.0,1437036162000.0,1437039762000.0,1437129786000.0,1437130686000.0,1437035736000.0,-1.0,1437036186000.0,1437039786000.0,1437036154000.0,1437039754000.0,1437036162000.0,1437039762000.0,"thomas.risberg",1437039786000.0,1437035736000.0,45,1437035736000.0,-1.0,"Update Groovy to 2.4.4","There is a vulnerability in Groovy that is fixed in 2.4.4:

CVE-2015-3253: Remote execution of untrusted code

See:

http://groovy-lang.org/security.html

http://mail-archives.apache.org/mod_mbox/incubator-groovy-users/201507.mbox/%3CCADQzvmmYC7RbZnsQ8O63XN4HCMYh9RGRdMiuWupVt=u=pjH8+g@mail.gmail.com%3E


",1.0,False,1.0,0,0.0,-1,2,False
"XD-3259","Story","Sprint 54","2015-07-15T10:39:31.000+0000","sabby","","Move message-bus implemenation from XD to spring-cloud-streams [Phase #2]","As a developer, I'd like to move message-bus from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency.",3.0,0.8580530087360967,0.8580530087360967,0.0,True,True,True,1436956771000.0,1438869493000.0,1438873093000.0,1439185013000.0,1439185913000.0,1436956771000.0,1438767877000.0,1436956771000.0,1436960371000.0,1438869493000.0,1438873093000.0,1438869493000.0,1438873093000.0,"sabby",1436960371000.0,1436956771000.0,45,1438767877000.0,1438767877000.0,"[SCS] Rename xd.messagebus binder properties ","replace with xd.messagebus prefix with spring.cloud.stream.binder",8.0,True,8.0,1,-62.5,-1,0,True
"XD-3258","Story","Sprint 53","2015-07-14T11:21:39.000+0000","thomas.risberg","","Add jars for Avro and Snappy compression to Sqoop job submission","We need to have some jars as part of the Sqoop job submission to YARN:

for Avro we need:
  avro-1.7.6.jar
  avro-mapred-1.7.6.jar

for Snappy we need:
  snappy-java-1.0.5.jar (note: the 1.1.0.1 version from xd/lib doesn't work)
  commons-compress-1.4.1.jar

We can either have these included using the --libjars option or automatically include them.
",3.0,0.0518738024337035,0.0518738024337035,0.0131556027439302,True,True,True,1436872899000.0,1436897914000.0,1436901514000.0,1437354227000.0,1437355127000.0,1436872899000.0,-1.0,1436879243000.0,1436882843000.0,1436897914000.0,1436901514000.0,1436897914000.0,1436901514000.0,"thomas.risberg",1436882843000.0,1436872899000.0,45,1436872899000.0,-1.0,"Add jars for Avro and Snappy compression to Sqoop job submission","We need to have some jars as part of the Sqoop job submission to YARN:

for Avro we need:
  avro-1.7.6.jar
  avro-mapred-1.7.6.jar

for Snappy we need:
  snappy-java-1.0.5.jar (note: the 1.1.0.1 version from xd/lib doesn't work)
  commons-compress-1.4.1.jar

We can either have these included using the --libjars option or automatically include them.
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3249","Story","Sprint 53","2015-07-10T02:51:03.000+0000","eric.bottard","eric.bottard","Change module option type from Class to String","This better aligns with boot. Moreover, using Class was a bad design choice (one can always get a Class from a String [modulo knowing which CL to use], while to converse is not always easy [CL not being available])
",3.0,0.6400738314810255,1.841184065472505e-05,0.6397562272297315,True,True,True,1436496663000.0,1436774777000.0,1436778377000.0,1436930266000.0,1436931166000.0,1436496663000.0,-1.0,1436774639000.0,1436778239000.0,1436496671000.0,1436500271000.0,1436774777000.0,1436778377000.0,"eric.bottard",1436778239000.0,1436496663000.0,45,1436496663000.0,-1.0,"Change module option type from Class to String","This better aligns with boot. Moreover, using Class was a bad design choice (one can always get a Class from a String [modulo knowing which CL to use], while to converse is not always easy [CL not being available])
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3241","Story","Sprint 53","2015-07-09T03:04:12.000+0000","jvalkeal","jvalkeal","Add support for update in gpfdist sink","Currently we can only do plain inserts, should follow same logic from native gpfdist sink and add upserts.",1.0,0.3498945742044782,0.0,0.3498842613209679,True,True,True,1436411052000.0,1436784259000.0,1436787859000.0,1437476779000.0,1437477679000.0,1436411052000.0,-1.0,1436784248000.0,1436787848000.0,1436411052000.0,1436414652000.0,1436784259000.0,1436787859000.0,"jvalkeal",1436787848000.0,1436411052000.0,45,1436411052000.0,-1.0,"Add support for update in gpfdist sink","Currently we can only do plain inserts, should follow same logic from native gpfdist sink and add upserts.",1.0,False,1.0,0,0.0,0,0,False
"XD-3240","Story","Sprint 53","2015-07-09T03:02:50.000+0000","jvalkeal","jvalkeal","Add better support for using control file with gpfdist","Currently only database connection info can be read from a control file yml format. Should add rest of the missing options to align how native format works.",2.0,0.3499400964082219,0.0,0.3499325967410196,True,True,True,1436410970000.0,1436784256000.0,1436787856000.0,1437476784000.0,1437477684000.0,1436410970000.0,-1.0,1436784248000.0,1436787848000.0,1436410970000.0,1436414570000.0,1436784256000.0,1436787856000.0,"jvalkeal",1436787848000.0,1436410970000.0,45,1436410970000.0,-1.0,"Add better support for using control file with gpfdist","Currently only database connection info can be read from a control file yml format. Should add rest of the missing options to align how native format works.",2.0,False,2.0,0,0.0,0,0,False
"XD-3239","Story","Sprint 53","2015-07-08T11:47:16.000+0000","sabby","","Move 1.2.x branch to EC2 CI infrastruucture","As a developer, I'd like to move 1.2.x branch to EC2 infrastructure, so I can reliably run CI test suites.",2.0,-1.0,-1.0,0.0,False,True,False,1436356036000.0,-1.0,-1.0,1437552659000.0,1437553559000.0,1436356036000.0,-1.0,1436356036000.0,1436359636000.0,-1.0,-1.0,-1.0,-1.0,"sabby",1436359636000.0,1436356036000.0,45,1436356036000.0,-1.0,"Move 1.2.x branch to EC2 CI infrastructure","As a developer, I'd like to move 1.2.x branch to EC2 infrastructure, so I can reliably run CI test suites.",2.0,False,2.0,0,0.0,-1,0,True
"XD-3238","Story","Sprint 53","2015-07-08T11:40:24.000+0000","sabby","","Complete remaining Kryo optimization changes","As a developer, I'd like to complete the remaining Kryo optimization changes, so I can polish and get the guidelines documented appropriately. ",1.0,0.8101018527721519,0.013483149728952,0.0,True,True,True,1436355624000.0,1436852807000.0,1436856407000.0,1436968453000.0,1436969353000.0,1436355624000.0,-1.0,1436355624000.0,1436359224000.0,1436363899000.0,1436367499000.0,1436852807000.0,1436856407000.0,"sabby",1436359224000.0,1436355624000.0,45,1436355624000.0,-1.0,"Complete remaining Kryo optimization changes","As a developer, I'd like to complete the remaining Kryo optimization changes, so I can polish and get the guidelines documented appropriately. ",1.0,False,1.0,0,0.0,-1,0,False
"XD-3235","Bug","Sprint 54","2015-07-08T08:51:59.000+0000","grenfro","grenfro","FileJdbc Job throws exception during Acceptance Tests ","Currently the testFileJdbcJobMultipleInvocations fails on line 156 stating data is different in table that what is expected.  Currently this is failing on the single admin/container deployment using redis as a transport.  

Also seeing the following exception in the attached log:
{noformat}
2.0.0.SNAP ERROR xdbus.job:ec2Job3.0.requests-1 step.AbstractStep - Encountered an error executing step step1 in job ec2Job3
org.springframework.batch.item.ItemStreamException: Failed to initialize the reader
...
Caused by: java.lang.IllegalStateException: Input resource must exist (reader is in 'strict' mode): URL [file:/tmp/xd/output/filejdbctest/filejdbctest1.out]
{noformat}
The file is should be present and data present for the test.  At least according to the checker on EC2 and local deployments.



",3.0,0.7616220996443546,0.6909154530821547,0.6999777323942766,True,True,True,1436345519000.0,1436944074000.0,1436947674000.0,1437130514000.0,1437131414000.0,1436345519000.0,-1.0,1436895628000.0,1436899228000.0,1436888506000.0,1436892106000.0,1436944074000.0,1436947674000.0,"grenfro",1436899228000.0,1436345519000.0,45,1436345519000.0,-1.0,"FileJdbc Job throws exception during Acceptance Tests ","Currently the testFileJdbcJobMultipleInvocations fails on line 156 stating data is different in table that what is expected.  Currently this is failing on the single admin/container deployment using redis as a transport.  

Also seeing the following exception in the attached log:
{noformat}
2.0.0.SNAP ERROR xdbus.job:ec2Job3.0.requests-1 step.AbstractStep - Encountered an error executing step step1 in job ec2Job3
org.springframework.batch.item.ItemStreamException: Failed to initialize the reader
...
Caused by: java.lang.IllegalStateException: Input resource must exist (reader is in 'strict' mode): URL [file:/tmp/xd/output/filejdbctest/filejdbctest1.out]
{noformat}
The file is should be present and data present for the test.  At least according to the checker on EC2 and local deployments.



",3.0,False,3.0,0,0.0,0,1,False
"XD-3234","Story","Sprint 60","2015-07-08T08:01:03.000+0000","hillert","","Remove XML REST Endpoints","The XML REST endpoints:

* are not working correctly
* interfere with security
* are not used


",3.0,0.998199733819992,0.9892254738179128,0.8560618739854846,True,True,True,1436342463000.0,1446040203000.0,1446043803000.0,1446056793000.0,1446057693000.0,1436342463000.0,-1.0,1444659301000.0,1444662901000.0,1445953016000.0,1445956616000.0,1446040203000.0,1446043803000.0,"hillert",1444662901000.0,1436342463000.0,45,1436342463000.0,-1.0,"Remove XML REST Endpoints","The XML REST endpoints:

* are not working correctly
* interfere with security
* are not used


",3.0,False,3.0,0,0.0,-1,0,False
"XD-3233","Story","Sprint 53","2015-07-08T07:53:10.000+0000","sabby","","Enable component model for S-Bus ","",8.0,0.1087996932418056,0.1087996932418056,0.0,True,True,True,1436341990000.0,1436417465000.0,1436421065000.0,1437034796000.0,1437035696000.0,1436341990000.0,-1.0,1436341990000.0,1436345590000.0,1436417465000.0,1436421065000.0,1436417465000.0,1436421065000.0,"sabby",1436345590000.0,1436341990000.0,45,1436341990000.0,-1.0,"Enable component model for spring-cloud-streams","As a developer, I'd like to create an annotation ({{@EnableModule}}) driven programming model for modules, so instead of explicitly defining I/O channels as beans on the module, for classes annotated with {{@EnableModule}}, the application would be responsible for creating the actual channel beans and channel adapters vs. the developer creating concrete channel instance types.

The {{@Input}} and {{@Output}} annotations will be used to indicate the input and output channels of the module. ",8.0,False,8.0,0,0.0,0,1,True
"XD-3232","Story","Sprint 53","2015-07-08T07:50:28.000+0000","grussell","","Update Spring Integration to 4.2.0.M2 (4.1.6 on 1.2.x)","",1.0,0.0490654012995513,0.0490654012995513,0.0037179501976036,True,True,True,1436341828000.0,1436371587000.0,1436375187000.0,1436947445000.0,1436948345000.0,1436341828000.0,-1.0,1436344083000.0,1436347683000.0,1436371587000.0,1436375187000.0,1436371587000.0,1436375187000.0,"grussell",1436347683000.0,1436341828000.0,45,1436341828000.0,-1.0,"Update Spring Integration to 4.2.0.M2 (4.1.6 on 1.2.x)","Also Spring Framework 4.2.0.RC2, Spring AMQP 1.5.0.M1

Also Batch 3.0.4",1.0,False,1.0,0,0.0,0,0,True
"XD-3231","Story","Sprint 53","2015-07-07T14:10:03.000+0000","sabby","","Add support to Ambari install multiple XD Admin's ","As a developer, I'd like to update Ambari installed Spring XD cluster to spin-up multiple instances of XD-Admin servers, so it is setup for HA. ",5.0,0.2212275247917436,0.1222403090711174,0.0,True,True,True,1436278203000.0,1436433270000.0,1436436870000.0,1436978242000.0,1436979142000.0,1436278203000.0,1436343797000.0,1436278203000.0,1436281803000.0,1436363886000.0,1436367486000.0,1436433270000.0,1436436870000.0,"sabby",1436281803000.0,1436278203000.0,45,1436343797000.0,1436343797000.0,"Add support to Ambari install multiple XD Admin's ","As a developer, I'd like to update Ambari installed Spring XD cluster to spin-up multiple instances of XD-Admin servers, so it is setup for HA. ",3.0,True,3.0,1,66.66666666666666,0,0,False
"XD-3230","Story","Sprint 53","2015-07-07T14:07:40.000+0000","sabby","","Update to Reactor 2.0.4","As a developer, I'd like to upgrade to Reactor 2.0.4 release, so I could leverage the latest improvements and bug-fixes.",1.0,0.3478249461374973,0.3478249461374973,0.0,True,True,True,1436278060000.0,1436516670000.0,1436520270000.0,1436963166000.0,1436964066000.0,1436278060000.0,-1.0,1436278060000.0,1436281660000.0,1436516670000.0,1436520270000.0,1436516670000.0,1436520270000.0,"sabby",1436281660000.0,1436278060000.0,45,1436278060000.0,-1.0,"Update to Reactor 2.0.4","As a developer, I'd like to upgrade to Reactor 2.0.4 release, so I could leverage the latest improvements and bug-fixes.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3229","Story","Sprint 53","2015-07-07T14:06:13.000+0000","sabby","","Spike: Discoverable XD Admin Server endpoints","As a developer, I'd like to investigate the possibility of service discovery (SD) discovering relevant XD Admin server endpoint, so we could conduct a feasibility study on what it means for Flo UI to consume XD exposed metrics when deployed to CloudFoundry. ",8.0,0.1958651752374171,0.053015838846157,0.0,True,True,True,1436277973000.0,1436514537000.0,1436518137000.0,1437484863000.0,1437485763000.0,1436277973000.0,-1.0,1436277973000.0,1436281573000.0,1436342005000.0,1436345605000.0,1436514537000.0,1436518137000.0,"sabby",1436281573000.0,1436277973000.0,45,1436277973000.0,-1.0,"Spike: XD Admin SPI to discover s-c-s modules","As a s-c-s user, I'd like to investigate the possibility of s-c-s modules self-registering themselves to service discovery, so I could use Spring XD runtime (running on CF) to discover and orchestrate such modules through streams.",8.0,False,8.0,0,0.0,0,0,True
"XD-3228","Story","Sprint 53","2015-07-07T13:56:53.000+0000","sabby","","Develop distributed implementation of Admin SPI","As a developer, I'd like to develop a distributedimplementation of XD Admin SPI (based on Module Launcher), so I can run data pipeline use-cases using Redis or Rabbit as the transport bus.
",8.0,0.1257585061637911,0.1257486018395631,0.0,True,True,True,1436277413000.0,1436429781000.0,1436433381000.0,1437488105000.0,1437489005000.0,1436277413000.0,-1.0,1436277413000.0,1436281013000.0,1436429769000.0,1436433369000.0,1436429781000.0,1436433381000.0,"sabby",1436281013000.0,1436277413000.0,45,1436277413000.0,-1.0,"Spike: Kickoff distributed Receptor implementation of Admin SPI","As a Spring XD user, I'd like to use Diego based Receptor implementation of XD Admin SPI (based on ModuleLauncher), so I can run data pipeline use-cases running on CF Lattice/Diego. 
",8.0,False,8.0,0,0.0,0,1,True
"XD-3227","Story","Sprint 53","2015-07-07T13:55:59.000+0000","sabby","","Develop singlenode implementation of Admin SPI","As a developer, I'd like to develop a singlenode (in a single JVM) implementation of XD Admin SPI (based on Module Launcher), so I can run data pipeline use-cases using local bus only.
",8.0,0.4256974682833911,0.4256974682833911,0.0,True,True,True,1436277359000.0,1436793163000.0,1436796763000.0,1437488127000.0,1437489027000.0,1436277359000.0,-1.0,1436277359000.0,1436280959000.0,1436793163000.0,1436796763000.0,1436793163000.0,1436796763000.0,"sabby",1436280959000.0,1436277359000.0,45,1436277359000.0,-1.0,"Spike: Kickoff singlenode implementation of Admin SPI","As a developer, I'd like to develop a singlenode (in a single JVM) implementation of XD Admin SPI (based on Module Launcher), so I can run data pipeline use-cases locally.
",8.0,False,8.0,0,0.0,0,1,True
"XD-3226","Story","Sprint 53","2015-07-07T13:49:11.000+0000","sabby","","Move serialization codec from XD to S-Bus","As a developer, I'd like to move 'serialization codec' from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency.",8.0,0.2063090252707581,0.2062960288808664,0.0,True,True,True,1436276951000.0,1436419820000.0,1436423420000.0,1436968551000.0,1436969451000.0,1436276951000.0,-1.0,1436276951000.0,1436280551000.0,1436419811000.0,1436423411000.0,1436419820000.0,1436423420000.0,"sabby",1436280551000.0,1436276951000.0,45,1436276951000.0,-1.0,"Move serialization codec from XD to spring-cloud-stream [Phase #1]","As a developer, I'd like to move 'serialization codec' from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency.",8.0,False,8.0,0,0.0,-1,0,True
"XD-3225","Story","Sprint 53","2015-07-07T13:48:26.000+0000","sabby","","Move input/output type-conversion from XD to S-Bus","As a developer, I'd like to move 'input/output type conversion' from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency. ",8.0,0.8947444034625087,0.8947444034625087,0.0,True,True,True,1436276906000.0,1442853629000.0,1442857229000.0,1443626399000.0,1443627299000.0,1436276906000.0,-1.0,1436276906000.0,1436280506000.0,1442853629000.0,1442857229000.0,1442853629000.0,1442857229000.0,"sabby",1436280506000.0,1436276906000.0,45,1436276906000.0,-1.0,"Move input/output type-conversion from XD to spring-cloud-stream","As a developer, I'd like to move input/output type conversion from Spring XD repo to spring-cloud-dataflow, so I can implement a custom module which produces or consumes a custom domain object.",8.0,False,8.0,0,0.0,0,0,True
"XD-3224","Story","Sprint 53","2015-07-07T13:47:27.000+0000","sabby","","Move message-bus implemenation from XD to S-Bus","As a developer, I'd like to move message-bus from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency. 
",8.0,0.2186206060186067,0.2186206060186067,0.0,True,True,True,1436276847000.0,1436428275000.0,1436431875000.0,1436968599000.0,1436969499000.0,1436276847000.0,-1.0,1436276847000.0,1436280447000.0,1436428275000.0,1436431875000.0,1436428275000.0,1436431875000.0,"sabby",1436280447000.0,1436276847000.0,45,1436276847000.0,-1.0,"Move message-bus implemenation from XD to spring-cloud-streams [Phase #1]","As a developer, I'd like to move message-bus from Spring XD repo into spring-bus, so I can update Spring XD to inherit the features/functionalities via maven dependency. 
",8.0,False,8.0,0,0.0,-1,1,True
"XD-3222","Story","Sprint 53","2015-07-07T08:09:22.000+0000","thomas.risberg","","Find a way to connect Sqoop job to Teradata","As a user I would like to connect the Sqoop batch job to Teradata for import jobs. 

I have tried the Teradata JDBC driver directly using:

{code}job create tdTest --definition ""sqoop --command=import --args='--table Frequent_Flyers --connect jdbc:teradata://tdexpress/DATABASE=transportation --driver com.teradata.jdbc.TeraDriver --username dbc --password dbc --target-dir=/xd/teradata --num-mappers 1'""
{code}

but that results in an NPE.

The only way so far is to use the Hortonworks Connector for Teradata - http://public-repo-1.hortonworks.com/HDP/tools/2.2.4.2/hdp-connector-for-teradata-1.3.4.2.2.4.2-2-distro.tar.gz

That one allows me to use the following:

{code}job create tdTest --definition ""sqoop --command=import --args='--table Frequent_Flyers --connect jdbc:teradata://tdexpress/DATABASE=transportation --connection-manager org.apache.sqoop.teradata.TeradataConnManager --username dbc --password dbc --target-dir=/xd/teradata --num-mappers 1'""
{code}",3.0,0.1619704513561872,0.1619704513561872,0.0328761676445107,True,True,True,1436256562000.0,1436368176000.0,1436371776000.0,1436944763000.0,1436945663000.0,1436256562000.0,-1.0,1436279217000.0,1436282817000.0,1436368176000.0,1436371776000.0,1436368176000.0,1436371776000.0,"thomas.risberg",1436282817000.0,1436256562000.0,45,1436256562000.0,-1.0,"Find a way to connect Sqoop job to Teradata","As a user I would like to connect the Sqoop batch job to Teradata for import jobs. 

I have tried the Teradata JDBC driver directly using:

{code}job create tdTest --definition ""sqoop --command=import --args='--table Frequent_Flyers --connect jdbc:teradata://tdexpress/DATABASE=transportation --driver com.teradata.jdbc.TeraDriver --username dbc --password dbc --target-dir=/xd/teradata --num-mappers 1'""
{code}

but that results in an NPE.

The only way so far is to use the Hortonworks Connector for Teradata - http://public-repo-1.hortonworks.com/HDP/tools/2.2.4.2/hdp-connector-for-teradata-1.3.4.2.2.4.2-2-distro.tar.gz

That one allows me to use the following:

{code}job create tdTest --definition ""sqoop --command=import --args='--table Frequent_Flyers --connect jdbc:teradata://tdexpress/DATABASE=transportation --connection-manager org.apache.sqoop.teradata.TeradataConnManager --username dbc --password dbc --target-dir=/xd/teradata --num-mappers 1'""
{code}",3.0,False,3.0,0,0.0,0,1,False
"XD-3220","Bug","Sprint 53","2015-07-07T03:26:38.000+0000","kdowbecki","hillert","Enabling security breaks job launching from Admin UI","After enabling security (see XD-3214) and granting user {{ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN}} privileges it's not possible to launch jobs from Admin UI. 

For {{bdl-sqoop-combo-lukasz-MONGO-DEV}} job, 403 error is returned when Admin UI attempts to access following URL after ""Launch Job"" button is pressed:
{code}
http://ilabphd12.isus.emc.com:9393/jobs/executions?jobParameters=%7B%7D&jobname=bdl-sqoop-combo-lukasz-MONGO-DEV
{code}

Please see attached screenshot.",2.0,0.3191560897705304,0.3189879801630663,0.5143943851391107,True,True,True,1436239598000.0,1436254786000.0,1436258386000.0,1436286286000.0,1436287186000.0,1436239598000.0,-1.0,1436264077000.0,1436267677000.0,1436254778000.0,1436258378000.0,1436254786000.0,1436258386000.0,"kdowbecki",1436267677000.0,1436239598000.0,45,1436239598000.0,-1.0,"Enabling security breaks job launching from Admin UI","After enabling security (see XD-3214) and granting user {{ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN}} privileges it's not possible to launch jobs from Admin UI. 

For {{bdl-sqoop-combo-lukasz-MONGO-DEV}} job, 403 error is returned when Admin UI attempts to access following URL after ""Launch Job"" button is pressed:
{code}
http://ilabphd12.isus.emc.com:9393/jobs/executions?jobParameters=%7B%7D&jobname=bdl-sqoop-combo-lukasz-MONGO-DEV
{code}

Please see attached screenshot.",2.0,False,2.0,0,0.0,-1,7,False
"XD-3219","Bug","Sprint 52","2015-07-06T12:36:26.000+0000","iperumal","iperumal","Fix random configuration in SecuredShellTests","Since the SecuredShellTests initialize singlenode app in a static way, the random configuration needs to be setup statically as well.",1.0,8.470269354565475e-05,0.0,0.99973530408267,True,True,True,1436186186000.0,1436186194000.0,1436189794000.0,1436279734000.0,1436280634000.0,1436186186000.0,-1.0,1436280609000.0,1436280634000.0,1436186186000.0,1436189786000.0,1436186194000.0,1436189794000.0,"iperumal",1436280634000.0,1436186186000.0,45,1436186186000.0,-1.0,"Fix random configuration in SecuredShellTests","Since the SecuredShellTests initialize singlenode app in a static way, the random configuration needs to be setup statically as well.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3218","Improvement","Sprint 52","2015-07-06T11:41:04.000+0000","iperumal","iperumal","Admin leader should exit if the recalculation of stream/job states failed","The admin leader election fails when the stream/job module definitions doesn't exist in `module-registry` but still some of the references still exist in ZK (via some of the previous deployments that had this module in module registry). 

Though this is expected, this behavior will make *all* the subsequent deployments in *deploying* state because the admin leader isn't elected.

",1.0,0.000852746066029,0.0,0.0287430576982412,True,True,True,1436182864000.0,1436185075000.0,1436188675000.0,1438774764000.0,1438775664000.0,1436182864000.0,1438768183000.0,1436257389000.0,1436260989000.0,1436182864000.0,1436186464000.0,1436185075000.0,1436188675000.0,"iperumal",1436260989000.0,1436182864000.0,45,1438768183000.0,1438768183000.0,"[Backport] Handle stream/job deployment status recalculation failures","The admin leader election fails when the stream/job module definitions doesn't exist in `module-registry` but still some of the references still exist in ZK (via some of the previous deployments that had this module in module registry). 

Though this is expected, this behavior will make *all* the subsequent deployments in *deploying* state because the admin leader isn't elected.

",3.0,True,3.0,1,-66.66666666666666,0,0,True
"XD-3217","Bug","Sprint 52","2015-07-03T08:13:14.000+0000","sabby","","Cannot connect to admin server with basic security enabled","As a user, I'm trying to connect to {{xd-admin}} server with basic security enabled; however, I'm unable to successfully connect to the server and I get the following error message.


{code:java}
server-unknown:>admin config server --uri http://localhost:9393 --username bob --password bobspwd
Unable to contact XD Admin Server at 'http://localhost:9393'.
{code}",5.0,0.869698610359141,0.8410936654033568,0.0,True,True,True,1435911194000.0,1435920832000.0,1435922276000.0,1435921376000.0,1435922276000.0,1435911194000.0,-1.0,1435911194000.0,1435914794000.0,1435920515000.0,1435922276000.0,1435920832000.0,1435922276000.0,"sabby",1435914794000.0,1435911194000.0,45,1435911194000.0,-1.0,"Cannot connect to admin server with basic security enabled","As a user, I'm trying to connect to {{xd-admin}} server with basic security enabled; however, I'm unable to successfully connect to the server and I get the following error message.


{code:java}
server-unknown:>admin config server --uri http://localhost:9393 --username bob --password bobspwd
Unable to contact XD Admin Server at 'http://localhost:9393'.
{code}",5.0,False,5.0,0,0.0,0,0,False
"XD-3216","Bug","Sprint 53","2015-07-02T14:27:26.000+0000","mbogoevici","","On specific shutdown scenarios, the stream resumes from the start of the bus topic","https://github.com/spring-projects/spring-xd/issues/1727",2.0,0.8124037456373907,0.1417130363494565,1.987777083857828e-05,True,True,True,1435847246000.0,1445615167000.0,1445618767000.0,1447869827000.0,1447870727000.0,1435847246000.0,1437550877000.0,1435847485000.0,1435851085000.0,1437551130000.0,1437554730000.0,1445615167000.0,1445618767000.0,"mbogoevici",1435851085000.0,1435847246000.0,45,1446478241000.0,1437550877000.0,"On specific shutdown scenarios, the stream resumes from the start of the bus topic","https://github.com/spring-projects/spring-xd/issues/1727",5.0,True,5.0,3,-60.0,-1,0,False
"XD-3214","Bug","Sprint 53","2015-07-02T05:21:13.000+0000","kdowbecki","hillert","Enabling security breaks Jobs page in Admin UI","After enabling Spring XD security in {{XD_HOME/config/servers.yml}}:

{code}
spring:
  profiles: admin
security:
  basic:
    enabled: true
    realm: SpringXD
xd:
  security:
    authentication:
      file:
        enabled: true
        users:
          user: password, ROLE_VIEW
          admin: password, ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN
{code}

after logging in as {{user}} with only {{ROLE_VIEW}} privilege, Jobs admin page is broken and is not displaying data. 403 error code is returned for following URLs:

{code}
http://localhost:9393/jobs/configurations.json?page=0&size=10
http://localhost:9393/jobs/definitions.json?page=0&size=10
{code}

Looks like {{/jobs/configurations.\*}} and {{/jobs/definitions.\*}} URLs are not covered in security section of applications.yml file.",2.0,0.0703835332444839,0.037020583444395,0.4140868608660701,True,True,True,1435814473000.0,1435847744000.0,1435851344000.0,1436286283000.0,1436287183000.0,1435814473000.0,-1.0,1436010216000.0,1436013816000.0,1435831973000.0,1435835573000.0,1435847744000.0,1435851344000.0,"kdowbecki",1436013816000.0,1435814473000.0,45,1435814473000.0,-1.0,"Enabling security breaks Jobs page in Admin UI","After enabling Spring XD security in {{XD_HOME/config/servers.yml}}:

{code}
spring:
  profiles: admin
security:
  basic:
    enabled: true
    realm: SpringXD
xd:
  security:
    authentication:
      file:
        enabled: true
        users:
          user: password, ROLE_VIEW
          admin: password, ROLE_VIEW, ROLE_CREATE, ROLE_ADMIN
{code}

after logging in as {{user}} with only {{ROLE_VIEW}} privilege, Jobs admin page is broken and is not displaying data. 403 error code is returned for following URLs:

{code}
http://localhost:9393/jobs/configurations.json?page=0&size=10
http://localhost:9393/jobs/definitions.json?page=0&size=10
{code}

Looks like {{/jobs/configurations.\*}} and {{/jobs/definitions.\*}} URLs are not covered in security section of applications.yml file.",2.0,False,2.0,0,0.0,-1,2,False
"XD-3208","Bug","Sprint 53","2015-06-24T10:07:01.000+0000","macquistapace","","Change in file source breaks backward compatibility ","With version 1.2.0 the option ref of the file source was removed and a new option mode was introduced.  see XD-2850 and PR  https://github.com/spring-projects/spring-xd/pull/1624.

This means you have to destroy all streams using the ref option before you do an upgrade.

It would have been much better to leave the ref option in the code and emit a deprecation warning if it is still used. This way an upgrade would be possible without interruption.




",1.0,-1.0,-1.0,0.0013172208851139,False,True,False,1435140421000.0,-1.0,-1.0,1438078284000.0,1438079184000.0,1435140421000.0,-1.0,1435144292000.0,1435147892000.0,-1.0,-1.0,-1.0,-1.0,"macquistapace",1435147892000.0,1435140421000.0,45,1435140421000.0,-1.0,"Change in file source breaks backward compatibility ","With version 1.2.0 the option ref of the file source was removed and a new option mode was introduced.  see XD-2850 and PR  https://github.com/spring-projects/spring-xd/pull/1624.

This means you have to destroy all streams using the ref option before you do an upgrade.

It would have been much better to leave the ref option in the code and emit a deprecation warning if it is still used. This way an upgrade would be possible without interruption.




",1.0,False,1.0,0,0.0,0,0,False
"XD-3206","Bug","Sprint 53","2015-06-24T01:28:42.000+0000","fmarchand","","An error message occurs about the shortDescription (header-enricher)","Here is an error I got using the header-enricher from spring-xd-modules :


{code:java}
Field error in object 'info' on field 'shortDescription': rejected value [A Header Enricher to set message headers in a stream]; codes [Pattern.info.shortDescription,Pattern.shortDescription,Pattern.java.lang.String,Pattern]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [info.shortDescription,shortDescription]; arguments []; default message [shortDescription],[Ljavax.validation.constraints.Pattern$Flag;@11eeec65,^\p{IsUppercase}.*\.$]; default message [Short description must start with a capital letter and end with a dot]
{code}

And if I look the config properties, indeed, short description doesn't end with a dot.
{code:java}
info.shortDescription = A Header Enricher to set message headers in a stream
{code}",1.0,0.9999139609863956,0.9999139609863956,0.8336320028124581,True,True,True,1435109322000.0,1436190134000.0,1436190227000.0,1436189327000.0,1436190227000.0,1435109322000.0,1436013602000.0,1436010399000.0,1436013999000.0,1436190134000.0,1436190227000.0,1436190134000.0,1436190227000.0,"fmarchand",1436013999000.0,1436013602000.0,45,1436013602000.0,-1.0,"An error message occurs about the shortDescription (header-enricher)","Here is an error I got using the header-enricher from spring-xd-modules :


{code:java}
Field error in object 'info' on field 'shortDescription': rejected value [A Header Enricher to set message headers in a stream]; codes [Pattern.info.shortDescription,Pattern.shortDescription,Pattern.java.lang.String,Pattern]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [info.shortDescription,shortDescription]; arguments []; default message [shortDescription],[Ljavax.validation.constraints.Pattern$Flag;@11eeec65,^\p{IsUppercase}.*\.$]; default message [Short description must start with a capital letter and end with a dot]
{code}

And if I look the config properties, indeed, short description doesn't end with a dot.
{code:java}
info.shortDescription = A Header Enricher to set message headers in a stream
{code}",1.0,False,1.0,0,0.0,0,1,False
"XD-3205","Story","Sprint 52","2015-06-23T08:51:51.000+0000","sabby","thomas.risberg","Investigate the steps to Ambari upgrade Spring XD","As a user, I'd like to upgrade Spring XD from 1.2 RC to 1.2 GA using the Ambari plugin, so I can work on the latest release bits. I'd like to refer to the documentation to do so.",1.0,0.1418105178643115,0.0,0.0,True,True,True,1435049511000.0,1435050924000.0,1435054524000.0,1435058575000.0,1435059475000.0,1435049511000.0,-1.0,1435049511000.0,1435053111000.0,1435049511000.0,1435053111000.0,1435050924000.0,1435054524000.0,"sabby",1435053111000.0,1435049511000.0,45,1435049511000.0,-1.0,"Investigate the steps to Ambari upgrade Spring XD","As a user, I'd like to upgrade Spring XD from 1.2 RC to 1.2 GA using the Ambari plugin, so I can work on the latest release bits. I'd like to refer to the documentation to do so.",1.0,False,1.0,0,0.0,0,0,False
"XD-3204","Story","Sprint 52","2015-06-23T08:48:21.000+0000","sabby","","Review spring-bus design specs","As a spring-bus lead, I'd like to review the current spring-bus architecture and the design specs, so I can address any foundation level gaps.",5.0,0.3280031800580894,0.0117055093490405,0.0,True,True,True,1435049301000.0,1435307573000.0,1435311173000.0,1435835808000.0,1435836708000.0,1435049301000.0,-1.0,1435049301000.0,1435052901000.0,1435058518000.0,1435062118000.0,1435307573000.0,1435311173000.0,"sabby",1435052901000.0,1435049301000.0,45,1435049301000.0,-1.0,"Review spring-bus design specs","As a spring-bus lead, I'd like to review the current spring-bus architecture and the design specs, so I can address any foundation level gaps.",5.0,False,5.0,0,0.0,-1,0,False
"XD-3202","Story","Sprint 52","2015-06-23T08:45:37.000+0000","sabby","","Investigate performance of channel metrics in SI 4.2 ","As a developer, I'd like to investigate channel performance issues in SI 4.2, so I can determine the bottlenecks and take corrective actions to improve overall channel performance. ",8.0,0.4321866277858779,0.99906186211206,0.0,True,True,True,1435049137000.0,1435308503000.0,1435312103000.0,1435648362000.0,1435649262000.0,1435049137000.0,1435648650000.0,1435049137000.0,1435052737000.0,1435648699000.0,1435649262000.0,1435308503000.0,1435312103000.0,"sabby",1435052737000.0,1435049137000.0,45,1435648650000.0,1435648650000.0,"Investigate performance of channel metrics in SI 4.2 ","As a developer, I'd like to investigate channel performance issues in SI 4.2, so I can determine the bottlenecks and take corrective actions to improve overall channel performance. ",5.0,True,5.0,1,60.0,0,0,False
"XD-3198","Story","Sprint 52","2015-06-23T06:19:55.000+0000","sabby","","Spike: Investigate the use of config server for spring-bus modules ","As a developer, I'd like to use spring-cloud-config server for spring-bus modules, so I can centrally manage external properties.",8.0,0.6067130588587423,0.5345266547079973,1.0264364876000311e-05,True,True,True,1435040395000.0,1436518112000.0,1436521712000.0,1437475106000.0,1437476006000.0,1435040395000.0,-1.0,1435040420000.0,1435044020000.0,1436342294000.0,1436345894000.0,1436518112000.0,1436521712000.0,"sabby",1435044020000.0,1435040395000.0,45,1435040395000.0,-1.0,"Spike: Investigate the use of config server for spring-cloud-stream modules ","As a developer, I'd like to use spring-cloud-config server for spring-bus modules, so I can centrally manage external properties.",8.0,False,8.0,0,0.0,0,0,True
"XD-3196","Story","Sprint 52","2015-06-22T13:30:21.000+0000","sabby","","Move CI builds to EC2 based infrastructure","As a developer, I'd like to migrate the current CI builds to EC2 instances, so I can manage them all in one-place reliably.",8.0,0.1207972246065362,0.1207972246065362,0.0001534442389866,True,True,True,1434979821000.0,1435134907000.0,1435138507000.0,1436262775000.0,1436263675000.0,1434979821000.0,-1.0,1434980018000.0,1434983618000.0,1435134907000.0,1435138507000.0,1435134907000.0,1435138507000.0,"sabby",1434983618000.0,1434979821000.0,45,1434979821000.0,-1.0,"Move MASTER branch CI builds to EC2 based infrastructure","As a developer, I'd like to migrate the current MASTER branch CI builds to EC2 instances, so I can manage them all in one-place reliably.",8.0,False,8.0,0,0.0,0,0,True
"XD-3194","Story","Sprint 52","2015-06-22T13:07:59.000+0000","sabby","","Spike: Investigate spring-cloud-config and XD fit","As a developer, I'd like to have a central place to manage external properties for applications across all the environments, so I can provide server and client-side support for externalized configuration for XD-Admin and XD-Container servers. ",8.0,0.636184678961844,0.6361823520925536,0.0,True,True,True,1434978479000.0,1436345519000.0,1436349119000.0,1437126389000.0,1437127289000.0,1434978479000.0,-1.0,1434978479000.0,1434982079000.0,1436345514000.0,1436349114000.0,1436345519000.0,1436349119000.0,"sabby",1434982079000.0,1434978479000.0,45,1434978479000.0,-1.0,"Spike: Investigate spring-cloud-config and the XD fit","As a developer, I'd like to have a central place to manage external properties for applications across all the environments, so I can provide server and client-side support for externalized configuration for XD-Admin and XD-Container servers. ",8.0,False,8.0,0,0.0,0,0,True
"XD-3193","Story","Sprint 52","2015-06-22T13:01:29.000+0000","sabby","","Spike: Investigate bootification of module options","As a developer, I'd like to handle module options via pure boot property source management, so I can leverage Boot's module [METADATA|http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#configuration-metadata] option to inject module options as opposed to maintaining them in core Spring XD runtime CP.
",8.0,0.2577423248130678,0.2577423248130678,0.0,True,True,True,1434978089000.0,1435308243000.0,1435311843000.0,1436258135000.0,1436259035000.0,1434978089000.0,-1.0,1434978089000.0,1434981689000.0,1435308243000.0,1435311843000.0,1435308243000.0,1435311843000.0,"sabby",1434981689000.0,1434978089000.0,45,1434978089000.0,-1.0,"Spike: Investigate bootification of module options","As a developer, I'd like to handle module options via pure boot property source management, so I can leverage Boot's module [METADATA|http://docs.spring.io/spring-boot/docs/current-SNAPSHOT/reference/htmlsingle/#configuration-metadata] option to inject module options as opposed to maintaining them in core Spring XD runtime CP.
",8.0,False,8.0,0,0.0,0,0,False
"XD-3192","Story","Sprint 52","2015-06-22T12:49:11.000+0000","sabby","","Spike: Investigate Boot export metrics and the XD fit","As a user, I'd like to have the module/app specific metrics consumed directly from Boot actuator [export()|https://github.com/spring-projects/spring-boot/blob/master/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/autoconfigure/MetricRepositoryAutoConfiguration.java] API, so I can have insight on how it is performing, being used and that it works etc.

",8.0,0.252951855703002,0.4613392026897654,0.0,True,True,True,1434977351000.0,1435308681000.0,1435312281000.0,1436286305000.0,1436287205000.0,1434977351000.0,-1.0,1434977351000.0,1434980951000.0,1435581638000.0,1435585238000.0,1435308681000.0,1435312281000.0,"sabby",1434980951000.0,1434977351000.0,45,1434977351000.0,-1.0,"Spike: Investigate Boot export metrics and the XD fit","As a user, I'd like to have the module/app specific metrics consumed directly from Boot actuator [export()|https://github.com/spring-projects/spring-boot/blob/master/spring-boot-actuator/src/main/java/org/springframework/boot/actuate/autoconfigure/MetricRepositoryAutoConfiguration.java] API, so I can have insight on how it is performing, being used and that it works etc.

",8.0,False,8.0,0,0.0,0,0,False
"XD-3189","Story","Sprint 51","2015-06-18T12:42:27.000+0000","grenfro","grenfro","Testers need ability to wait for a file to be created in XD directory","User's need ability to wait for user specified time in millis for a file to be created in the XD directory.  If file is not created in allotted time then return false else return true.  Also check to see if a file exists in the XD directory.  ",3.0,5.265211816374189e-05,4.026338447815557e-05,0.2237126557108965,True,True,True,1434631347000.0,1434631364000.0,1434634964000.0,1434953321000.0,1434954221000.0,1434631347000.0,-1.0,1434703578000.0,1434707178000.0,1434631360000.0,1434634960000.0,1434631364000.0,1434634964000.0,"grenfro",1434707178000.0,1434631347000.0,45,1434631347000.0,-1.0,"Testers need ability to wait for a file to be created in XD directory","User's need ability to wait for user specified time in millis for a file to be created in the XD directory.  If file is not created in allotted time then return false else return true.  Also check to see if a file exists in the XD directory.  ",3.0,False,3.0,0,0.0,-1,0,False
"XD-3188","Bug","Sprint 52","2015-06-18T08:00:46.000+0000","mminella","mminella","fileDeletionListener resolves resources once","In the {{filejdbc}} job, there is the option to delete the imported files.  This functionality is created using a listener called the {{FileDeletionStepExecutionListener}}.  When you run the job the first time with the {{--deleteFiles=true}}, everything works as expected.  The second time you run the job, the files are not deleted.

I believe the issue here is that since the {{FileDeletionStepExecutionListener}} is a singleton, the resources are resolved only once (the first time the job runs) and so it works the first time, but if the job is run again later and new files match the expression, they are not picked up.  I believe the fix is to make the {{FileDeletionStepExecutionListener}} used in this job step scoped.",1.0,0.0281594635303351,0.0,0.7525175870407027,True,True,True,1434614446000.0,1434633964000.0,1434637564000.0,1435306670000.0,1435307570000.0,1434614446000.0,-1.0,1435136034000.0,1435139634000.0,1434614446000.0,1434618046000.0,1434633964000.0,1434637564000.0,"mminella",1435139634000.0,1434614446000.0,45,1434614446000.0,-1.0,"FileDeletionListener resolves resources once","In the {{filejdbc}} job, there is the option to delete the imported files.  This functionality is created using a listener called the {{FileDeletionStepExecutionListener}}.  When you run the job the first time with the {{--deleteFiles=true}}, everything works as expected.  The second time you run the job, the files are not deleted.

I believe the issue here is that since the {{FileDeletionStepExecutionListener}} is a singleton, the resources are resolved only once (the first time the job runs) and so it works the first time, but if the job is run again later and new files match the expression, they are not picked up.  I believe the fix is to make the {{FileDeletionStepExecutionListener}} used in this job step scoped.",1.0,False,1.0,0,0.0,0,0,True
"XD-3187","Bug","","2015-06-17T12:33:00.000+0000","iperumal","iperumal","XD admin leader should cleanup deployment after initializing the container path cache","The admin leader starts cleaning up the deployments for the container(s) that is/are no longer connected to the ZK. This clean up needs to happen after the container path cache is started by the admin leader. ",1.0,0.0050136736554238,0.0,-1.0,True,False,True,1434544380000.0,1434544391000.0,1434546574000.0,1434545674000.0,1434546574000.0,1434544380000.0,-1.0,-1.0,-1.0,1434544380000.0,1434546574000.0,1434544391000.0,1434546574000.0,"iperumal",-1.0,-1.0,0,1434544380000.0,-1.0,"XD admin leader should cleanup deployment after initializing the container path cache","The admin leader starts cleaning up the deployments for the container(s) that is/are no longer connected to the ZK. This clean up needs to happen after the container path cache is started by the admin leader. ",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-3184","Story","Sprint 51","2015-06-16T05:59:32.000+0000","thomas.risberg","thomas.risberg","Update spring-xd-yarn servers.yml with settings for HDP 2.2.6.0","We need to add the settings needed to run XD on YARN when using Hortonworks HDP 2.2.6.0 which is the version you now get when installing with Ambari.",1.0,0.0082457074889996,0.0082457074889996,0.4945253610233434,True,True,True,1434434372000.0,1434438892000.0,1434442492000.0,1434981636000.0,1434982536000.0,1434434372000.0,-1.0,1434705453000.0,1434709053000.0,1434438892000.0,1434442492000.0,1434438892000.0,1434442492000.0,"thomas.risberg",1434709053000.0,1434434372000.0,45,1434434372000.0,-1.0,"Update spring-xd-yarn servers.yml with settings for HDP 2.2.6.0","We need to add the settings needed to run XD on YARN when using Hortonworks HDP 2.2.6.0 which is the version you now get when installing with Ambari.",1.0,False,1.0,0,0.0,-1,1,False
"XD-3183","Story","Sprint 52","2015-06-16T04:08:11.000+0000","pharris","","Upgrade to Spring Boot 1.2.5","Spring Boot 1.2.4 (and earlier) does not allow for the retrieval of boolean values from the vcap environment. This means that when XD Admin tries to retrieve the value of (for example) {{vcap.services.rabbitmq.credentials.protocols.amqp.ssl}} it will fail, as that value returns a boolean.

Spring Boot 1.2.5 (as yet unreleased) contains a fix for this issue (https://github.com/spring-projects/spring-boot/pull/3237)",1.0,0.9321614006330994,0.9321614006330994,0.0002287162822168,True,True,True,1434427691000.0,1436775250000.0,1436778850000.0,1436945195000.0,1436946095000.0,1434427691000.0,-1.0,1434428267000.0,1434431867000.0,1436775250000.0,1436778850000.0,1436775250000.0,1436778850000.0,"pharris",1434431867000.0,1434427691000.0,45,1434427691000.0,-1.0,"Upgrade to Spring Boot 1.2.5","Spring Boot 1.2.4 (and earlier) does not allow for the retrieval of boolean values from the vcap environment. This means that when XD Admin tries to retrieve the value of (for example) {{vcap.services.rabbitmq.credentials.protocols.amqp.ssl}} it will fail, as that value returns a boolean.

Spring Boot 1.2.5 (as yet unreleased) contains a fix for this issue (https://github.com/spring-projects/spring-boot/pull/3237)",1.0,False,1.0,0,0.0,0,1,False
"XD-3178","Bug","Sprint 51","2015-06-12T13:31:35.000+0000","thomas.risberg","thomas.risberg","Hadoop Distro log message shows wrong version when set via env var","If we export HADOOP_DISTRO env var instead of using --hadoopDistro parameter then the logging message is wrong, it always says

Hadoop Distro: hadoop26

even if we set HADOOP_DISTRO to something else

The classpath is built correctly. Maybe we should just remove this logging message since we log the actual version used in the next log message.
",1.0,0.3648226059944472,0.3648226059944472,0.6802661429352805,True,True,True,1434115895000.0,1434432051000.0,1434435651000.0,1434981597000.0,1434982497000.0,1434115895000.0,-1.0,1434705415000.0,1434709015000.0,1434432051000.0,1434435651000.0,1434432051000.0,1434435651000.0,"thomas.risberg",1434709015000.0,1434115895000.0,45,1434115895000.0,-1.0,"Hadoop Distro log message shows wrong version when set via env var","If we export HADOOP_DISTRO env var instead of using --hadoopDistro parameter then the logging message is wrong, it always says

Hadoop Distro: hadoop26

even if we set HADOOP_DISTRO to something else

The classpath is built correctly. Maybe we should just remove this logging message since we log the actual version used in the next log message.
",1.0,False,1.0,0,0.0,-1,0,False
"XD-3177","Story","Sprint 52","2015-06-10T12:13:51.000+0000","grussell","grussell","Make RabbitMessageBus RabbitMQ Config Properties Optional","When the bus is used outside of the XD container (e.g. spring-bus), the inheritance from Spring Boot configuration is broken (no application.yml or servers.yml on the cp).

Make the bus properties optional (Add "":"")",1.0,0.2989302212496961,0.2918794067590566,0.1821055190858254,True,True,True,1433938431000.0,1433940890000.0,1433944490000.0,1433945757000.0,1433946657000.0,1433938431000.0,-1.0,1433939929000.0,1433943529000.0,1433940832000.0,1433944432000.0,1433940890000.0,1433944490000.0,"grussell",1433943529000.0,1433938431000.0,45,1433938431000.0,-1.0,"Make RabbitMessageBus RabbitMQ Config Properties Optional","When the bus is used outside of the XD container (e.g. spring-bus), the inheritance from Spring Boot configuration is broken (no application.yml or servers.yml on the cp).

Make the bus properties optional (Add "":"")",1.0,False,1.0,0,0.0,-1,0,False
"XD-3176","Bug","Sprint 51","2015-06-10T11:59:23.000+0000","thomas.risberg","","Using HDFS for custom module home doesn't work with Kerberized Hadoop cluster","I tried setting the xd.customModule.home property to point to a Kerberized Hadoop cluster with all usual security config settings provided. It failed with the following exception:

{code}
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1139) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1042) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:755) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95) [spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79) [spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	... 22 common frames omitted
Caused by: org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.7.0_67]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[na:1.7.0_67]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.7.0_67]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526) ~[na:1.7.0_67]
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2755) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2724) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:870) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:866) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:866) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:859) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1817) ~[hadoop-common-2.6.0.jar:na]
	at org.springframework.xd.dirt.module.ExtendedResource$HdfsExtendedResource.mkdirs(ExtendedResource.java:127) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:79) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1633) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1570) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	... 25 common frames omitted
Caused by: org.apache.hadoop.ipc.RemoteException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]
	at org.apache.hadoop.ipc.Client.call(Client.java:1468) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.ipc.Client.call(Client.java:1399) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) ~[hadoop-common-2.6.0.jar:na]
	at com.sun.proxy.$Proxy79.mkdirs(Unknown Source) ~[na:na]
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:539) ~[hadoop-hdfs-2.6.0.jar:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_67]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_67]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) ~[hadoop-common-2.6.0.jar:na]
	at com.sun.proxy.$Proxy80.mkdirs(Unknown Source) ~[na:na]
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2753) ~[hadoop-hdfs-2.6.0.jar:na]
	... 37 common frames omitted
2015-06-10T14:49:20-0400 1.2.0.SNAP ERROR main boot.SpringApplication - Application startup failed
{code}",3.0,0.2629577329182579,0.2629577329182579,0.0439680349176802,True,True,True,1433937563000.0,1433953950000.0,1433957550000.0,1433998981000.0,1433999881000.0,1433937563000.0,-1.0,1433940303000.0,1433943903000.0,1433953950000.0,1433957550000.0,1433953950000.0,1433957550000.0,"thomas.risberg",1433943903000.0,1433937563000.0,45,1433937563000.0,-1.0,"Using HDFS for custom module home doesn't work with Kerberized Hadoop cluster","I tried setting the xd.customModule.home property to point to a Kerberized Hadoop cluster with all usual security config settings provided. It failed with the following exception:

{code}
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1139) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1042) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:755) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480) ~[spring-context-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:686) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129) [spring-boot-1.2.3.RELEASE.jar:1.2.3.RELEASE]
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.run(AdminServerApplication.java:95) [spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.server.admin.AdminServerApplication.main(AdminServerApplication.java:79) [spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#19f459aa' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	... 22 common frames omitted
Caused by: org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.7.0_67]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) ~[na:1.7.0_67]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.7.0_67]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526) ~[na:1.7.0_67]
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2755) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2724) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:870) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:866) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:866) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:859) ~[hadoop-hdfs-2.6.0.jar:na]
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1817) ~[hadoop-common-2.6.0.jar:na]
	at org.springframework.xd.dirt.module.ExtendedResource$HdfsExtendedResource.mkdirs(ExtendedResource.java:127) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
	at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:79) ~[spring-xd-dirt-1.2.0.BUILD-SNAPSHOT.jar:1.2.0.BUILD-SNAPSHOT]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1633) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1570) ~[spring-beans-4.1.6.RELEASE.jar:4.1.6.RELEASE]
	... 25 common frames omitted
Caused by: org.apache.hadoop.ipc.RemoteException: SIMPLE authentication is not enabled.  Available:[TOKEN, KERBEROS]
	at org.apache.hadoop.ipc.Client.call(Client.java:1468) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.ipc.Client.call(Client.java:1399) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232) ~[hadoop-common-2.6.0.jar:na]
	at com.sun.proxy.$Proxy79.mkdirs(Unknown Source) ~[na:na]
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:539) ~[hadoop-hdfs-2.6.0.jar:na]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.7.0_67]
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) ~[na:1.7.0_67]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:1.7.0_67]
	at java.lang.reflect.Method.invoke(Method.java:606) ~[na:1.7.0_67]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187) ~[hadoop-common-2.6.0.jar:na]
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102) ~[hadoop-common-2.6.0.jar:na]
	at com.sun.proxy.$Proxy80.mkdirs(Unknown Source) ~[na:na]
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2753) ~[hadoop-hdfs-2.6.0.jar:na]
	... 37 common frames omitted
2015-06-10T14:49:20-0400 1.2.0.SNAP ERROR main boot.SpringApplication - Application startup failed
{code}",3.0,False,3.0,0,0.0,-1,0,False
"XD-3172","Story","Sprint 51","2015-06-10T02:53:26.000+0000","hillert","hillert","Provide a deployment property to enable the SOF/EOF markers when splitting a file into lines","Depends on INT-3727",2.0,0.1639272052857308,0.0003013793902863,0.454387388431668,True,True,True,1433904806000.0,1433911877000.0,1433915477000.0,1433947041000.0,1433947941000.0,1433904806000.0,-1.0,1433924406000.0,1433928006000.0,1433904819000.0,1433908419000.0,1433911877000.0,1433915477000.0,"hillert",1433928006000.0,1433904806000.0,45,1433904806000.0,-1.0,"Provide a source option to enable the SOF/EOF markers when splitting a file into lines","Depends on INT-3727",2.0,False,2.0,0,0.0,-1,0,True
"XD-3171","Bug","","2015-06-09T16:21:58.000+0000","iperumal","","Composed modules not working on YARN","From https://github.com/spring-projects/spring-xd/issues/1704

I am trying to use composed modules when running on YARN.  

In ZK, each child module definition of the composed module gets serialized as follows:

```
{""@class"":""org.springframework.xd.module.SimpleModuleDefinition"",""name"":""transform"",""type"":""processor"",""location"":""file:/tmp/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1433789137218_0001/filecache/17/spring-xd-yarn-1.1.2.RELEASE.zip/modules/processor/transform/""}
```

When I try to use the composed module on YARN, it may be deployed to a different container where the ""location"" file path is not valid.  In this case I get the following exception:

```
java.lang.IllegalArgumentException: File must exist
	at org.springframework.boot.loader.data.RandomAccessDataFile.<init>(RandomAccessDataFile.java:67)
	at org.springframework.boot.loader.data.RandomAccessDataFile.<init>(RandomAccessDataFile.java:51)
	at org.springframework.boot.loader.jar.JarFile.<init>(JarFile.java:95)
	at org.springframework.boot.loader.archive.JarFileArchive.<init>(JarFileArchive.java:61)
	at org.springframework.boot.loader.archive.JarFileArchive.<init>(JarFileArchive.java:57)
	at org.springframework.xd.module.support.ModuleUtils.createModuleClassLoader(ModuleUtils.java:54)
	at org.springframework.xd.module.support.ModuleUtils.createModuleClassLoader(ModuleUtils.java:47)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:186)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:164)
	at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)
	at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveComposedModuleMetadata(DefaultModuleOptionsMetadataResolver.java:175)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:167)
	at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)
	at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)
	at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:174)
```

I think the issue may be related to the following line in ArchiveModuleRegistry:

```
String filename = resource.getFile().getCanonicalFile().getName();
```

",5.0,0.9502202869510488,0.9502202869510488,-1.0,True,False,True,1433866918000.0,1434014873000.0,1434018473000.0,1434021724000.0,1434022624000.0,1433866918000.0,-1.0,-1.0,-1.0,1434014873000.0,1434018473000.0,1434014873000.0,1434018473000.0,"iperumal",-1.0,-1.0,0,1433866918000.0,-1.0,"Composed modules not working on YARN","From https://github.com/spring-projects/spring-xd/issues/1704

I am trying to use composed modules when running on YARN.  

In ZK, each child module definition of the composed module gets serialized as follows:

```
{""@class"":""org.springframework.xd.module.SimpleModuleDefinition"",""name"":""transform"",""type"":""processor"",""location"":""file:/tmp/hadoop-hduser/nm-local-dir/usercache/hduser/appcache/application_1433789137218_0001/filecache/17/spring-xd-yarn-1.1.2.RELEASE.zip/modules/processor/transform/""}
```

When I try to use the composed module on YARN, it may be deployed to a different container where the ""location"" file path is not valid.  In this case I get the following exception:

```
java.lang.IllegalArgumentException: File must exist
	at org.springframework.boot.loader.data.RandomAccessDataFile.<init>(RandomAccessDataFile.java:67)
	at org.springframework.boot.loader.data.RandomAccessDataFile.<init>(RandomAccessDataFile.java:51)
	at org.springframework.boot.loader.jar.JarFile.<init>(JarFile.java:95)
	at org.springframework.boot.loader.archive.JarFileArchive.<init>(JarFileArchive.java:61)
	at org.springframework.boot.loader.archive.JarFileArchive.<init>(JarFileArchive.java:57)
	at org.springframework.xd.module.support.ModuleUtils.createModuleClassLoader(ModuleUtils.java:54)
	at org.springframework.xd.module.support.ModuleUtils.createModuleClassLoader(ModuleUtils.java:47)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:186)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:164)
	at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)
	at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveComposedModuleMetadata(DefaultModuleOptionsMetadataResolver.java:175)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:167)
	at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)
	at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)
	at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:174)
```

I think the issue may be related to the following line in ArchiveModuleRegistry:

```
String filename = resource.getFile().getCanonicalFile().getName();
```

",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-3170","Story","Sprint 51","2015-06-09T15:15:35.000+0000","iperumal","iperumal","Update Spark streaming documentation","As a user I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated.
The documentation also needs some more information on `Reliable` receiver.",1.0,0.0339805825242718,0.0,1.4077669902912622,False,False,False,1433862935000.0,1433862942000.0,1433863141000.0,1433862935000.0,1433863141000.0,1433862935000.0,-1.0,1433863225000.0,1433863141000.0,1433862935000.0,1433863141000.0,1433862942000.0,1433863141000.0,"iperumal",1433863141000.0,1433862935000.0,45,1433862935000.0,-1.0,"Update Spark streaming documentation","As a user I need to know the Spark streaming features like adding tap at the spark module output and the examples need to be updated.
The documentation also needs some more information on `Reliable` receiver.",1.0,False,1.0,0,0.0,0,0,False
"XD-3169","Story","Sprint 51","2015-06-09T08:42:07.000+0000","sabby","mminella","Spike: Explore options for batch modules to be short lived","As a developer, I'd like a job module to be bootstrapped when the job is launched and shut down once it is complete instead of the current behavior of bootstrapping the context when the module is deployed regardless of if it's being used so that I can achieve better resource utilization.",8.0,0.4318491741271341,0.0005165612125036,0.0,True,True,True,1433839327000.0,1434702923000.0,1434706523000.0,1435838190000.0,1435839090000.0,1433839327000.0,-1.0,1433839327000.0,1433842927000.0,1433840360000.0,1433843960000.0,1434702923000.0,1434706523000.0,"sabby",1433842927000.0,1433839327000.0,45,1433839327000.0,-1.0,"Spike: Explore options for batch modules to be short lived","As a developer, I'd like a job module to be bootstrapped when the job is launched and shut down once it is complete instead of the current behavior of bootstrapping the context when the module is deployed regardless of if it's being used so that I can achieve better resource utilization.",8.0,False,8.0,0,0.0,0,1,False
"XD-3166","Story","Sprint 51","2015-06-08T19:21:47.000+0000","sabby","","Publish performance benchmarks","As a developer, I'd like to publish performance benchmarks along with the infrastructure specifics, so the users can use it as a reference while setting up Spring XD cluster.
",8.0,0.2127830698812554,0.2127776078478496,0.0,True,True,True,1433791307000.0,1433947134000.0,1433950734000.0,1434522735000.0,1434523635000.0,1433791307000.0,1433791337000.0,1433791307000.0,1433794907000.0,1433947130000.0,1433950730000.0,1433947134000.0,1433950734000.0,"sabby",1433794907000.0,1433791337000.0,45,1433791337000.0,-1.0,"Publish performance benchmarks","As a developer, I'd like to publish performance benchmarks along with the infrastructure specifics, so the users can use it as a reference while setting up Spring XD cluster.
",8.0,False,8.0,0,0.0,0,0,False
"XD-3165","Story","Sprint 51","2015-06-08T19:18:54.000+0000","sabby","","Synchronize XD and XD + Ambari RPMs into a single repo","As a PM, I'd like to have XD and XD + Ambari RPM scripts into a single public repo, so that users can go to a single location to use the respective build scripts.",3.0,-1.0,-1.0,0.0,False,True,False,1433791134000.0,-1.0,-1.0,1436012499000.0,1436013399000.0,1433791134000.0,-1.0,1433791134000.0,1433794734000.0,-1.0,-1.0,-1.0,-1.0,"sabby",1433794734000.0,1433791134000.0,45,1433791134000.0,-1.0,"Synchronize XD and XD + Ambari RPMs into a single repo","As a PM, I'd like to have XD and XD + Ambari RPM scripts into a single public repo, so that users can go to a single location to use the respective build scripts.",3.0,False,3.0,0,0.0,0,0,False
"XD-3164","Story","Sprint 51","2015-06-08T19:04:52.000+0000","mbogoevici","","Kafka bus defaults configurable at producer/consumer level","As a developer, I want to be able to override Kafka bus defaults for module consumers and producers, so that I can finely tune performance and behaviour. 

Such properties should include
- autoCommitEnabled,queueSize,maxWait,fetchSize for consumers
- batchSize,batchTimeout for producers",3.0,0.4880049884057202,0.4880049884057202,0.0002523284786474,True,True,True,1433790292000.0,1434360824000.0,1434364424000.0,1434958503000.0,1434959403000.0,1433790292000.0,-1.0,1433790587000.0,1433794187000.0,1434360824000.0,1434364424000.0,1434360824000.0,1434364424000.0,"mbogoevici",1433794187000.0,1433790292000.0,45,1433790292000.0,-1.0,"Kafka bus defaults configurable at producer/consumer level","As a developer, I want to be able to override Kafka bus defaults for module consumers and producers, so that I can finely tune performance and behaviour. 

Such properties should include
- autoCommitEnabled,queueSize,maxWait,fetchSize for consumers
- batchSize,batchTimeout for producers",3.0,False,3.0,0,0.0,-1,0,False
"XD-3162","Story","Sprint 51","2015-06-08T13:55:49.000+0000","grenfro","grenfro","Update Master Environment for 1.3 CI Acceptance Tests","",3.0,0.302034853713133,8.434876527465765e-05,0.0001265231479119,True,True,True,1433771749000.0,1434523714000.0,1434527314000.0,1436260512000.0,1436261412000.0,1433771749000.0,-1.0,1433772064000.0,1433775664000.0,1433771959000.0,1433775559000.0,1434523714000.0,1434527314000.0,"grenfro",1433775664000.0,1433771749000.0,45,1433771749000.0,-1.0,"Update Master Environment for 2.0 CI Acceptance Tests","",3.0,False,3.0,0,0.0,0,0,True
"XD-3161","Story","Sprint 51","2015-06-08T13:40:09.000+0000","grenfro","grenfro","Add CI Acceptance Test for 1.2.x","Need acceptance tests to run on the 1.2.X branch.  Needs to be setup as a child of the Publish 1.2.x",3.0,0.968364500225729,0.0012463327078089,0.0016141873563469,True,True,True,1433770809000.0,1434523694000.0,1434527294000.0,1434547390000.0,1434548290000.0,1433770809000.0,-1.0,1433772064000.0,1433775664000.0,1433771778000.0,1433775378000.0,1434523694000.0,1434527294000.0,"grenfro",1433775664000.0,1433770809000.0,45,1433770809000.0,-1.0,"Add CI Acceptance Test for 1.2.x","Need acceptance tests to run on the 1.2.X branch.  Needs to be setup as a child of the Publish 1.2.x",3.0,False,3.0,0,0.0,-1,0,False
"XD-3160","Story","Sprint 50","2015-06-08T08:36:53.000+0000","eric.bottard","eric.bottard","Reorganize OOTB module list in docs","Sort alphabetically, nest ""Available modules"" section appropriately. Optionally, move to a whole different ""PART"" in reference doc",2.0,0.8980927556867113,5.741253200748659e-05,0.0010965793613429,True,True,True,1433752613000.0,1433909041000.0,1433912641000.0,1433925891000.0,1433926791000.0,1433752613000.0,1433836431000.0,1433752804000.0,1433756404000.0,1433752623000.0,1433756223000.0,1433909041000.0,1433912641000.0,"eric.bottard",1433756404000.0,1433752613000.0,45,1433836431000.0,1433836431000.0,"Reorganize OOTB module list in docs","Sort alphabetically, nest ""Available modules"" section appropriately. Optionally, move to a whole different ""PART"" in reference doc",1.0,True,1.0,1,100.0,0,0,False
"XD-3154","Story","Sprint 50","2015-06-05T12:16:45.000+0000","sabby","","Update to Spring Hadoop 2.2.0 GA ","As a developer, I'd like to update to Spring Hadoop 2.2.0 GA release, so I can leverage the latest improvements. ",1.0,0.9254855975725204,0.9254855975725204,0.0,True,True,True,1433506605000.0,1433916224000.0,1433919824000.0,1433948304000.0,1433949204000.0,1433506605000.0,-1.0,1433506605000.0,1433510205000.0,1433916224000.0,1433919824000.0,1433916224000.0,1433919824000.0,"sabby",1433510205000.0,1433506605000.0,45,1433506605000.0,-1.0,"Update to Spring Hadoop 2.2.0 GA ","As a developer, I'd like to update to Spring Hadoop 2.2.0 GA release, so I can leverage the latest improvements. ",1.0,False,1.0,0,0.0,0,0,False
"XD-3153","Story","Sprint 50","2015-06-05T12:15:26.000+0000","sabby","","Update to Spring Integration Kafka 1.2.0 GA","As a developer, I'd like to update to SI Kafka extension 1.2.0, so I can leverage the latest performance improvements.",1.0,0.806993743195875,0.806993743195875,0.0,True,True,True,1433506526000.0,1433843805000.0,1433847405000.0,1433923571000.0,1433924471000.0,1433506526000.0,-1.0,1433506526000.0,1433510126000.0,1433843805000.0,1433847405000.0,1433843805000.0,1433847405000.0,"sabby",1433510126000.0,1433506526000.0,45,1433506526000.0,-1.0,"Update to Spring Integration Kafka 1.2.0 GA","As a developer, I'd like to update to SI Kafka extension 1.2.0, so I can leverage the latest performance improvements.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3152","Story","Sprint 50","2015-06-05T12:13:51.000+0000","sabby","","Update to Spring Integration 4.1.5 ","As a developer, I'd like to update to the 4.1.5 SI release, so I can pickup the latest improvements to message channels.",1.0,0.8070311827236804,0.8070311827236804,0.0,True,True,True,1433506431000.0,1433843812000.0,1433847412000.0,1433923583000.0,1433924483000.0,1433506431000.0,-1.0,1433506431000.0,1433510031000.0,1433843812000.0,1433847412000.0,1433843812000.0,1433847412000.0,"sabby",1433510031000.0,1433506431000.0,45,1433506431000.0,-1.0,"Update to Spring Integration 4.1.5 ","As a developer, I'd like to update to the 4.1.5 SI release, so I can pickup the latest improvements to message channels.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3151","Story","Sprint 50","2015-06-05T08:06:33.000+0000","dturanski","dturanski","Update Modules - Build and Package sections","Some info is obsolete and add more content re. dependency management",1.0,0.0119433632835838,6.380621472531425e-05,1.7401694925085702e-05,True,True,True,1433491593000.0,1433495711000.0,1433499311000.0,1433835487000.0,1433836387000.0,1433491593000.0,-1.0,1433491599000.0,1433495199000.0,1433491615000.0,1433495215000.0,1433495711000.0,1433499311000.0,"dturanski",1433495199000.0,1433491593000.0,45,1433491593000.0,-1.0,"Update Modules - Build and Package sections","Some info is obsolete and add more content re. dependency management",1.0,False,1.0,0,0.0,-1,0,False
"XD-3150","Bug","Sprint 50","2015-06-05T05:59:25.000+0000","thomas.risberg","jvalkeal","the 'filepollhdfs' job fails on second submission","Definitions:

>job create pollHdfs --definition ""filepollhdfs --names=name,age"" --deploy true

>stream create csvStream --definition ""file --mode=ref --dir=/Users/trisberg/Test/files --pattern=*.csv > queue:job:pollHdfs"" --deploy

Here is the exception:

{code}
org.springframework.data.hadoop.store.StoreException: Error while flushing stream; nested exception is java.nio.channels.ClosedChannelException
	at org.springframework.xd.batch.item.hadoop.HdfsTextItemWriter.update(HdfsTextItemWriter.java:135)
	at org.springframework.batch.item.support.CompositeItemStream.update(CompositeItemStream.java:74)
	at org.springframework.batch.core.step.tasklet.TaskletStep.doExecute(TaskletStep.java:250)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy54.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunching
{code}",3.0,0.6365362704139764,0.6365362704139764,0.6350930497531333,True,True,True,1433483965000.0,1433492345000.0,1433495945000.0,1433496230000.0,1433497130000.0,1433483965000.0,-1.0,1433492326000.0,1433495926000.0,1433492345000.0,1433495945000.0,1433492345000.0,1433495945000.0,"thomas.risberg",1433495926000.0,1433483965000.0,45,1433483965000.0,-1.0,"the 'filepollhdfs' job fails on second submission","Definitions:

>job create pollHdfs --definition ""filepollhdfs --names=name,age"" --deploy true

>stream create csvStream --definition ""file --mode=ref --dir=/Users/trisberg/Test/files --pattern=*.csv > queue:job:pollHdfs"" --deploy

Here is the exception:

{code}
org.springframework.data.hadoop.store.StoreException: Error while flushing stream; nested exception is java.nio.channels.ClosedChannelException
	at org.springframework.xd.batch.item.hadoop.HdfsTextItemWriter.update(HdfsTextItemWriter.java:135)
	at org.springframework.batch.item.support.CompositeItemStream.update(CompositeItemStream.java:74)
	at org.springframework.batch.core.step.tasklet.TaskletStep.doExecute(TaskletStep.java:250)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy54.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunching
{code}",3.0,False,3.0,0,0.0,-1,1,False
"XD-3149","Story","Sprint 50","2015-06-05T05:52:06.000+0000","thomas.risberg","","Batch job filepollhdfs docs are outdated","The stream definition example uses old style syntax, should be --mode=ref instead of --ref=true ",1.0,-1.0,-1.0,0.0546299026835741,False,True,False,1433483526000.0,-1.0,-1.0,1433496190000.0,1433497090000.0,1433483526000.0,-1.0,1433484267000.0,1433487867000.0,-1.0,-1.0,-1.0,-1.0,"thomas.risberg",1433487867000.0,1433483526000.0,45,1433483526000.0,-1.0,"Batch job filepollhdfs docs are outdated","The stream definition example uses old style syntax, should be --mode=ref instead of --ref=true ",1.0,False,1.0,0,0.0,-1,0,False
"XD-3148","Story","Sprint 50","2015-06-04T14:30:49.000+0000","thomas.risberg","thomas.risberg","Remove mr1 jar from cdh5 hadoop build","There is an hadoop-core-2.5.0-mr1-cdh5.3.3.jar in the lib/cdh5 directory - we need to remove that from the dist",3.0,0.0080949558598745,0.0080949558598745,0.0037090183111332,True,True,True,1433428249000.0,1433430844000.0,1433434444000.0,1433747919000.0,1433748819000.0,1433428249000.0,-1.0,1433429438000.0,1433433038000.0,1433430844000.0,1433434444000.0,1433430844000.0,1433434444000.0,"thomas.risberg",1433433038000.0,1433428249000.0,45,1433428249000.0,-1.0,"Remove mr1 jar from cdh5 hadoop build","There is an hadoop-core-2.5.0-mr1-cdh5.3.3.jar in the lib/cdh5 directory - we need to remove that from the dist",3.0,False,3.0,0,0.0,-1,0,False
"XD-3147","Story","Sprint 52","2015-06-04T13:42:44.000+0000","mminella","","Composing Jobs via the DSL","h2. Narrative
As a developer, I want to be able to construct jobs using a DSL similar to the current syntax for streams.

h2.  Back story
Streams currently provide a DSL for assembling modules into flows (streams) that consist of a source, n processors, and a sink.  While constructing the steps of jobs themselves would be difficult in this manor, creating flows of jobs (essentially a job that consists only of job steps) would be very useful.  It would allow a developer to create something like the following:

{code}
filejdbc | mycustomjob | jdbchdfs
{code}

This approach also allows the existing packaging/module registry/etc to work out of the box.  This gets us closer to what Oozie provides out of the box without the need to create custom jobs to do the orchestration.",8.0,-1.0,-1.0,0.0042007495214473,False,True,False,1433425364000.0,-1.0,-1.0,1447444343000.0,1447445243000.0,1433425364000.0,-1.0,1433484258000.0,1433487858000.0,-1.0,-1.0,-1.0,-1.0,"mminella",1433487858000.0,1433425364000.0,45,1433425364000.0,-1.0,"Composing Jobs via the DSL","h2. Narrative
As a developer, I want to be able to construct jobs using a DSL similar to the current syntax for streams.

h2.  Back story
Streams currently provide a DSL for assembling modules into flows (streams) that consist of a source, n processors, and a sink.  While constructing the steps of jobs themselves would be difficult in this manor, creating flows of jobs (essentially a job that consists only of job steps) would be very useful.  It would allow a developer to create something like the following:

{code}
filejdbc | mycustomjob | jdbchdfs
{code}

This approach also allows the existing packaging/module registry/etc to work out of the box.  This gets us closer to what Oozie provides out of the box without the need to create custom jobs to do the orchestration.",8.0,False,8.0,0,0.0,-1,0,False
"XD-3142","Bug","Sprint 50","2015-06-03T16:51:47.000+0000","iperumal","iperumal","Enable/Disable Boot and Integration MBeans when JMX is enabled/disabled","Spring Integration MBeans are enabled by default even though XD_JMX_ENABLED is set to false. We need to disable JMX on these MBeans as well as Spring Boot MBeans.",1.0,1.694144070011714e-05,0.0,0.3556661858524449,True,True,True,1433350307000.0,1433350314000.0,1433353914000.0,1433762595000.0,1433763495000.0,1433350307000.0,-1.0,1433497264000.0,1433500864000.0,1433350307000.0,1433353907000.0,1433350314000.0,1433353914000.0,"iperumal",1433500864000.0,1433350307000.0,45,1433350307000.0,-1.0,"Enable/Disable Boot and Integration MBeans when JMX is enabled/disabled","Spring Integration MBeans are enabled by default even though XD_JMX_ENABLED is set to false. We need to disable JMX on these MBeans as well as Spring Boot MBeans.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3141","Bug","Sprint 50","2015-06-03T16:15:31.000+0000","iperumal","iperumal","Uploaded custom module requires restart to get in effect","When a custom module is uploaded to module registry, though the module registry is updated with the changed module after deleting the existing one, the module changes aren't loaded when deployed.",5.0,0.1449659787547707,0.1449659787547707,0.2658256279713997,True,True,True,1433348131000.0,1433429453000.0,1433433053000.0,1433908204000.0,1433909104000.0,1433348131000.0,-1.0,1433497252000.0,1433500852000.0,1433429453000.0,1433433053000.0,1433429453000.0,1433433053000.0,"iperumal",1433500852000.0,1433348131000.0,45,1433348131000.0,-1.0,"Uploaded custom module requires restart to get in effect","When a custom module is uploaded to module registry, though the module registry is updated with the changed module after deleting the existing one, the module changes aren't loaded when deployed.",5.0,False,5.0,0,0.0,0,0,False
"XD-3140","Story","Sprint 50","2015-06-03T14:30:54.000+0000","sabby","dturanski","Create a landing page with links for all OOTB modules","As a user, I'd like to have a landing page with higher-order links for sources, processors, sinks and jobs, so I can jump to right section from one place. ",1.0,0.0175264044083881,0.0174345629879075,0.0,True,True,True,1433341854000.0,1433342999000.0,1433346599000.0,1433406284000.0,1433407184000.0,1433341854000.0,-1.0,1433341854000.0,1433345454000.0,1433342993000.0,1433346593000.0,1433342999000.0,1433346599000.0,"sabby",1433345454000.0,1433341854000.0,45,1433341854000.0,-1.0,"Create a landing page with links for all OOTB modules","As a user, I'd like to have a landing page with higher-order links for sources, processors, sinks and jobs, so I can jump to right section from one place. ",1.0,False,1.0,0,0.0,-1,0,False
"XD-3139","Story","Sprint 50","2015-06-03T14:28:45.000+0000","sabby","","Document the new analytics tab features","As a user, I'd like to refer to the analytics tab docs, so I can understand how to use various widgets from streaming pipeline.  ",2.0,0.6921821171378131,0.6921669704420648,0.0,True,True,True,1433341725000.0,1433753012000.0,1433756612000.0,1433935014000.0,1433935914000.0,1433341725000.0,1433341876000.0,1433341725000.0,1433345325000.0,1433753003000.0,1433756603000.0,1433753012000.0,1433756612000.0,"sabby",1433345325000.0,1433341876000.0,45,1433341876000.0,-1.0,"Document the new analytics tab features","As a user, I'd like to refer to the analytics tab docs, so I can understand how to use various widgets from streaming pipeline.  ",2.0,False,2.0,0,0.0,-1,0,False
"XD-3137","Story","Sprint 50","2015-06-03T08:52:16.000+0000","dturanski","dturanski","Upgrade to 3.1.1 of the Gradle Artifactory Plugin ","This addresses The plugin issue https://www.jfrog.com/jira/browse/GAP-172 to disable spring-xd/pom.xml",1.0,0.0016677079084464,0.0016367288141719,0.0014663437956618,True,True,True,1433321536000.0,1433321859000.0,1433325459000.0,1433514315000.0,1433515215000.0,1433321536000.0,-1.0,1433321820000.0,1433325420000.0,1433321853000.0,1433325453000.0,1433321859000.0,1433325459000.0,"dturanski",1433325420000.0,1433321536000.0,45,1433321536000.0,-1.0,"Upgrade to 3.1.1 of the Gradle Artifactory Plugin ","This addresses The plugin issue https://www.jfrog.com/jira/browse/GAP-172 to disable spring-xd/pom.xml",1.0,False,1.0,0,0.0,-1,1,False
"XD-3136","Bug","Sprint 50","2015-06-02T13:59:53.000+0000","thomas.risberg","","Example hashtag-count MR job fails when running XD on YARN with PHD 3.0","Running XD on YARN on PHD 3.0 Ambari install.

Uploading and submitting a custom job fails with the following:

{code}
2015-06-02 16:54:15,580 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1433273561345_0009_m_000000_0: Error: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2076)
	at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:742)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1982)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2074)
	... 8 more
{code}

Same example jar works fine when submitted from XD cluster.",5.0,-1.0,-1.0,0.0018045276532195,False,True,False,1433253593000.0,-1.0,-1.0,1433766955000.0,1433767855000.0,1433253593000.0,-1.0,1433254521000.0,1433258121000.0,-1.0,-1.0,-1.0,-1.0,"thomas.risberg",1433258121000.0,1433253593000.0,45,1433253593000.0,-1.0,"Example hashtag-count MR job fails when running XD on YARN with PHD 3.0","Running XD on YARN on PHD 3.0 Ambari install.

Uploading and submitting a custom job fails with the following:

{code}
2015-06-02 16:54:15,580 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1433273561345_0009_m_000000_0: Error: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2076)
	at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:742)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.lang.ClassNotFoundException: Class org.springframework.xd.examples.hadoop.HashtagCount$TokenizerMapper not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1982)
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2074)
	... 8 more
{code}

Same example jar works fine when submitted from XD cluster.",5.0,False,5.0,0,0.0,0,1,False
"XD-3135","Bug","Sprint 50","2015-06-02T11:21:14.000+0000","iperumal","iperumal","Spark streaming module includes logback jar when using dist zip","When running spark streaming module on spark standalone cluster from XD distribution, I see the following error:

[Stage 3:=============================>                             (1 + 1) / 2]2015-06-02T10:05:53-0700 1.2.0.SNAP WARN task-result-getter-3 scheduler.TaskSetManager - Lost task 0.0 in stage 3.0 (TID 50, 192.168.2.8): java.lang.IllegalArgumentException: LoggerFactory is not a Logback LoggerContext but Logback is on the classpath. Either remove Logback or the competing implementation (class org.slf4j.impl.Log4jLoggerFactory loaded from file:/Users/igopinatha/workspace/git/ilayaperumalg/spark/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.2.0.jar). If you are using Weblogic you will need to add 'org.slf4j' to prefer-application-packages in WEB-INF/weblogic.xml Object of class [org.slf4j.impl.Log4jLoggerFactory] must be an instance of class ch.qos.logback.classic.LoggerContext
     at org.springframework.util.Assert.isInstanceOf(Assert.java:339)
     at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLoggerContext(LogbackLoggingSystem.java:151)
     at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLogger(LogbackLoggingSystem.java:143)
     at org.springframework.boot.logging.logback.LogbackLoggingSystem.beforeInitialize(LogbackLoggingSystem.java:89)
     at org.springframework.boot.logging.LoggingApplicationListener.onApplicationStartedEvent(LoggingApplicationListener.java:152)
     at org.springframework.boot.logging.LoggingApplicationListener.onApplicationEvent(LoggingApplicationListener.java:139)
     at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:151)
     at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:128)
     at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:100)
     at org.springframework.boot.context.event.EventPublishingRunListener.started(EventPublishingRunListener.java:54)
     at org.springframework.boot.SpringApplication.run(SpringApplication.java:277)
     at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
     at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusConfiguration.createApplicationContext(MessageBusConfiguration.java:82)
     at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.start(MessageBusSender.java:105)
     at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:58)
     at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:53)
",2.0,0.5808847271553116,0.0,0.5766871165644172,True,True,True,1433244074000.0,1433245873000.0,1433247171000.0,1433246271000.0,1433247171000.0,1433244074000.0,-1.0,1433245860000.0,1433247171000.0,1433244074000.0,1433247171000.0,1433245873000.0,1433247171000.0,"iperumal",1433247171000.0,1433244074000.0,45,1433244074000.0,-1.0,"Spark streaming module includes logback jar when using dist zip","When running spark streaming module on spark standalone cluster from XD distribution, I see the following error:

[Stage 3:=============================>                             (1 + 1) / 2]2015-06-02T10:05:53-0700 1.2.0.SNAP WARN task-result-getter-3 scheduler.TaskSetManager - Lost task 0.0 in stage 3.0 (TID 50, 192.168.2.8): java.lang.IllegalArgumentException: LoggerFactory is not a Logback LoggerContext but Logback is on the classpath. Either remove Logback or the competing implementation (class org.slf4j.impl.Log4jLoggerFactory loaded from file:/Users/igopinatha/workspace/git/ilayaperumalg/spark/assembly/target/scala-2.10/spark-assembly-1.2.1-hadoop2.2.0.jar). If you are using Weblogic you will need to add 'org.slf4j' to prefer-application-packages in WEB-INF/weblogic.xml Object of class [org.slf4j.impl.Log4jLoggerFactory] must be an instance of class ch.qos.logback.classic.LoggerContext
     at org.springframework.util.Assert.isInstanceOf(Assert.java:339)
     at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLoggerContext(LogbackLoggingSystem.java:151)
     at org.springframework.boot.logging.logback.LogbackLoggingSystem.getLogger(LogbackLoggingSystem.java:143)
     at org.springframework.boot.logging.logback.LogbackLoggingSystem.beforeInitialize(LogbackLoggingSystem.java:89)
     at org.springframework.boot.logging.LoggingApplicationListener.onApplicationStartedEvent(LoggingApplicationListener.java:152)
     at org.springframework.boot.logging.LoggingApplicationListener.onApplicationEvent(LoggingApplicationListener.java:139)
     at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:151)
     at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:128)
     at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:100)
     at org.springframework.boot.context.event.EventPublishingRunListener.started(EventPublishingRunListener.java:54)
     at org.springframework.boot.SpringApplication.run(SpringApplication.java:277)
     at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
     at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusConfiguration.createApplicationContext(MessageBusConfiguration.java:82)
     at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.start(MessageBusSender.java:105)
     at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:58)
     at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:53)
",2.0,False,2.0,0,0.0,-1,0,False
"XD-3133","Bug","Sprint 50","2015-06-01T16:24:41.000+0000","thomas.risberg","thomas.risberg","Update YARN deployment classpath settings for HDP 2.2 and PHD 3.0","Need to update classpath settings for PHD 3.0 and HDP 2.2 ",1.0,0.397602119057577,0.397602119057577,0.5499790882475951,True,True,True,1433175881000.0,1433181585000.0,1433185185000.0,1433189327000.0,1433190227000.0,1433175881000.0,-1.0,1433183771000.0,1433187371000.0,1433181585000.0,1433185185000.0,1433181585000.0,1433185185000.0,"thomas.risberg",1433187371000.0,1433175881000.0,45,1433175881000.0,-1.0,"Update YARN deployment classpath settings for HDP 2.2 and PHD 3.0","Need to update classpath settings for PHD 3.0 and HDP 2.2 ",1.0,False,1.0,0,0.0,-1,1,False
"XD-3132","Bug","Sprint 50","2015-06-01T16:19:04.000+0000","thomas.risberg","","Sqoop job doesn't run when deployed on YARN on Ambari deployed HDP","Got this error when submitting Sqoop job:

{code}
2015-06-01 19:09:42,932 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(75)) - Sqoop command: import
2015-06-01 19:09:42,939 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(76)) - Using args: [--table, XD_JOB_REGISTRY, --target-dir, /xd/sqoop2, -m=1]
2015-06-01 19:09:42,939 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(77)) - Mapreduce home: /usr/hdp/2.2.4.2-2/hadoop-mapreduce
2015-06-01 19:09:42,977 WARN [main] conf.Configuration (Configuration.java:(646)) - DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively
2015-06-01 19:09:42,984 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: fs.defaultFS=hdfs://hawaii:8020
2015-06-01 19:09:43,743 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.hostname=hawaii
2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.address=hawaii:8050
2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.scheduler.address=hawaii
2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: mapreduce.framework.name=yarn
2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: mapreduce.jobhistory.address=hawaii
2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: yarn.resourcemanager.scheduler.address=hawaii:8030
2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: mapreduce.application.classpath=$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:/usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar:/etc/hadoop/conf/secure
2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: mapreduce.application.framework.path=/hdp/apps/2.2.4.2-2/mapreduce/mapreduce.tar.gz#mr-framework
2015-06-01 19:09:44,141 INFO [main] sqoop.Sqoop (Sqoop.java:(92)) - Running Sqoop version: 1.4.5
2015-06-01 19:09:44,691 INFO [main] manager.SqlManager (SqlManager.java:initOptionDefaults(98)) - Using default fetchSize of 1000
2015-06-01 19:09:44,691 INFO [main] tool.CodeGenTool (CodeGenTool.java:generateORM(92)) - Beginning code generation
2015-06-01 19:09:45,057 INFO [main] manager.SqlManager (SqlManager.java:execute(749)) - Executing SQL statement: SELECT t.* FROM XD_JOB_REGISTRY AS t WHERE 1=0
2015-06-01 19:09:45,074 INFO [main] manager.SqlManager (SqlManager.java:execute(749)) - Executing SQL statement: SELECT t.* FROM XD_JOB_REGISTRY AS t WHERE 1=0
2015-06-01 19:09:45,148 INFO [main] orm.CompilationManager (CompilationManager.java:findHadoopJars(94)) - HADOOP_MAPRED_HOME is /usr/hdp/2.2.4.2-2/hadoop-mapreduce
2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(184)) - It seems as though you are running sqoop with a JRE.
2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(185)) - Sqoop requires a JDK that can compile Java code.
2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(186)) - Please install a JDK and set $JAVA_HOME to use it.
2015-06-01 19:09:45,232 ERROR [main] tool.ImportTool (ImportTool.java:run(609)) - Encountered IOException running import job: java.io.IOException: Could not start Java compiler.
at org.apache.sqoop.orm.CompilationManager.compile(CompilationManager.java:187)
at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:97)
at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)
at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601)
at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
at org.springframework.xd.sqoop.SqoopRunner.main(SqoopRunner.java:87)
{code}
",3.0,0.9902105447807508,0.9901987520324064,0.1164607603669313,True,True,True,1433175544000.0,1433847286000.0,1433850886000.0,1433853027000.0,1433853927000.0,1433175544000.0,-1.0,1433254549000.0,1433258149000.0,1433847278000.0,1433850878000.0,1433847286000.0,1433850886000.0,"thomas.risberg",1433258149000.0,1433175544000.0,45,1433175544000.0,-1.0,"Sqoop job doesn't run when deployed on YARN on Ambari deployed HDP","Got this error when submitting Sqoop job:

{code}
2015-06-01 19:09:42,932 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(75)) - Sqoop command: import
2015-06-01 19:09:42,939 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(76)) - Using args: [--table, XD_JOB_REGISTRY, --target-dir, /xd/sqoop2, -m=1]
2015-06-01 19:09:42,939 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:main(77)) - Mapreduce home: /usr/hdp/2.2.4.2-2/hadoop-mapreduce
2015-06-01 19:09:42,977 WARN [main] conf.Configuration (Configuration.java:(646)) - DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively
2015-06-01 19:09:42,984 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: fs.defaultFS=hdfs://hawaii:8020
2015-06-01 19:09:43,743 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.hostname=hawaii
2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.address=hawaii:8050
2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: yarn.resourcemanager.scheduler.address=hawaii
2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: mapreduce.framework.name=yarn
2015-06-01 19:09:43,758 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:setConfigurationProperty(168)) - Setting configuration property: mapreduce.jobhistory.address=hawaii
2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: yarn.resourcemanager.scheduler.address=hawaii:8030
2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: mapreduce.application.classpath=$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:/usr/hdp/2.2.4.2-2/hadoop/lib/hadoop-lzo-0.6.0.2.2.4.2-2.jar:/etc/hadoop/conf/secure
2015-06-01 19:09:43,760 INFO [main] sqoop.SqoopRunner (SqoopRunner.java:createConfiguration(158)) - Setting configuration property: mapreduce.application.framework.path=/hdp/apps/2.2.4.2-2/mapreduce/mapreduce.tar.gz#mr-framework
2015-06-01 19:09:44,141 INFO [main] sqoop.Sqoop (Sqoop.java:(92)) - Running Sqoop version: 1.4.5
2015-06-01 19:09:44,691 INFO [main] manager.SqlManager (SqlManager.java:initOptionDefaults(98)) - Using default fetchSize of 1000
2015-06-01 19:09:44,691 INFO [main] tool.CodeGenTool (CodeGenTool.java:generateORM(92)) - Beginning code generation
2015-06-01 19:09:45,057 INFO [main] manager.SqlManager (SqlManager.java:execute(749)) - Executing SQL statement: SELECT t.* FROM XD_JOB_REGISTRY AS t WHERE 1=0
2015-06-01 19:09:45,074 INFO [main] manager.SqlManager (SqlManager.java:execute(749)) - Executing SQL statement: SELECT t.* FROM XD_JOB_REGISTRY AS t WHERE 1=0
2015-06-01 19:09:45,148 INFO [main] orm.CompilationManager (CompilationManager.java:findHadoopJars(94)) - HADOOP_MAPRED_HOME is /usr/hdp/2.2.4.2-2/hadoop-mapreduce
2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(184)) - It seems as though you are running sqoop with a JRE.
2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(185)) - Sqoop requires a JDK that can compile Java code.
2015-06-01 19:09:45,230 ERROR [main] orm.CompilationManager (CompilationManager.java:compile(186)) - Please install a JDK and set $JAVA_HOME to use it.
2015-06-01 19:09:45,232 ERROR [main] tool.ImportTool (ImportTool.java:run(609)) - Encountered IOException running import job: java.io.IOException: Could not start Java compiler.
at org.apache.sqoop.orm.CompilationManager.compile(CompilationManager.java:187)
at org.apache.sqoop.tool.CodeGenTool.generateORM(CodeGenTool.java:97)
at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:478)
at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:601)
at org.apache.sqoop.Sqoop.run(Sqoop.java:143)
at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:179)
at org.apache.sqoop.Sqoop.runTool(Sqoop.java:218)
at org.springframework.xd.sqoop.SqoopRunner.main(SqoopRunner.java:87)
{code}
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3131","Bug","Sprint 50","2015-06-01T16:00:46.000+0000","iperumal","iperumal","Spark streaming plugin shouldn't need tap listener cache","Since Spark streaming doesn't use ZK to keep track taps being created, we don't need the tap listener cache setup at the container startup.",1.0,0.0116483324820291,0.0,0.0577807123547072,True,True,True,1433174446000.0,1433176316000.0,1433179916000.0,1433334084000.0,1433334984000.0,1433174446000.0,-1.0,1433183722000.0,1433187322000.0,1433174446000.0,1433178046000.0,1433176316000.0,1433179916000.0,"iperumal",1433187322000.0,1433174446000.0,45,1433174446000.0,-1.0,"Spark streaming plugin shouldn't need tap listener cache","Since Spark streaming doesn't use ZK to keep track taps being created, we don't need the tap listener cache setup at the container startup.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3130","Bug","Sprint 50","2015-06-01T15:51:59.000+0000","iperumal","iperumal","Move Bus cleaner util method from BusUtils","Since `Spark-streaming` uses `BusUtils`, we need to move the bus cleaner util method that builds rest template so that spark streaming doesn't depend on `httpClient`",1.0,0.0039308502539827,0.0,0.137081482096638,True,True,True,1433173919000.0,1433174203000.0,1433177803000.0,1433245268000.0,1433246168000.0,1433173919000.0,-1.0,1433183823000.0,1433187423000.0,1433173919000.0,1433177519000.0,1433174203000.0,1433177803000.0,"iperumal",1433187423000.0,1433173919000.0,45,1433173919000.0,-1.0,"Move Bus cleaner util method from BusUtils","Since `Spark-streaming` uses `BusUtils`, we need to move the bus cleaner util method that builds rest template so that spark streaming doesn't depend on `httpClient`",1.0,False,1.0,0,0.0,-1,0,False
"XD-3127","Story","Sprint 50","2015-06-01T10:18:09.000+0000","sabby","","Create a landing section for OOTB batch jobs","As a user, I'd like to refer to OOTB batch jobs and the documentation, so I can jump to the right job section and review details. ",1.0,0.7386166680484983,0.7386166680484983,0.0,True,True,True,1433153889000.0,1433340971000.0,1433344571000.0,1433406276000.0,1433407176000.0,1433153889000.0,-1.0,1433153889000.0,1433157489000.0,1433340971000.0,1433344571000.0,1433340971000.0,1433344571000.0,"sabby",1433157489000.0,1433153889000.0,45,1433153889000.0,-1.0,"Create a landing section for OOTB batch jobs","As a user, I'd like to refer to OOTB batch jobs and the documentation, so I can jump to the right job section and review details. ",1.0,False,1.0,0,0.0,-1,0,False
"XD-3126","Story","Sprint 51","2015-06-01T08:07:57.000+0000","dturanski","","Support for registering custom Kryo Serializers","This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10% more performant than Serializable.",3.0,0.2853953380035131,0.2853953380035131,6.650258284782588e-05,True,True,True,1433146077000.0,1434008667000.0,1434012267000.0,1436167616000.0,1436168516000.0,1433146077000.0,1434433222000.0,1433146278000.0,1433149878000.0,1434008667000.0,1434012267000.0,1434008667000.0,1434012267000.0,"dturanski",1433149878000.0,1433146077000.0,45,1434433222000.0,1434433222000.0,"Support for registering custom Kryo Serializers","This is an enhancement to KryoClassRegistrar or a related mechanism to initialize codecs using custom serializers to improve serialization performance. Currently XD will support POJOs that implement the kryo Serializable interface to gain a 2x performance improvement, however initial benchmarks show that custom serializers are about 10% more performant than Serializable.",2.0,True,2.0,1,50.0,-1,0,False
"XD-3125","Story","Sprint 51","2015-06-01T08:03:34.000+0000","dturanski","","Move XD default kryo registration to declaration in config/ext XML ","Currently kryo class registration is hard coded in spring-xd-codec. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. ",2.0,0.2854491511629907,0.2854491511629907,0.0001535039255982,True,True,True,1433145814000.0,1434008648000.0,1434012248000.0,1436167638000.0,1436168538000.0,1433145814000.0,1434433135000.0,1433146278000.0,1433149878000.0,1434008648000.0,1434012248000.0,1434008648000.0,1434012248000.0,"dturanski",1433149878000.0,1433145814000.0,45,1434433135000.0,1434433135000.0,"Fail fast on Kryo registration conflicts","Currently kryo class registration is hard coded in spring-xd-codec. Users may register their own classes using an extension mechanism, but it is possible to conflict with internal XD class registration, e.g., Tuple. Exposing this using the same extension mechanism will make it more transparent. ",1.0,True,1.0,1,100.0,0,0,True
"XD-3124","Bug","Sprint 50","2015-06-01T07:11:45.000+0000","mbogoevici","mbogoevici","`minPartitionCount` is ignored by the consumer","`minPartitionCount` is ignored by the consumer, so downstream modules end up listening to fewer partitions",3.0,0.0016767751535155,0.0016616690710514,0.0012160396383603,True,True,True,1433142705000.0,1433143149000.0,1433146749000.0,1433406599000.0,1433407499000.0,1433142705000.0,-1.0,1433143027000.0,1433146627000.0,1433143145000.0,1433146745000.0,1433143149000.0,1433146749000.0,"mbogoevici",1433146627000.0,1433142705000.0,45,1433142705000.0,-1.0,"`minPartitionCount` is ignored by the consumer","`minPartitionCount` is ignored by the consumer, so downstream modules end up listening to fewer partitions",3.0,False,3.0,0,0.0,-1,0,False
"XD-3123","Bug","Sprint 50","2015-06-01T06:38:47.000+0000","eric.bottard","eric.bottard","Prevent classloader leakage thru javabeans infrastructure","",1.0,0.583726362971124,0.583726362971124,0.583631413777128,True,True,True,1433140727000.0,1433497298000.0,1433500898000.0,1433750680000.0,1433751580000.0,1433140727000.0,-1.0,1433497240000.0,1433500840000.0,1433497298000.0,1433500898000.0,1433497298000.0,1433500898000.0,"eric.bottard",1433500840000.0,1433140727000.0,45,1433140727000.0,-1.0,"Prevent classloader leakage thru javabeans infrastructure","",1.0,False,1.0,0,0.0,0,0,False
"XD-3118","Story","Sprint 50","2015-05-28T08:30:36.000+0000","sabby","thomas.risberg","Update RPM script to include number of containers to be started","As a user, I'd like to start multiple instances of {{xd-container}}'s through the RPM scripts, so I can easily spin-up instances on the same node/vm.",2.0,0.5909722031994741,6.484379280118336e-05,0.0,True,True,True,1432801836000.0,1435563312000.0,1435566912000.0,1437473704000.0,1437474604000.0,1432801836000.0,-1.0,1432801836000.0,1432805436000.0,1432802139000.0,1432805739000.0,1435563312000.0,1435566912000.0,"sabby",1432805436000.0,1432801836000.0,45,1432801836000.0,-1.0,"Update RPM script to include number of containers to be started","As a user, I'd like to start multiple instances of {{xd-container}}'s through the RPM scripts, so I can easily spin-up instances on the same node/vm.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3117","Story","Sprint 50","2015-05-28T07:49:25.000+0000","grussell","pperalta","Add Logging to ZooKeeperContainerRepository","Occasional CI test build failures:

{quote}
Caused by: java.lang.IllegalStateException: Container cache not initialized (likely as a result of a ZooKeeper connection error)
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache(ZooKeeperContainerRepository.java:184)
	at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne(ZooKeeperContainerRepository.java:263)
{quote}

e.g. https://build.spring.io/browse/XD-JDK8-JOB1-1514

Add logging to {{ensureCache()}} (e.g. in {{childEvent()}} ) and {{closeCache()}} to log that the cache was closed; it appears that's the only way the ""cache not initialized"" message can be emitted.",1.0,0.0258856756922699,0.0226069871676797,0.0087922883281144,True,True,True,1432799365000.0,1432801473000.0,1432805073000.0,1432879900000.0,1432880800000.0,1432799365000.0,-1.0,1432800081000.0,1432803681000.0,1432801206000.0,1432804806000.0,1432801473000.0,1432805073000.0,"grussell",1432803681000.0,1432799365000.0,45,1432799365000.0,-1.0,"Add Logging to ZooKeeperContainerRepository","Occasional CI test build failures:

{quote}
Caused by: java.lang.IllegalStateException: Container cache not initialized (likely as a result of a ZooKeeper connection error)
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.ensureCache(ZooKeeperContainerRepository.java:184)
	at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findOne(ZooKeeperContainerRepository.java:263)
{quote}

e.g. https://build.spring.io/browse/XD-JDK8-JOB1-1514

Add logging to {{ensureCache()}} (e.g. in {{childEvent()}} ) and {{closeCache()}} to log that the cache was closed; it appears that's the only way the ""cache not initialized"" message can be emitted.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3116","Story","Sprint 50","2015-05-28T07:39:44.000+0000","mark.pollack","","Update redis.tar.gz bundled in distribution to be version 3.0.1","RPM scripts will need to change.",3.0,0.3476491011496027,0.3476491011496027,0.0,True,True,True,1432798784000.0,1433136725000.0,1433140325000.0,1433769959000.0,1433770859000.0,1432798784000.0,-1.0,1432798784000.0,1432802384000.0,1433136725000.0,1433140325000.0,1433136725000.0,1433140325000.0,"mark.pollack",1432802384000.0,1432798784000.0,45,1432798784000.0,-1.0,"Update redis.tar.gz bundled in distribution to be version 3.0.1","RPM scripts will need to change.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3115","Story","Sprint 50","2015-05-28T07:34:12.000+0000","mark.pollack","Stephane Maldini","Improve ReactorReflectionUtils.extractGeneric to support classes with inheritance","Provide unit tests",1.0,0.7510578371550435,0.7510578371550435,0.0,True,True,True,1432798452000.0,1433774161000.0,1433777761000.0,1434096665000.0,1434097565000.0,1432798452000.0,-1.0,1432798452000.0,1432802052000.0,1433774161000.0,1433777761000.0,1433774161000.0,1433777761000.0,"mark.pollack",1432802052000.0,1432798452000.0,45,1432798452000.0,-1.0,"Improve ReactorReflectionUtils.extractGeneric to support classes with inheritance","Provide unit tests",1.0,False,1.0,0,0.0,0,0,False
"XD-3114","Story","Sprint 50","2015-05-28T07:33:19.000+0000","mark.pollack","Stephane Maldini","Ensure proper lifecycle shutdown of processors in BroadcasterMessageHandler and MultipleBroadcasterMessageHandler ","",2.0,0.7421425910360264,0.7421425910360264,0.0,True,True,True,1432798399000.0,1433762558000.0,1433766158000.0,1434096655000.0,1434097555000.0,1432798399000.0,1433836290000.0,1432798399000.0,1432801999000.0,1433762558000.0,1433766158000.0,1433762558000.0,1433766158000.0,"mark.pollack",1432801999000.0,1432798399000.0,45,1433836290000.0,1433836290000.0,"Ensure proper lifecycle shutdown of processors in BroadcasterMessageHandler and MultipleBroadcasterMessageHandler ","",1.0,True,1.0,1,100.0,0,0,False
"XD-3113","Story","Sprint 50","2015-05-27T13:48:33.000+0000","mark.pollack","","Review 'critical' sonar warning...","https://sonar.spring.io/drilldown/issues/org.springframework.xd:spring-xd?severity=CRITICAL",3.0,0.1448015782949526,0.1448015782949526,0.0,True,True,True,1432734513000.0,1432817451000.0,1432821051000.0,1433306383000.0,1433307283000.0,1432734513000.0,-1.0,1432734513000.0,1432738113000.0,1432817451000.0,1432821051000.0,1432817451000.0,1432821051000.0,"mark.pollack",1432738113000.0,1432734513000.0,45,1432734513000.0,-1.0,"Review 'critical' sonar warning...","https://sonar.spring.io/drilldown/issues/org.springframework.xd:spring-xd?severity=CRITICAL",3.0,False,3.0,0,0.0,-1,0,False
"XD-3112","Story","Sprint 50","2015-05-27T10:37:39.000+0000","svennela","hillert","Hide the passwords used as module properties in streams from being displayed.","This type is used in password field in the jdbc sink module provided by Spring XD (defined in org.springframework.xd.jdbc.JdbcConnectionMixin class). It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot.
Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI?
This is similar issue as below and fixed in batch jobs but not in streams.
https://github.com/spring-projects/spring-xd/pull/1325",2.0,0.1658829522751976,0.0067376743989501,0.006693069585856,True,True,True,1432723059000.0,1432793719000.0,1432797319000.0,1433148122000.0,1433149022000.0,1432723059000.0,-1.0,1432725910000.0,1432729510000.0,1432725929000.0,1432729529000.0,1432793719000.0,1432797319000.0,"svennela",1432729510000.0,1432723059000.0,45,1432723059000.0,-1.0,"Hide the passwords used as module properties in streams from being displayed.","This type is used in password field in the jdbc sink module provided by Spring XD (defined in org.springframework.xd.jdbc.JdbcConnectionMixin class). It seems that Spring XD Admin UI is always displaying the password in plain text please see attached screen shot.
Is there a way to somehow hide the passwords used as module properties in streams from being displayed in Spring XD Admin UI?
This is similar issue as below and fixed in batch jobs but not in streams.
https://github.com/spring-projects/spring-xd/pull/1325",2.0,False,2.0,0,0.0,0,0,False
"XD-3111","Story","Sprint 50","2015-05-27T08:03:25.000+0000","mark.pollack","","Upgrade to 1.2.0 RC1 SIK release","",1.0,0.983693105494986,0.983693105494986,0.0,True,True,True,1432713805000.0,1432745294000.0,1432745816000.0,1432744916000.0,1432745816000.0,1432713805000.0,-1.0,1432713805000.0,1432717405000.0,1432745294000.0,1432745816000.0,1432745294000.0,1432745816000.0,"mark.pollack",1432717405000.0,1432713805000.0,45,1432713805000.0,-1.0,"Upgrade to 1.2.0 RC1 SIK release","",1.0,False,1.0,0,0.0,-1,0,False
"XD-3110","Story","Sprint 50","2015-05-26T21:39:27.000+0000","sabby","","Clean-up compiler and javadoc warnings from the build","As a developer, I'd like to clean-up compiler and javadoc warnings from the build, so we don't see  the warnings in build sysout.",2.0,0.272107131873969,0.272107131873969,0.0,True,True,True,1432676367000.0,1432802896000.0,1432806496000.0,1433140464000.0,1433141364000.0,1432676367000.0,-1.0,1432676367000.0,1432679967000.0,1432802896000.0,1432806496000.0,1432802896000.0,1432806496000.0,"sabby",1432679967000.0,1432676367000.0,45,1432676367000.0,-1.0,"Clean-up compiler and javadoc warnings from the build","As a developer, I'd like to clean-up compiler and javadoc warnings from the build, so we don't see  the warnings in build sysout.",2.0,False,2.0,0,0.0,-1,0,False
"XD-3107","Story","Sprint 50","2015-05-26T09:36:35.000+0000","sabby","dturanski","Fix gradle build issues","As a XD build master, I'd like to fix (local ./gradlew dist and distZip targets) the outstanding build issues, so I can evaluate that publish builds works as expected. ",3.0,0.0973919774279329,0.0001872682555337,0.0,True,True,True,1432632995000.0,1432640796000.0,1432644396000.0,1432712194000.0,1432713094000.0,1432632995000.0,-1.0,1432632995000.0,1432636595000.0,1432633010000.0,1432636610000.0,1432640796000.0,1432644396000.0,"sabby",1432636595000.0,1432632995000.0,45,1432632995000.0,-1.0,"Fix gradle build issues","As a XD build master, I'd like to fix (local ./gradlew dist and distZip targets) the outstanding build issues, so I can evaluate that publish builds works as expected. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-3106","Story","Sprint 50","2015-05-26T08:36:36.000+0000","grenfro","","Support XD_JMX_ENABLED configuraiton","XD-EC2 needs  to allow user to set the XD_JMX_ENABLED flag in the environment prior to admin or container  startups. ",2.0,0.9974258317781068,0.9974258317781068,0.1854044661818649,True,True,True,1432629396000.0,1432644895000.0,1432644935000.0,1432644035000.0,1432644935000.0,1432629396000.0,-1.0,1432632277000.0,1432635877000.0,1432644895000.0,1432644935000.0,1432644895000.0,1432644935000.0,"grenfro",1432635877000.0,1432629396000.0,45,1432629396000.0,-1.0,"Support XD_JMX_ENABLED configuration","XD-EC2 needs  to allow user to set the XD_JMX_ENABLED flag in the environment prior to admin or container  startups. ",2.0,False,2.0,0,0.0,0,0,True
"XD-3105","Story","Sprint 50","2015-05-26T08:29:19.000+0000","sabby","iperumal","Turn-off JMX by default","",1.0,0.2369217979929933,0.0449498248322546,0.0,True,True,True,1432628959000.0,1432632949000.0,1432636549000.0,1432644900000.0,1432645800000.0,1432628959000.0,-1.0,1432628959000.0,1432632559000.0,1432629716000.0,1432633316000.0,1432632949000.0,1432636549000.0,"sabby",1432632559000.0,1432628959000.0,45,1432628959000.0,-1.0,"Turn-off JMX by default","As a developer, I'd like to have JMX turned-off by default, so I can take advantage of the performance throughput benefits. ",1.0,False,1.0,0,0.0,-1,0,True
"XD-3104","Story","Sprint 50","2015-05-26T08:26:55.000+0000","sabby","","Update to Reactor 2.0.3 ","",1.0,0.7649039039554815,0.7649039039554815,0.0,True,True,True,1432628815000.0,1432717885000.0,1432721485000.0,1432744361000.0,1432745261000.0,1432628815000.0,-1.0,1432628815000.0,1432632415000.0,1432717885000.0,1432721485000.0,1432717885000.0,1432721485000.0,"sabby",1432632415000.0,1432628815000.0,45,1432628815000.0,-1.0,"Update to Reactor 2.0.3 ","",1.0,False,1.0,0,0.0,-1,0,False
"XD-3103","Story","Sprint 50","2015-05-26T08:26:18.000+0000","sabby","","Adapt to XD Reactor processor fixes and improvements","",3.0,0.8342839235349421,0.8342839235349421,0.0,True,True,True,1432628778000.0,1432642089000.0,1432644733000.0,1432643833000.0,1432644733000.0,1432628778000.0,-1.0,1432628778000.0,1432632378000.0,1432642089000.0,1432644733000.0,1432642089000.0,1432644733000.0,"sabby",1432632378000.0,1432628778000.0,45,1432628778000.0,-1.0,"Adapt to XD Reactor processor fixes and improvements","As a developer, I'd like to upgrade to 2.0.3 release of Reactor, so I can inherit the latest optimizations to further improve XD performance characteristics. ",3.0,False,3.0,0,0.0,-1,0,True
"XD-3102","Story","Sprint 50","2015-05-26T08:13:59.000+0000","grenfro","","Benchmark XD RC1 using Kafka 0.8.2 as transport","As a developer, I'd like to rerun _baseline_, _Tuple_, and _Serialized_ payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. 

Sinks to be included in test:
In-Memory Transport > Hdfs sink
Direct Binding Transport > Hdfs Sink
Kafka > Hdfs Sink",8.0,0.2327398720017947,0.2327398720017947,0.0037808344542894,True,True,True,1432628039000.0,1432889475000.0,1432893075000.0,1433750436000.0,1433751336000.0,1432628039000.0,-1.0,1432632286000.0,1432635886000.0,1432889475000.0,1432893075000.0,1432889475000.0,1432893075000.0,"grenfro",1432635886000.0,1432628039000.0,45,1432628039000.0,-1.0,"Benchmark XD RC1 using Kafka 0.8.2 as transport","As a developer, I'd like to rerun _baseline_, _Tuple_, and _Serialized_ payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. 

Sinks to be included in test:
In-Memory Transport > Hdfs sink
Direct Binding Transport > Hdfs Sink
Kafka > Hdfs Sink",8.0,False,8.0,0,0.0,0,0,False
"XD-3101","Story","Sprint 50","2015-05-26T07:55:41.000+0000","hillert","hillert","Fix Gradle dist build task","",1.0,3.8511158608206726e-05,0.0,0.1870178884331735,True,True,True,1432626941000.0,1432626951000.0,1432630551000.0,1432885706000.0,1432886606000.0,1432626941000.0,-1.0,1432675503000.0,1432679103000.0,1432626941000.0,1432630541000.0,1432626951000.0,1432630551000.0,"hillert",1432679103000.0,1432626941000.0,45,1432626941000.0,-1.0,"Fix Gradle dist build task","",1.0,False,1.0,0,0.0,-1,1,False
"XD-3100","Bug","","2015-05-26T07:42:48.000+0000","lukasz.nowanski@emc.com","","module.*.count > 1 duplicates messages on taps","Using module.name.count > 1 when deploying taps causes duplication of messages in those modules.

To demonstrate the problem please see test case scenario set up below:

h4. 1. Environment

- Spring-XD version 1.1.1-RELEASE
- Running two spring-xd containers and one spring-xd admin

h4. 2. Set up

Stream definition is as follows:


{quote}stream create --name test-module-count --definition ""syslog-udp --port=5140 | transform | log""
stream deploy --name test-module-count --properties ""module.*.count=2""
stream create --name tap-test-module-count --definition ""tap:stream:test-module-count.syslog-udp > transform --expression='payload.toString() + \""TAPPED\""' | log""
stream deploy --name tap-test-module-count --properties ""module.*.count=2""{quote}


Please refer to the screen shots attached to see that after deploying those two streams we have:

- streams successfully deployed ( module-count-spring-xd-streams.png )
- streams successfully deployed with count=2 to both containers ( module-count-spring-xd-containers.png ) 
- 5 queues created in Rabbit ( module-count-rabbit.png ) where two were created for the syslog-udp collector as a result of using module.syslog-udp.count=2 - this is causing messages to be duplicated. Normally the expectation would be to have only one queue for the tap

h4. 3. Test input data

I have sent a very simple UDP message to the listening udp collector running on second container:


{quote}echo test-module-count >> /dev/udp/host02/5140{quote}

h4. 4. Test output data in the logs ( module-count-container01.log and module-count-container02.log )

h5. Expected result:

Below messages logged only on 1 container (it does not matter which one)
{quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count}{quote}
Below message logged only on one container (it does not matter which one)

{quote}2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count
}TAPPED{quote}

h5. Actual result:

Stream that has been create as a tap has duplicated the same message and as a result the same message was proccessed twice on both containers by the same module ( transformer ) and logged twice to the console on both containers

Container01:
{quote}2015-05-26 14:52:21,143 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count
}TAPPED{quote}

Container02:
{quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count
}
2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count
}TAPPED{quote}


",5.0,-1.0,-1.0,-1.0,False,False,False,1432626168000.0,-1.0,-1.0,1432806564000.0,1432807464000.0,1432626168000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"lukasz.nowanski@emc.com",-1.0,-1.0,0,1432626168000.0,-1.0,"module.*.count > 1 duplicates messages on taps","Using module.name.count > 1 when deploying taps causes duplication of messages in those modules. This impacts balancing of the containers and modules in a cluster as messages should not be duplicated across modules if the same module is deployed twice to two containers in order to spread the load.

We use taps quite heavily in our project mainly for analytics of the life feed in real time but due to issue we have discovered and described in this bug we are currently facing a limitation where heavily processing modules can not be load balanced across the cluster as they are causing duplication of the messages and therefore the same module deployed to two 
containers would still process the same message twice.

To demonstrate the problem please see test case scenario set up below:

h4. 1. Environment

- Spring-XD version 1.1.1-RELEASE
- Running two spring-xd containers and one spring-xd admin

h4. 2. Set up

Stream definition is as follows:


{quote}stream create --name test-module-count --definition ""syslog-udp --port=5140 | transform | log""
stream deploy --name test-module-count --properties ""module.*.count=2""
stream create --name tap-test-module-count --definition ""tap:stream:test-module-count.syslog-udp > transform --expression='payload.toString() + \""TAPPED\""' | log""
stream deploy --name tap-test-module-count --properties ""module.*.count=2""{quote}


Please refer to the screen shots attached to see that after deploying those two streams we have:

- streams successfully deployed ( module-count-spring-xd-streams.png )
- streams successfully deployed with count=2 to both containers ( module-count-spring-xd-containers.png ) 
- 5 queues created in Rabbit ( module-count-rabbit.png ) where two were created for the syslog-udp collector as a result of using module.syslog-udp.count=2 - this is causing messages to be duplicated. Normally the expectation would be to have only one queue for the tap

h4. 3. Test input data

I have sent a very simple UDP message to the listening udp collector running on second container:


{quote}echo test-module-count >> /dev/udp/host02/5140{quote}

h4. 4. Test output data in the logs ( module-count-container01.log and module-count-container02.log )

h5. Expected result:

Below messages logged only on 1 container (it does not matter which one)
{quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count}{quote}
Below message logged only on one container (it does not matter which one)

{quote}2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count
}TAPPED{quote}

h5. Actual result:

Stream that has been create as a tap has duplicated the same message and as a result the same message was proccessed twice on both containers by the same module ( transformer ) and logged twice to the console on both containers

Container01:
{quote}2015-05-26 14:52:21,143 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count
}TAPPED{quote}

Container02:
{quote}2015-05-26 09:52:21,630 1.1.1.RELEASE  INFO xdbus.test-module-count.1-1 sink.test-module-count - {UNDECODED=test-module-count
}
2015-05-26 09:52:21,843 1.1.1.RELEASE  INFO xdbus.tap-test-module-count.0-1 sink.tap-test-module-count - {UNDECODED=test-module-count
}TAPPED{quote}


",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-3098","Story","Sprint 50","2015-05-22T13:46:38.000+0000","mbogoevici","mbogoevici","Message Bus optimizations (Kafka + Redis)","",5.0,0.0059049242205403,3.129393186588865e-05,0.0051634987578716,True,True,True,1432302398000.0,1432304851000.0,1432308451000.0,1432716914000.0,1432717814000.0,1432302398000.0,-1.0,1432304543000.0,1432308143000.0,1432302411000.0,1432306011000.0,1432304851000.0,1432308451000.0,"mbogoevici",1432308143000.0,1432302398000.0,45,1432302398000.0,-1.0,"Message Bus optimizations (Kafka + Redis)","",5.0,False,5.0,0,0.0,-1,1,False
"XD-3096","Story","Sprint 49","2015-05-21T19:54:36.000+0000","mark.pollack","smaldini","Support for BroadcasterMessageHandler to work with concurrent producing threads","Also provide better lifecycle (shutdown) mgmt of handler.",3.0,0.0036670333700036,0.0,0.0,True,True,True,1432238076000.0,1432238086000.0,1432240803000.0,1432239903000.0,1432240803000.0,1432238076000.0,-1.0,1432238076000.0,1432240803000.0,1432238076000.0,1432240803000.0,1432238086000.0,1432240803000.0,"mark.pollack",1432240803000.0,1432238076000.0,45,1432238076000.0,-1.0,"Support for BroadcasterMessageHandler to work with concurrent producing threads","Also provide better lifecycle (shutdown) mgmt of handler.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3095","Story","Sprint 49","2015-05-21T19:49:27.000+0000","mark.pollack","smaldini","Update to Reactor 2.0.2","",1.0,0.053440213760855,0.053440213760855,0.0013360053440213,True,True,True,1432237767000.0,1432237927000.0,1432240761000.0,1432239861000.0,1432240761000.0,1432237767000.0,-1.0,1432237771000.0,1432240761000.0,1432237927000.0,1432240761000.0,1432237927000.0,1432240761000.0,"mark.pollack",1432240761000.0,1432237767000.0,45,1432237767000.0,-1.0,"Update to Reactor 2.0.2","",1.0,False,1.0,0,0.0,-1,0,False
"XD-3094","Bug","Sprint 50","2015-05-21T12:42:44.000+0000","iperumal","Ilayaperumal Gopinathan","SparkApp batch job is not running","From the latest master, I couldn't run sparkApp as batch job. The spark application process gets launched and it doesn't complete and there are no errors at the output.",2.0,0.0006460499530699,0.9629435498616478,0.0986506088716067,True,True,True,1432212164000.0,1432212217000.0,1432215817000.0,1432293301000.0,1432294201000.0,1432212164000.0,-1.0,1432220257000.0,1432223857000.0,1432291161000.0,1432294201000.0,1432212217000.0,1432215817000.0,"iperumal",1432223857000.0,1432212164000.0,45,1432212164000.0,-1.0,"SparkApp batch job is not running","From the latest master, I couldn't run sparkApp as batch job. The spark application process gets launched and it doesn't complete and there are no errors at the output.",2.0,False,2.0,0,0.0,0,2,False
"XD-3093","Bug","Sprint 49","2015-05-21T11:18:02.000+0000","jvalkeal","thomas.risberg","Sqoop list-tables doesn't work oob","Commands from docs:

xd:>job create sqoopListTables --definition ""sqoop --command=list-tables"" --deploy
xd:>job launch --name sqoopListTables

2015-05-21 19:12:36,211 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop job for 'list-tables' finished with exit code: 1
2015-05-21 19:12:36,212 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop err: Error: Required argument --connect is missing.

Adding --connect results

xd:>job create sqoopListTables --definition ""sqoop --command=list-tables --connect=jdbc:hsqldb:hsql://localhost:9101/xdjob"" --deploy
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module sqoop of type job:
    connect: option named 'connect' is not supported


This is with singlenode.",1.0,0.4791111111111111,0.4791111111111111,0.4734814814814815,True,True,True,1432207082000.0,1432210316000.0,1432213832000.0,1432212932000.0,1432213832000.0,1432207082000.0,-1.0,1432210278000.0,1432213832000.0,1432210316000.0,1432213832000.0,1432210316000.0,1432213832000.0,"jvalkeal",1432213832000.0,1432207082000.0,45,1432207082000.0,-1.0,"Sqoop list-tables doesn't work oob","Commands from docs:

xd:>job create sqoopListTables --definition ""sqoop --command=list-tables"" --deploy
xd:>job launch --name sqoopListTables

2015-05-21 19:12:36,211 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop job for 'list-tables' finished with exit code: 1
2015-05-21 19:12:36,212 1.2.0.M1 ERROR task-scheduler-1 sqoop.SqoopTasklet - Sqoop err: Error: Required argument --connect is missing.

Adding --connect results

xd:>job create sqoopListTables --definition ""sqoop --command=list-tables --connect=jdbc:hsqldb:hsql://localhost:9101/xdjob"" --deploy
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module sqoop of type job:
    connect: option named 'connect' is not supported


This is with singlenode.",1.0,False,1.0,0,0.0,0,2,False
"XD-3092","Story","Sprint 49","2015-05-21T10:40:07.000+0000","pperalta","pperalta","Synchronous deployment/undeployments","There are a range of issues (such as XD-3083, XD-2671) that are caused by asynchronous deployments issued by the REST API. The flow of events is:
* deploy/undeploy request received by REST API
* controller queues up request to be processed by supervisor
* controller returns HTTP 2xx

This proposal is to have the thread executing the deploy/undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment/undeployment will have been fulfilled. ",2.0,0.0220922416301305,0.0084175253077563,0.0119591062775275,True,True,True,1432204807000.0,1432207009000.0,1432210609000.0,1432303580000.0,1432304480000.0,1432204807000.0,-1.0,1432205999000.0,1432209599000.0,1432205646000.0,1432209246000.0,1432207009000.0,1432210609000.0,"pperalta",1432209599000.0,1432204807000.0,45,1432204807000.0,-1.0,"Synchronous deployment/undeployments","There are a range of issues (such as XD-3083, XD-2671) that are caused by asynchronous deployments issued by the REST API. The flow of events is:
* deploy/undeploy request received by REST API
* controller queues up request to be processed by supervisor
* controller returns HTTP 2xx

This proposal is to have the thread executing the deploy/undeploy request block until the request has been processed by the supervisor. This will have the side effect of deploys appearing to take longer, but when the HTTP request completes, the deployment/undeployment will have been fulfilled. ",2.0,False,2.0,0,0.0,-1,0,False
"XD-3091","Story","Sprint 49","2015-05-21T07:34:56.000+0000","thomas.risberg","thomas.risberg","Update build to use SHDP 2.2.0.RC1","",1.0,0.011396192780289,0.011396192780289,0.0027376329025275,True,True,True,1432193696000.0,1432194233000.0,1432197833000.0,1432239917000.0,1432240817000.0,1432193696000.0,-1.0,1432193825000.0,1432197425000.0,1432194233000.0,1432197833000.0,1432194233000.0,1432197833000.0,"thomas.risberg",1432197425000.0,1432193696000.0,45,1432193696000.0,-1.0,"Update build to use SHDP 2.2.0.RC1","",1.0,False,1.0,0,0.0,-1,0,False
"XD-3090","Bug","Sprint 49","2015-05-21T06:36:03.000+0000","grenfro","grenfro","JdbcHdfsTests sporadically fail","Acceptance tests sporadically fail after https://github.com/spring-projects/spring-xd/pull/1623 was merged XD-2309.

Additional tests were added but used fixed timeouts.  Will replace them with waitForJob.

 ",2.0,0.9999073655107544,0.0001140116790713,0.0066589946307624,True,True,True,1432190163000.0,1432751457000.0,1432751509000.0,1432750609000.0,1432751509000.0,1432190163000.0,-1.0,1432193901000.0,1432197501000.0,1432190227000.0,1432193827000.0,1432751457000.0,1432751509000.0,"grenfro",1432197501000.0,1432190163000.0,45,1432190163000.0,-1.0,"JdbcHdfsTests sporadically fail","Acceptance tests sporadically fail after https://github.com/spring-projects/spring-xd/pull/1623 was merged XD-2309.

Additional tests were added but used fixed timeouts.  Will replace them with waitForJob.

 ",2.0,False,2.0,0,0.0,0,0,False
"XD-3089","Story","Sprint 50","2015-05-21T05:48:41.000+0000","thomas.risberg","","Add incremental load feature to batch docs","The incremental load introduced with XD-2309 should be added to the batch docs",1.0,0.8561612399847887,0.852774286126415,0.0617144955255866,True,True,True,1432187321000.0,1432644351000.0,1432647951000.0,1432720234000.0,1432721134000.0,1432187321000.0,-1.0,1432220265000.0,1432223865000.0,1432642543000.0,1432646143000.0,1432644351000.0,1432647951000.0,"thomas.risberg",1432223865000.0,1432187321000.0,45,1432187321000.0,-1.0,"Add incremental load feature to batch docs","The incremental load introduced with XD-2309 should be added to the batch docs",1.0,False,1.0,0,0.0,-1,0,False
"XD-3086","Bug","Sprint 49","2015-05-20T10:00:33.000+0000","sabby","iperumal","Fix random Spark streaming test failures","The {{testTapSparkProcessor}} has the test that checks the contents at the output of spark streaming word count processor. It turns out that the order in which these messages are processed are not always in order.",1.0,0.0008858376702469,0.0,0.0,True,True,True,1432116033000.0,1432116041000.0,1432119641000.0,1432124164000.0,1432125064000.0,1432116033000.0,-1.0,1432116033000.0,1432119633000.0,1432116033000.0,1432119633000.0,1432116041000.0,1432119641000.0,"sabby",1432119633000.0,1432116033000.0,45,1432116033000.0,-1.0,"Fix random Spark streaming test failures","The {{testTapSparkProcessor}} has the test that checks the contents at the output of spark streaming word count processor. It turns out that the order in which these messages are processed are not always in order.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3082","Story","Sprint 49","2015-05-19T12:16:26.000+0000","hillert","hillert","UI: Add Analytics Tab","",3.0,0.0013020833333333,0.0013020833333333,0.6123754528985508,True,True,True,1432037786000.0,1432037809000.0,1432041409000.0,1432054550000.0,1432055450000.0,1432037786000.0,-1.0,1432048603000.0,1432052203000.0,1432037809000.0,1432041409000.0,1432037809000.0,1432041409000.0,"hillert",1432052203000.0,1432037786000.0,45,1432037786000.0,-1.0,"UI: Add Analytics Tab","",3.0,False,3.0,0,0.0,-1,0,False
"XD-3081","Bug","Sprint 49","2015-05-19T11:45:57.000+0000","grenfro","hillert","When using file as a source and sink user can not use file sink --mode","Cluster Type: SingleNode
Machine: Mac
PR: https://github.com/spring-projects/spring-xd/pull/1624,https://github.com/spring-projects/spring-xd/pull/1626
Stream that reproduces the problem:
{noformat}
stream create foo --definition ""filein: file --dir=/tmp/xd/a0180520-c7fa-4d9d-8cc3-e36fbf59496a --pattern=de59d1b8-f99c-4c43-a8c0-2f6043546689.out --mode=contents | fileout: file --binary=true --mode=replace ""
{noformat}
Error Message:
{noformat}
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module file of type sink:
    mode: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found
{noformat}
Stacktrace:
{noformat}
2015-05-19 14:30:56,329 1.2.0.SNAP ERROR qtp671416633-35 rest.RestControllerAdvice - Caught exception while handling a request
org.springframework.xd.dirt.plugins.ModuleConfigurationException: Error with option(s) for module file of type sink:
    mode: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found
	at org.springframework.xd.dirt.plugins.ModuleConfigurationException.fromBindException(ModuleConfigurationException.java:55)
	at org.springframework.xd.dirt.stream.XDStreamParser.buildModuleDescriptors(XDStreamParser.java:191)
	at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:122)
	at org.springframework.xd.dirt.stream.AbstractDeployer.validateBeforeSave(AbstractDeployer.java:115)
	at org.springframework.xd.dirt.rest.XDController.save(XDController.java:260)
	at sun.reflect.GeneratedMethodAccessor191.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:776)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:705)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:868)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:755)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.validation.BindException: org.springframework.validation.BeanPropertyBindingResult: 1 errors
Field error in object 'target' on field 'mode': rejected value [replace]; codes [typeMismatch.target.mode,typeMismatch.mode,typeMismatch.org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode,typeMismatch]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [target.mode,mode]; arguments []; default message [mode]]; default message [Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found]
	at org.springframework.xd.module.options.PojoModuleOptionsMetadata.bindAndValidate(PojoModuleOptionsMetadata.java:205)
	at org.springframework.xd.module.options.PojoModuleOptionsMetadata.interpolate(PojoModuleOptionsMetadata.java:139)
	at org.springframework.xd.module.options.FlattenedCompositeModuleOptionsMetadata.interpolate(FlattenedCompositeModuleOptionsMetadata.java:152)
	at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver$ModuleOptionsMetadataWithDefaults.interpolate(EnvironmentAwareModuleOptionsMetadataResolver.java:168)
	at org.springframework.xd.dirt.stream.XDStreamParser.buildModuleDescriptors(XDStreamParser.java:188)
	... 61 more
{noformat}",3.0,0.0049256677047283,0.0048865307309109,0.0924023951827976,True,True,True,1432035957000.0,1432036838000.0,1432040438000.0,1432213916000.0,1432214816000.0,1432035957000.0,-1.0,1432052484000.0,1432056084000.0,1432036831000.0,1432040431000.0,1432036838000.0,1432040438000.0,"grenfro",1432056084000.0,1432035957000.0,45,1432035957000.0,-1.0,"When using file as a source and sink user can not use file sink --mode","Cluster Type: SingleNode
Machine: Mac
PR: https://github.com/spring-projects/spring-xd/pull/1624,https://github.com/spring-projects/spring-xd/pull/1626
Stream that reproduces the problem:
{noformat}
stream create foo --definition ""filein: file --dir=/tmp/xd/a0180520-c7fa-4d9d-8cc3-e36fbf59496a --pattern=de59d1b8-f99c-4c43-a8c0-2f6043546689.out --mode=contents | fileout: file --binary=true --mode=replace ""
{noformat}
Error Message:
{noformat}
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module file of type sink:
    mode: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found
{noformat}
Stacktrace:
{noformat}
2015-05-19 14:30:56,329 1.2.0.SNAP ERROR qtp671416633-35 rest.RestControllerAdvice - Caught exception while handling a request
org.springframework.xd.dirt.plugins.ModuleConfigurationException: Error with option(s) for module file of type sink:
    mode: Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found
	at org.springframework.xd.dirt.plugins.ModuleConfigurationException.fromBindException(ModuleConfigurationException.java:55)
	at org.springframework.xd.dirt.stream.XDStreamParser.buildModuleDescriptors(XDStreamParser.java:191)
	at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:122)
	at org.springframework.xd.dirt.stream.AbstractDeployer.validateBeforeSave(AbstractDeployer.java:115)
	at org.springframework.xd.dirt.rest.XDController.save(XDController.java:260)
	at sun.reflect.GeneratedMethodAccessor191.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:776)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:705)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:959)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:893)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:868)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:755)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:102)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.validation.BindException: org.springframework.validation.BeanPropertyBindingResult: 1 errors
Field error in object 'target' on field 'mode': rejected value [replace]; codes [typeMismatch.target.mode,typeMismatch.mode,typeMismatch.org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode,typeMismatch]; arguments [org.springframework.context.support.DefaultMessageSourceResolvable: codes [target.mode,mode]; arguments []; default message [mode]]; default message [Failed to convert property value of type 'java.lang.String' to required type 'org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode' for property 'mode'; nested exception is java.lang.IllegalStateException: Cannot convert value of type [java.lang.String] to required type [org.springframework.xd.dirt.modules.metadata.FileSinkOptionsMetadata$Mode] for property 'mode': no matching editors or conversion strategy found]
	at org.springframework.xd.module.options.PojoModuleOptionsMetadata.bindAndValidate(PojoModuleOptionsMetadata.java:205)
	at org.springframework.xd.module.options.PojoModuleOptionsMetadata.interpolate(PojoModuleOptionsMetadata.java:139)
	at org.springframework.xd.module.options.FlattenedCompositeModuleOptionsMetadata.interpolate(FlattenedCompositeModuleOptionsMetadata.java:152)
	at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver$ModuleOptionsMetadataWithDefaults.interpolate(EnvironmentAwareModuleOptionsMetadataResolver.java:168)
	at org.springframework.xd.dirt.stream.XDStreamParser.buildModuleDescriptors(XDStreamParser.java:188)
	... 61 more
{noformat}",3.0,False,3.0,0,0.0,-1,1,False
"XD-3079","Bug","Sprint 50","2015-05-19T08:01:17.000+0000","cgiha","","Create a new Kerberos ticket instead of renew the current one","Running Spring-XD singlenode with a kerberized hadoop cluster on CDH 5.3.2. with JDK 1.7 and JCE 1.7.
The kerberos ticket policies are:
* expiration: 24 hours
* renew: 7 days

I need to keep the Spring XD server running constantly because my flows are always waiting for incoming files to be ingested into the HDFS, but the kerberos session expires if there aren't jobs to run before the expiration date. The expiration policies can't be changed due internal company policies.

Is there a way which Spring XD can generate a new ticket instead of renew the current one when a job or stream start executing?

The Spring XD server has configured the hadoop.properties like:

# Use servers.yml to change URI for namenode
# You can add additional properties in this file
dfs.namenode.kerberos.principal=hdfs/_HOST@EDA.COMPANY.COM
yarn.resourcemanager.principal=yarn/_HOST@EDA.COMPANY.COM

yarn.application.classpath=/opt/cloudera/parcels/CDH/lib/hadoop/*,/opt/cloudera/parcels/CDH/lib/hadoop/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/lib/*

hadoop.security.authorization=true
hadoop.security.authentication=kerberos

spring.hadoop.userKeytab=file:///export/home/user/user.keytab
spring.hadoop.userPrincipal=user@ERS.COMPANY.COM

#Connecting to Kerberized Hadoop (Spring XD doc configuration Appendix D)
spring.hadoop.security.authMethod=kerberos
spring.hadoop.security.userKeytab=/export/home/user/user.keytab
spring.hadoop.security.userPrincipal=user@ERS.COMPANY.COM
spring.hadoop.security.namenodePrincipal=hdfs/_HOST@EDA.COMPANY.COM
spring.hadoop.security.rmManagerPrincipal=yarn/_HOST@EDA.COMPANY.COM",5.0,0.4673199378827231,0.407554437384293,0.1342390972650766,True,True,True,1432022477000.0,1432711601000.0,1432715201000.0,1433496207000.0,1433497107000.0,1432022477000.0,-1.0,1432220430000.0,1432224030000.0,1432623469000.0,1432627069000.0,1432711601000.0,1432715201000.0,"cgiha",1432224030000.0,1432022477000.0,45,1432022477000.0,-1.0,"Create a new Kerberos ticket instead of renew the current one","Running Spring-XD singlenode with a kerberized hadoop cluster on CDH 5.3.2. with JDK 1.7 and JCE 1.7.
The kerberos ticket policies are:
* expiration: 24 hours
* renew: 7 days

I need to keep the Spring XD server running constantly because my flows are always waiting for incoming files to be ingested into the HDFS, but the kerberos session expires if there aren't jobs to run before the expiration date. The expiration policies can't be changed due internal company policies.

Is there a way which Spring XD can generate a new ticket instead of renew the current one when a job or stream start executing?

The Spring XD server has configured the hadoop.properties like:

# Use servers.yml to change URI for namenode
# You can add additional properties in this file
dfs.namenode.kerberos.principal=hdfs/_HOST@EDA.COMPANY.COM
yarn.resourcemanager.principal=yarn/_HOST@EDA.COMPANY.COM

yarn.application.classpath=/opt/cloudera/parcels/CDH/lib/hadoop/*,/opt/cloudera/parcels/CDH/lib/hadoop/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/*,/opt/cloudera/parcels/CDH/lib/hadoop-hdfs/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/*,/opt/cloudera/parcels/CDH/lib/hadoop-yarn/lib/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/*,/opt/cloudera/parcels/CDH/lib/hadoop-mapreduce/lib/*

hadoop.security.authorization=true
hadoop.security.authentication=kerberos

spring.hadoop.userKeytab=file:///export/home/user/user.keytab
spring.hadoop.userPrincipal=user@ERS.COMPANY.COM

#Connecting to Kerberized Hadoop (Spring XD doc configuration Appendix D)
spring.hadoop.security.authMethod=kerberos
spring.hadoop.security.userKeytab=/export/home/user/user.keytab
spring.hadoop.security.userPrincipal=user@ERS.COMPANY.COM
spring.hadoop.security.namenodePrincipal=hdfs/_HOST@EDA.COMPANY.COM
spring.hadoop.security.rmManagerPrincipal=yarn/_HOST@EDA.COMPANY.COM",5.0,False,5.0,0,0.0,0,0,False
"XD-3078","Bug","Sprint 50","2015-05-19T01:48:44.000+0000","lukasz.nowanski@emc.com","","Spring XD admin fails to redeploy modules after Spring XD container successfully reconnectes to Zookeeper","We are running Spring XD 1.1.1 in our production environment and Zookeeper 3.4.5.  Zookeeper is running in failover mode and consists of three independent nodes set up on three separate VMs. From time to time we get ""Connection to Zookeeper Suspended"" event which causes one of the containers in the cluster to be removed from the SpringXD cluster. Modules being deployed on this removed node fail to be re-deployed to other containers in the cluster.

Affected versions:
- SpringXD 1.1.1
- Zookeeper 3.4.5 and 3.4.6

Cluster set up in PROD environment where error occurs:
- 4 Spring-XD dedicated servers
- 4 spring-xd containers (each running on designated server )
- 2 spring-xd admins ( each running alongside one spring-xd container)
- 3 Zookeeper nodes ( 3 designated servers on PAITO environment )

Cluster set up in TEST environment where error also occurred:
- 2 Spring-XD dedicated servers running one spring-xd container and one spring-xd admin each
- 3 Zookeeper nodes running on 3 dedicated servers (PAITO Test environment)

Cluster set up to reproduce error found in PROD environment:
- 1 spring-xd admin
- 3 spring xd-containers (each running on a designated VM )
- 3 zookeeper servers running on one VM

Steps to reproduce:

1)	Set up three node Zookeeper cluster. Attached is example zoo.cfg, we are using default configuration values. In this particular test case we run all Zookeeper nodes on a single VM as we were not testing network layer interruptions.
2)	Set up one Spring XD admin node. Please note that we have also observed this on two node Spring XD admin cluster. 
3)	Set up three Spring XD container nodes. All of them belong to one group (SA) and two of them also belong to second group (HA1). This is configured in $XD_HOME/config/servers.yml however so far group configuration never influenced test outcome.
4)	Create and deploy a test stream using following XD Shell commands:
stream create --name test-zookeeper-failover --definition ""syslog-udp --port=5140 | transform | file --dir='/opt/pivotal/spring-xd/xd/output'""
stream deploy --name test-zookeeper-failover --properties ""module.syslog-udp.criteria=groups.contains('HA1'),module.syslog-udp.count=2,module.file.criteria=groups.contains('SA'),module.file.count=3,module.transform.criteria=groups.contains('SA')""
5)	Ensure that test stream works and handles traffic on UDP port 5140
6)	Shutdown one of the Zookeeper nodes by issuing a stop command.
7)	Two Spring XD containers were not affected and remained in Spring XD cluster.
8)	One Spring XD container was kicked out of Spring XD cluster and was no longer visible on Spring XD admin Web UI. Modules previously deployed to this container were not redeployed to other cluster members.
9)	On the failed Spring XD container we have observed CONNECTION_SUSPEND, CONNECTION_RECONECTED and CHILD_REMOVE Zookeeper events (attached is container-log.txt). Please note that Java process is still running and we see ConnectionStateManager-0 server.ContainerRegistrar - Waiting for supervisor to clean up prior deployments messages.
10)	Spring XD admin failed with exception in DepartingContainerModuleRedeployer (attached is admin-log.txt). 
11)	We have observed that departing container node in Zookeeper (/sa/deployments/modules/allocated/1d3fd4cc-5a70-47ed-b4f3-22deef1f4d4f/) had no children. We did this after few minutes so we are not sure at which point it was cleared. 
12)	Restarting failed Spring XD container fixed the problem, modules were correctly redeployed.
Exception from point 10 is very similar to XD-1983 and this code was rewritten in XD-2004.
",8.0,0.0805565613147135,0.079808935127294,0.0283189560382458,True,True,True,1432000124000.0,1432627982000.0,1432631582000.0,1439793226000.0,1439794126000.0,1432000124000.0,1438768566000.0,1432220842000.0,1432224442000.0,1432622155000.0,1432625755000.0,1432627982000.0,1432631582000.0,"lukasz.nowanski@emc.com",1432224442000.0,1432000124000.0,45,1438768566000.0,1438768566000.0,"Spring XD admin fails to redeploy modules after Spring XD container successfully reconnectes to Zookeeper","We are running Spring XD 1.1.1 in our production environment and Zookeeper 3.4.5.  Zookeeper is running in failover mode and consists of three independent nodes set up on three separate VMs. From time to time we get ""Connection to Zookeeper Suspended"" event which causes one of the containers in the cluster to be removed from the SpringXD cluster. Modules being deployed on this removed node fail to be re-deployed to other containers in the cluster.

Affected versions:
- SpringXD 1.1.1
- Zookeeper 3.4.5 and 3.4.6

Cluster set up in PROD environment where error occurs:
- 4 Spring-XD dedicated servers
- 4 spring-xd containers (each running on designated server )
- 2 spring-xd admins ( each running alongside one spring-xd container)
- 3 Zookeeper nodes ( 3 designated servers on PAITO environment )

Cluster set up in TEST environment where error also occurred:
- 2 Spring-XD dedicated servers running one spring-xd container and one spring-xd admin each
- 3 Zookeeper nodes running on 3 dedicated servers (PAITO Test environment)

Cluster set up to reproduce error found in PROD environment:
- 1 spring-xd admin
- 3 spring xd-containers (each running on a designated VM )
- 3 zookeeper servers running on one VM

Steps to reproduce:

1)	Set up three node Zookeeper cluster. Attached is example zoo.cfg, we are using default configuration values. In this particular test case we run all Zookeeper nodes on a single VM as we were not testing network layer interruptions.
2)	Set up one Spring XD admin node. Please note that we have also observed this on two node Spring XD admin cluster. 
3)	Set up three Spring XD container nodes. All of them belong to one group (SA) and two of them also belong to second group (HA1). This is configured in $XD_HOME/config/servers.yml however so far group configuration never influenced test outcome.
4)	Create and deploy a test stream using following XD Shell commands:
stream create --name test-zookeeper-failover --definition ""syslog-udp --port=5140 | transform | file --dir='/opt/pivotal/spring-xd/xd/output'""
stream deploy --name test-zookeeper-failover --properties ""module.syslog-udp.criteria=groups.contains('HA1'),module.syslog-udp.count=2,module.file.criteria=groups.contains('SA'),module.file.count=3,module.transform.criteria=groups.contains('SA')""
5)	Ensure that test stream works and handles traffic on UDP port 5140
6)	Shutdown one of the Zookeeper nodes by issuing a stop command.
7)	Two Spring XD containers were not affected and remained in Spring XD cluster.
8)	One Spring XD container was kicked out of Spring XD cluster and was no longer visible on Spring XD admin Web UI. Modules previously deployed to this container were not redeployed to other cluster members.
9)	On the failed Spring XD container we have observed CONNECTION_SUSPEND, CONNECTION_RECONECTED and CHILD_REMOVE Zookeeper events (attached is container-log.txt). Please note that Java process is still running and we see ConnectionStateManager-0 server.ContainerRegistrar - Waiting for supervisor to clean up prior deployments messages.
10)	Spring XD admin failed with exception in DepartingContainerModuleRedeployer (attached is admin-log.txt). 
11)	We have observed that departing container node in Zookeeper (/sa/deployments/modules/allocated/1d3fd4cc-5a70-47ed-b4f3-22deef1f4d4f/) had no children. We did this after few minutes so we are not sure at which point it was cleared. 
12)	Restarting failed Spring XD container fixed the problem, modules were correctly redeployed.
Exception from point 10 is very similar to XD-1983 and this code was rewritten in XD-2004.
",5.0,True,5.0,1,60.0,0,0,False
"XD-3077","Story","Sprint 49","2015-05-18T15:20:13.000+0000","sabby","","Default HDFS as custom module registry for YARN deployments","As a developer, I'd like to default to HDFS as distributed remote location for custom module registry, so I can use xd-shell or the REST-API directly to upload the custom module bits. I would also like to remove custom-modules.zip artifact from YARN distribution.",3.0,0.455617167216898,0.455617167216898,0.0,True,True,True,1431962413000.0,1432033487000.0,1432037087000.0,1432117508000.0,1432118408000.0,1431962413000.0,-1.0,1431962413000.0,1431966013000.0,1432033487000.0,1432037087000.0,1432033487000.0,1432037087000.0,"sabby",1431966013000.0,1431962413000.0,45,1431962413000.0,-1.0,"Default HDFS as custom module registry for YARN deployments","As a developer, I'd like to default to HDFS as distributed remote location for custom module registry, so I can use xd-shell or the REST-API directly to upload the custom module bits. I would also like to remove custom-modules.zip artifact from YARN distribution.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3076","Story","Sprint 49","2015-05-18T14:42:15.000+0000","sabby","","Add SSL properties to the Mail source","As a user, I'd like to use the _Mail_ source to connect to secured IMAP and/or SMTP mail servers. 

_Mail_ source config file requires a <util:properties/> bean (with ssl/tls properties), provided to the adapter via the java-mail-properties attribute. [Ref. Example|http://docs.spring.io/spring-integration/docs/latest-ga/reference/html/mail.html].

{code:xml}
   <beans:beans profile=""default"">
        <util:properties id=""javaMailProperties"">
            <beans:prop key=""mail.imap.socketFactory.class"">javax.net.ssl.SSLSocketFactory</beans:prop>
            <beans:prop key=""mail.imap.socketFactory.fallback"">false</beans:prop>
            <beans:prop key=""mail.store.protocol"">imaps</beans:prop>
            <beans:prop key=""mail.debug"">false</beans:prop>
        </util:properties>
    </beans:beans>
{code}

[List of all java-mail properties|https://javamail.java.net/nonav/docs/api/com/sun/mail/smtp/package-summary.html]",1.0,0.3340601940433052,0.3340423788298255,0.0,True,True,True,1431960135000.0,1432053892000.0,1432057492000.0,1432239894000.0,1432240794000.0,1431960135000.0,1431960246000.0,1431960135000.0,1431963735000.0,1432053887000.0,1432057487000.0,1432053892000.0,1432057492000.0,"sabby",1431963735000.0,1431960246000.0,45,1431960246000.0,-1.0,"Add SSL properties to the Mail source","As a user, I'd like to use the _Mail_ source to connect to secured IMAP and/or SMTP mail servers. 

_Mail_ source config file requires a <util:properties/> bean (with ssl/tls properties), provided to the adapter via the java-mail-properties attribute. [Ref. Example|http://docs.spring.io/spring-integration/docs/latest-ga/reference/html/mail.html].

{code:xml}
   <beans:beans profile=""default"">
        <util:properties id=""javaMailProperties"">
            <beans:prop key=""mail.imap.socketFactory.class"">javax.net.ssl.SSLSocketFactory</beans:prop>
            <beans:prop key=""mail.imap.socketFactory.fallback"">false</beans:prop>
            <beans:prop key=""mail.store.protocol"">imaps</beans:prop>
            <beans:prop key=""mail.debug"">false</beans:prop>
        </util:properties>
    </beans:beans>
{code}

[List of all java-mail properties|https://javamail.java.net/nonav/docs/api/com/sun/mail/smtp/package-summary.html]",1.0,False,1.0,0,0.0,0,0,False
"XD-3075","Bug","Sprint 49","2015-05-18T13:01:48.000+0000","mbogoevici","mbogoevici","Backport Kafka Sink input fix","As part of XD-2958, we've changed the input type of the Kafka sink from String to byte[]. The main reason for the change was that it required an arbitrary and often unneeded (but expensive) conversion to String for the bus payloads. 

Apply the same change to 1.1 branch.",1.0,0.1239502855223379,0.1234464225730601,0.0167954316425932,True,True,True,1431954108000.0,1431955584000.0,1431959184000.0,1431965116000.0,1431966016000.0,1431954108000.0,-1.0,1431954308000.0,1431957908000.0,1431955578000.0,1431959178000.0,1431955584000.0,1431959184000.0,"mbogoevici",1431957908000.0,1431954108000.0,45,1431954108000.0,-1.0,"Backport Kafka Sink input fix","As part of XD-2958, we've changed the input type of the Kafka sink from String to byte[]. The main reason for the change was that it required an arbitrary and often unneeded (but expensive) conversion to String for the bus payloads. 

Apply the same change to 1.1 branch.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3074","Bug","Sprint 49","2015-05-18T12:58:54.000+0000","mbogoevici","mbogoevici","Backport metadata retrieval stability improvements","Backport stability improvements added as part of XD-2958 to the 1.1.x branch.",1.0,0.1384564425306393,0.0,0.0303080490228552,True,True,True,1431953934000.0,1431955606000.0,1431959206000.0,1431965110000.0,1431966010000.0,1431953934000.0,-1.0,1431954300000.0,1431957900000.0,1431953934000.0,1431957534000.0,1431955606000.0,1431959206000.0,"mbogoevici",1431957900000.0,1431953934000.0,45,1431953934000.0,-1.0,"Backport metadata retrieval stability improvements","Backport stability improvements added as part of XD-2958 to the 1.1.x branch.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3072","Story","Sprint 49","2015-05-18T08:07:46.000+0000","sabby","aclement","Flo parser improvements","As a Flo developer, I'd like to add improvements to existing Flo parser endpoints, so I can streamline the error reporting strategy.",3.0,0.0001058316911174,8.0286110502883e-05,0.0,True,True,True,1431936466000.0,1431936495000.0,1431940095000.0,1432209586000.0,1432210486000.0,1431936466000.0,-1.0,1431936466000.0,1431940066000.0,1431936488000.0,1431940088000.0,1431936495000.0,1431940095000.0,"sabby",1431940066000.0,1431936466000.0,45,1431936466000.0,-1.0,"Flo parser improvements","As a Flo developer, I'd like to add improvements to existing Flo parser endpoints, so I can streamline the error reporting strategy.",3.0,False,3.0,0,0.0,0,0,False
"XD-3071","Story","Sprint 49","2015-05-18T07:13:44.000+0000","sabby","","Spike: Produce Rabbit baseline on rackspace infrastructure","As a developer, I'd like to bench Rabbit on rackspace infrastructure, so I can have a sense on how it scales as we add more _xd-container_ nodes.",8.0,0.4401864159569596,0.4401864159569596,0.0,True,True,True,1431933224000.0,1432047606000.0,1432051206000.0,1432192173000.0,1432193073000.0,1431933224000.0,-1.0,1431933224000.0,1431936824000.0,1432047606000.0,1432051206000.0,1432047606000.0,1432051206000.0,"sabby",1431936824000.0,1431933224000.0,45,1431933224000.0,-1.0,"Spike: Produce Rabbit baseline on rackspace infrastructure","As a developer, I'd like to bench Rabbit on rackspace infrastructure, so I can have a sense on how it scales as we add more _xd-container_ nodes.",8.0,False,8.0,0,0.0,0,1,False
"XD-3070","Story","Sprint 49","2015-05-18T07:08:17.000+0000","pperalta","pperalta","Spike: introduce xolpoc-admin to XD Admin","The POC for XD on Lattice uses the following interface for module deployment:

https://github.com/markfisher/xolpoc-admin/blob/master/src/main/java/xolpoc/spi/ModuleDeployer.java

{code}
public interface ModuleDeployer {

	void deploy(ModuleDescriptor descriptor);

	void undeploy(ModuleDescriptor descriptor);

	ModuleStatus getStatus(ModuleDescriptor descriptor);

}
{code}

This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to:
* Demo a POC showing simple stream deployment with the existing shell/admin to Lattice
* Learn from the experience to help guide the re-architecture/splitting of stream/job repositories (especially in regard to {{AbstractDeployer}} and related classes).

Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR.",5.0,0.0001217631301241,0.0001217631301241,0.0001192781682849,True,True,True,1431932897000.0,1431933044000.0,1431936644000.0,1433139259000.0,1433140159000.0,1431932897000.0,-1.0,1431933041000.0,1431936641000.0,1431933044000.0,1431936644000.0,1431933044000.0,1431936644000.0,"pperalta",1431936641000.0,1431932897000.0,45,1431932897000.0,-1.0,"Spike: introduce xolpoc-admin to XD Admin","The POC for XD on Lattice uses the following interface for module deployment:

https://github.com/markfisher/xolpoc-admin/blob/master/src/main/java/xolpoc/spi/ModuleDeployer.java

{code}
public interface ModuleDeployer {

	void deploy(ModuleDescriptor descriptor);

	void undeploy(ModuleDescriptor descriptor);

	ModuleStatus getStatus(ModuleDescriptor descriptor);

}
{code}

This spike is to introduce this interface and the Lattice implementation in the XD admin. The goals are to:
* Demo a POC showing simple stream deployment with the existing shell/admin to Lattice
* Learn from the experience to help guide the re-architecture/splitting of stream/job repositories (especially in regard to {{AbstractDeployer}} and related classes).

Note that this work will not necessarily be merged into XD itself, although some of the concepts may be included in a future PR.",5.0,False,5.0,0,0.0,0,0,False
"XD-3066","Story","","2015-05-15T08:34:42.000+0000","hillert","","Make Enum Conversions for ModuleOptions more lenient","If you have a an option *--mode=textLine*, presently the enum MUST be named *textLine*.

I think it would improve the user-experience if we allowed users to pass in values such as:

* --mode=textLine
* --mode=text_line
* --mode=TEXT_LINE

",3.0,0.8470066149702549,0.8470066149702549,-1.0,True,False,True,1431678882000.0,1432136767000.0,1432140367000.0,1432218574000.0,1432219474000.0,1431678882000.0,-1.0,-1.0,-1.0,1432136767000.0,1432140367000.0,1432136767000.0,1432140367000.0,"hillert",-1.0,-1.0,0,1431678882000.0,-1.0,"Make Enum Conversions for ModuleOptions more lenient","If you have a an option *--mode=textLine*, presently the enum MUST be named *textLine*.

I think it would improve the user-experience if we allowed users to pass in values such as:

* --mode=textLine
* --mode=text_line
* --mode=TEXT_LINE

",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-3064","Bug","Sprint 49","2015-05-15T06:25:23.000+0000","grenfro","","HdfsMongoDB Job failing due because of missing ID in Default Tuple","Looks to have been introduced by https://github.com/spring-projects/spring-xd/pull/1577
Deployment: single admin, 2 container deployment using +RabbitMQ+ as the transport.
Below is a partial stacktrace (please check log for full stacktrace).
Log is attached.
{noformat)
2015-05-15 10:50:15,843 1.2.0.SNAP ERROR xdbus.job:ec2Job3-1 step.AbstractStep - Encountered an error executing step readResourcesStep in job ec2Job3
org.springframework.dao.InvalidDataAccessApiUsageException: Cannot autogenerate id of type java.util.UUID for entity of type org.springframework.xd.tuple.DefaultTuple!
        at org.springframework.data.mongodb.core.MongoTemplate.assertUpdateableIdIfNotSet(MongoTemplate.java:1153)
        at org.springframework.data.mongodb.core.MongoTemplate.doSave(MongoTemplate.java:882)
        at org.springframework.data.mongodb.core.MongoTemplate.save(MongoTemplate.java:837)
        at org.springframework.batch.item.data.MongoItemWriter.doWrite(MongoItemWriter.java:128)
        at org.springframework.batch.item.data.MongoItemWriter$1.beforeCommit(MongoItemWriter.java:156)
        at org.springframework.transaction.support.TransactionSynchronizationUtils.triggerBeforeCommit(TransactionSynchronizationUtils.java:95)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.triggerBeforeCommit(AbstractPlatformTransactionManager.java:928)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:740)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:726)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
{noformat)",3.0,0.9986325021675306,0.9986325021675306,0.0067277172274958,True,True,True,1431671123000.0,1432207866000.0,1432208601000.0,1432207701000.0,1432208601000.0,1431671123000.0,-1.0,1431674739000.0,1431678339000.0,1432207866000.0,1432208601000.0,1432207866000.0,1432208601000.0,"grenfro",1431678339000.0,1431671123000.0,45,1431671123000.0,-1.0,"HdfsMongoDB Job failing due because of missing ID in Default Tuple","Looks to have been introduced by https://github.com/spring-projects/spring-xd/pull/1577
Deployment: single admin, 2 container deployment using +RabbitMQ+ as the transport.
Below is a partial stacktrace (please check log for full stacktrace).
Log is attached.
{noformat)
2015-05-15 10:50:15,843 1.2.0.SNAP ERROR xdbus.job:ec2Job3-1 step.AbstractStep - Encountered an error executing step readResourcesStep in job ec2Job3
org.springframework.dao.InvalidDataAccessApiUsageException: Cannot autogenerate id of type java.util.UUID for entity of type org.springframework.xd.tuple.DefaultTuple!
        at org.springframework.data.mongodb.core.MongoTemplate.assertUpdateableIdIfNotSet(MongoTemplate.java:1153)
        at org.springframework.data.mongodb.core.MongoTemplate.doSave(MongoTemplate.java:882)
        at org.springframework.data.mongodb.core.MongoTemplate.save(MongoTemplate.java:837)
        at org.springframework.batch.item.data.MongoItemWriter.doWrite(MongoItemWriter.java:128)
        at org.springframework.batch.item.data.MongoItemWriter$1.beforeCommit(MongoItemWriter.java:156)
        at org.springframework.transaction.support.TransactionSynchronizationUtils.triggerBeforeCommit(TransactionSynchronizationUtils.java:95)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.triggerBeforeCommit(AbstractPlatformTransactionManager.java:928)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:740)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:726)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
{noformat)",3.0,False,3.0,0,0.0,0,0,False
"XD-3063","Story","Sprint 49","2015-05-15T05:34:49.000+0000","grussell","grussell","Add Property maxMessagesPerPoll to All Polled Sources","Polled message sources return only one message per poll by default.

When polling, say, a file directory with many files, files will be emitted once per {{fixedDelay}}.

As a user I need to configure a limit for the number of messages that will be emitted per poll.",3.0,0.6168816425120773,0.030903381642512,0.6168478260869565,True,True,True,1431668089000.0,1431923478000.0,1431927078000.0,1432081189000.0,1432082089000.0,1431668089000.0,-1.0,1431923464000.0,1431927064000.0,1431680883000.0,1431684483000.0,1431923478000.0,1431927078000.0,"grussell",1431927064000.0,1431668089000.0,45,1431668089000.0,-1.0,"Add Property maxMessagesPerPoll to All Polled Sources","Polled message sources return only one message per poll by default.

When polling, say, a file directory with many files, files will be emitted once per {{fixedDelay}}.

As a user I need to configure a limit for the number of messages that will be emitted per poll.",3.0,False,3.0,0,0.0,-1,0,False
"XD-3061","Improvement","Sprint 49","2015-05-14T15:32:48.000+0000","mark.pollack","mark.pollack","Remove id/timestamp from tuple and related methods on tuplebuilder","As a developer, I'd like to remove _ID_ and _TimeStamp_ attributes from the {{Tuple}} class, so I can improve performance characteristics by not having them go through _serde_; instead, we could leverage message headers to collect such information. 
",3.0,0.1032194638274049,0.0004266312321042,0.0,True,True,True,1431617568000.0,1431678779000.0,1431682379000.0,1432209686000.0,1432210586000.0,1431617568000.0,-1.0,1431617568000.0,1431621168000.0,1431617821000.0,1431621421000.0,1431678779000.0,1431682379000.0,"mark.pollack",1431621168000.0,1431617568000.0,45,1431617568000.0,-1.0,"Remove id/timestamp from tuple and related methods on tuplebuilder","As a developer, I'd like to remove _ID_ and _TimeStamp_ attributes from the {{Tuple}} class, so I can improve performance characteristics by not having them go through _serde_; instead, we could leverage message headers to collect such information. 
",3.0,False,3.0,0,0.0,-1,0,False
"XD-3056","Story","Sprint 50","2015-05-14T02:23:49.000+0000","simontao","","Add a new source module to capture video frame from camera or video files","This is a source module for video ingestion: the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image (encoded with JPEG) as the payload.   ",8.0,0.9999788510777492,0.9999788510777492,0.5107929999887205,True,True,True,1431570229000.0,1432279470000.0,1432279485000.0,1432278585000.0,1432279485000.0,1431570229000.0,-1.0,1431932512000.0,1431936112000.0,1432279470000.0,1432279485000.0,1432279470000.0,1432279485000.0,"simontao",1431936112000.0,1431932512000.0,45,1431932512000.0,-1.0,"Add a new source module to capture video frame from camera or video files","This is a source module for video ingestion: the modules captures video frames from a camera or from a video file. For camera, the frames are grabbed from the rtsp video stream. This module will generate message with the frame image (encoded with JPEG) as the payload.   ",8.0,False,8.0,0,0.0,0,2,False
"XD-3054","Bug","Sprint 50","2015-05-13T11:07:59.000+0000","iperumal","iperumal","Deployment validation when processing the deployment message","Currently, the deployment message is being validated before pushed the ZK distributed queue for deployment. When the message is consumed, there is no validation done. Since, the consumer consumes the messages asynchronously, we need validation at both sides.",1.0,0.6414347650736035,0.0,0.0054320427505545,True,True,True,1431515279000.0,1431617067000.0,1431620667000.0,1431673067000.0,1431673967000.0,1431515279000.0,-1.0,1431516141000.0,1431519741000.0,1431515279000.0,1431518879000.0,1431617067000.0,1431620667000.0,"iperumal",1431519741000.0,1431515279000.0,45,1431515279000.0,-1.0,"Deployment validation when processing the deployment message","Currently, the deployment message is being validated before pushed the ZK distributed queue for deployment. When the message is consumed, there is no validation done. Since, the consumer consumes the messages asynchronously, we need validation at both sides.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3052","Story","Sprint 49","2015-05-13T09:33:54.000+0000","sabby","mark.pollack","Move gpfdist sink from spring-xd-modules repo to the core","As a developer, I'd like to move the project reactor based [gpfdist|https://github.com/spring-projects/spring-xd-modules/tree/master/gpfdist] from spring-xd-module repo to the core, so I can natively use this sink to write to GPDB/HAWQ.
",2.0,0.6558595747789874,0.0,0.0,True,True,True,1431509634000.0,1431617355000.0,1431620955000.0,1431672978000.0,1431673878000.0,1431509634000.0,-1.0,1431509634000.0,1431513234000.0,1431509634000.0,1431513234000.0,1431617355000.0,1431620955000.0,"sabby",1431513234000.0,1431509634000.0,45,1431509634000.0,-1.0,"Move gpfdist sink from spring-xd-modules repo to the core","As a developer, I'd like to move the project reactor based [gpfdist|https://github.com/spring-projects/spring-xd-modules/tree/master/gpfdist] from spring-xd-module repo to the core, so I can natively use this sink to write to GPDB/HAWQ.
",2.0,False,2.0,0,0.0,-1,0,False
"XD-3051","Bug","Sprint 49","2015-05-13T09:13:12.000+0000","mminella","mminella","Gradle launch task is broken","Spring XD has a gradle task available in the build called launch that starts a single node instance.  This is currently broken.  The command I was using for this command was:
{code}
$ ./gradlew clean build -x test -x javadoc launch
{code}",1.0,0.0057601247841345,0.0,0.0101052866135591,True,True,True,1431508392000.0,1431508909000.0,1431512509000.0,1431597247000.0,1431598147000.0,1431508392000.0,-1.0,1431509299000.0,1431512899000.0,1431508392000.0,1431511992000.0,1431508909000.0,1431512509000.0,"mminella",1431512899000.0,1431508852000.0,45,1431508852000.0,-1.0,"Gradle launch task is broken","Spring XD has a gradle task available in the build called launch that starts a single node instance.  This is currently broken.  The command I was using for this command was:
{code}
$ ./gradlew clean build -x test -x javadoc launch
{code}",1.0,False,1.0,0,0.0,-1,0,False
"XD-3050","Story","Sprint 49","2015-05-12T11:53:10.000+0000","sabby","","Move Reactor based processor module from spring-xd-modules to core","As a developer, I'd like to move the project reactor based [data processor module|https://github.com/spring-projects/spring-xd-modules/tree/master/spring-xd-reactor] from _spring-xd-module_ repo to the core, so I can natively use Reactor's Stream API to build processor modules. ",3.0,0.2161937670277094,0.2161937670277094,0.0,True,True,True,1431431590000.0,1431457618000.0,1431461218000.0,1431551082000.0,1431551982000.0,1431431590000.0,-1.0,1431431590000.0,1431435190000.0,1431457618000.0,1431461218000.0,1431457618000.0,1431461218000.0,"sabby",1431435190000.0,1431431590000.0,45,1431431590000.0,-1.0,"Move Reactor based processor module from spring-xd-modules to core","As a developer, I'd like to move the project reactor based [data processor module|https://github.com/spring-projects/spring-xd-modules/tree/master/spring-xd-reactor] from _spring-xd-module_ repo to the core, so I can natively use Reactor's Stream API to build processor modules. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-3049","Story","Sprint 49","2015-05-11T09:00:29.000+0000","grenfro","","Profile byte array on In-Memory and Kafka Transport","Identify and report hotspots while running the load-generator source and the throughput sink on :
# Singlenode -> In Memory Transport
# Singlenode -> Kafka Transport
# Admin/Container -> Kafka Transport",3.0,0.0292178156958441,0.0291991878248235,0.0092254531229625,True,True,True,1431334829000.0,1431341103000.0,1431344703000.0,1431548661000.0,1431549561000.0,1431334829000.0,-1.0,1431336810000.0,1431340410000.0,1431341099000.0,1431344699000.0,1431341103000.0,1431344703000.0,"grenfro",1431340410000.0,1431334829000.0,45,1431334829000.0,-1.0,"Profile byte array on In-Memory and Kafka Transport","Identify and report hotspots while running the load-generator source and the throughput sink on :
# Singlenode -> In Memory Transport
# Singlenode -> Kafka Transport
# Admin/Container -> Kafka Transport",3.0,False,3.0,0,0.0,0,0,False
"XD-3048","Bug","Sprint 49","2015-05-11T08:44:44.000+0000","pharris","grussell","RabbitMQ queue cleanup uses wildcard unexpectedly","Calling the API to delete queues uses a wildcard-like behaviour unexpectedly. If I request to delete:

{{test-1}}

I expect it to delete streams named with the pattern:

{{test-1.*}}

For example, it would delete:

{{test-1.0, test-1.1, etc}}

In fact I believe it wildcards before and after the period, e.g.:

{{test-1*.*}}

And hence would delete:

{{test-1.0, test-11.0, test-123.0, etc}}

That way of working is potentially helpful, but it's also dangerous because it removes the ability to know that you're only deleting the exact queue you want to in all cases.

For the record the commit (https://github.com/spring-projects/spring-xd/commit/2d5f3f706330a6ead8e91c9a7a23d4372715614d) implies that it should work in the more restricted way above, not the less restricted way.

(Note: I've marked this as an improvement because, absent documentation, I don't know what the correct functionality is and hence can't say this is a bug)",1.0,0.5251707946680909,0.3923270356276565,0.3922981062376215,True,True,True,1431333884000.0,1431569880000.0,1431573480000.0,1431782354000.0,1431783254000.0,1431333884000.0,-1.0,1431510171000.0,1431513771000.0,1431510184000.0,1431513784000.0,1431569880000.0,1431573480000.0,"pharris",1431513771000.0,1431333884000.0,45,1431333884000.0,-1.0,"RabbitMQ queue cleanup uses wildcard unexpectedly","Calling the API to delete queues uses a wildcard-like behaviour unexpectedly. If I request to delete:

{{test-1}}

I expect it to delete streams named with the pattern:

{{test-1.*}}

For example, it would delete:

{{test-1.0, test-1.1, etc}}

In fact I believe it wildcards before and after the period, e.g.:

{{test-1*.*}}

And hence would delete:

{{test-1.0, test-11.0, test-123.0, etc}}

That way of working is potentially helpful, but it's also dangerous because it removes the ability to know that you're only deleting the exact queue you want to in all cases.

For the record the commit (https://github.com/spring-projects/spring-xd/commit/2d5f3f706330a6ead8e91c9a7a23d4372715614d) implies that it should work in the more restricted way above, not the less restricted way.

(Note: I've marked this as an improvement because, absent documentation, I don't know what the correct functionality is and hence can't say this is a bug)",1.0,False,1.0,0,0.0,0,0,False
"XD-3047","Story","Sprint 49","2015-05-11T08:22:27.000+0000","pperalta","pperalta","Complete Camera Ready DEBS submission","Complete and submit DEBS 2015 paper as described here:

http://www.debs2015.org/camera-ready-instructions.html",5.0,0.3386478429913208,0.0,0.0020932423160563,True,True,True,1431332547000.0,1431419909000.0,1431423509000.0,1431589620000.0,1431590520000.0,1431332547000.0,-1.0,1431333087000.0,1431336687000.0,1431332547000.0,1431336147000.0,1431419909000.0,1431423509000.0,"pperalta",1431336687000.0,1431332547000.0,45,1431332547000.0,-1.0,"Complete Camera Ready DEBS submission","Complete and submit DEBS 2015 paper as described here:

http://www.debs2015.org/camera-ready-instructions.html",5.0,False,5.0,0,0.0,-1,0,False
"XD-3046","Bug","","2015-05-10T05:32:09.000+0000","dturanski","","Fix compilation errors after moving SingleNodeApplication package","Samples including Singlenode tests need to update for package changes",2.0,0.9975683186660492,0.0016211208893006,-1.0,True,False,True,1431235929000.0,1431244544000.0,1431244565000.0,1431243665000.0,1431244565000.0,1431235929000.0,-1.0,-1.0,-1.0,1431235943000.0,1431239543000.0,1431244544000.0,1431244565000.0,"dturanski",-1.0,-1.0,0,1431235929000.0,-1.0,"Fix compilation errors after moving SingleNodeApplication package","Samples including Singlenode tests need to update for package changes",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-3043","Story","Sprint 49","2015-05-08T15:32:22.000+0000","sabby","","Document GF specific configuration properties","As a user, I'd like to use the GF source along with native GF authentication enabled, so I can consume data from GF in a secured way. I'd like to refer to documentation on where the GF specific native and security properties needs configured. 

See this [SC post|https://gopivotal-com.socialcast.com/messages/24377202] for more details.",1.0,0.8565109802825267,0.1947731980399492,0.0,True,True,True,1431099142000.0,1432131648000.0,1432135248000.0,1432303721000.0,1432304621000.0,1431099142000.0,-1.0,1431099142000.0,1431102742000.0,1431333937000.0,1431337537000.0,1432131648000.0,1432135248000.0,"sabby",1431102742000.0,1431099142000.0,45,1431099142000.0,-1.0,"Document GF specific configuration properties","As a user, I'd like to use the GF source along with native GF authentication enabled, so I can consume data from GF in a secured way. I'd like to refer to documentation on where the GF specific native and security properties needs configured. 

See this [SC post|https://gopivotal-com.socialcast.com/messages/24377202] for more details.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3042","Story","Sprint 48","2015-05-08T13:16:03.000+0000","mark.pollack","","Upgrade to Reactor 2.0.1","",5.0,0.9973295853829937,0.9973295853829937,0.0,True,True,True,1431090963000.0,1431098059000.0,1431098078000.0,1431097178000.0,1431098078000.0,1431090963000.0,-1.0,1431090963000.0,1431094563000.0,1431098059000.0,1431098078000.0,1431098059000.0,1431098078000.0,"mark.pollack",1431094563000.0,1431090963000.0,45,1431090963000.0,-1.0,"Upgrade to Reactor 2.0.1","",5.0,False,5.0,0,0.0,-1,0,False
"XD-3041","Improvement","Sprint 49","2015-05-08T10:25:22.000+0000","iperumal","iperumal","Disable MongoDB boot autoconfiguration at XD runtime","XD runtime (admin, container and singlenode) have MongoDB boot autoconfiguration enabled. This spins off MongoDB client and thereby the cleaner thread running on all these runtime.

The MongoDB based modules won't have any impact when we disable this autoconfiguration.
 ",1.0,0.0002077389690607,0.0,0.6995568235326705,True,True,True,1431080722000.0,1431080797000.0,1431084397000.0,1431440852000.0,1431441752000.0,1431080722000.0,-1.0,1431333283000.0,1431336883000.0,1431080722000.0,1431084322000.0,1431080797000.0,1431084397000.0,"iperumal",1431336883000.0,1431080722000.0,45,1431080722000.0,-1.0,"Disable MongoDB boot autoconfiguration at XD runtime","XD runtime (admin, container and singlenode) have MongoDB boot autoconfiguration enabled. This spins off MongoDB client and thereby the cleaner thread running on all these runtime.

The MongoDB based modules won't have any impact when we disable this autoconfiguration.
 ",1.0,False,1.0,0,0.0,-1,0,False
"XD-3036","Story","Sprint 49","2015-05-07T11:45:05.000+0000","dturanski","","Fix section headers in reference TOC","See:
http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#_introduction_26

There should be chapter/section title before this.",1.0,0.1975449423995443,0.1463462127501185,0.1439500889391874,True,True,True,1430999105000.0,1431457656000.0,1431461256000.0,1433319454000.0,1433320354000.0,1430999105000.0,-1.0,1431333249000.0,1431336849000.0,1431338811000.0,1431342411000.0,1431457656000.0,1431461256000.0,"dturanski",1431336849000.0,1430999105000.0,45,1430999105000.0,-1.0,"Fix section headers in reference TOC","See:
http://docs.spring.io/spring-xd/docs/current-SNAPSHOT/reference/html/#_introduction_26

There should be chapter/section title before this.",1.0,False,1.0,0,0.0,0,0,False
"XD-3033","Story","Sprint 49","2015-05-07T06:50:47.000+0000","mark.pollack","","Upgrade to Spring Data Fowler Release","Spring Data Gemfire is version 8.0.0 in Folwer, which is the same as in BDS (Should check minor version number in BDS).  ATM we are using gemfire 7.0.x",3.0,0.9011843243204608,0.9011843243204608,0.6812973962018712,True,True,True,1430981447000.0,1431611875000.0,1431615475000.0,1431680102000.0,1431681002000.0,1430981447000.0,-1.0,1431458052000.0,1431461652000.0,1431611875000.0,1431615475000.0,1431611875000.0,1431615475000.0,"mark.pollack",1431461652000.0,1430981447000.0,45,1430981447000.0,-1.0,"Upgrade to Spring Data Fowler Release","Spring Data Gemfire is version 8.0.0 in Folwer, which is the same as in BDS (Should check minor version number in BDS).  ATM we are using gemfire 7.0.x",3.0,False,3.0,0,0.0,-1,0,False
"XD-3030","Bug","Sprint 49","2015-05-06T08:09:47.000+0000","pharris","grussell","RabbitMQ queues not being removed on stream destroy","As part of p-spring-xd testing we create, deploy, exercise, undeploy and destroy a RabbitMQ to RabbitMQ stream every minute. Spring XD does not appear to be deleting the queues it creates internally for each stream. We have seen as many as ~9800 xdbus queues (via the RabbitMQ web ui) before RabbitMQ runs out of memory and blocks.",5.0,0.9103802871556071,0.9103802871556071,0.9103220799379124,True,True,True,1430899787000.0,1431509760000.0,1431513360000.0,1431568907000.0,1431569807000.0,1430899787000.0,-1.0,1431509721000.0,1431513321000.0,1431509760000.0,1431513360000.0,1431509760000.0,1431513360000.0,"pharris",1431513321000.0,1430899787000.0,45,1430899787000.0,-1.0,"RabbitMQ queues not being removed on stream destroy","As part of p-spring-xd testing we create, deploy, exercise, undeploy and destroy a RabbitMQ to RabbitMQ stream every minute. Spring XD does not appear to be deleting the queues it creates internally for each stream. We have seen as many as ~9800 xdbus queues (via the RabbitMQ web ui) before RabbitMQ runs out of memory and blocks.",5.0,False,5.0,0,0.0,0,0,False
"XD-3027","Story","Sprint 48","2015-05-06T04:29:05.000+0000","grussell","grussell","Update Spring Integration / Spring AMQP Versions","4.1.4 and 1.4.5 respectively.",1.0,0.8710000660411386,0.0,0.0,True,True,True,1430886545000.0,1431572360000.0,1431575960000.0,1431673033000.0,1431673933000.0,1430886545000.0,-1.0,1430886545000.0,1430890145000.0,1430886545000.0,1430890145000.0,1431572360000.0,1431575960000.0,"grussell",1430890145000.0,1430886545000.0,45,1430886545000.0,-1.0,"Update Spring Integration / Spring AMQP Versions","4.1.4 and 1.4.5 respectively.",1.0,False,1.0,0,0.0,0,0,False
"XD-3025","Bug","Sprint 48","2015-05-04T12:39:44.000+0000","bzeyben","thomas.risberg","SpringXD sqoop module is hanging","The SpringXD Sqoop module is in execution status until it times out, it is hanging. 

The container logs show:

2015-05-04 15:15:45,365 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying job 'sqoop_lookup'
2015-05-04 15:15:45,536 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying module [ModuleDescriptor@239b037a moduleName = 'sqoop', moduleLabel = 'sqoop', group = 'sqoop_lookup', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map['args' -> '--connect=jdbc:oracle:thin:@************:****/******* username=******** --password-file=/user/zeybeb/workspace/secure-files/gdw.password --table=MASTERDATA.W_LOOKUP_D --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d -m 1', 'command' -> 'import'], children = list[[empty]]]
2015-05-04 15:16:17,061 1.2.0.M1  INFO inbound.job:sqoop_lookup-redis:queue-inbound-channel-adapter1 sqoop.SqoopTasklet - Sqoop system.out: /tmp/Sqoop-948322291323951735.out

The /tmp/Sqoop-948322291323951735.out file content is:

15:16:17,612  INFO main sqoop.SqoopRunner - Sqoop command: import
15:16:17,613  INFO main sqoop.SqoopRunner - Using args: [--connect=jdbc:oracle:thin:@************:****/*******, username=*********, --password-file=/user/zeybeb/workspace/secure-files/gdw.password, --table=MASTERDATA.W_LOOKUP_D, --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d, -m, 1]
15:16:17,613  INFO main sqoop.SqoopRunner - Mapreduce home: /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21
15:16:17,631  INFO main sqoop.SqoopRunner - Setting configuration property: fs.defaultFS=hdfs://ilabphd07.isus.emc.com:8020
15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.resourcemanager.address=ilabphd08.isus.emc.com:8050
15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.application.classpath=/usr/hdp/2.2.0.0-2041/etc/hadoop/conf.empty,/usr/hdp/2.2.0.0-2041/hadoop/*,/usr/hdp/2.2.0.0-2041/hadoop/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/lib/*,/usr/hdp/2.2.0.0-2041/sqoop/*,/usr/hdp/2.2.0.0-2041/sqoop/lib/*,/usr/hdp/2.2.0.0-2041/flume/*,/usr/hdp/2.2.0.0-2041/flume/lib/*,/usr/hdp/2.2.0.0-2041/storm/*,/usr/hdp/2.2.0.0-2041/storm/lib/*
15:16:17,754  INFO main sqoop.SqoopRunner - Setting configuration property: mapreduce.framework.name=yarn
15:16:17,837  WARN main tool.SqoopTool - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration.
15:16:17,907  INFO main sqoop.Sqoop - Running Sqoop version: 1.4.5
15:16:18,282  WARN main util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15:16:19,552  WARN main sqoop.ConnFactory - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration.
15:16:19,657  INFO main oracle.OraOopManagerFactory - Data Connector for Oracle and Hadoop is disabled.
15:16:19,673  INFO main manager.SqlManager - Using default fetchSize of 1000
15:16:19,673  INFO main tool.CodeGenTool - Beginning code generation
15:16:20,639  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:20,853  INFO main manager.SqlManager - Executing SQL statement: SELECT t.* FROM MASTERDATA.W_LOOKUP_D t WHERE 1=0
15:16:21,018  INFO main orm.CompilationManager - HADOOP_MAPRED_HOME is /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21
15:16:23,171  INFO main orm.CompilationManager - Writing jar file: /tmp/sqoop-spring-xd/compile/4e11123a52fa36d6677efdb47bcdc43b/MASTERDATA.W_LOOKUP_D.jar
15:16:23,191  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:24,109  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:24,825  INFO main mapreduce.ImportJobBase - Beginning import of MASTERDATA.W_LOOKUP_D
15:16:24,848  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:24,876  WARN main mapreduce.JobBase - SQOOP_HOME is unset. May not be able to find all job dependencies.
15:16:25,083  INFO main client.RMProxy - Connecting to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050
15:16:25,977  INFO main db.DBInputFormat - Using read commited transaction isolation
15:16:26,117  INFO main mapreduce.JobSubmitter - number of splits:1
15:16:26,361  INFO main mapreduce.JobSubmitter - Submitting tokens for job: job_1429280992648_0019
15:16:26,717  INFO main impl.YarnClientImpl - Submitted application application_1429280992648_0019 to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050
15:16:26,782  INFO main mapreduce.Job - The url to track the job: http://http://ilabphd08.isus.emc.com:8088/proxy/application_1429280992648_0019/
15:16:26,783  INFO main mapreduce.Job - Running job: job_1429280992648_0019

The logs on the Hadoop side are:

Showing 4096 bytes. Click here for full log
mumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:04,026 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:05,033 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:06,042 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:07,049 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:08,056 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:09,062 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:10,069 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:41,093 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:42,100 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:43,107 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:44,113 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:45,120 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:46,129 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:47,136 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:48,143 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:49,150 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:50,156 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)

Even though  the maxRetries is set to 10 the process is going into several sets of retrials. 

",5.0,0.2377279261557085,0.0254634183248846,0.0254634183248846,True,True,True,1430743184000.0,1430806025000.0,1430809625000.0,1431006624000.0,1431007524000.0,1430743184000.0,-1.0,1430749915000.0,1430753515000.0,1430749915000.0,1430753515000.0,1430806025000.0,1430809625000.0,"bzeyben",1430753515000.0,1430743184000.0,45,1430743184000.0,-1.0,"SpringXD sqoop module is hanging","The SpringXD Sqoop module is in execution status until it times out, it is hanging. 

The container logs show:

2015-05-04 15:15:45,365 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying job 'sqoop_lookup'
2015-05-04 15:15:45,536 1.2.0.M1  INFO DeploymentsPathChildrenCache-0 container.DeploymentListener - Deploying module [ModuleDescriptor@239b037a moduleName = 'sqoop', moduleLabel = 'sqoop', group = 'sqoop_lookup', sourceChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map['args' -> '--connect=jdbc:oracle:thin:@************:****/******* username=******** --password-file=/user/zeybeb/workspace/secure-files/gdw.password --table=MASTERDATA.W_LOOKUP_D --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d -m 1', 'command' -> 'import'], children = list[[empty]]]
2015-05-04 15:16:17,061 1.2.0.M1  INFO inbound.job:sqoop_lookup-redis:queue-inbound-channel-adapter1 sqoop.SqoopTasklet - Sqoop system.out: /tmp/Sqoop-948322291323951735.out

The /tmp/Sqoop-948322291323951735.out file content is:

15:16:17,612  INFO main sqoop.SqoopRunner - Sqoop command: import
15:16:17,613  INFO main sqoop.SqoopRunner - Using args: [--connect=jdbc:oracle:thin:@************:****/*******, username=*********, --password-file=/user/zeybeb/workspace/secure-files/gdw.password, --table=MASTERDATA.W_LOOKUP_D, --target-dir=/user/zeybeb/workspace/ent/masterdata_src/lookup_d, -m, 1]
15:16:17,613  INFO main sqoop.SqoopRunner - Mapreduce home: /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21
15:16:17,631  INFO main sqoop.SqoopRunner - Setting configuration property: fs.defaultFS=hdfs://ilabphd07.isus.emc.com:8020
15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.resourcemanager.address=ilabphd08.isus.emc.com:8050
15:16:17,753  INFO main sqoop.SqoopRunner - Setting configuration property: yarn.application.classpath=/usr/hdp/2.2.0.0-2041/etc/hadoop/conf.empty,/usr/hdp/2.2.0.0-2041/hadoop/*,/usr/hdp/2.2.0.0-2041/hadoop/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/*,/usr/hdp/2.2.0.0-2041/hadoop-hdfs/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/*,/usr/hdp/2.2.0.0-2041/hadoop-yarn/lib/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/*,/usr/hdp/2.2.0.0-2041/hadoop-mapreduce/lib/*,/usr/hdp/2.2.0.0-2041/sqoop/*,/usr/hdp/2.2.0.0-2041/sqoop/lib/*,/usr/hdp/2.2.0.0-2041/flume/*,/usr/hdp/2.2.0.0-2041/flume/lib/*,/usr/hdp/2.2.0.0-2041/storm/*,/usr/hdp/2.2.0.0-2041/storm/lib/*
15:16:17,754  INFO main sqoop.SqoopRunner - Setting configuration property: mapreduce.framework.name=yarn
15:16:17,837  WARN main tool.SqoopTool - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration.
15:16:17,907  INFO main sqoop.Sqoop - Running Sqoop version: 1.4.5
15:16:18,282  WARN main util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
15:16:19,552  WARN main sqoop.ConnFactory - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration.
15:16:19,657  INFO main oracle.OraOopManagerFactory - Data Connector for Oracle and Hadoop is disabled.
15:16:19,673  INFO main manager.SqlManager - Using default fetchSize of 1000
15:16:19,673  INFO main tool.CodeGenTool - Beginning code generation
15:16:20,639  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:20,853  INFO main manager.SqlManager - Executing SQL statement: SELECT t.* FROM MASTERDATA.W_LOOKUP_D t WHERE 1=0
15:16:21,018  INFO main orm.CompilationManager - HADOOP_MAPRED_HOME is /opt/pivotal/spring-xd-1.2.0.M1/xd/lib/phd21
15:16:23,171  INFO main orm.CompilationManager - Writing jar file: /tmp/sqoop-spring-xd/compile/4e11123a52fa36d6677efdb47bcdc43b/MASTERDATA.W_LOOKUP_D.jar
15:16:23,191  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:24,109  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:24,825  INFO main mapreduce.ImportJobBase - Beginning import of MASTERDATA.W_LOOKUP_D
15:16:24,848  INFO main manager.OracleManager - Time zone has been set to GMT
15:16:24,876  WARN main mapreduce.JobBase - SQOOP_HOME is unset. May not be able to find all job dependencies.
15:16:25,083  INFO main client.RMProxy - Connecting to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050
15:16:25,977  INFO main db.DBInputFormat - Using read commited transaction isolation
15:16:26,117  INFO main mapreduce.JobSubmitter - number of splits:1
15:16:26,361  INFO main mapreduce.JobSubmitter - Submitting tokens for job: job_1429280992648_0019
15:16:26,717  INFO main impl.YarnClientImpl - Submitted application application_1429280992648_0019 to ResourceManager at ilabphd08.isus.emc.com/10.15.232.191:8050
15:16:26,782  INFO main mapreduce.Job - The url to track the job: http://http://ilabphd08.isus.emc.com:8088/proxy/application_1429280992648_0019/
15:16:26,783  INFO main mapreduce.Job - Running job: job_1429280992648_0019

The logs on the Hadoop side are:

Showing 4096 bytes. Click here for full log
mumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:04,026 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:05,033 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:06,042 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:07,049 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:08,056 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:09,062 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:10,069 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:41,093 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:42,100 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:43,107 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:44,113 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:45,120 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:46,129 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:47,136 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:48,143 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:49,150 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
2015-05-04 15:36:50,156 INFO [main] org.apache.hadoop.ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8030. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)

Even though  the maxRetries is set to 10 the process is going into several sets of retrials. 

",5.0,False,5.0,0,0.0,0,1,False
"XD-3024","Story","Sprint 49","2015-05-01T14:08:37.000+0000","sabby","eric.bottard","Add new REST-API to get all the counters, gauges, and rich-gauges","As a user, I'd like to have a REST-API to get all the _counters_, _gauges_, and _rich-gauges_ in a single request, so I don't have to issue multiple request to fetch each one of the metrics by name/id for custom dashboards.

*Example:*
{code}
/metrics/counters/all (fetches all available counters)
/metrics/gauges/all (fetches all available gauges)
/metrics/rich-gauges/all (fetches all available rich-gauges)
{code}",5.0,0.9999881584400718,0.9999881584400718,0.9999684225068582,True,True,True,1430489317000.0,1431502685000.0,1431502697000.0,1431501797000.0,1431502697000.0,1430489317000.0,-1.0,1431502665000.0,1431502697000.0,1431502685000.0,1431502697000.0,1431502685000.0,1431502697000.0,"sabby",1431502697000.0,1430489317000.0,45,1430489317000.0,-1.0,"Add new REST-API to get all the counters, gauges, and rich-gauges","As a user, I'd like to have a REST-API to get all the _counters_, _gauges_, and _rich-gauges_ in a single request, so I don't have to issue multiple request to fetch each one of the metrics by name/id for custom dashboards.

*Example:*
{code}
/metrics/counters/all (fetches all available counters)
/metrics/gauges/all (fetches all available gauges)
/metrics/rich-gauges/all (fetches all available rich-gauges)
{code}",5.0,False,5.0,0,0.0,0,0,False
"XD-3023","Story","Sprint 48","2015-05-01T10:15:00.000+0000","grussell","grussell","SSL Config for RabbitMessageBus Connections is Ignored","",1.0,0.0018106794980254,1.5047198044533018e-05,0.0,True,True,True,1430475300000.0,1430476383000.0,1430479983000.0,1431072518000.0,1431073418000.0,1430475300000.0,-1.0,1430475300000.0,1430478900000.0,1430475309000.0,1430478909000.0,1430476383000.0,1430479983000.0,"grussell",1430478900000.0,1430475300000.0,45,1430475300000.0,-1.0,"SSL Config for RabbitMessageBus Connections is Ignored","",1.0,False,1.0,0,0.0,-1,0,False
"XD-3022","Bug","Sprint 49","2015-04-30T12:36:52.000+0000","mbogoevici","","Kafka Message Bus ignores consumer concurrency when computing partition count","This is a combination of two issues:
- the internal property `next.module.concurrency` is computed from `concurrency` when it should be computed from `consumer.concurrency`
- even if `next.module.concurrency` is set, the KafkaMessageBus rejects it, since it's not set in SUPPORTED_CONSUMER_PROPERTIES

As a result, the value used in partition calculation is always 1.

A workaround exists, by setting the `module.[moduleName].producer.minPartitionCount` property to the expected total value. ",3.0,0.8349479601361383,0.8349447100821874,0.0058916978021835,True,True,True,1430397412000.0,1431681926000.0,1431685526000.0,1431934948000.0,1431935848000.0,1430397412000.0,-1.0,1430406476000.0,1430410076000.0,1431681921000.0,1431685521000.0,1431681926000.0,1431685526000.0,"mbogoevici",1430410076000.0,1430397412000.0,45,1430397412000.0,-1.0,"Kafka Message Bus ignores consumer concurrency when computing partition count","This is a combination of two issues:
- the internal property `next.module.concurrency` is computed from `concurrency` when it should be computed from `consumer.concurrency`
- even if `next.module.concurrency` is set, the KafkaMessageBus rejects it, since it's not set in SUPPORTED_CONSUMER_PROPERTIES

As a result, the value used in partition calculation is always 1.

A workaround exists, by setting the `module.[moduleName].producer.minPartitionCount` property to the expected total value. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-3019","Story","Sprint 48","2015-04-30T11:51:50.000+0000","mark.pollack","","Document how to use the module registry backed by HDFS","",1.0,0.8429391598375091,0.0608720300292845,0.0004251800515918,True,True,True,1430394710000.0,1431399861000.0,1431403461000.0,1431586246000.0,1431587146000.0,1430394710000.0,-1.0,1430395217000.0,1430398817000.0,1430467296000.0,1430470896000.0,1431399861000.0,1431403461000.0,"mark.pollack",1430398817000.0,1430394710000.0,45,1430394710000.0,-1.0,"Document how to use the module registry backed by HDFS","As a user, I'd like to refer to the documentation, so I can configure HDFS backed module registry (XD-2287) as recommended. ",1.0,False,1.0,0,0.0,-1,0,True
"XD-3018","Story","Sprint 48","2015-04-30T11:17:44.000+0000","thomas.risberg","","Update to spring-data-hadoop 2.2.0.M1","We should update to use spring-data-hadoop 2.2.0.M1in order to use the fixes available for the HDFS writing there (syncable writes, timeout).

A few things to keep in mind:
- this updates Cloudera CDH to 5.3.3
- Kite version is now 1.0 - need to test the hdfs-dataset sink
",2.0,0.0478480018015169,0.0478480018015169,0.0061352191765726,True,True,True,1430392664000.0,1430412637000.0,1430416237000.0,1430809190000.0,1430810090000.0,1430392664000.0,-1.0,1430395225000.0,1430398825000.0,1430412637000.0,1430416237000.0,1430412637000.0,1430416237000.0,"thomas.risberg",1430398825000.0,1430392664000.0,45,1430392664000.0,-1.0,"Update to spring-data-hadoop 2.2.0.M1","We should update to use spring-data-hadoop 2.2.0.M1in order to use the fixes available for the HDFS writing there (syncable writes, timeout).

A few things to keep in mind:
- this updates Cloudera CDH to 5.3.3
- Kite version is now 1.0 - need to test the hdfs-dataset sink
",2.0,False,2.0,0,0.0,0,0,False
"XD-3017","Story","Sprint 48","2015-04-30T10:45:31.000+0000","hillert","iperumal","Fix package tangles","See: 

https://sonar43.spring.io/drilldown/measures/7173?metric=package_tangle_index",5.0,0.0240208509439278,0.0240208509439278,0.6375035221189067,True,True,True,1430390731000.0,1430391072000.0,1430394672000.0,1430404027000.0,1430404927000.0,1430390731000.0,-1.0,1430399781000.0,1430403381000.0,1430391072000.0,1430394672000.0,1430391072000.0,1430394672000.0,"hillert",1430403381000.0,1430390731000.0,45,1430390731000.0,-1.0,"Fix package tangles","See: 

https://sonar43.spring.io/drilldown/measures/7173?metric=package_tangle_index",5.0,False,5.0,0,0.0,-1,0,False
"XD-3016","Story","Sprint 48","2015-04-30T10:26:02.000+0000","mark.pollack","","Document Kafka message bus properties","For example, how to specify the partition count for topics that are created by the message bus.",1.0,0.0233469225415133,0.0233469225415133,0.0016350281655433,True,True,True,1430389562000.0,1430470568000.0,1430474168000.0,1433858327000.0,1433859227000.0,1430389562000.0,-1.0,1430395235000.0,1430398835000.0,1430470568000.0,1430474168000.0,1430470568000.0,1430474168000.0,"mark.pollack",1430398835000.0,1430389562000.0,45,1430389562000.0,-1.0,"Document Kafka message bus properties","For example, how to specify the partition count for topics that are created by the message bus.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3015","Bug","Sprint 48","2015-04-30T06:42:41.000+0000","dturanski","","RemoteFileToHadoopTests fails on 1.1.x","This error surfaced recently as a result of a fix to a bug in HostNotWindowsRule which disabled this test in all environments. Now the test has been reactivated it is failing on the 1.1.x branch.  The test runs OK on master.
{noformat}
Encountered an error executing step step1-master in job job
org.springframework.messaging.MessageDeliveryException: failed to send Message to channel 'null'; nested exception is java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:292)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.xd.dirt.integration.bus.local.LocalMessageBus$3.handleMessage(LocalMessageBus.java:262)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:85)
	at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:224)
	at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:161)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)
	at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.getThreadPoolExecutor(ThreadPoolTaskExecutor.java:221)
	at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.execute(ThreadPoolTaskExecutor.java:252)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:89)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	... 76 more

java.lang.AssertionError: 
Expected :exitCode=COMPLETED;exitDescription=
Actual   :exitCode=FAILED;exitDescription=
   <Click to see difference>


	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:162)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)
	at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
{noformat}",2.0,0.0787591694069492,0.0787591694069492,0.0073125301814335,True,True,True,1430376161000.0,1430383011000.0,1430386611000.0,1430462235000.0,1430463135000.0,1430376161000.0,-1.0,1430376797000.0,1430380397000.0,1430383011000.0,1430386611000.0,1430383011000.0,1430386611000.0,"dturanski",1430380397000.0,1430376161000.0,45,1430376161000.0,-1.0,"RemoteFileToHadoopTests fails on 1.1.x","This error surfaced recently as a result of a fix to a bug in HostNotWindowsRule which disabled this test in all environments. Now the test has been reactivated it is failing on the 1.1.x branch.  The test runs OK on master.
{noformat}
Encountered an error executing step step1-master in job job
org.springframework.messaging.MessageDeliveryException: failed to send Message to channel 'null'; nested exception is java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:292)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.xd.dirt.integration.bus.local.LocalMessageBus$3.handleMessage(LocalMessageBus.java:262)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:85)
	at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:224)
	at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:165)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:144)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:161)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)
	at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: java.lang.IllegalStateException: ThreadPoolTaskExecutor not initialized
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.getThreadPoolExecutor(ThreadPoolTaskExecutor.java:221)
	at org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor.execute(ThreadPoolTaskExecutor.java:252)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:89)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	... 76 more

java.lang.AssertionError: 
Expected :exitCode=COMPLETED;exitDescription=
Actual   :exitCode=FAILED;exitDescription=
   <Click to see difference>


	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.springframework.batch.integration.x.RemoteFileToHadoopTests.testSimple(RemoteFileToHadoopTests.java:162)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.springframework.test.context.junit4.statements.RunBeforeTestMethodCallbacks.evaluate(RunBeforeTestMethodCallbacks.java:73)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.springframework.test.context.junit4.statements.RunAfterTestMethodCallbacks.evaluate(RunAfterTestMethodCallbacks.java:82)
	at org.springframework.test.context.junit4.statements.SpringRepeat.evaluate(SpringRepeat.java:73)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:217)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.runChild(SpringJUnit4ClassRunner.java:83)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.springframework.test.context.junit4.statements.RunBeforeTestClassCallbacks.evaluate(RunBeforeTestClassCallbacks.java:61)
	at org.springframework.test.context.junit4.statements.RunAfterTestClassCallbacks.evaluate(RunAfterTestClassCallbacks.java:68)
	at org.springframework.xd.test.HostNotWindowsRule$1.evaluate(HostNotWindowsRule.java:38)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.springframework.test.context.junit4.SpringJUnit4ClassRunner.run(SpringJUnit4ClassRunner.java:163)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:74)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:211)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:67)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
{noformat}",2.0,False,2.0,0,0.0,0,2,False
"XD-3014","Story","Sprint 49","2015-04-30T06:35:50.000+0000","thomas.risberg","","Add documentation for connecting to HDFS with HA Namenode","",1.0,0.9970400733073306,0.9970400733073306,0.0054730561018439,True,True,True,1430375750000.0,1433926109000.0,1433929709000.0,1433935749000.0,1433936649000.0,1430375750000.0,-1.0,1430395239000.0,1430398839000.0,1433926109000.0,1433929709000.0,1433926109000.0,1433929709000.0,"thomas.risberg",1430398839000.0,1430375750000.0,45,1430375750000.0,-1.0,"Add documentation for connecting to HDFS with HA Namenode","",1.0,False,1.0,0,0.0,-1,0,False
"XD-3013","Story","Sprint 48","2015-04-30T06:33:09.000+0000","eric.bottard","eric.bottard","Support --ref=true/false for sftp source","The file and ftp sources allow working with either the java.io.File or its contents.
For consistency, the sftp source should support the same mechanism.
",2.0,0.999894401269468,0.999894401269468,0.999815915726505,True,True,True,1430375589000.0,1431076281000.0,1431076355000.0,1431075455000.0,1431076355000.0,1430375589000.0,-1.0,1431076226000.0,1431076355000.0,1431076281000.0,1431076355000.0,1431076281000.0,1431076355000.0,"eric.bottard",1431076355000.0,1430375589000.0,45,1430375589000.0,-1.0,"Support --ref=true/false for sftp source","The file and ftp sources allow working with either the java.io.File or its contents.
For consistency, the sftp source should support the same mechanism.
",2.0,False,2.0,0,0.0,0,0,False
"XD-3009","Bug","Sprint 48","2015-04-28T18:03:35.000+0000","iperumal","iperumal","Manual acknowledgement with Kafka bus doesn't work","When the kafka message headers are expected to be set with acknowledgement flags to manually acknowledge the messages at the consumer side, the message headers are missing.",1.0,0.006536961119346,0.0,0.3606864429380353,True,True,True,1430244215000.0,1430245269000.0,1430248869000.0,1430404552000.0,1430405452000.0,1430244215000.0,-1.0,1430302371000.0,1430305971000.0,1430244215000.0,1430247815000.0,1430245269000.0,1430248869000.0,"iperumal",1430305971000.0,1430244215000.0,45,1430244215000.0,-1.0,"Manual acknowledgement with Kafka bus doesn't work","When the kafka message headers are expected to be set with acknowledgement flags to manually acknowledge the messages at the consumer side, the message headers are missing.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3008","Bug","Sprint 48","2015-04-28T14:06:38.000+0000","iperumal","iperumal","Version info not available when security enabled","When security is enabled, the VersionController REST endpoint isn't visible.",1.0,0.0012290045063498,0.0,0.8390004096681688,True,True,True,1430229998000.0,1430230004000.0,1430233604000.0,1430233980000.0,1430234880000.0,1430229998000.0,-1.0,1430234094000.0,1430234880000.0,1430229998000.0,1430233598000.0,1430230004000.0,1430233604000.0,"iperumal",1430234880000.0,1430229998000.0,45,1430229998000.0,-1.0,"Version info not available when security enabled","When security is enabled, the VersionController REST endpoint isn't visible.",1.0,False,1.0,0,0.0,-1,0,False
"XD-3007","Bug","Sprint 48","2015-04-28T13:09:54.000+0000","iperumal","iperumal","Message rate collection throws warning level exception","When the container doesn't have any modules deployed, the jolokia response returns stacktrace with ""NoInstanceFoundException"". This exception is thrown at the admin log as:

2015-04-28 13:09:35,952 1.2.0.SNAP  WARN qtp1648225666-27 rest.ContainersController - Error getting message rate metrics for 713255e5-49b2-4158-b69c-2d203cfe50d3
org.codehaus.jettison.json.JSONException: JSONObject[""value""] not found.
	at org.codehaus.jettison.json.JSONObject.get(JSONObject.java:360)
	at org.codehaus.jettison.json.JSONObject.getJSONObject(JSONObject.java:454)
	at org.springframework.xd.dirt.rest.ContainersController.setMessageRates(ContainersController.java:134)
	at org.springframework.xd.dirt.rest.ContainersController.list(ContainersController.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:857)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)",1.0,0.0772568398216222,0.0,0.9319030975051223,True,True,True,1430226594000.0,1430227235000.0,1430230835000.0,1430233991000.0,1430234891000.0,1430226594000.0,-1.0,1430234326000.0,1430234891000.0,1430226594000.0,1430230194000.0,1430227235000.0,1430230835000.0,"iperumal",1430234891000.0,1430226594000.0,45,1430226594000.0,-1.0,"Message rate collection throws warning level exception","When the container doesn't have any modules deployed, the jolokia response returns stacktrace with ""NoInstanceFoundException"". This exception is thrown at the admin log as:

2015-04-28 13:09:35,952 1.2.0.SNAP  WARN qtp1648225666-27 rest.ContainersController - Error getting message rate metrics for 713255e5-49b2-4158-b69c-2d203cfe50d3
org.codehaus.jettison.json.JSONException: JSONObject[""value""] not found.
	at org.codehaus.jettison.json.JSONObject.get(JSONObject.java:360)
	at org.codehaus.jettison.json.JSONObject.getJSONObject(JSONObject.java:454)
	at org.springframework.xd.dirt.rest.ContainersController.setMessageRates(ContainersController.java:134)
	at org.springframework.xd.dirt.rest.ContainersController.list(ContainersController.java:114)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)
	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:857)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:735)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConfiguration.java:291)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:87)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:100)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)
	at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:90)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)
	at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)
	at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
	at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
	at org.eclipse.jetty.server.Server.handle(Server.java:370)
	at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
	at org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)
	at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
	at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)",1.0,False,1.0,0,0.0,-1,0,False
"XD-3006","Story","","2015-04-28T09:42:58.000+0000","monasj1","","db.password not being passed when Sqoop job --create called from Spring-XD Batch job","Created a single definition Sqoop Job --create statement. When processed through Sqoop direct (see attach image for props in repos. jobname sqoop2) 

Notice: job definition set db.require.password = false and has a propname called db.password that holds the value for password. 

When same definition is created as a batch job via Spring-XD (see image jobname lookupd_job), the db.require.password prop is set to ""TRUE"" and the db.password is not even created as a row.

This causes a password prompt when trying to execute the job in Sqoop direct, that was created via a spring-XD batch job. 

Note the job definition for Spring-XD job contains meta-connect with username and password defined as well as import jdbc: --connect username and password. When created via Sqoop (no Spring-XD) password is stored and job is able to execute with no prompt.

Please see attachment for repository job values. Highlighted in yellow is the discrepant property and value attribution. Please reach out if there are questions or need further details.

To automate Spring-XD batch job using Sqoop via Spring we can not be prompted for password.

Spring-XD Job definition:
job create sqoop_lookup_d_job_v1 --definition ""sqoop --command=job  --args='--create lookupd_job --meta-connect \""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx\"" -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query \""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), ''-'')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), ''-'')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), ''-'')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), ''-'')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND $CONDITIONS\"" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/monasj1/bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by \""|\"" --append'""

Sqoop Direct Definition:
sqoop job --create sqoop2 --meta-connect 'jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx' -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query ""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), '-')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), '-')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), '-')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), '-')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND \$CONDITIONS "" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/monasj1/bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by '|' --append
",5.0,-1.0,-1.0,-1.0,False,False,False,1430214178000.0,-1.0,-1.0,1432633002000.0,1432633902000.0,1430214178000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"monasj1",-1.0,-1.0,0,1430214178000.0,-1.0,"db.password not being passed when Sqoop job --create called from Spring-XD Batch job","Created a single definition Sqoop Job --create statement. When processed through Sqoop direct (see attach image for props in repos. jobname sqoop2) 

Notice: job definition set db.require.password = false and has a propname called db.password that holds the value for password. 

When same definition is created as a batch job via Spring-XD (see image jobname lookupd_job), the db.require.password prop is set to ""TRUE"" and the db.password is not even created as a row.

This causes a password prompt when trying to execute the job in Sqoop direct, that was created via a spring-XD batch job. 

Note the job definition for Spring-XD job contains meta-connect with username and password defined as well as import jdbc: --connect username and password. When created via Sqoop (no Spring-XD) password is stored and job is able to execute with no prompt.

Please see attachment for repository job values. Highlighted in yellow is the discrepant property and value attribution. Please reach out if there are questions or need further details.

To automate Spring-XD batch job using Sqoop via Spring we can not be prompted for password.

Spring-XD Job definition:
job create sqoop_lookup_d_job_v1 --definition ""sqoop --command=job  --args='--create lookupd_job --meta-connect \""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx\"" -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query \""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), ''-'')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), ''-'')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), ''-'')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), ''-'')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND $CONDITIONS\"" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/monasj1/bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by \""|\"" --append'""

Sqoop Direct Definition:
sqoop job --create sqoop2 --meta-connect 'jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx' -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query ""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), '-')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), '-')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), '-')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), '-')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND \$CONDITIONS "" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/monasj1/bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by '|' --append
",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-3004","Story","Sprint 48","2015-04-28T06:42:03.000+0000","eric.bottard","eric.bottard","Detailed module list performance improvement","The call to /modules?detailed=true that was introduced for Flo proves to be a performance hog, most certainly because of all the metadata resolution that has to occur there (and no caching takes place)",3.0,0.9999816771374191,0.0,0.9998992242558056,True,True,True,1430203323000.0,1431076533000.0,1431076549000.0,1431075649000.0,1431076549000.0,1430203323000.0,-1.0,1431076461000.0,1431076549000.0,1430203323000.0,1430206923000.0,1431076533000.0,1431076549000.0,"eric.bottard",1431076549000.0,1430203323000.0,45,1430203323000.0,-1.0,"Detailed module list performance improvement","The call to /modules?detailed=true that was introduced for Flo proves to be a performance hog, most certainly because of all the metadata resolution that has to occur there (and no caching takes place)",3.0,False,3.0,0,0.0,-1,0,False
"XD-3003","Story","Sprint 48","2015-04-27T14:58:06.000+0000","sabby","","Add support for pluggable Sqoop metastore","As a user, I'd like to have the option to change the default Sqoop _metastore_, so I can implement a DB of my choice and not tied to default specifications.

Refer to this [thread|http://stackoverflow.com/questions/24078668/how-to-change-sqoop-metastore] for more details. ",1.0,0.1176751294728293,0.1176751294728293,0.0,True,True,True,1430146686000.0,1430288971000.0,1430292571000.0,1431354920000.0,1431355820000.0,1430146686000.0,-1.0,1430146686000.0,1430150286000.0,1430288971000.0,1430292571000.0,1430288971000.0,1430292571000.0,"sabby",1430150286000.0,1430146686000.0,45,1430146686000.0,-1.0,"Add support for using Sqoop metastore","As a user, I'd like to have the option to change the default Sqoop _metastore_, so I can implement a DB of my choice and not tied to default specifications.

Refer to this [thread|http://stackoverflow.com/questions/24078668/how-to-change-sqoop-metastore] for more details. ",1.0,False,1.0,0,0.0,-1,0,True
"XD-3001","Story","Sprint 48","2015-04-27T13:28:24.000+0000","monasj1","thomas.risberg","Unable to call Sqoop Job commands other than --create from within Spring-Xd Job","Running a (SQOOP Job --create) from within (Spring-XD JOB create) statement runs successfully.Unable to run the (Sqoop Job --exec) from within (Spring-XD Job)

CREATE DEFINITION:
job create sqoop_lookup_d_job_v1 --definition ""sqoop --command=job  --args='--create lookupd_job --meta-connect \""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx\"" -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query \""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), ''-'')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), ''-'')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), ''-'')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), ''-'')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND $CONDITIONS\"" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/monasj1/bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by \""|\"" --append'""

Validated job definition details stored in PostgreSQL repository

Trying to run EXECUTE In Same fashion.

EXECUTION DEFINITION:
job create sqoop_lookup_d_exec_v1 --definition ""sqoop --command=job --args='--exec lookupd_job --meta-connect \""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxx\""'""


Stack Trace:
batch.stepType	org.springframework.batch.core.step.tasklet.TaskletStep
batch.taskletType	org.springframework.xd.sqoop.SqoopTasklet
sqoop.command	job --exec lookupd_job --meta-connect ""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxxx&password=xxxxxxx"" -- exec
sqoop.errors	No job operation specified
Try --help for usage instructions.
Exception in thread ""main"" java.lang.RuntimeException: Sqoop failed - return code 1
at org.springframework.xd.sqoop.SqoopRunner.main(SqoopRunner.java:81)
sqoop.log	16:01:45,668 INFO main sqoop.SqoopRunner - Sqoop command: job
16:01:45,668 INFO main sqoop.SqoopRunner - Using args: [--exec, lookupd_job, --meta-connect, ""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxxxx&password=xxxxx""]
16:01:45,668 INFO main sqoop.SqoopRunner - Mapreduce home: /opt/pivotal/spring-xd-1.1.1.RELEASE/xd/lib/phd21
16:01:45,679 INFO main sqoop.SqoopRunner - Setting configuration property: fs.defaultFS=hdfs://xxxxxxxxxxxxx.com:8020
16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: yarn.resourcemanager.address=xxxxxxxxxxxx.com:8032
16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: yarn.application.classpath=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,$USS_HOME/*,$USS_CONF
16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: mapreduce.framework.name=yarn
16:01:45,817 WARN main tool.SqoopTool - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration.
16:01:45,863 INFO main sqoop.Sqoop - Running Sqoop version: 1.4.5

I have tested --list, --exec,--delete they are all erroring same way. Looking at SqoopRunner not sure what makes --create any different from other saved-job functionality.

Please let me know how i can call or execute save job definition's through spring-xd Sqoop tasklet
",5.0,-1.0,0.000358621881314,0.0003092298676603,False,True,True,1430141304000.0,-1.0,-1.0,1436274999000.0,1436275899000.0,1430141304000.0,-1.0,1430143201000.0,1430146801000.0,1430143504000.0,1430147104000.0,-1.0,-1.0,"monasj1",1430146801000.0,1430141304000.0,45,1430141304000.0,-1.0,"Unable to call Sqoop Job commands other than --create from within Spring-Xd Job","Running a (SQOOP Job --create) from within (Spring-XD JOB create) statement runs successfully.Unable to run the (Sqoop Job --exec) from within (Spring-XD Job)

CREATE DEFINITION:
job create sqoop_lookup_d_job_v1 --definition ""sqoop --command=job  --args='--create lookupd_job --meta-connect \""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxxx\"" -- import  --connect jdbc:oracle:thin:@xxxxxx.com:xxxx/GDWP --username xxxxx --password xxxxx --query \""SELECT ROW_WID, LKUP_ID, TRIM(TRANSLATE(LKUP_CD, CHR(10)||CHR(13), ''-'')) AS LKUP_CD, TRIM(TRANSLATE(LKUP_DESC, CHR(10)||CHR(13), ''-'')) AS LKUP_DESC, TRIM(TRANSLATE(LKUP_TYPE, CHR(10)||CHR(13), ''-'')) AS LKUP_TYPE, TRIM(TRANSLATE(LKUP_VAL, CHR(10)||CHR(13), ''-'')) AS LKUP_VAL, SRC_STRT_DT, SRC_END_DT, W_INSERT_DT, W_UPDATE_DT FROM MASTERDATA.W_LOOKUP_D WHERE 1=1 AND $CONDITIONS\"" --incremental append --check-column ROW_WID --last-value 10150 --target-dir /user/monasj1/bdl/data/sqoop/jobtest --num-mappers 1 --fields-terminated-by \""|\"" --append'""

Validated job definition details stored in PostgreSQL repository

Trying to run EXECUTE In Same fashion.

EXECUTION DEFINITION:
job create sqoop_lookup_d_exec_v1 --definition ""sqoop --command=job --args='--exec lookupd_job --meta-connect \""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxx&password=xxxxx\""'""


Stack Trace:
batch.stepType	org.springframework.batch.core.step.tasklet.TaskletStep
batch.taskletType	org.springframework.xd.sqoop.SqoopTasklet
sqoop.command	job --exec lookupd_job --meta-connect ""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxxx&password=xxxxxxx"" -- exec
sqoop.errors	No job operation specified
Try --help for usage instructions.
Exception in thread ""main"" java.lang.RuntimeException: Sqoop failed - return code 1
at org.springframework.xd.sqoop.SqoopRunner.main(SqoopRunner.java:81)
sqoop.log	16:01:45,668 INFO main sqoop.SqoopRunner - Sqoop command: job
16:01:45,668 INFO main sqoop.SqoopRunner - Using args: [--exec, lookupd_job, --meta-connect, ""jdbc:postgresql://xx.xxx.xx.xx:5433/bdl?user=xxxxxxx&password=xxxxx""]
16:01:45,668 INFO main sqoop.SqoopRunner - Mapreduce home: /opt/pivotal/spring-xd-1.1.1.RELEASE/xd/lib/phd21
16:01:45,679 INFO main sqoop.SqoopRunner - Setting configuration property: fs.defaultFS=hdfs://xxxxxxxxxxxxx.com:8020
16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: yarn.resourcemanager.address=xxxxxxxxxxxx.com:8032
16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: yarn.application.classpath=$HADOOP_CONF_DIR,$HADOOP_COMMON_HOME/*,$HADOOP_COMMON_HOME/lib/*,$HADOOP_HDFS_HOME/*,$HADOOP_HDFS_HOME/lib/*,$HADOOP_MAPRED_HOME/*,$HADOOP_MAPRED_HOME/lib/*,$HADOOP_YARN_HOME/*,$HADOOP_YARN_HOME/lib/*,$USS_HOME/*,$USS_CONF
16:01:45,759 INFO main sqoop.SqoopRunner - Setting configuration property: mapreduce.framework.name=yarn
16:01:45,817 WARN main tool.SqoopTool - $SQOOP_CONF_DIR has not been set in the environment. Cannot check for additional configuration.
16:01:45,863 INFO main sqoop.Sqoop - Running Sqoop version: 1.4.5

I have tested --list, --exec,--delete they are all erroring same way. Looking at SqoopRunner not sure what makes --create any different from other saved-job functionality.

Please let me know how i can call or execute save job definition's through spring-xd Sqoop tasklet
",5.0,False,5.0,0,0.0,0,0,False
"XD-3000","Story","Sprint 48","2015-04-27T12:23:51.000+0000","David Turanski","dturanski","Enhance TupleCodec performance","Profile TupleCodec and implement performance optimizations",5.0,0.000115281357716,1.3562512672472778e-05,9.697196560818036e-05,True,True,True,1430137431000.0,1430137601000.0,1430141201000.0,1431611184000.0,1431612084000.0,1430137431000.0,-1.0,1430137574000.0,1430141174000.0,1430137451000.0,1430141051000.0,1430137601000.0,1430141201000.0,"sabby",1430141174000.0,1430137431000.0,45,1430137431000.0,-1.0,"Enhance TupleCodec performance","Profile TupleCodec and implement performance optimizations",5.0,False,5.0,0,0.0,-1,0,False
"XD-2999","Story","Sprint 48","2015-04-27T09:51:54.000+0000","sabby","","Benchmark with and without JMX activated","As a developer, I'd like to benchmark a stream with and without {{JMX}} enabled, so I can test in isolation, and document the differences in performance.",3.0,0.999964302686446,0.3485831513142189,0.0,True,True,True,1430128314000.0,1432817496000.0,1432817592000.0,1432816692000.0,1432817592000.0,1430128314000.0,-1.0,1430128314000.0,1430131914000.0,1431065751000.0,1431069351000.0,1432817496000.0,1432817592000.0,"sabby",1430131914000.0,1430128314000.0,45,1430128314000.0,-1.0,"Benchmark with and without JMX activated","As a developer, I'd like to benchmark a stream with and without {{JMX}} enabled, so I can test in isolation, and document the differences in performance.",3.0,False,3.0,0,0.0,0,0,False
"XD-2996","Bug","Sprint 48","2015-04-27T08:55:00.000+0000","mbogoevici","","Align Spring XD partitioning with Kafka partitioning for the Kafka message bus","As a developer, I want that the Spring XD partitioning process targets Kafka bus partitions directly, so that the design of my stream processing application is easier to understand.

Current situation
- Spring XD partitioning logic that builds on top of Kafka partitioning;
- The number of Spring XD partitions is not explicitly configured (it's inferred from the number of consumer modules)
- If the concurrency of the consumer modules is 1, then Spring XD partitions are matched 1:1 with Kafka partitions;
-  If the concurrency of the consumer modules is n, then a Spring XD partition uses n Kafka partitions, and the message bus distributes messages across the Kafka partitions that correspond to the Spring XD partitions;
- this could be confusing to the end user, especially if they are used to the Kafka partitioning process;

Improvement:
- *For the Kafka message bus* the number of Spring XD partitions does not need to be equal to the number of modules (must be higher or equal, though, so that consumers can be created), and should be configured explicitly - using the `partitionCount` property - (as an option, the module count * concurrency can be used as a default)
 - as a result, in the case of Kafka there will always be a 1:1 match between Kafka partitions and Spring XD partitions, optionally processed by fewer modules than the partition count;",5.0,0.9821947613655024,0.9821947613655024,0.0016220473433869,True,True,True,1430124900000.0,1433707808000.0,1433711408000.0,1433771859000.0,1433772759000.0,1430124900000.0,-1.0,1430130817000.0,1430134417000.0,1433707808000.0,1433711408000.0,1433707808000.0,1433711408000.0,"mbogoevici",1430134417000.0,1430124900000.0,45,1430124900000.0,-1.0,"Align Spring XD partitioning with Kafka partitioning for the Kafka message bus","As a developer, I want that the Spring XD partitioning process targets Kafka bus partitions directly, so that the design of my stream processing application is easier to understand and the order of messages is not altered

Current situation
- Spring XD partitioning logic that builds on top of Kafka partitioning;
- The number of Spring XD partitions is not explicitly configured (it's inferred from the number of consumer modules)
- If the concurrency of the consumer modules is 1, then Spring XD partitions are matched 1:1 with Kafka partitions;
-  If the concurrency of the consumer modules is n, then a Spring XD partition uses n Kafka partitions, and the message bus distributes messages across the Kafka partitions that correspond to the Spring XD partitions;
- this could be confusing to the end user, especially if they are used to the Kafka partitioning process;
- this can also lead to changes of ordering between messages, as messages within the same Spring XD partitions will be sent to different Kafka partitions (this only happens if the concurrency of the receiving module is higher than 1)

Improvement:
- *For the Kafka message bus* the number of Spring XD partitions does not need to be equal to the number of modules (must be higher or equal, though, so that consumers can be created), and should be configured explicitly - using the `partitionCount` property - (as an option, the module count * concurrency can be used as a default)
 - as a result, in the case of Kafka there will always be a 1:1 match between Kafka partitions and Spring XD partitions, optionally processed by fewer modules than the partition count;",5.0,False,5.0,0,0.0,-1,0,True
"XD-2995","Bug","Sprint 48","2015-04-27T06:23:15.000+0000","grenfro","iperumal","Jobs are failing to be deployed","Error Started:
Commit: 7087dc67e058edd6cbb1630ebd95b52e2c7e21e1 
https://github.com/spring-projects/spring-xd/pull/1564

This can be reproduced by running the test with a admin and single container on Mac OSX.

Issue All Jobs fail to deploy with the following exception:
{noformat}
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.2.0.BUILD-SNAPSHOT             eXtreme Data


Started : AdminServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki

2015-04-27 09:08:51,082 1.2.0.SNAP  WARN main config.IntegrationRegistrar - The '#jsonPath' SpEL function cannot be registered. The version of json-path found on the classpath is not supported. Supported json-path version is '0.9.1'. Upgrade to Spring Integration 4.2 or later to use json-path 1.0 or later.
2015-04-27 09:09:02,767 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: /Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd
2015-04-27 09:09:02,768 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: redis
2015-04-27 09:09:02,768 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop Distro: hadoop26
2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop version detected from classpath 2.6.0
2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:/Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd/config//
2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application
2015-04-27 09:09:02,772 1.2.0.SNAP  INFO LeaderSelector-0 zk.DeploymentSupervisor - Leader Admin 172.31.99.83:9393 is watching for stream/job deployment requests.
2015-04-27 09:09:02,773 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:/Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd/config//modules/
2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules
2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Admin web UI: http://Glenns-MacBook-Pro.local:9393/admin-ui
2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:2181
2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd
2015-04-27 09:09:02,776 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: redis
2015-04-27 09:09:02,813 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Path cache event: type=INITIALIZED
2015-04-27 09:09:02,845 1.2.0.SNAP  INFO main admin.AdminServerApplication - Started AdminServerApplication in 6.777 seconds (JVM running for 14.213)
2015-04-27 09:09:04,010 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Path cache event: path=/containers/dc7692b1-979b-4fa3-a11a-3f35cdedc319, type=CHILD_ADDED
2015-04-27 09:09:04,017 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Container arrived: Container{name='dc7692b1-979b-4fa3-a11a-3f35cdedc319', attributes={groups=, host=Glenns-MacBook-Pro.local, id=dc7692b1-979b-4fa3-a11a-3f35cdedc319, managementPort=9395, ip=172.31.99.83, pid=99669}}
2015-04-27 09:09:04,018 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Scheduling deployments to new container(s) in 15000 ms
2015-04-27 09:10:35,003 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ZKJobDeploymentHandler - Deployment status for job 'tfphj4ffb45d5-0d6c-4f22-b407-c313ce82b449': DeploymentStatus{state=failed,error(s)=org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'objectNameProperties' defined in null: Could not resolve placeholder 'xd.stream.name' in string value ""${xd.stream.name}""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'xd.stream.name' in string value ""${xd.stream.name}""
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.processProperties(PropertySourcesPlaceholderConfigurer.java:180)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.postProcessBeanFactory(PropertySourcesPlaceholderConfigurer.java:155)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:606)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:462)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:214)
	at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)
	at org.springframework.xd.dirt.server.container.DeploymentListener.deployModule(DeploymentListener.java:365)
	at org.springframework.xd.dirt.server.container.DeploymentListener.deployJobModule(DeploymentListener.java:291)
	at org.springframework.xd.dirt.server.container.DeploymentListener.onChildAdded(DeploymentListener.java:181)
	at org.springframework.xd.dirt.server.container.DeploymentListener.childEvent(DeploymentListener.java:149)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'xd.stream.name' in string value ""${xd.stream.name}""
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:204)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:178)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$2.resolveStringValue(PropertySourcesPlaceholderConfigurer.java:175)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:204)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitMap(BeanDefinitionVisitor.java:262)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:198)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)
	... 31 more
}
{noformat}",1.0,0.3398008680112331,0.3391626244574929,0.3309931069696196,True,True,True,1430115795000.0,1430118457000.0,1430122057000.0,1430122729000.0,1430123629000.0,1430115795000.0,1430123611000.0,1430118388000.0,1430121988000.0,1430118452000.0,1430122052000.0,1430118457000.0,1430122057000.0,"grenfro",1430121988000.0,1430115795000.0,45,1430123611000.0,1430123611000.0,"Jobs are failing to be deployed","Error Started:
Commit: 7087dc67e058edd6cbb1630ebd95b52e2c7e21e1 
https://github.com/spring-projects/spring-xd/pull/1564

This can be reproduced by running the test with a admin and single container on Mac OSX.

Issue All Jobs fail to deploy with the following exception:
{noformat}
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.2.0.BUILD-SNAPSHOT             eXtreme Data


Started : AdminServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki

2015-04-27 09:08:51,082 1.2.0.SNAP  WARN main config.IntegrationRegistrar - The '#jsonPath' SpEL function cannot be registered. The version of json-path found on the classpath is not supported. Supported json-path version is '0.9.1'. Upgrade to Spring Integration 4.2 or later to use json-path 1.0 or later.
2015-04-27 09:09:02,767 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: /Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd
2015-04-27 09:09:02,768 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: redis
2015-04-27 09:09:02,768 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop Distro: hadoop26
2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop version detected from classpath 2.6.0
2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:/Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd/config//
2015-04-27 09:09:02,771 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application
2015-04-27 09:09:02,772 1.2.0.SNAP  INFO LeaderSelector-0 zk.DeploymentSupervisor - Leader Admin 172.31.99.83:9393 is watching for stream/job deployment requests.
2015-04-27 09:09:02,773 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:/Users/glennrenfro/project/spring-xd/build/dist/spring-xd/xd/config//modules/
2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules
2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Admin web UI: http://Glenns-MacBook-Pro.local:9393/admin-ui
2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:2181
2015-04-27 09:09:02,775 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd
2015-04-27 09:09:02,776 1.2.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: redis
2015-04-27 09:09:02,813 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Path cache event: type=INITIALIZED
2015-04-27 09:09:02,845 1.2.0.SNAP  INFO main admin.AdminServerApplication - Started AdminServerApplication in 6.777 seconds (JVM running for 14.213)
2015-04-27 09:09:04,010 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Path cache event: path=/containers/dc7692b1-979b-4fa3-a11a-3f35cdedc319, type=CHILD_ADDED
2015-04-27 09:09:04,017 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Container arrived: Container{name='dc7692b1-979b-4fa3-a11a-3f35cdedc319', attributes={groups=, host=Glenns-MacBook-Pro.local, id=dc7692b1-979b-4fa3-a11a-3f35cdedc319, managementPort=9395, ip=172.31.99.83, pid=99669}}
2015-04-27 09:09:04,018 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ContainerListener - Scheduling deployments to new container(s) in 15000 ms
2015-04-27 09:10:35,003 1.2.0.SNAP  INFO DeploymentSupervisor-0 zk.ZKJobDeploymentHandler - Deployment status for job 'tfphj4ffb45d5-0d6c-4f22-b407-c313ce82b449': DeploymentStatus{state=failed,error(s)=org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'objectNameProperties' defined in null: Could not resolve placeholder 'xd.stream.name' in string value ""${xd.stream.name}""; nested exception is java.lang.IllegalArgumentException: Could not resolve placeholder 'xd.stream.name' in string value ""${xd.stream.name}""
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:211)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.processProperties(PropertySourcesPlaceholderConfigurer.java:180)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer.postProcessBeanFactory(PropertySourcesPlaceholderConfigurer.java:155)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:265)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:162)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:606)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:462)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:214)
	at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)
	at org.springframework.xd.dirt.server.container.DeploymentListener.deployModule(DeploymentListener.java:365)
	at org.springframework.xd.dirt.server.container.DeploymentListener.deployJobModule(DeploymentListener.java:291)
	at org.springframework.xd.dirt.server.container.DeploymentListener.onChildAdded(DeploymentListener.java:181)
	at org.springframework.xd.dirt.server.container.DeploymentListener.childEvent(DeploymentListener.java:149)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Could not resolve placeholder 'xd.stream.name' in string value ""${xd.stream.name}""
	at org.springframework.util.PropertyPlaceholderHelper.parseStringValue(PropertyPlaceholderHelper.java:174)
	at org.springframework.util.PropertyPlaceholderHelper.replacePlaceholders(PropertyPlaceholderHelper.java:126)
	at org.springframework.core.env.AbstractPropertyResolver.doResolvePlaceholders(AbstractPropertyResolver.java:204)
	at org.springframework.core.env.AbstractPropertyResolver.resolveRequiredPlaceholders(AbstractPropertyResolver.java:178)
	at org.springframework.context.support.PropertySourcesPlaceholderConfigurer$2.resolveStringValue(PropertySourcesPlaceholderConfigurer.java:175)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveStringValue(BeanDefinitionVisitor.java:282)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:204)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitMap(BeanDefinitionVisitor.java:262)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.resolveValue(BeanDefinitionVisitor.java:198)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitPropertyValues(BeanDefinitionVisitor.java:141)
	at org.springframework.beans.factory.config.BeanDefinitionVisitor.visitBeanDefinition(BeanDefinitionVisitor.java:82)
	at org.springframework.beans.factory.config.PlaceholderConfigurerSupport.doProcessProperties(PlaceholderConfigurerSupport.java:208)
	... 31 more
}
{noformat}",3.0,True,3.0,1,-66.66666666666666,-1,1,False
"XD-2994","Story","Sprint 48","2015-04-27T05:45:57.000+0000","grenfro","grenfro","Verify module count works on 10+ Containers","",3.0,0.9992017266357212,0.0,0.2657954646246637,True,True,True,1430113557000.0,1430147353000.0,1430147380000.0,1430146480000.0,1430147380000.0,1430113557000.0,-1.0,1430122547000.0,1430126147000.0,1430113557000.0,1430117157000.0,1430147353000.0,1430147380000.0,"grenfro",1430126147000.0,1430113557000.0,45,1430113557000.0,-1.0,"Verify module count works on 10+ Containers","This tests should prove that Kafka can handle a module count greater than one with the proper concurrency settings.
load-generator should be used as the foundation for this test with the following settings:
module.load-generator.count=10,module.throughput.consumer.concurrency=10

An environment should be provisioned to support the containers, Zookeeper and Kafka.
",3.0,False,3.0,0,0.0,0,0,True
"XD-2993","Story","Sprint 48","2015-04-24T16:29:56.000+0000","sabby","","Add a new variation of DSL parser for Flo","As a Flo developer, I'd like to have a new DSL parser, so I can easily  detect incorrect module/option values when supplied from the Flo UI.

Example:
MyStream = mail | log
tap:stream:MyStream.bar > log

If parsed separately (which Flo UI does), the current parser endpoint will barf on the second stream because it doesnt know about the first stream (MyStream). ",8.0,0.2242909081592725,0.1369354282733048,0.0,True,True,True,1429892996000.0,1430278195000.0,1430281795000.0,1431609504000.0,1431610404000.0,1429892996000.0,1430122864000.0,1429892996000.0,1429896596000.0,1430128170000.0,1430131770000.0,1430278195000.0,1430281795000.0,"sabby",1429896596000.0,1429892996000.0,45,1430122864000.0,1430122864000.0,"Add a new variation of DSL parser for Flo","As a Flo developer, I'd like to have a new DSL parser, so I can easily  detect incorrect module/option values when supplied from the Flo UI.

Example:
MyStream = mail | log
tap:stream:MyStream.bar > log

If parsed separately (which Flo UI does), the current parser endpoint will barf on the second stream because it doesnt know about the first stream (MyStream). ",5.0,True,5.0,1,60.0,0,1,False
"XD-2992","Story","Sprint 48","2015-04-24T15:53:12.000+0000","sabby","","Add support for multiple topics in Kafka source","As a user, I'd like to consume multiple topic-partitions, so I can have the option to consume from multiple data endpoints and still be able to serve the data via single queue.",3.0,0.3418707316910028,0.3418707316910028,0.0,True,True,True,1429890792000.0,1430401899000.0,1430405499000.0,1431384922000.0,1431385822000.0,1429890792000.0,1430120993000.0,1429890792000.0,1429894392000.0,1430401899000.0,1430405499000.0,1430401899000.0,1430405499000.0,"sabby",1429894392000.0,1429890792000.0,45,1430120993000.0,1430120993000.0,"Add support for multiple topics in Kafka source","As a user, I'd like to consume multiple topic-partitions, so I can have the option to consume from multiple data endpoints and still be able to serve the data via single queue.",1.0,True,1.0,1,200.0,0,0,False
"XD-2987","Story","Sprint 47","2015-04-24T12:14:25.000+0000","grenfro","grenfro","Update acceptance test to use new JMX Module name format","the update to the JMX was introduced in XD-2941.   Also noticed that we should have been checking source and not sink .  This was also resolved.",2.0,0.117639159336601,0.0,0.1229436648089706,True,True,True,1429877665000.0,1429879417000.0,1429883017000.0,1429891658000.0,1429892558000.0,1429877665000.0,-1.0,1429879496000.0,1429883096000.0,1429877665000.0,1429881265000.0,1429879417000.0,1429883017000.0,"grenfro",1429883096000.0,1429877665000.0,45,1429877665000.0,-1.0,"Update acceptance test to use new JMX Module name format","the update to the JMX was introduced in XD-2941.   Also noticed that we should have been checking source and not sink .  This was also resolved.",2.0,False,2.0,0,0.0,-1,0,False
"XD-2986","Story","Sprint 48","2015-04-24T10:11:03.000+0000","sabby","","Refactor deployment interfaces/class hierarchy (Phase II)","As a developer, I'd like to continue the remaining work from XD-2877, so I can refactor deployment interfaces and the class hierarchy.
",8.0,0.4099924803351544,0.1748899198926894,0.0,True,True,True,1429870263000.0,1430470558000.0,1430474158000.0,1431333524000.0,1431334424000.0,1429870263000.0,-1.0,1429870263000.0,1429873863000.0,1430126330000.0,1430129930000.0,1430470558000.0,1430474158000.0,"sabby",1429873863000.0,1429870263000.0,45,1429870263000.0,-1.0,"Experiment with re-parsing of streams when needed","As a follow up to XD-2877, experiment with the removal of the list of modules from BaseDefinition and reparse as needed.

Branch is here: https://github.com/pperalta/spring-xd/tree/deploy-refactor-2",8.0,False,8.0,0,0.0,-1,0,True
"XD-2984","Bug","Sprint 47","2015-04-23T12:31:28.000+0000","thomas.risberg","iperumal","xd-admin script fails when providing --hadoopDistro option","XD-2837 added back the --hadoopDistro option for xd-admin scripts. However, if I try to use it I get an error message saying: ""--hadoopDistro"" is not a valid option
",3.0,0.7848200825850429,0.759716851281379,0.7588757073255992,True,True,True,1429792288000.0,1429864132000.0,1429867732000.0,1429882930000.0,1429883830000.0,1429792288000.0,-1.0,1429861757000.0,1429865357000.0,1429861834000.0,1429865434000.0,1429864132000.0,1429867732000.0,"thomas.risberg",1429865357000.0,1429792288000.0,45,1429792288000.0,-1.0,"xd-admin script fails when providing --hadoopDistro option","XD-2837 added back the --hadoopDistro option for xd-admin scripts. However, if I try to use it I get an error message saying: ""--hadoopDistro"" is not a valid option
",3.0,False,3.0,0,0.0,-1,0,False
"XD-2980","Story","Sprint 48","2015-04-22T07:20:14.000+0000","sabby","","Create Boot based ModuleRunner (phase 2)","As a user, I'd like to use Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without _xd-containers_.

Scope:
* Complete the remaining deployment properties work",8.0,0.993716656022398,0.3148037046355252,0.0,True,True,True,1429687214000.0,1431073565000.0,1431077165000.0,1431081431000.0,1431082331000.0,1429687214000.0,-1.0,1429687214000.0,1429690814000.0,1430126402000.0,1430130002000.0,1431073565000.0,1431077165000.0,"sabby",1429690814000.0,1429687214000.0,45,1429687214000.0,-1.0,"Create Boot based ModuleRunner (phase 2)","As a user, I'd like to use Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without _xd-containers_.

Scope:
* Complete the remaining deployment properties work",8.0,False,8.0,0,0.0,0,0,False
"XD-2979","Story","Sprint 48","2015-04-22T07:15:16.000+0000","sabby","","Submit Java receptor client for CF incubation","As a user, I'd like to use the java receptor client, so I can interact with Diego runtime using the receptor REST APIs.

Scope:
* Complete the remaining refactoring and polishing work
* Document
* Submit for CF incubation process",8.0,0.9999909182758346,0.090700830461754,0.0,True,True,True,1429686916000.0,1434531767000.0,1434531811000.0,1434530911000.0,1434531811000.0,1429686916000.0,1433839572000.0,1429686916000.0,1429690516000.0,1430126352000.0,1430129952000.0,1434531767000.0,1434531811000.0,"sabby",1429690516000.0,1429686916000.0,45,1433839572000.0,1433839572000.0,"Submit java receptor client for CF incubation","As a user, I'd like to use the Java receptor client, so I can interact with Diego runtime using the Java receptor REST APIs.",5.0,True,5.0,1,60.0,-1,0,True
"XD-2973","Story","Sprint 49","2015-04-21T08:26:38.000+0000","sabby","","Run the sqoop job against secured cluster","As a user, I'd like to run the sqoop jobs against secured hdfs cluster, so I can restrict access to only authorized users. ",3.0,-1.0,-1.0,0.0,False,True,False,1429604798000.0,-1.0,-1.0,1433666295000.0,1433667195000.0,1429604798000.0,-1.0,1429604798000.0,1429608398000.0,-1.0,-1.0,-1.0,-1.0,"sabby",1429608398000.0,1429604798000.0,45,1429604798000.0,-1.0,"Run the sqoop job against secured cluster","As a user, I'd like to run the sqoop jobs against secured hdfs cluster, so I can restrict access to only authorized users. ",3.0,False,3.0,0,0.0,0,0,False
"XD-2972","Bug","Sprint 47","2015-04-20T20:21:14.000+0000","hillert","hillert","STS - Spring XD Imported with Compilation Error","Cannot import Spring XD into STS without compilation errors in class:

*org.springframework.xd.dirt.rest.ModulesController#list*

Error is in:

{code}
return assembler.toResource(page, detailed ? detailedAssembler: simpleAssembler);
{code}

{code}
The method toResource(Page<ModuleDefinition>, Link) in the type PagedResourcesAssembler<ModuleDefinition> is not applicable for the arguments (Page<ModuleDefinition>, (detailed ? detailedAssembler : simpleAssembler))
{code}

Seems to be an STS specific issue.",3.0,0.4235533548066599,0.4234972677595628,1.0104161658894604,True,True,True,1429561274000.0,1429614136000.0,1429617736000.0,1429685180000.0,1429686080000.0,1429561274000.0,-1.0,1429687380000.0,1429686080000.0,1429614129000.0,1429617729000.0,1429614136000.0,1429617736000.0,"hillert",1429686080000.0,1429561274000.0,45,1429561274000.0,-1.0,"STS - Spring XD Imported with Compilation Error","Cannot import Spring XD into STS without compilation errors in class:

*org.springframework.xd.dirt.rest.ModulesController#list*

Error is in:

{code}
return assembler.toResource(page, detailed ? detailedAssembler: simpleAssembler);
{code}

{code}
The method toResource(Page<ModuleDefinition>, Link) in the type PagedResourcesAssembler<ModuleDefinition> is not applicable for the arguments (Page<ModuleDefinition>, (detailed ? detailedAssembler : simpleAssembler))
{code}

Seems to be an STS specific issue.",3.0,False,3.0,0,0.0,0,0,False
"XD-2971","Story","Sprint 47","2015-04-20T15:19:23.000+0000","sabby","","Document the use of properties file as deployment manifest","As a user, I'd like to refer to the documentation to configure the properties file, so I can use it as recommended to represent the deployment manifest.",1.0,0.8546241867117863,0.4758855320460267,0.0,True,True,True,1429543163000.0,1430719037000.0,1430722637000.0,1430918159000.0,1430919059000.0,1429543163000.0,-1.0,1429543163000.0,1429546763000.0,1430197932000.0,1430201532000.0,1430719037000.0,1430722637000.0,"sabby",1429546763000.0,1429543163000.0,45,1429543163000.0,-1.0,"Document the use of properties file as deployment manifest","As a user, I'd like to refer to the documentation to configure the properties file, so I can use it as recommended to represent the deployment manifest.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2970","Story","Sprint 49","2015-04-20T09:52:49.000+0000","dturanski","dturanski","Standardize XD logging to align with Spring Boot","In XD today we use  commons-logging or slf4j  APIs bound to log4j at runtime (configured with log4j.properties).  

Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring-xd-dirt and spring-boot, requiring specific dependency exclusions.  In order to simplify building and troubleshooting log dependencies, XD should standardize on 
slf4j APIs (replace any commons-logging Loggers with Slf4j). This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA.

",8.0,0.7100743082780634,0.6764588356820984,0.6759429040631102,True,True,True,1429523569000.0,1431433867000.0,1431437467000.0,1432212948000.0,1432213848000.0,1429523569000.0,1431601371000.0,1431342044000.0,1431345644000.0,1431343432000.0,1431347032000.0,1431433867000.0,1431437467000.0,"dturanski",1431345644000.0,1429523569000.0,45,1431601371000.0,1431601371000.0,"Standardize XD logging to align with Spring Boot","In XD today we use  commons-logging or slf4j  APIs bound to log4j at runtime (configured with log4j.properties).  

Boot uses slf4j APIs backed by logback. This causes some build incompatibilities building a component that depends on spring-xd-dirt and spring-boot, requiring specific dependency exclusions.  In order to simplify building and troubleshooting log dependencies, XD should standardize on 
slf4j APIs (replace any commons-logging Loggers with Slf4j). This is internal only, and would not impact users who are used to seeing log4j.properties. An additional step is to replace log4j with logback. This change would be visible to end users but will provide us greater affinity with boot and improve the developer experience. If we make this change it should go into 1.2 GA.

",3.0,True,3.0,1,166.66666666666669,-1,1,False
"XD-2962","Story","Sprint 49","2015-04-17T09:21:52.000+0000","sabby","","Publish performance results","As a developer, I'd like to publish performance results along with the infrastructure specifics, so the customers/users can use it as a reference while setting up Spring XD cluster.",8.0,0.7908142327410699,0.4011152179043432,0.0,True,True,True,1429262512000.0,1432817857000.0,1432821457000.0,1433757415000.0,1433758315000.0,1429262512000.0,-1.0,1429262512000.0,1429266112000.0,1431065847000.0,1431069447000.0,1432817857000.0,1432821457000.0,"sabby",1429266112000.0,1429262512000.0,45,1429262512000.0,-1.0,"Document performance benchmark results","As a developer, I'd like to document performance benchmark results along with the infrastructure specifics, so I can publish the blog for customers/users to use it as a reference while setting up Spring XD cluster.",8.0,False,8.0,0,0.0,0,0,True
"XD-2961","Story","Sprint 49","2015-04-17T09:19:52.000+0000","sabby","","Benchmark against Kafka 0.8.2 release","As a developer, I'd like to rerun _baseline_, _Tuple_, and _Serialized_ payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. 

Note:
1.1.1 > Benched against 0.8.1 
1.2 > Benched against 0.8.2 

",8.0,-1.0,0.5357963289777825,0.0,False,True,True,1429262392000.0,-1.0,-1.0,1432627312000.0,1432628212000.0,1429262392000.0,-1.0,1429262392000.0,1429265992000.0,1431065786000.0,1431069386000.0,-1.0,-1.0,"sabby",1429265992000.0,1429262392000.0,45,1429262392000.0,-1.0,"Benchmark against Kafka 0.8.2 release","As a developer, I'd like to rerun _baseline_, _Tuple_, and _Serialized_ payloads, so I can compare the differences in performance between 0.8.1 and 0.8.2 Kafka releases. 

Note:
1.1.1 > Benched against 0.8.1 
1.2 > Benched against 0.8.2 

",8.0,False,8.0,0,0.0,0,0,False
"XD-2958","Story","Sprint 48","2015-04-17T09:05:00.000+0000","sabby","","Upgrade to Kafka 0.8.2","As a developer, I'd like to upgrade to Kafka 0.8.2, so I can leverage the latest features in order to test the performance characteristics.",8.0,0.4521067238221589,0.4521067238221589,0.0,True,True,True,1429261500000.0,1430470588000.0,1430474188000.0,1431934942000.0,1431935842000.0,1429261500000.0,1430118901000.0,1429261500000.0,1429265100000.0,1430470588000.0,1430474188000.0,1430470588000.0,1430474188000.0,"sabby",1429265100000.0,1429261500000.0,45,1430118901000.0,1430118901000.0,"Upgrade to Kafka 0.8.2","As a developer, I'd like to upgrade to Kafka 0.8.2, so I can leverage the latest features in order to test the performance characteristics.",1.0,True,1.0,1,700.0,-1,0,False
"XD-2957","Story","Sprint 48","2015-04-17T08:47:13.000+0000","sabby","","Document Kryo optimization guidelines","As a developer, I'd like to document the Kryo optimization guidelines, so the end-users can refer to it while tuning to improve performance.",3.0,0.4349244473813239,0.4174895683238195,0.0,True,True,True,1429260433000.0,1432796180000.0,1432799780000.0,1437389099000.0,1437389999000.0,1429260433000.0,1432628371000.0,1429260433000.0,1429264033000.0,1432654442000.0,1432658042000.0,1432796180000.0,1432799780000.0,"sabby",1429264033000.0,1429260433000.0,45,1432628371000.0,1432628371000.0,"Create samples and document Kryo optimization guidelines","As a developer, I'd like to document the Kryo optimization guidelines, so the end-users can refer to it while tuning to improve performance.",1.0,True,1.0,1,200.0,-1,0,True
"XD-2956","Story","Sprint 48","2015-04-17T08:42:44.000+0000","sabby","","Revisit the requirement for ID and Timestamp attributes in Tuple","As a developer, I'd like revisit the design to determine the necessity for _ID_ and _TimeStamp_ attributes in {{Tuple}}, so I can refactor in order to improve performance throughput.",1.0,0.3718812464154695,0.3718812464154695,0.0,True,True,True,1429260164000.0,1430136819000.0,1430140419000.0,1431616616000.0,1431617516000.0,1429260164000.0,1430126266000.0,1429260164000.0,1429263764000.0,1430136819000.0,1430140419000.0,1430136819000.0,1430140419000.0,"sabby",1429263764000.0,1429260164000.0,45,1430126266000.0,1430126266000.0,"Revisit the requirement for ID and Timestamp attributes in Tuple","As a developer, I'd like revisit the design to determine the necessity for _ID_ and _TimeStamp_ attributes in {{Tuple}}, so I can refactor in order to improve performance throughput.",5.0,True,5.0,1,-80.0,0,0,False
"XD-2955","Story","Sprint 47","2015-04-17T08:38:18.000+0000","sabby","dturanski","Refactor MessageBus to avoid unnecessary use of MessageBuilder  ","As a developer, I'd like to refactor the programmatic means by which the MessageBus transforms the Message so throughput performance can be optimized.
",5.0,0.0062088405152562,0.0001740944319852,0.0,True,True,True,1429259898000.0,1429263821000.0,1429267421000.0,1429890839000.0,1429891739000.0,1429259898000.0,-1.0,1429259898000.0,1429263498000.0,1429260008000.0,1429263608000.0,1429263821000.0,1429267421000.0,"sabby",1429263498000.0,1429259898000.0,45,1429259898000.0,-1.0,"Refactor MessageBus to avoid unnecessary use of MessageBuilder  ","As a developer, I'd like to refactor the programmatic means by which the MessageBus transforms the Message so throughput performance can be optimized.
",5.0,False,5.0,0,0.0,-1,2,False
"XD-2953","Story","Sprint 48","2015-04-16T13:27:34.000+0000","eric.bottard","eric.bottard","Get rid of SparkStreamingDriverModule","Code that is in there could be moved to the SparkStreamingModule.

Then, as part of a later refactoring, that plugin should be made part of the module (and loaded by the module classloader)",3.0,0.999938969667142,0.0,0.8860555861141909,True,True,True,1429190854000.0,1430468829000.0,1430468907000.0,1430468007000.0,1430468907000.0,1429190854000.0,-1.0,1430323280000.0,1430326880000.0,1429190854000.0,1429194454000.0,1430468829000.0,1430468907000.0,"eric.bottard",1430326880000.0,1429190854000.0,45,1429190854000.0,-1.0,"Get rid of SparkStreamingDriverModule","Code that is in there could be moved to the SparkStreamingModule.

Then, as part of a later refactoring, that plugin should be made part of the module (and loaded by the module classloader)",3.0,False,3.0,0,0.0,-1,0,False
"XD-2949","Story","Sprint 47","2015-04-15T06:02:14.000+0000","grenfro","grenfro","Error Message for ""Missing Job Description"" needs to be updated","When using the rest interface to create a Job with an empty description, used to generate the following exception, ""Definition can not be empty"".   Now generates ""XD112E:(pos 0): Unexpectedly ran out of input^"". 
The correct error should be, ""definition cannot be blank or null""

",2.0,0.7269572657698341,0.7269572657698341,0.7615105077589075,True,True,True,1429077734000.0,1429521231000.0,1429524831000.0,1429686907000.0,1429687807000.0,1429077734000.0,-1.0,1429542311000.0,1429545911000.0,1429521231000.0,1429524831000.0,1429521231000.0,1429524831000.0,"grenfro",1429545911000.0,1429077734000.0,45,1429077734000.0,-1.0,"Error Message for ""Missing Job Description"" needs to be updated","When using the rest interface to create a Job with an empty description, used to generate the following exception, ""Definition can not be empty"".   Now generates ""XD112E:(pos 0): Unexpectedly ran out of input^"". 
The correct error should be, ""definition cannot be blank or null""

",2.0,False,2.0,0,0.0,-1,1,False
"XD-2948","Improvement","","2015-04-14T13:35:34.000+0000","thomasd","","Document how to specify custom-modules location via Environment variable.","It is possible to specify the location of custom modules via the environment variable {{XD_CUSTOMMODULE_HOME}} which is provided by Spring Boot property key derivation mechanism (in this case derived from {{xd.customModule.home}}).

This allows a user to specify a custom modules location that survives a complete wipe of spring-xd installations.",1.0,-1.0,0.2081869388210362,-1.0,False,False,True,1429018534000.0,-1.0,-1.0,1430219011000.0,1430219911000.0,1429018534000.0,-1.0,-1.0,-1.0,1429268645000.0,1429272245000.0,-1.0,-1.0,"thomasd",-1.0,-1.0,0,1429018534000.0,-1.0,"Document how to specify custom-modules location via Environment variable.","It is possible to specify the location of custom modules via the environment variable {{XD_CUSTOMMODULE_HOME}} which is provided by Spring Boot property key derivation mechanism (in this case derived from {{xd.customModule.home}}).

This allows a user to specify a custom modules location that survives a complete wipe of spring-xd installations.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2942","Improvement","Sprint 48","2015-04-10T12:33:44.000+0000","fmarchand","eric.bottard","Add ftp source to default source modules","It would be nice to have a simple ftp source. I have to do it for one of my projects. Same as XD-2139 but for source modules.",2.0,0.9999896138469854,0.9999896138469854,0.9999555472650974,True,True,True,1428669224000.0,1431076250000.0,1431076275000.0,1431075375000.0,1431076275000.0,1428669224000.0,-1.0,1431076168000.0,1431076275000.0,1431076250000.0,1431076275000.0,1431076250000.0,1431076275000.0,"fmarchand",1431076275000.0,1428669224000.0,45,1428669224000.0,-1.0,"Add ftp source to default source modules","It would be nice to have a simple ftp source. I have to do it for one of my projects. Same as XD-2139 but for source modules.",2.0,False,2.0,0,0.0,0,0,False
"XD-2941","Bug","Sprint 47","2015-04-10T07:48:10.000+0000","aboyko","iperumal","Failure to get message rates for modules with labels.","Start XD distributed XD with specified management port and
xd:
  messageRateMonitoring:
    enabled: true
in servers.yml
to gather stats.

Create stream {{file | log}}, deploy it, navigate to Containers tab in Admin UI. Rates are shown correctly.
Create stream {{MYFILE: file | log}}, deploy it, navigate to Containers tab in Admin UI - none of the message rates are shown. Open browser dev tools console and note 500 error response.

spring-xd-dirt -> ContainersController lines 109-112 creates request to get message rates for modules. 
Typical request:
{{http://192.168.0.10:9292/management/jolokia/read/xd.str4:module=log.*,component=*,name=input/MeanSendRate}}

Typical response:
{code:json}
{""request"":{""mbean"":""xd.str4:component=*,module=log.1,name=input"",""attribute"":""MeanSendRate"",""type"":""read""},""value"":{""xd.str4:component=MessageChannel,module=log.1,name=input"":{""MeanSendRate"":0.0}},""timestamp"":1428675070,""status"":200}
{code}

For file module with label `MYFILE` the request is:
{{http://192.168.0.10:9292/management/jolokia/read/xd.str4:module=MYFILE.*,component=*,name=output/MeanSendRate}}

Response:
{code:json}
{""mbean"":""xd.str4:component=*,module=MYFILE.1,name=output"",""attribute"":""MeanSendRate"",""type"":""read""},""stacktrace"":""javax.management.InstanceNotFoundException: No MBean with pattern xd.str4:module=MYFILE.1,component=*,name=output found for reading attributes\n\tat org.jolokia.handler.ReadHandler.searchMBeans(ReadHandler.java:160)\n\tat org.jolokia.handler.ReadHandler.fetchAttributesForMBeanPattern(ReadHandler.java:126)\n\tat org.jolokia.handler.ReadHandler.doHandleRequest(ReadHandler.java:116)\n\tat org.jolokia.handler.ReadHandler.doHandleRequest(ReadHandler.java:37)\n\tat org.jolokia.handler.JsonRequestHandler.handleRequest(JsonRequestHandler.java:160)\n\tat org.jolokia.backend.MBeanServerHandler.dispatchRequest(MBeanServerHandler.java:97)\n\tat org.jolokia.backend.LocalRequestDispatcher.dispatchRequest(LocalRequestDispatcher.java:98)\n\tat org.jolokia.backend.BackendManager.callRequestDispatcher(BackendManager.java:411)\n\tat org.jolokia.backend.BackendManager.handleRequest(BackendManager.java:158)\n\tat org.jolokia.http.HttpRequestHandler.executeRequest(HttpRequestHandler.java:197)\n\tat org.jolokia.http.HttpRequestHandler.handleGetRequest(HttpRequestHandler.java:86)\n\tat org.jolokia.http.AgentServlet$4.handleRequest(AgentServlet.java:435)\n\tat org.jolokia.http.AgentServlet.handleSecurely(AgentServlet.java:320)\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:291)\n\tat org.jolokia.http.AgentServlet.doGet(AgentServlet.java:252)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:735)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:158)\n\tat org.springframework.web.servlet.mvc.AbstractController.handleRequest(AbstractController.java:146)\n\tat org.springframework.boot.actuate.endpoint.mvc.JolokiaMvcEndpoint.handle(JolokiaMvcEndpoint.java:130)\n\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:857)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:735)\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:744)\n"",""error_type"":""javax.management.InstanceNotFoundException"",""error"":""javax.management.InstanceNotFoundException : No MBean with pattern xd.str4:module=MYFILE.1,component=*,name=output found for reading attributes"",""status"":404}
{code}

This reponse results in JSONException in the ContainersController because it's missing 'value' property.

The module id is somewhat problematic in the request: {{xd.str4:module=log.*}} index is {{\*}} but should be index within the stream, also node type (source/sink/processor) is missing. Therefore, stream {{mail | mail}} is suffering from the same problem.

Would be nice to have some sort of a bulk request to query more than one module for input/output message rates, such that I could get all message rates for modules in the stream.",3.0,0.7162812048061942,0.0018155673174848,0.0145500403916426,True,True,True,1428652090000.0,1429522800000.0,1429526400000.0,1429866788000.0,1429867688000.0,1428652090000.0,1429534709000.0,1428669777000.0,1428673377000.0,1428654297000.0,1428657897000.0,1429522800000.0,1429526400000.0,"aboyko",1428673377000.0,1428652090000.0,45,1429534709000.0,1429534709000.0,"Failure to get message rates for modules with labels.","Start XD distributed XD with specified management port and
xd:
  messageRateMonitoring:
    enabled: true
in servers.yml
to gather stats.

Create stream {{file | log}}, deploy it, navigate to Containers tab in Admin UI. Rates are shown correctly.
Create stream {{MYFILE: file | log}}, deploy it, navigate to Containers tab in Admin UI - none of the message rates are shown. Open browser dev tools console and note 500 error response.

spring-xd-dirt -> ContainersController lines 109-112 creates request to get message rates for modules. 
Typical request:
{{http://192.168.0.10:9292/management/jolokia/read/xd.str4:module=log.*,component=*,name=input/MeanSendRate}}

Typical response:
{code:json}
{""request"":{""mbean"":""xd.str4:component=*,module=log.1,name=input"",""attribute"":""MeanSendRate"",""type"":""read""},""value"":{""xd.str4:component=MessageChannel,module=log.1,name=input"":{""MeanSendRate"":0.0}},""timestamp"":1428675070,""status"":200}
{code}

For file module with label `MYFILE` the request is:
{{http://192.168.0.10:9292/management/jolokia/read/xd.str4:module=MYFILE.*,component=*,name=output/MeanSendRate}}

Response:
{code:json}
{""mbean"":""xd.str4:component=*,module=MYFILE.1,name=output"",""attribute"":""MeanSendRate"",""type"":""read""},""stacktrace"":""javax.management.InstanceNotFoundException: No MBean with pattern xd.str4:module=MYFILE.1,component=*,name=output found for reading attributes\n\tat org.jolokia.handler.ReadHandler.searchMBeans(ReadHandler.java:160)\n\tat org.jolokia.handler.ReadHandler.fetchAttributesForMBeanPattern(ReadHandler.java:126)\n\tat org.jolokia.handler.ReadHandler.doHandleRequest(ReadHandler.java:116)\n\tat org.jolokia.handler.ReadHandler.doHandleRequest(ReadHandler.java:37)\n\tat org.jolokia.handler.JsonRequestHandler.handleRequest(JsonRequestHandler.java:160)\n\tat org.jolokia.backend.MBeanServerHandler.dispatchRequest(MBeanServerHandler.java:97)\n\tat org.jolokia.backend.LocalRequestDispatcher.dispatchRequest(LocalRequestDispatcher.java:98)\n\tat org.jolokia.backend.BackendManager.callRequestDispatcher(BackendManager.java:411)\n\tat org.jolokia.backend.BackendManager.handleRequest(BackendManager.java:158)\n\tat org.jolokia.http.HttpRequestHandler.executeRequest(HttpRequestHandler.java:197)\n\tat org.jolokia.http.HttpRequestHandler.handleGetRequest(HttpRequestHandler.java:86)\n\tat org.jolokia.http.AgentServlet$4.handleRequest(AgentServlet.java:435)\n\tat org.jolokia.http.AgentServlet.handleSecurely(AgentServlet.java:320)\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:291)\n\tat org.jolokia.http.AgentServlet.doGet(AgentServlet.java:252)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:735)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:158)\n\tat org.springframework.web.servlet.mvc.AbstractController.handleRequest(AbstractController.java:146)\n\tat org.springframework.boot.actuate.endpoint.mvc.JolokiaMvcEndpoint.handle(JolokiaMvcEndpoint.java:130)\n\tat sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.doInvoke(InvocableHandlerMethod.java:221)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:137)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:110)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:777)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:706)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:85)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:857)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:735)\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:848)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1496)\n\tat org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)\n\tat org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1467)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:499)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:971)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:1033)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:744)\n"",""error_type"":""javax.management.InstanceNotFoundException"",""error"":""javax.management.InstanceNotFoundException : No MBean with pattern xd.str4:module=MYFILE.1,component=*,name=output found for reading attributes"",""status"":404}
{code}

This reponse results in JSONException in the ContainersController because it's missing 'value' property.

The module id is somewhat problematic in the request: {{xd.str4:module=log.*}} index is {{\*}} but should be index within the stream, also node type (source/sink/processor) is missing. Therefore, stream {{mail | mail}} is suffering from the same problem.

Would be nice to have some sort of a bulk request to query more than one module for input/output message rates, such that I could get all message rates for modules in the stream.",2.0,True,2.0,1,50.0,-1,13,False
"XD-2935","Story","Sprint 49","2015-04-09T07:33:16.000+0000","sabby","","Parameterize Merge Options","As a user, I'd like to parameterize Merge Options, so I can incrementally consume the delta with the help of megastore. 
",5.0,0.7873883594802081,0.7873883594802081,0.0,True,True,True,1428564796000.0,1429868082000.0,1429871682000.0,1430219097000.0,1430219997000.0,1428564796000.0,-1.0,1428564796000.0,1428568396000.0,1429868082000.0,1429871682000.0,1429868082000.0,1429871682000.0,"sabby",1428568396000.0,1428564796000.0,45,1428564796000.0,-1.0,"Parameterize Merge Options","As a user, I'd like to parameterize Merge Options, so I can incrementally consume the delta with the help of megastore. 
",5.0,False,5.0,0,0.0,0,0,False
"XD-2934","Story","Sprint 49","2015-04-09T07:28:52.000+0000","sabby","","Parameterize codegen options","As a user, I'd like to parameterize CodeGen Options, so I can generate code on the fly as needed. 

",5.0,0.787423986797699,0.787423986797699,0.0,True,True,True,1428564532000.0,1429868088000.0,1429871688000.0,1430219101000.0,1430220001000.0,1428564532000.0,-1.0,1428564532000.0,1428568132000.0,1429868088000.0,1429871688000.0,1429868088000.0,1429871688000.0,"sabby",1428568132000.0,1428564532000.0,45,1428564532000.0,-1.0,"Parameterize codegen options","As a user, I'd like to parameterize CodeGen Options, so I can generate code on the fly as needed. 

",5.0,False,5.0,0,0.0,-1,0,False
"XD-2931","Bug","Sprint 47","2015-04-08T16:05:16.000+0000","iperumal","hillert","Login page is missing style info when secured","If the admin UI is secured, the login page is displayed without any styles.",1.0,0.8577911703587247,0.8577613281946582,0.8579258240258535,True,True,True,1428509116000.0,1429687631000.0,1429691231000.0,1429882111000.0,1429883011000.0,1428509116000.0,-1.0,1429687816000.0,1429691416000.0,1429687590000.0,1429691190000.0,1429687631000.0,1429691231000.0,"iperumal",1429691416000.0,1428509116000.0,45,1428509116000.0,-1.0,"Login page is missing style info when secured","If the admin UI is secured, the login page is displayed without any styles.",1.0,False,1.0,0,0.0,0,0,False
"XD-2929","Story","Sprint 48","2015-04-08T11:32:11.000+0000","sabby","","Document the use of nested jobs with example","As a developer, I'd like to document how to nest batch jobs and workflows in XD, so it will be easy for end-users to use it as reference. ",1.0,0.7841237152812013,0.2155271262097301,0.0,True,True,True,1428492731000.0,1432636145000.0,1432639745000.0,1433775964000.0,1433776864000.0,1428492731000.0,-1.0,1428492731000.0,1428496331000.0,1429631605000.0,1429635205000.0,1432636145000.0,1432639745000.0,"sabby",1428496331000.0,1428492731000.0,45,1428492731000.0,-1.0,"Document the use of nested jobs with example","As a developer, I'd like to document how to nest batch jobs and workflows in XD, so it will be easy for end-users to use it as reference. ",1.0,False,1.0,0,0.0,-1,0,False
"XD-2924","Bug","Sprint 47","2015-04-07T18:56:14.000+0000","iperumal","iperumal","Ability to tap spark streaming processor output","We need to support adding a tap stream that connects to spark streaming processor module's output channel.",3.0,9.730124024555066e-06,0.0,0.0677034584627283,True,True,True,1428432974000.0,1428433005000.0,1428436605000.0,1431618056000.0,1431618956000.0,1428432974000.0,-1.0,1428648676000.0,1428652276000.0,1428432974000.0,1428436574000.0,1428433005000.0,1428436605000.0,"iperumal",1428652276000.0,1428432974000.0,45,1428432974000.0,-1.0,"Ability to tap spark streaming processor output","We need to support adding a tap stream that connects to spark streaming processor module's output channel.",3.0,False,3.0,0,0.0,-1,0,False
"XD-2923","Bug","Sprint 47","2015-04-07T18:10:37.000+0000","iperumal","iperumal","Not able to connect a pubsub channel to spark streaming module","If a spark streaming module is setup to connect to a pub/sub channel (a topic or a tap channel), then it doesn't bind to it.

For instance, if I have a stream ""ingest"" with a definition ""http | log"" and want to create another stream as,
""tap:stream:ingest > spark-processor | count"" then this stream doesn't work.",3.0,0.0015383089767745,0.0,0.1820296178643838,True,True,True,1428430237000.0,1428432083000.0,1428435683000.0,1429629356000.0,1429630256000.0,1428430237000.0,-1.0,1428648676000.0,1428652276000.0,1428430237000.0,1428433837000.0,1428432083000.0,1428435683000.0,"iperumal",1428652276000.0,1428430237000.0,45,1428430237000.0,-1.0,"Not able to connect a pubsub channel to spark streaming module","If a spark streaming module is setup to connect to a pub/sub channel (a topic or a tap channel), then it doesn't bind to it.

For instance, if I have a stream ""ingest"" with a definition ""http | log"" and want to create another stream as,
""tap:stream:ingest > spark-processor | count"" then this stream doesn't work.",3.0,False,3.0,0,0.0,0,0,False
"XD-2922","Story","Sprint 55","2015-04-07T14:06:03.000+0000","iperumal","iperumal","Upgrade Spark version to 1.4.1","As a Spring XD user, I'd like to create streaming pipelines, so I can take advantage of latest specs from both XD and Spark/Spark Streaming.",3.0,1.0149875085487323e-06,6.766583390324881e-07,0.6891273929091755,True,True,True,1428415563000.0,1428415578000.0,1428419178000.0,1443193170000.0,1443194070000.0,1428415563000.0,1438768227000.0,1438599837000.0,1438603437000.0,1428415573000.0,1428419173000.0,1428415578000.0,1428419178000.0,"iperumal",1438603437000.0,1428415563000.0,45,1438768227000.0,1438768227000.0,"Upgrade Spark version to 1.3.1","As a Spring XD user, I'd like to create streaming pipelines, so I can take advantage of latest specs from both XD and Spark/Spark Streaming.",1.0,True,1.0,1,200.0,0,2,True
"XD-2921","Story","Sprint 47","2015-04-07T07:30:13.000+0000","sabby","","Clarify the use of escape quotes for properties in the Sqoop job","As a developer, I'd like to add documentation on escape quotes, so when someone using Sqoop job can double escape {{\\\\N}} instead of sending quotes {{'\N'}} to successfully submit the job.",1.0,0.8943284675793924,0.4761573192163996,0.0,True,True,True,1428391813000.0,1430720444000.0,1430724044000.0,1430994689000.0,1430995589000.0,1428391813000.0,-1.0,1428391813000.0,1428395413000.0,1429631620000.0,1429635220000.0,1430720444000.0,1430724044000.0,"sabby",1428395413000.0,1428391813000.0,45,1428391813000.0,-1.0,"Clarify the use of escape quotes for properties in the Sqoop job","As a developer, I'd like to add documentation on escape quotes, so when someone using Sqoop job can double escape {{\\\\N}} instead of sending quotes {{'\N'}} to successfully submit the job.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2920","Improvement","Sprint 49","2015-04-07T04:57:44.000+0000","kdowbecki","iperumal","Dynamic router should allow to discard messages","Currently dynamic router sink has to return a valid queue name. This is problematic when the message should be discarded as part of the routing process. In this case one have to define a stream with {{filter | router}} steps where part of the SpEL is duplicated between {{filter}} and {{router}} modules.

Instead the dynamic router should allow to return null to discard the message and stop further processing. Spring Integration is already providing {{resolution-required}} attribute on {{<router/>}}. ",2.0,0.9999957790221622,0.9999957790221622,0.9999439212944412,True,True,True,1428382664000.0,1431699417000.0,1431699431000.0,1431698531000.0,1431699431000.0,1428382664000.0,-1.0,1431699245000.0,1431699431000.0,1431699417000.0,1431699431000.0,1431699417000.0,1431699431000.0,"kdowbecki",1431699431000.0,1428382664000.0,45,1428382664000.0,-1.0,"Dynamic router should allow to discard messages","Currently dynamic router sink has to return a valid queue name. This is problematic when the message should be discarded as part of the routing process. In this case one have to define a stream with {{filter | router}} steps where part of the SpEL is duplicated between {{filter}} and {{router}} modules.

Instead the dynamic router should allow to return null to discard the message and stop further processing. Spring Integration is already providing {{resolution-required}} attribute on {{<router/>}}. ",2.0,False,2.0,0,0.0,0,0,False
"XD-2919","Story","Sprint 48","2015-04-06T09:56:55.000+0000","sabby","","Evaluate stream/job repository implementation for Lattice","As a developer, I'd like to evaluate whether another implementation of stream/job definition repository is required for Lattice.",5.0,0.9666950053378156,0.96580994630146,0.0,True,True,True,1428314215000.0,1440142059000.0,1440145659000.0,1440548657000.0,1440549557000.0,1428314215000.0,1438769856000.0,1428314215000.0,1428317815000.0,1440131230000.0,1440134830000.0,1440142059000.0,1440145659000.0,"sabby",1428317815000.0,1428314215000.0,45,1438769856000.0,1438769856000.0,"Create persistent stream repository","As a developer, I'd like to create persistent repository for streams, so I could leverage the persisted metadata and reestablish the streaming pipe under failure conditions.",3.0,True,3.0,1,66.66666666666666,-1,0,True
"XD-2916","Story","Sprint 47","2015-04-06T09:51:42.000+0000","sabby","mark.fisher","Create a Java client for Receptor","As a developer, I'd like to create a [java client|https://github.com/markfisher/receptor-client] for Receptor, so I can interact with Diego runtime via Receptor API calls from XD. ",8.0,0.0004405113134955,0.0003815337657382,0.0,True,True,True,1428313902000.0,1428314507000.0,1428318107000.0,1429686406000.0,1429687306000.0,1428313902000.0,-1.0,1428313902000.0,1428317502000.0,1428314426000.0,1428318026000.0,1428314507000.0,1428318107000.0,"sabby",1428317502000.0,1428313902000.0,45,1428313902000.0,-1.0,"Create a Java client for Receptor","As a developer, I'd like to create a [java client|https://github.com/markfisher/receptor-client] for Receptor, so I can interact with Diego runtime via Receptor API calls from XD. ",8.0,False,8.0,0,0.0,0,0,False
"XD-2915","Story","Sprint 47","2015-04-06T09:44:52.000+0000","sabby","mark.fisher","Create Boot based ModuleRunner","As a developer, I'd like to build isolated Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without the hard requirement for running _xd-containers_.",8.0,0.0007104138961616,0.0006732918585548,0.0,True,True,True,1428313492000.0,1428314468000.0,1428318068000.0,1429686439000.0,1429687339000.0,1428313492000.0,-1.0,1428313492000.0,1428317092000.0,1428314417000.0,1428318017000.0,1428314468000.0,1428318068000.0,"sabby",1428317092000.0,1428313492000.0,45,1428313492000.0,-1.0,"Create Boot based ModuleRunner","As a developer, I'd like to build isolated Boot-based {{ModuleRunner}} for use in container-managed environments, so I can run XD without the hard requirement for running _xd-containers_.",8.0,False,8.0,0,0.0,0,0,False
"XD-2914","Story","Sprint 47","2015-04-06T09:39:50.000+0000","sabby","pperalta","Create a pluggable runtime SPI","As a developer, I'd like to migrate module deployment from the ""repository"" abstraction (used for stream/job definitions), so I can create it as a pluggable runtime SPI.",8.0,-1.0,0.0001049046459041,0.0,False,True,True,1428313190000.0,-1.0,-1.0,1439903769000.0,1439904669000.0,1428313190000.0,-1.0,1428313190000.0,1428316790000.0,1428314406000.0,1428318006000.0,-1.0,-1.0,"sabby",1428316790000.0,1428313190000.0,45,1428313190000.0,-1.0,"Create a pluggable runtime SPI","As a developer, I'd like to migrate module deployment from the ""repository"" abstraction (used for stream/job definitions), so I can create it as a pluggable runtime SPI.",8.0,False,8.0,0,0.0,-1,0,False
"XD-2911","Story","Sprint 47","2015-04-06T08:32:58.000+0000","sabby","","Research the impacts of TupleBuilder wrt performance","As a developer, I'd like to bench test cases around {{TupleBuilder}}, so I can identify the bottlenecks and tune for performance optimizations. ",8.0,0.0162626398930914,0.014773929281893,0.0,True,True,True,1428309178000.0,1428331594000.0,1428335194000.0,1429686652000.0,1429687552000.0,1428309178000.0,-1.0,1428309178000.0,1428312778000.0,1428329542000.0,1428333142000.0,1428331594000.0,1428335194000.0,"sabby",1428312778000.0,1428309178000.0,45,1428309178000.0,-1.0,"Improve performance of TupleBuilder ","As a developer, I'd like to bench test cases around {{TupleBuilder}}, so I can identify the bottlenecks and tune for performance optimizations. ",8.0,False,8.0,0,0.0,0,0,True
"XD-2910","Story","Sprint 47","2015-04-06T08:31:02.000+0000","sabby","","Revisit benchmark matrix for Sqoop vs. jdbshdfs jobs","As a developer, I'd like to revisit performance benchmarks with new improvements, so I can verify the optimizations around _jdbchdfs_.",1.0,-1.0,0.0008486273601718,0.0,False,True,True,1428309062000.0,-1.0,-1.0,1436012368000.0,1436013268000.0,1428309062000.0,-1.0,1428309062000.0,1428312662000.0,1428315600000.0,1428319200000.0,-1.0,-1.0,"sabby",1428312662000.0,1428309062000.0,45,1428309062000.0,-1.0,"Revisit benchmark matrix for Sqoop vs. jdbshdfs jobs","As a developer, I'd like to revisit performance benchmarks with new improvements, so I can verify the optimizations around _jdbchdfs_.",1.0,False,1.0,0,0.0,0,0,False
"XD-2909","Story","Sprint 47","2015-04-06T08:21:29.000+0000","grenfro","","Produce Kafka Baseline numbers on Rackspace","",5.0,0.0127957893515833,0.005206037582647,0.0001043962417352,True,True,True,1428308489000.0,1428326139000.0,1428329739000.0,1429686949000.0,1429687849000.0,1428308489000.0,-1.0,1428308633000.0,1428312233000.0,1428315670000.0,1428319270000.0,1428326139000.0,1428329739000.0,"grenfro",1428312233000.0,1428308489000.0,45,1428308489000.0,-1.0,"Produce Kafka Baseline numbers on Rackspace","",5.0,False,5.0,0,0.0,0,0,False
"XD-2908","Story","Sprint 47","2015-04-06T07:09:51.000+0000","grenfro","","Acceptance Tests needs to wait for JobDefinitionResources to be populated ","After the Introduction to XD-2861 the acquisition of JobResources takes more time.  We have to introduce a pause to wait for getJobDefinitionResource to be populated. ",5.0,0.8908683579088665,0.1885998139191902,0.0031901624894501,True,True,True,1428304191000.0,1429541290000.0,1429544890000.0,1429691935000.0,1429692835000.0,1428304191000.0,-1.0,1428308621000.0,1428312221000.0,1428566089000.0,1428569689000.0,1429541290000.0,1429544890000.0,"grenfro",1428312221000.0,1428304191000.0,45,1428304191000.0,-1.0,"Acceptance Tests needs to wait for JobDefinitionResources to be populated ","After the Introduction to XD-2861 the acquisition of JobResources takes more time.  We have to introduce a pause to wait for getJobDefinitionResource to be populated. ",5.0,False,5.0,0,0.0,-1,0,False
"XD-2906","Story","Sprint 47","2015-04-04T12:47:13.000+0000","sabby","","Create a new CI build to verify 'install' target","As a developer, I'd like to add a new CI build to include _install_ target, so I can verify the target expectations, as it is often time consuming to verify it in the development environment.",2.0,0.9941510220561446,0.9941510220561446,0.0,True,True,True,1428151633000.0,1429513263000.0,1429516863000.0,1429520374000.0,1429521274000.0,1428151633000.0,1428308255000.0,1428151633000.0,1428155233000.0,1429513263000.0,1429516863000.0,1429513263000.0,1429516863000.0,"sabby",1428155233000.0,1428151633000.0,45,1428308255000.0,1428308255000.0,"Create a new CI build to verify 'install' target","As a developer, I'd like to add a new CI build to include _install_ target, so I can verify the target expectations, as it is often time consuming to verify it in the development environment.",1.0,True,1.0,1,100.0,-1,0,False
"XD-2905","Story","Sprint 47","2015-04-03T16:24:40.000+0000","sabby","","Complete remaining work for the DEBS challenge","As a developer, I'd like to complete the remaining work with DEBS challenge, so I can submit by the deadline.",2.0,-1.0,-1.0,0.0,False,True,False,1428078280000.0,-1.0,-1.0,1432372876000.0,1432373776000.0,1428078280000.0,-1.0,1428078280000.0,1428081880000.0,-1.0,-1.0,-1.0,-1.0,"sabby",1428081880000.0,1428078280000.0,45,1428078280000.0,-1.0,"Complete remaining work for the DEBS challenge","As a developer, I'd like to complete the remaining work with DEBS challenge, so I can submit by the deadline.",2.0,False,2.0,0,0.0,-1,0,False
"XD-2904","Story","Sprint 47","2015-04-03T14:53:02.000+0000","sabby","","Upgrade to Boot 1.2.3 release","As a user, I'd like to upgrade to Spring Boot 1.2.3 release, do I can leverage the latest improvements and bug fixes.",1.0,0.9045731614642232,0.9045731614642232,0.0,True,True,True,1428072782000.0,1430721270000.0,1430724870000.0,1430999769000.0,1431000669000.0,1428072782000.0,1430291520000.0,1428072782000.0,1428076382000.0,1430721270000.0,1430724870000.0,1430721270000.0,1430724870000.0,"sabby",1428076382000.0,1428072782000.0,45,1430291520000.0,1430291520000.0,"Upgrade to Boot 1.2.3 release","As a user, I'd like to upgrade to Spring Boot 1.2.3 release, do I can leverage the latest improvements and bug fixes.

We should also sync-up the following dependency updates to [synchronize with Boot|https://github.com/spring-projects/spring-boot/blob/master/spring-boot-dependencies/pom.xml]:
{code}
<logback.version>1.1.3</logback.version>
<jackson.version>2.5.1</jackson.version>
<gemfire.version>8.0.0</gemfire.version>
<h2.version>1.4.185</h2.version>
<javax-mail.version>1.5.3</javax-mail.version>
<undertow.version>1.2.3.Final</undertow.version>
<joda-time.version>2.7</joda-time.version>
<nekohtml.version>1.9.21</nekohtml.version>
<activemq.version>5.11.1</activemq.version>
<antlr2.version>2.7.7</antlr2.version>
<commons-dbcp2.version>2.0.1</commons-dbcp2.version>
<tomcat.version>8.0.21</tomcat.version>
<aspectj.version>1.8.5</aspectj.version>
<groovy.version>2.4.3</groovy.version>
<crashub.version>1.3.1</crashub.version>
<jetty.version>9.2.9.v20150224</jetty.version>
<elasticsearch.version>1.4.4</elasticsearch.version>
<flyway.version>3.2.1</flyway.version>
<freemarker.version>2.3.22</freemarker.version>
<jdom2.version>2.0.6</jdom2.version>
<liquibase.version>3.3.2</liquibase.version>
<mockito.version>1.10.19</mockito.version>
mongodb.version>2.13.0</mongodb.version>
<slf4j.version>1.7.11</slf4j.version>
<spring-cloud-connectors.version>1.1.1.RELEASE</spring-cloud-connectors.version>
<spring-security.version>4.0.1.RELEASE</spring-security.version>
<jedis.version>2.6.2</jedis.version>
<spring-ws.version>2.2.1.RELEASE</spring-ws.version>
{code}",2.0,True,2.0,1,-50.0,0,0,True
"XD-2903","Story","Sprint 47","2015-04-03T12:00:39.000+0000","sabby","","Upgrade to latest SIK release","As a developer, I'd like to upgrade to SI Kafka release, so I can synchronize with latest improvements and bug fixes.  ",1.0,0.9928337934219472,0.8594281826234966,0.0,True,True,True,1428062439000.0,1429874731000.0,1429878331000.0,1429886912000.0,1429887812000.0,1428062439000.0,-1.0,1428062439000.0,1428066039000.0,1429631216000.0,1429634816000.0,1429874731000.0,1429878331000.0,"sabby",1428066039000.0,1428062439000.0,45,1428062439000.0,-1.0,"Upgrade to 1.1.2 SIK release","As a developer, I'd like to upgrade to SI Kafka release, so I can synchronize with latest improvements and bug fixes.  ",1.0,False,1.0,0,0.0,-1,0,True
"XD-2902","Story","Sprint 47","2015-04-03T10:50:20.000+0000","sabby","","Upgrade to latest SI release ","As a developer, I'd like to upgrade to SI milestone/GA release, so I can synchronize with JMX improvements.  

This is dependent on SI Milestone and GA release timelines.",1.0,0.9857398137429452,0.8601465953484357,0.0,True,True,True,1428058220000.0,1429862598000.0,1429866198000.0,1429887801000.0,1429888701000.0,1428058220000.0,-1.0,1428058220000.0,1428061820000.0,1429632702000.0,1429636302000.0,1429862598000.0,1429866198000.0,"sabby",1428061820000.0,1428058220000.0,45,1428058220000.0,-1.0,"Upgrade to latest SI release ","As a developer, I'd like to upgrade to SI milestone/GA release, so I can synchronize with JMX improvements.  

This is dependent on SI Milestone and GA release timelines.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2899","Story","Sprint 47","2015-04-03T10:09:13.000+0000","sabby","","Add HA support for Rabbit","As a developer, I would like to connect to the broker that hosts the Rabbit queue, so I can connect to a Rabbit cluster that's setup for HA/FT. Perhaps consider having this feature natively supported in spring amqp itself.",3.0,0.6404123793207245,0.5559828965816236,0.0,True,True,True,1428055753000.0,1430442853000.0,1430446453000.0,1431782295000.0,1431783195000.0,1428055753000.0,1430128141000.0,1428055753000.0,1428059353000.0,1430128147000.0,1430131747000.0,1430442853000.0,1430446453000.0,"sabby",1428059353000.0,1428055753000.0,45,1430128141000.0,1430128141000.0,"Improve HA support for Rabbit","As a developer, I would like to connect to the broker that hosts the Rabbit queue, so I can connect to a Rabbit cluster that's setup for HA/FT. Perhaps consider having this feature natively supported in spring amqp itself.",5.0,True,5.0,1,-40.0,-1,0,True
"XD-2896","Story","Sprint 47","2015-04-02T09:25:52.000+0000","sabby","","Add support to capture errors/stacktrace via DLQ","As a user, I'd like to have the configuration option to use an alternative DLQ, so I can publish the message this time with additional headers, including one that contains the exception (and stack trace).
",3.0,0.3032963471422587,0.3032963471422587,0.0,True,True,True,1427966752000.0,1428462731000.0,1428466331000.0,1429601147000.0,1429602047000.0,1427966752000.0,-1.0,1427966752000.0,1427970352000.0,1428462731000.0,1428466331000.0,1428462731000.0,1428466331000.0,"sabby",1427970352000.0,1427966752000.0,45,1427966752000.0,-1.0,"Add support to capture errors/stacktrace via DLQ","As a user, I'd like to have the configuration option to use an alternative DLQ, so I can publish the message this time with additional headers, including one that contains the exception (and stack trace).
",3.0,False,3.0,0,0.0,0,0,False
"XD-2893","Story","Sprint 50","2015-04-02T02:23:22.000+0000","eric.bottard","eric.bottard","Properly render defaults for ""module info"" that use \n \t etc.","Characters line \t, \n, etc. should be either escaped, or rendered as human readable variants in module info (eg <newline>)",2.0,0.9999972777138156,0.0,0.9999916237348176,True,True,True,1427941402000.0,1432716787000.0,1432716800000.0,1432715900000.0,1432716800000.0,1427941402000.0,-1.0,1432716760000.0,1432716800000.0,1427941402000.0,1427945002000.0,1432716787000.0,1432716800000.0,"eric.bottard",1432716800000.0,1427941402000.0,45,1427941402000.0,-1.0,"Properly render defaults for ""module info"" that use \n \t etc.","Characters line \t, \n, etc. should be either escaped, or rendered as human readable variants in module info (eg <newline>)",2.0,False,2.0,0,0.0,-1,1,False
"XD-2892","Story","Sprint 47","2015-04-01T15:17:14.000+0000","sabby","","Add support for PHD 3.0 ","As a developer, I'd like to certify Spring XD against PHD 3.0, so I can synchronize with the latest ODP based bits. ",3.0,0.9563036341120306,0.8631084533598117,0.0,True,True,True,1427901434000.0,1429787194000.0,1429790794000.0,1429872460000.0,1429873360000.0,1427901434000.0,-1.0,1427901434000.0,1427905034000.0,1429603420000.0,1429607020000.0,1429787194000.0,1429790794000.0,"sabby",1427905034000.0,1427901434000.0,45,1427901434000.0,-1.0,"Add support for PHD 3.0 ","As a developer, I'd like to certify Spring XD against PHD 3.0, so I can synchronize with the latest ODP based bits. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-2891","Story","Sprint 47","2015-04-01T13:22:48.000+0000","mark.pollack","","Provide an --override option to the module upload command","When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar.

This would be an optional parameter.",3.0,0.7669126934777335,0.0774457453720098,0.0016813845909327,True,True,True,1427894568000.0,1429269769000.0,1429273369000.0,1429686833000.0,1429687733000.0,1427894568000.0,-1.0,1427897583000.0,1427901183000.0,1428033441000.0,1428037041000.0,1429269769000.0,1429273369000.0,"mark.pollack",1427901183000.0,1427894568000.0,45,1427894568000.0,-1.0,"Provide an --override option to the module upload command","When uploading a new version of a module the admin container if there is already an existing module, the behavior should be to delete the existing contents of the module directory and replace it with that of the new upload jar.

This would be an optional parameter.",3.0,False,3.0,0,0.0,-1,0,False
"XD-2887","Story","Sprint 47","2015-03-31T10:21:10.000+0000","eric.bottard","eric.bottard","Have a version of GET /modules that returns full info","Similar to the DetailedModuleDefinitionResource that is returned when querying a single module, but would be returned when listing (provided a ?full flag has been turned on)",3.0,0.7848997853623144,0.0,0.7153133407902295,True,True,True,1427797270000.0,1428316544000.0,1428320144000.0,1428457950000.0,1428458850000.0,1427797270000.0,-1.0,1428270507000.0,1428274107000.0,1427797270000.0,1427800870000.0,1428316544000.0,1428320144000.0,"eric.bottard",1428274107000.0,1427797270000.0,45,1427797270000.0,-1.0,"Have a version of GET /modules that returns full info","Similar to the DetailedModuleDefinitionResource that is returned when querying a single module, but would be returned when listing (provided a ?full flag has been turned on)",3.0,False,3.0,0,0.0,-1,0,False
"XD-2885","Story","Sprint 50","2015-03-31T02:35:48.000+0000","eric.bottard","eric.bottard","Only ship relevant modules files","The current build ships everything that is found in the modules directory, including build artifacts such as build/ or IDEA *.iml files.

Restrict the build to only include config/, lib/ at the moment.",1.0,0.9999145820720896,0.0,0.9999937400572926,True,True,True,1427769348000.0,1432721047000.0,1432721470000.0,1432720570000.0,1432721470000.0,1427769348000.0,-1.0,1432721439000.0,1432721470000.0,1427769348000.0,1427772948000.0,1432721047000.0,1432721470000.0,"eric.bottard",1432721470000.0,1427769348000.0,45,1427769348000.0,-1.0,"Only ship relevant modules files","The current build ships everything that is found in the modules directory, including build artifacts such as build/ or IDEA *.iml files.

Restrict the build to only include config/, lib/ at the moment.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2884","Story","Sprint 46","2015-03-31T01:24:18.000+0000","eric.bottard","eric.bottard","Document dynamic classpath feature","",3.0,-1.0,2.8302754565588097e-05,0.9992818176028982,False,True,True,1427765058000.0,-1.0,-1.0,1428046816000.0,1428047716000.0,1427765058000.0,-1.0,1428047513000.0,1428047716000.0,1427765066000.0,1427768666000.0,-1.0,-1.0,"eric.bottard",1428047716000.0,1427765058000.0,45,1427765058000.0,-1.0,"Document dynamic classpath feature","",3.0,False,3.0,0,0.0,-1,0,False
"XD-2882","Story","Sprint 48","2015-03-30T12:47:53.000+0000","thomas.risberg","","Provide an option for hdfs sink to use ""Syncable"" writes","As a user, I'd like to have an option to have the hdfs sink use ""Syncable"" writes to provide better resiliency in the case of sink/container failures. I'm willing to accept the performance penalty if I choose this option.
",3.0,0.9734255466766196,0.9440540989458492,0.1019106533971569,True,True,True,1427719673000.0,1445365613000.0,1445369213000.0,1445846446000.0,1445847346000.0,1427719673000.0,1443552036000.0,1429567076000.0,1429570676000.0,1444833177000.0,1444836777000.0,1445365613000.0,1445369213000.0,"thomas.risberg",1429570676000.0,1427719673000.0,45,1443552036000.0,1443552036000.0,"Provide an option for hdfs sink to use ""Syncable"" writes","As a user, I'd like to have an option to have the hdfs sink use ""Syncable"" writes to provide better resiliency in the case of sink/container failures. I'm willing to accept the performance penalty if I choose this option.
",5.0,True,5.0,1,-40.0,-1,0,False
"XD-2879","Story","Sprint 46","2015-03-30T08:41:01.000+0000","sabby","","Add support for explicit partition count configuration for Kafka bus","As a developer, I'd like to add support for explicit partition count configuration, so I can use this option to cleverly route the payload to the intended consumer (module).",5.0,0.0533466716018952,0.0533466716018952,0.0,True,True,True,1427704861000.0,1427807622000.0,1427811222000.0,1429630248000.0,1429631148000.0,1427704861000.0,-1.0,1427704861000.0,1427708461000.0,1427807622000.0,1427811222000.0,1427807622000.0,1427811222000.0,"sabby",1427708461000.0,1427704861000.0,45,1427704861000.0,-1.0,"Add support for explicit partition count configuration for Kafka bus","As a developer, I'd like to add support for explicit partition count configuration, so I can use this option to cleverly route the payload to the intended consumer (module).",5.0,False,5.0,0,0.0,-1,0,False
"XD-2876","Improvement","Sprint 68","2015-03-27T07:54:55.000+0000","hillert","grussell","Update Documentation Link","{code}
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.1.1.RELEASE                    eXtreme Data


Started : ContainerServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki
{code}

This should probably be changed to:

Documentation: http://docs.spring.io/spring-xd/docs/current/reference/html/

",1.0,0.9999964522876598,0.9999964522876598,0.9999956443927704,True,True,True,1427442895000.0,1455911844000.0,1455911945000.0,1455911045000.0,1455911945000.0,1427442895000.0,-1.0,1455911821000.0,1455911945000.0,1455911844000.0,1455911945000.0,1455911844000.0,1455911945000.0,"hillert",1455911945000.0,1427442895000.0,45,1427442895000.0,-1.0,"Update Documentation Link","{code}
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.1.1.RELEASE                    eXtreme Data


Started : ContainerServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki
{code}

This should probably be changed to:

Documentation: http://docs.spring.io/spring-xd/docs/current/reference/html/

",1.0,False,1.0,0,0.0,0,3,False
"XD-2872","Bug","Sprint 48","2015-03-26T09:54:14.000+0000","hillert","hillert","Able to bypass authorization checks by appending "".json"" or "".xml""","How to reproduce:

1) Enable security
2) Use a user that has the following role only: ""ROLE_CREATE""
3) Make a normal REST call:

{code}
http://localhost:9393/runtime/containers
{code}

yields the *desired response*:

{code}
    {
       ""timestamp"": ""2015-03-26T16:51:17.010Z"",
       ""status"": 403,
       ""error"": ""Forbidden"",
       ""message"": ""Access is denied"",
       ""path"": ""/runtime/containers""
    }
{code}

Now try:

{code}
http://localhost:9393/runtime/containers.json
{code}

This produces:

{code}


    {
       ""links"":
       [
           {
               ""rel"": ""self"",
               ""href"": ""http://localhost:9393/runtime/containers{?page,size,sort}""
           }
       ],
       ""content"":
       [
           {
               ""containerId"": ""86eea5aa-b18e-41c5-a3f5-42dfa10713c1"",
               ""groups"": """",
               ""deploymentSize"": 0,
               ""deployedModules"":
               [
               ],
               ""messageRates"": null,
               ""attributes"":
               {
                   ""ip"": ""10.0.1.119"",
                   ""host"": ""INTEGRATION.local"",
                   ""groups"": """",
                   ""pid"": ""52686"",
                   ""id"": ""86eea5aa-b18e-41c5-a3f5-42dfa10713c1""
               },
               ""links"":
               [
                   {
                       ""rel"": ""self"",
                       ""href"": ""http://localhost:9393/runtime/containers/86eea5aa-b18e-41c5-a3f5-42dfa10713c1""
                   }
               ]
           }
       ],
       ""page"":
       {
           ""size"": 20,
           ""totalElements"": 1,
           ""totalPages"": 1,
           ""number"": 0
       }
    }
{code}",3.0,0.5112719305187802,0.5112719305187802,1.0494797614013824,True,True,True,1427363654000.0,1429172530000.0,1429176130000.0,1430900746000.0,1430901646000.0,1427363654000.0,-1.0,1431076705000.0,1430901646000.0,1429172530000.0,1429176130000.0,1429172530000.0,1429176130000.0,"hillert",1430901646000.0,1427363654000.0,45,1427363654000.0,-1.0,"Able to bypass authorization checks by appending "".json"" or "".xml""","How to reproduce:

1) Enable security
2) Use a user that has the following role only: ""ROLE_CREATE""
3) Make a normal REST call:

{code}
http://localhost:9393/runtime/containers
{code}

yields the *desired response*:

{code}
    {
       ""timestamp"": ""2015-03-26T16:51:17.010Z"",
       ""status"": 403,
       ""error"": ""Forbidden"",
       ""message"": ""Access is denied"",
       ""path"": ""/runtime/containers""
    }
{code}

Now try:

{code}
http://localhost:9393/runtime/containers.json
{code}

This produces:

{code}


    {
       ""links"":
       [
           {
               ""rel"": ""self"",
               ""href"": ""http://localhost:9393/runtime/containers{?page,size,sort}""
           }
       ],
       ""content"":
       [
           {
               ""containerId"": ""86eea5aa-b18e-41c5-a3f5-42dfa10713c1"",
               ""groups"": """",
               ""deploymentSize"": 0,
               ""deployedModules"":
               [
               ],
               ""messageRates"": null,
               ""attributes"":
               {
                   ""ip"": ""10.0.1.119"",
                   ""host"": ""INTEGRATION.local"",
                   ""groups"": """",
                   ""pid"": ""52686"",
                   ""id"": ""86eea5aa-b18e-41c5-a3f5-42dfa10713c1""
               },
               ""links"":
               [
                   {
                       ""rel"": ""self"",
                       ""href"": ""http://localhost:9393/runtime/containers/86eea5aa-b18e-41c5-a3f5-42dfa10713c1""
                   }
               ]
           }
       ],
       ""page"":
       {
           ""size"": 20,
           ""totalElements"": 1,
           ""totalPages"": 1,
           ""number"": 0
       }
    }
{code}",3.0,False,3.0,0,0.0,0,0,False
"XD-2868","Story","Sprint 46","2015-03-26T05:30:27.000+0000","grussell","","Support Partitioned Batch Jobs with a LocalMessageBus","Initial support for partitioned batch jobs (initially tested with a local bus) had an {{ExecutorChannel}} in the job context to enable multiple partitions to run. Otherwise, with a local bus, only one partition would run at a time.

When further work was done to support other buses, this was removed and the bus was used to control partition concurrency.

The {{LocalMessageBus}} was changed to use an unbounded task executor; this was wrong because now all partitions ran at once.

Further changes to the local bus changed the task executor to be pooled, but with default properties that mean only one thread is used.

Further, the pool configuration is bus-wide so you can't use that configuration to select the concurrency for an individual job.

The bottom line is that the local bus is not suitable for partitioned batch jobs; it was not anticipated that it would be used for this scenario. With 1.0.x too many partitions run (all); with 1.1.x only one thread runs (by default).

In the local bus, we need to use a configurable, dedicated, bounded task executor for each batch job. ",5.0,0.0219114108456291,0.0219114108456291,0.0100181284658451,True,True,True,1427347827000.0,1427362597000.0,1427366197000.0,1428021005000.0,1428021905000.0,1427347827000.0,-1.0,1427354580000.0,1427358180000.0,1427362597000.0,1427366197000.0,1427362597000.0,1427366197000.0,"grussell",1427358180000.0,1427347827000.0,45,1427347827000.0,-1.0,"Support Partitioned Batch Jobs with a LocalMessageBus","Initial support for partitioned batch jobs (initially tested with a local bus) had an {{ExecutorChannel}} in the job context to enable multiple partitions to run. Otherwise, with a local bus, only one partition would run at a time.

When further work was done to support other buses, this was removed and the bus was used to control partition concurrency.

The {{LocalMessageBus}} was changed to use an unbounded task executor; this was wrong because now all partitions ran at once.

Further changes to the local bus changed the task executor to be pooled, but with default properties that mean only one thread is used.

Further, the pool configuration is bus-wide so you can't use that configuration to select the concurrency for an individual job.

The bottom line is that the local bus is not suitable for partitioned batch jobs; it was not anticipated that it would be used for this scenario. With 1.0.x too many partitions run (all); with 1.1.x only one thread runs (by default).

In the local bus, we need to use a configurable, dedicated, bounded task executor for each batch job. ",5.0,False,5.0,0,0.0,-1,0,False
"XD-2865","Bug","Sprint 46","2015-03-25T13:45:35.000+0000","mbogoevici","mbogoevici","Message Bus: Shut down Kafka Consumers completely before unbinding","This causes the following exception to be thrown in the log (without functional adverse effects)

org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'unknown.channel.name'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:43)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$AutoAcknowledgingChannelForwardingMessageListener.doOnMessage(KafkaMessageDrivenChannelAdapter.java:172)
	at org.springframework.integration.kafka.listener.AbstractDecodingMessageListener.onMessage(AbstractDecodingMessageListener.java:50)
	at org.springframework.integration.kafka.listener.QueueingMessageListenerInvoker.run(QueueingMessageListenerInvoker.java:121)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 13 more

",2.0,0.0001623999763781,0.0001623999763781,0.0149703250952254,True,True,True,1427291135000.0,1427291146000.0,1427294746000.0,1427357969000.0,1427358869000.0,1427291135000.0,-1.0,1427292149000.0,1427295749000.0,1427291146000.0,1427294746000.0,1427291146000.0,1427294746000.0,"mbogoevici",1427295749000.0,1427291135000.0,45,1427291135000.0,-1.0,"Message Bus: Shut down Kafka Consumers completely before unbinding","This causes the following exception to be thrown in the log (without functional adverse effects)

org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'unknown.channel.name'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:101)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter.access$300(KafkaMessageDrivenChannelAdapter.java:43)
	at org.springframework.integration.kafka.inbound.KafkaMessageDrivenChannelAdapter$AutoAcknowledgingChannelForwardingMessageListener.doOnMessage(KafkaMessageDrivenChannelAdapter.java:172)
	at org.springframework.integration.kafka.listener.AbstractDecodingMessageListener.onMessage(AbstractDecodingMessageListener.java:50)
	at org.springframework.integration.kafka.listener.QueueingMessageListenerInvoker.run(QueueingMessageListenerInvoker.java:121)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 13 more

",2.0,False,2.0,0,0.0,-1,0,False
"XD-2864","Story","Sprint 46","2015-03-25T12:52:04.000+0000","dturanski","dturanski","JavaConfiguredModule should throw an exception when no @Configuration class is present ","I had a custom module with a typo:
base_packages=base_packages=com.acme.config

The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. ",2.0,0.0821640387675394,0.0820917112686243,0.9997685520034716,True,True,True,1427287924000.0,1427293604000.0,1427297204000.0,1427356154000.0,1427357054000.0,1427287924000.0,-1.0,1427357038000.0,1427357054000.0,1427293599000.0,1427297199000.0,1427293604000.0,1427297204000.0,"dturanski",1427357054000.0,1427287924000.0,45,1427287924000.0,-1.0,"JavaConfiguredModule should throw an exception when no @Configuration class is present ","I had a custom module with a typo:
base_packages=base_packages=com.acme.config

The module deploys without error but the stream hangs since the channels, etc. are not found in the stream plugin. Very hard to debug. ",2.0,False,2.0,0,0.0,-1,0,False
"XD-2861","Bug","","2015-03-25T00:32:04.000+0000","iperumal","iperumal","Admin leader election issue when using different management port","When the admin is started with the different management port (default is the same as that of admin http port), then the leadership is requested when the management context is setup. The leadership election should happen only using the Admin server application context.

With this, the following exception is thrown when deployment requests are handled:

2015-03-24 21:48:26,340 1.2.0.SNAP ERROR DeploymentSupervisor-0 queue.DistributedQueue - Exception processing queue item: queue-0000000004
org.springframework.xd.dirt.server.admin.deployment.DeploymentException: testStream
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:164)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:83)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployAll(AbstractInstancePersistingDeployer.java:109)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:117)
	at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.processDeploymentMessage(DeploymentMessageConsumer.java:115)
	at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:73)
	at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:43)
	at org.apache.curator.framework.recipes.queue.DistributedQueue.processMessageBytes(DistributedQueue.java:678)
	at org.apache.curator.framework.recipes.queue.DistributedQueue.processNormally(DistributedQueue.java:712)
	at org.apache.curator.framework.recipes.queue.DistributedQueue.access$300(DistributedQueue.java:65)
	at org.apache.curator.framework.recipes.queue.DistributedQueue$5.run(DistributedQueue.java:629)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Module deployment request path cache shouldn't be null.
	at org.springframework.util.Assert.notNull(Assert.java:112)
	at org.springframework.xd.dirt.server.admin.deployment.zk.ZKDeploymentHandler.undeploy(ZKDeploymentHandler.java:81)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:161)
	... 17 more",1.0,0.0013530879500156,0.0,-1.0,True,False,True,1427243524000.0,1427245454000.0,1427249054000.0,1428668991000.0,1428669891000.0,1427243524000.0,-1.0,-1.0,-1.0,1427243524000.0,1427247124000.0,1427245454000.0,1427249054000.0,"iperumal",-1.0,-1.0,0,1427243524000.0,-1.0,"Admin leader election issue when using different management port","When the admin is started with the different management port (default is the same as that of admin http port), then the leadership is requested when the management context is setup. The leadership election should happen only using the Admin server application context.

With this, the following exception is thrown when deployment requests are handled:

2015-03-24 21:48:26,340 1.2.0.SNAP ERROR DeploymentSupervisor-0 queue.DistributedQueue - Exception processing queue item: queue-0000000004
org.springframework.xd.dirt.server.admin.deployment.DeploymentException: testStream
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:164)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:83)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployAll(AbstractInstancePersistingDeployer.java:109)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:117)
	at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.processDeploymentMessage(DeploymentMessageConsumer.java:115)
	at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:73)
	at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:43)
	at org.apache.curator.framework.recipes.queue.DistributedQueue.processMessageBytes(DistributedQueue.java:678)
	at org.apache.curator.framework.recipes.queue.DistributedQueue.processNormally(DistributedQueue.java:712)
	at org.apache.curator.framework.recipes.queue.DistributedQueue.access$300(DistributedQueue.java:65)
	at org.apache.curator.framework.recipes.queue.DistributedQueue$5.run(DistributedQueue.java:629)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Module deployment request path cache shouldn't be null.
	at org.springframework.util.Assert.notNull(Assert.java:112)
	at org.springframework.xd.dirt.server.admin.deployment.zk.ZKDeploymentHandler.undeploy(ZKDeploymentHandler.java:81)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:161)
	... 17 more",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2860","Improvement","Sprint 46","2015-03-24T18:00:25.000+0000","mark.fisher","eric.bottard","remove ModuleDefinitions.dummy()","This method should be replaced with a utility method in a test support class so that it is only available in a testing context.
",2.0,0.6528591432963018,0.0,0.307124063954665,True,True,True,1427220025000.0,1427697440000.0,1427701040000.0,1427950393000.0,1427951293000.0,1427220025000.0,-1.0,1427444615000.0,1427448215000.0,1427220025000.0,1427223625000.0,1427697440000.0,1427701040000.0,"mark.fisher",1427448215000.0,1427220025000.0,45,1427220025000.0,-1.0,"remove ModuleDefinitions.dummy()","This method should be replaced with a utility method in a test support class so that it is only available in a testing context.
",2.0,False,2.0,0,0.0,-1,0,False
"XD-2859","Story","Sprint 46","2015-03-24T15:50:00.000+0000","hillert","hillert","UI: Deploy Stream - Return key does not submit form","*http://localhost:9393/admin-ui/#/streams/definitions/test/deploy*",1.0,0.0075678629286272,0.0075678629286272,0.0075601159821184,True,True,True,1427212200000.0,1427270813000.0,1427274413000.0,1434956287000.0,1434957187000.0,1427212200000.0,-1.0,1427270753000.0,1427274353000.0,1427270813000.0,1427274413000.0,1427270813000.0,1427274413000.0,"hillert",1427274353000.0,1427212200000.0,45,1427212200000.0,-1.0,"UI: Deploy Stream - Return key does not submit form","*http://localhost:9393/admin-ui/#/streams/definitions/test/deploy*",1.0,False,1.0,0,0.0,0,0,False
"XD-2858","Story","Sprint 46","2015-03-24T10:31:22.000+0000","sabby","","Add dynamic classpath support for modules","As a developer, I'd like to add support for dynamic classpath for modules, so we can have the flexibility to load the right dependencies either based on module options (0) or via other properties such as including the dependencies from a specific location (1). 

(0):
{code:xml}
lib/*.jar:lib/${distro}/*.jar
{code}

(1):
{code:xml}
${xd.home}/lib/hadoop/${distro}/*.jar
{code}

*Example:*
{code:xml}
http | hdfs --distro=PHD22

http | myCustomModule --classpath=/my/funky/dir

http | jpa --provider=eclipse

jpa:
config/
lib/something-that-is-common.jar
lib/eclipse/eclipse-link-3.2.jar
lib/hibernate/hibernate-core-5.0.jar

module.classpath = lib/*.jar:lib/${provider}/*.jar
{code}",5.0,0.1928749703084759,0.1928749703084759,0.0,True,True,True,1427193082000.0,1427357917000.0,1427361517000.0,1428046803000.0,1428047703000.0,1427193082000.0,-1.0,1427193082000.0,1427196682000.0,1427357917000.0,1427361517000.0,1427357917000.0,1427361517000.0,"sabby",1427196682000.0,1427193082000.0,45,1427193082000.0,-1.0,"Add dynamic classpath support for modules","As a developer, I'd like to add support for dynamic classpath for modules, so we can have the flexibility to load the right dependencies either based on module options (0) or via other properties such as including the dependencies from a specific location (1). 

(0):
{code}
/lib/*.jar:lib/${distro}/*.jar
{code}

(1):
{code}
${xd.home}/lib/hadoop/${distro}/*.jar
{code}

*Example:*
{code}
http | hdfs --distro=PHD22

http | myCustomModule --classpath=/my/funky/dir

http | jpa --provider=eclipse

jpa:
/config/
/lib/something-that-is-common.jar
    /eclipse/eclipse-link-3.2.jar
    /hibernate/hibernate-core-5.0.jar

module.classpath = /lib/*.jar:/lib/${provider}/*.jar
{code}",5.0,False,5.0,0,0.0,0,0,True
"XD-2855","Improvement","Sprint 47","2015-03-24T05:44:44.000+0000","kdowbecki","","Basic security makes xd-shell throw 403 Forbidden error","After enabling admin endpoint security in servers.yml using basic authentication and single user
{code}
spring:
  profiles: admin
security:
  basic:
    enabled: true # false to disable security settings (default)
    realm: SpringXD
  user: # valid only if security.basic.enabled=true
    name: myadmin
    password: myadmin
{code}

Spring XD UI is secured however xd-shell commands are resulting in a 403 error:

{code}
server-unknown:>admin config server --uri http://localhost:9393 --username myadmin --password myadmin
Successfully targeted http://localhost:9393
xd:>admin config info
  -------------  -------------------------------------------
  Credentials    [username='myadmin, password=****']
  Result         Successfully targeted http://localhost:9393
  Target         http://localhost:9393
  Timezone used  Greenwich Mean Time (UTC 0:00)
  -------------  -------------------------------------------
xd:>stream list
Command failed org.springframework.web.client.HttpClientErrorException: 403 Forbidden
xd:>stream create --name ""t1"" --definition ""time | log""
Command failed org.springframework.web.client.HttpClientErrorException: 403 Forbidden
{code}

This can be fixed by adding configuration explained in ""File based authentication"" docs section:

{code}
xd:
  security:
    authentication:
      file:
        enabled: true
        users:
            myadmin: myadmin, ROLE_VIEW, ROLE_ADMIN, ROLE_CREATE
{code}

Following is the problem:
# Configuration explained in ""Single user authentication"" chapter should work out of the box without additional role setup
# Docs should be more clear on authorization",2.0,0.8897944675070775,0.7570392256300575,0.0096221625778656,True,True,True,1427175884000.0,1428507594000.0,1428511194000.0,1428671633000.0,1428672533000.0,1427175884000.0,-1.0,1427190285000.0,1427193885000.0,1428308906000.0,1428312506000.0,1428507594000.0,1428511194000.0,"kdowbecki",1427193885000.0,1427175884000.0,45,1427175884000.0,-1.0,"Basic security makes xd-shell throw 403 Forbidden error","After enabling admin endpoint security in servers.yml using basic authentication and single user
{code}
spring:
  profiles: admin
security:
  basic:
    enabled: true # false to disable security settings (default)
    realm: SpringXD
  user: # valid only if security.basic.enabled=true
    name: myadmin
    password: myadmin
{code}

Spring XD UI is secured however xd-shell commands are resulting in a 403 error:

{code}
server-unknown:>admin config server --uri http://localhost:9393 --username myadmin --password myadmin
Successfully targeted http://localhost:9393
xd:>admin config info
  -------------  -------------------------------------------
  Credentials    [username='myadmin, password=****']
  Result         Successfully targeted http://localhost:9393
  Target         http://localhost:9393
  Timezone used  Greenwich Mean Time (UTC 0:00)
  -------------  -------------------------------------------
xd:>stream list
Command failed org.springframework.web.client.HttpClientErrorException: 403 Forbidden
xd:>stream create --name ""t1"" --definition ""time | log""
Command failed org.springframework.web.client.HttpClientErrorException: 403 Forbidden
{code}

This can be fixed by adding configuration explained in ""File based authentication"" docs section:

{code}
xd:
  security:
    authentication:
      file:
        enabled: true
        users:
            myadmin: myadmin, ROLE_VIEW, ROLE_ADMIN, ROLE_CREATE
{code}

Following is the problem:
# Configuration explained in ""Single user authentication"" chapter should work out of the box without additional role setup
# Docs should be more clear on authorization",2.0,False,2.0,0,0.0,0,2,False
"XD-2854","Bug","Sprint 46","2015-03-23T15:26:24.000+0000","iperumal","iperumal","Launching XD admin fails with ZK holding existing stream data","Following exception is thrown when starting XD admin withe ZK holding the stream data:

2015-03-23 17:21:13,831 1.2.0.SNAP ERROR LeaderSelector-0 leader.LeaderSelector - The leader threw an exception
java.lang.NullPointerException
at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:822)
at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:896)
at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperStreamDefinitionRepository.findOne(ZooKeeperStreamDefinitionRepository.java:157)
at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperStreamDefinitionRepository.findOne(ZooKeeperStreamDefinitionRepository.java:56)
at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:654)
at org.springframework.xd.dirt.stream.dsl.ChannelNode.resolve(ChannelNode.java:144)
at org.springframework.xd.dirt.stream.dsl.SourceChannelNode.resolve(SourceChannelNode.java:54)
at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:125)
at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:110)
at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:121)
at org.springframework.xd.dirt.stream.StreamFactory.createStream(StreamFactory.java:84)
at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentLoader.loadStream(DeploymentLoader.java:101)
at org.springframework.xd.dirt.server.admin.deployment.zk.DefaultDeploymentStateRecalculator.recalculateStreamStates(DefaultDeploymentStateRecalculator.java:96)
at org.springframework.xd.dirt.server.admin.deployment.zk.DefaultDeploymentStateRecalculator.onSupervisorElected(DefaultDeploymentStateRecalculator.java:182)
at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:468)
at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)
at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)
at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)
at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)
at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)
at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)",5.0,0.0888768951616403,0.0,0.0885932901366223,True,True,True,1427124384000.0,1427185807000.0,1427189407000.0,1427814586000.0,1427815486000.0,1427124384000.0,1427701830000.0,1427185611000.0,1427189211000.0,1427124384000.0,1427127984000.0,1427185807000.0,1427189407000.0,"iperumal",1427189211000.0,1427124384000.0,45,1427701830000.0,1427701830000.0,"Launching XD admin fails with ZK holding existing stream data","Following exception is thrown when starting XD admin withe ZK holding the stream data:

2015-03-23 17:21:13,831 1.2.0.SNAP ERROR LeaderSelector-0 leader.LeaderSelector - The leader threw an exception
java.lang.NullPointerException
at com.fasterxml.jackson.core.JsonFactory.createParser(JsonFactory.java:822)
at com.fasterxml.jackson.databind.ObjectReader.readValue(ObjectReader.java:896)
at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperStreamDefinitionRepository.findOne(ZooKeeperStreamDefinitionRepository.java:157)
at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperStreamDefinitionRepository.findOne(ZooKeeperStreamDefinitionRepository.java:56)
at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:654)
at org.springframework.xd.dirt.stream.dsl.ChannelNode.resolve(ChannelNode.java:144)
at org.springframework.xd.dirt.stream.dsl.SourceChannelNode.resolve(SourceChannelNode.java:54)
at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:125)
at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:110)
at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:121)
at org.springframework.xd.dirt.stream.StreamFactory.createStream(StreamFactory.java:84)
at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentLoader.loadStream(DeploymentLoader.java:101)
at org.springframework.xd.dirt.server.admin.deployment.zk.DefaultDeploymentStateRecalculator.recalculateStreamStates(DefaultDeploymentStateRecalculator.java:96)
at org.springframework.xd.dirt.server.admin.deployment.zk.DefaultDeploymentStateRecalculator.onSupervisorElected(DefaultDeploymentStateRecalculator.java:182)
at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:468)
at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)
at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)
at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)
at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)
at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)
at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)",1.0,True,1.0,1,400.0,0,0,False
"XD-2853","Bug","Sprint 46","2015-03-23T14:45:50.000+0000","iperumal","iperumal","XD admin ZK distributed queue consumer initialization issue","The ZK distributed queue consumer is initialized even before the module, stream, job deployment requests path cache are started. This could lead to issue when the consumer start processing the requests before the cache are initialized.

On such scenario, the following exception could be thrown:

2015-03-23 21:00:25,919 1.2.0.SNAP ERROR DeploymentSupervisor-0 queue.DistributedQueue - Exception processing queue item: queue-0000000002
org.springframework.xd.dirt.server.admin.deployment.DeploymentException: dataSender
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:164)
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:83)
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployAll(AbstractInstancePersistingDeployer.java:109)
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:117)
        at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.processDeploymentMessage(DeploymentMessageConsumer.java:115)
        at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:70)
        at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:43)
        at org.apache.curator.framework.recipes.queue.DistributedQueue.processMessageBytes(DistributedQueue.java:678)
        at org.apache.curator.framework.recipes.queue.DistributedQueue.processNormally(DistributedQueue.java:712)
        at org.apache.curator.framework.recipes.queue.DistributedQueue.access$300(DistributedQueue.java:65)
        at org.apache.curator.framework.recipes.queue.DistributedQueue$5.run(DistributedQueue.java:629)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Module deployment request path cache shouldn't be null.
        at org.springframework.util.Assert.notNull(Assert.java:112)
        at org.springframework.xd.dirt.server.admin.deployment.zk.ZKDeploymentHandler.undeploy(ZKDeploymentHandler.java:81)
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:161)",1.0,6.576946159474503e-05,0.0,0.0015784670782738,True,True,True,1427121950000.0,1427121958000.0,1427125558000.0,1427242687000.0,1427243587000.0,1427121950000.0,-1.0,1427122142000.0,1427125742000.0,1427121950000.0,1427125550000.0,1427121958000.0,1427125558000.0,"iperumal",1427125742000.0,1427121950000.0,45,1427121950000.0,-1.0,"XD admin ZK distributed queue consumer initialization issue","The ZK distributed queue consumer is initialized even before the module, stream, job deployment requests path cache are started. This could lead to issue when the consumer start processing the requests before the cache are initialized.

On such scenario, the following exception could be thrown:

2015-03-23 21:00:25,919 1.2.0.SNAP ERROR DeploymentSupervisor-0 queue.DistributedQueue - Exception processing queue item: queue-0000000002
org.springframework.xd.dirt.server.admin.deployment.DeploymentException: dataSender
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:164)
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:83)
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployAll(AbstractInstancePersistingDeployer.java:109)
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.deleteAll(AbstractInstancePersistingDeployer.java:117)
        at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.processDeploymentMessage(DeploymentMessageConsumer.java:115)
        at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:70)
        at org.springframework.xd.dirt.server.admin.deployment.zk.DeploymentMessageConsumer.consumeMessage(DeploymentMessageConsumer.java:43)
        at org.apache.curator.framework.recipes.queue.DistributedQueue.processMessageBytes(DistributedQueue.java:678)
        at org.apache.curator.framework.recipes.queue.DistributedQueue.processNormally(DistributedQueue.java:712)
        at org.apache.curator.framework.recipes.queue.DistributedQueue.access$300(DistributedQueue.java:65)
        at org.apache.curator.framework.recipes.queue.DistributedQueue$5.run(DistributedQueue.java:629)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Module deployment request path cache shouldn't be null.
        at org.springframework.util.Assert.notNull(Assert.java:112)
        at org.springframework.xd.dirt.server.admin.deployment.zk.ZKDeploymentHandler.undeploy(ZKDeploymentHandler.java:81)
        at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeployResource(AbstractInstancePersistingDeployer.java:161)",1.0,False,1.0,0,0.0,0,0,False
"XD-2852","Story","Sprint 46","2015-03-23T13:12:59.000+0000","sabby","","Create a gpload batch job","As a developer, I'd like to create a _gpload_ tasklet, so I can ingest data from various sources into GPDB in an efficient manner.",5.0,0.0088268907475618,0.0088268907475618,0.0,True,True,True,1427116379000.0,1427129957000.0,1427133557000.0,1428653733000.0,1428654633000.0,1427116379000.0,-1.0,1427116379000.0,1427119979000.0,1427129957000.0,1427133557000.0,1427129957000.0,1427133557000.0,"sabby",1427119979000.0,1427116379000.0,45,1427116379000.0,-1.0,"Create a gpload batch job","As a developer, I'd like to create a _gpload_ tasklet, so I can ingest data from various sources into GPDB in an efficient manner.",5.0,False,5.0,0,0.0,-1,0,False
"XD-2850","Story","Sprint 47","2015-03-23T10:35:57.000+0000","sabby","","Create a File source to efficiently read files","As a developer, I'd like to use an efficient approach to read files, so I don't have to read line-by-line and keep it in-memory in order to consume/write the file content. 

Would the _tasklet_ approach be better as opposed to transmitting data via message bus (as streams)? ",5.0,0.9097341747152344,0.9097327255796852,0.0,True,True,True,1427106957000.0,1431501397000.0,1431504997000.0,1431936523000.0,1431937423000.0,1427106957000.0,1428315105000.0,1427106957000.0,1427110557000.0,1431501390000.0,1431504990000.0,1431501397000.0,1431504997000.0,"sabby",1427110557000.0,1427106957000.0,45,1428315105000.0,1428315105000.0,"Create a File source to efficiently read files","As a developer, I'd like to use an efficient approach to read files, so I don't have to read line-by-line and keep it in-memory in order to consume/write the file content. 

Would the _tasklet_ approach be better as opposed to transmitting data via message bus (as streams)? ",8.0,True,8.0,1,-37.5,-1,0,False
"XD-2845","Story","Sprint 46","2015-03-21T12:32:15.000+0000","sabby","","Add support for admin-ui and Flo integration ","As a developer, I'd like to setup UI infrastructure, so I can integrate admin_ui and Flo.",5.0,0.2120178112581196,0.983037170627384,0.0,True,True,True,1426941135000.0,1427273245000.0,1427276845000.0,1428506660000.0,1428507560000.0,1426941135000.0,-1.0,1426941135000.0,1426944735000.0,1428480989000.0,1428484589000.0,1427273245000.0,1427276845000.0,"sabby",1426944735000.0,1426941135000.0,45,1426941135000.0,-1.0,"Add support for admin-ui and Flo integration ","As a developer, I'd like to setup UI infrastructure, so I can integrate admin_ui and Flo.",5.0,False,5.0,0,0.0,-1,0,False
"XD-2844","Story","Sprint 46","2015-03-21T12:17:52.000+0000","sabby","jvalkeal","Create a POC for gpfdist sink","As a user, I'd like to have the OOTB _gpfdist_ sink module, so I can use this module to do ultra fast data movement from various sources into GPDB/HAWQ.",8.0,0.1685323264836771,0.0,0.0,True,True,True,1426940272000.0,1427126405000.0,1427130005000.0,1428043807000.0,1428044707000.0,1426940272000.0,-1.0,1426940272000.0,1426943872000.0,1426940272000.0,1426943872000.0,1427126405000.0,1427130005000.0,"sabby",1426943872000.0,1426940272000.0,45,1426940272000.0,-1.0,"Create a POC for gpfdist sink","As a user, I'd like to have the OOTB _gpfdist_ sink module, so I can use this module to do ultra fast data movement from various sources into GPDB/HAWQ.",8.0,False,8.0,0,0.0,0,1,False
"XD-2842","Bug","Sprint 46","2015-03-20T21:39:33.000+0000","mbogoevici","mbogoevici","Kafka source should not try to decode payloads as Strings","Currently, the Kafka source uses a StringDecoder by default - which is an invalid assumption if the payload is not the result of String conversion.

 ",2.0,0.0002907849613603,0.0,0.7261627447571788,True,True,True,1426887573000.0,1426887665000.0,1426891265000.0,1427203058000.0,1427203958000.0,1426887573000.0,-1.0,1427117320000.0,1427120920000.0,1426887573000.0,1426891173000.0,1426887665000.0,1426891265000.0,"mbogoevici",1427120920000.0,1426887573000.0,45,1426887573000.0,-1.0,"Kafka source should not try to decode payloads as Strings","Currently, the Kafka source uses a StringDecoder by default - which is an invalid assumption if the payload is not the result of String conversion.

 ",2.0,False,2.0,0,0.0,-1,0,False
"XD-2838","Story","Sprint 47","2015-03-20T15:44:23.000+0000","sabby","","Update all the module documentation to include ""shortDescription""","As a developer, I'd like to update all the module docs to also include _shortDescription_ so that it's available for users to learn more about the module.",2.0,0.956199378827148,0.8061014194883762,0.0,True,True,True,1426866263000.0,1428577399000.0,1428580999000.0,1428654881000.0,1428655781000.0,1426866263000.0,1427900764000.0,1426866263000.0,1426869863000.0,1428308796000.0,1428312396000.0,1428577399000.0,1428580999000.0,"sabby",1426869863000.0,1426866263000.0,45,1428308116000.0,1427900764000.0,"Update all the module documentation to include ""shortDescription""","As a developer, I'd like to update all the module docs to also include _shortDescription_ so that it's available for users to learn more about the module.",2.0,True,2.0,2,0.0,-1,0,False
"XD-2837","Bug","Sprint 45","2015-03-20T11:29:00.000+0000","grenfro","iperumal","XD-Admin fails to start","When starting xd-admin getting the following exception:
{noformat}
2015-03-20 14:25:53,904 1.2.0.SNAP  WARN main annotation.AnnotationConfigApplicationContext - Exception encountered during context initialization - cancelling refresh attempt
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:89)
	at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:73)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)
	... 22 more
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.springframework.xd.dirt.module.ExtendedResource.<clinit>(ExtendedResource.java:47)
	at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123)
	at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:70)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)
	... 25 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 32 more
2015-03-20 14:25:53,911 1.2.0.SNAP ERROR main boot.SpringApplication - Application startup failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:89)
	at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:73)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)
	... 22 more
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.springframework.xd.dirt.module.ExtendedResource.<clinit>(ExtendedResource.java:47)
	at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123)
	at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:70)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)
	... 25 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 32 more
2015-03-20 14:25:53,915 1.2.0.SNAP ERROR main server.AdminServerApplication - Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
{noformat}

Reproduced Locally (mac) and on EC2.
xd-singlenode works fine.
Commit: 4673b5ab97",3.0,0.0208945923315093,0.0208945923315093,0.0207465970829357,True,True,True,1426850940000.0,1426856305000.0,1426859905000.0,1427106805000.0,1427107705000.0,1426850940000.0,-1.0,1426856267000.0,1426859867000.0,1426856305000.0,1426859905000.0,1426856305000.0,1426859905000.0,"grenfro",1426859867000.0,1426850940000.0,45,1426850940000.0,-1.0,"XD-Admin fails to start","When starting xd-admin getting the following exception:
{noformat}
2015-03-20 14:25:53,904 1.2.0.SNAP  WARN main annotation.AnnotationConfigApplicationContext - Exception encountered during context initialization - cancelling refresh attempt
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:89)
	at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:73)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)
	... 22 more
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.springframework.xd.dirt.module.ExtendedResource.<clinit>(ExtendedResource.java:47)
	at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123)
	at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:70)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)
	... 25 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 32 more
2015-03-20 14:25:53,911 1.2.0.SNAP ERROR main boot.SpringApplication - Application startup failed
org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:313)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:122)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:382)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:157)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:648)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:140)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1131)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:1034)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:504)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:303)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:230)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:299)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:762)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:757)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:480)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:89)
	at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:73)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1566)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:476)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:299)
	... 22 more
Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.springframework.xd.dirt.module.ExtendedResource.<clinit>(ExtendedResource.java:47)
	at org.springframework.xd.dirt.module.WritableResourceModuleRegistry.afterPropertiesSet(WritableResourceModuleRegistry.java:123)
	at org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean.afterPropertiesSet(CustomModuleRegistryFactoryBean.java:70)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)
	... 25 more
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 32 more
2015-03-20 14:25:53,915 1.2.0.SNAP ERROR main server.AdminServerApplication - Error creating bean with name 'moduleRegistry' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Cannot create inner bean 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' of type [org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean] while setting constructor argument with key [1]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'org.springframework.xd.dirt.module.CustomModuleRegistryFactoryBean#175e137b' defined in class path resource [META-INF/spring-xd/internal/repositories.xml]: Invocation of init method failed; nested exception is java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
{noformat}

Reproduced Locally (mac) and on EC2.
xd-singlenode works fine.
Commit: 4673b5ab97",3.0,False,3.0,0,0.0,-1,1,False
"XD-2834","Story","Sprint 53","2015-03-20T06:42:22.000+0000","eric.bottard","mark.pollack","Make doc generation part of the standard build","Following the recent move of the doco to the main repo, it makes sense to have the doc generation be part of the ""main"" build, at an early stage, as an incentive for developers to push doc changes as soon as they change the code.

",2.0,0.9142433573860264,0.9142442754511052,0.9142452853226916,True,True,True,1426833742000.0,1436792114000.0,1436795714000.0,1437725316000.0,1437726216000.0,1426833742000.0,1436947938000.0,1436792135000.0,1436795735000.0,1436792124000.0,1436795724000.0,1436792114000.0,1436795714000.0,"eric.bottard",1436795735000.0,1426833742000.0,45,1437550717000.0,1436947938000.0,"Make doc generation part of the standard build","Following the recent move of the doco to the main repo, it makes sense to have the doc generation be part of the ""main"" build, at an early stage, as an incentive for developers to push doc changes as soon as they change the code.

",3.0,True,3.0,2,-33.33333333333333,0,0,False
"XD-2833","Story","Sprint 45","2015-03-20T06:40:12.000+0000","eric.bottard","eric.bottard","Document MongoDB source","",1.0,0.0087241460865319,0.0087241460865319,0.0086840920708307,True,True,True,1426833612000.0,1426838186000.0,1426841786000.0,1427357004000.0,1427357904000.0,1426833612000.0,-1.0,1426838165000.0,1426841765000.0,1426838186000.0,1426841786000.0,1426838186000.0,1426841786000.0,"eric.bottard",1426841765000.0,1426833612000.0,45,1426833612000.0,-1.0,"Document MongoDB source","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2832","Story","Sprint 45","2015-03-19T09:20:47.000+0000","sabby","dturanski","Add support to create custom jobs using Java Config","As a developer, I'd like to create a custom job module using Java Config so that I don't have to deal with XML configurations. While deploying/launching the following job, I get the error attached below.

{code:xml}
job create --name CDK_Global --definition ""customBatchJob"" --deploy
module upload --type job --name customBatchJob --file /Users/mminella/Documents/IntelliJWorkspace/CustomBatchModule/build/libs/CustomBatchModule-1.1.0.RELEASE.jar
job launch --name CDK_Global
{code}

*Error:*
I'm getting an exception that the job doesn't exist asking if it's deployed",5.0,0.0781000956327701,0.0,0.0,True,True,True,1426756847000.0,1426765912000.0,1426769512000.0,1426872016000.0,1426872916000.0,1426756847000.0,-1.0,1426756847000.0,1426760447000.0,1426756847000.0,1426760447000.0,1426765912000.0,1426769512000.0,"sabby",1426760447000.0,1426756847000.0,45,1426756847000.0,-1.0,"Add support to create custom jobs using Java Config","As a developer, I'd like to create a custom job module using Java Config so that I don't have to deal with XML configurations. While deploying/launching the following job, I get the error attached below.

{code:xml}
job create --name CDK_Global --definition ""customBatchJob"" --deploy
module upload --type job --name customBatchJob --file /Users/mminella/Documents/IntelliJWorkspace/CustomBatchModule/build/libs/CustomBatchModule-1.1.0.RELEASE.jar
job launch --name CDK_Global
{code}

*Error:*
I'm getting an exception that the job doesn't exist asking if it's deployed",5.0,False,5.0,0,0.0,-1,0,False
"XD-2831","Improvement","Sprint 45","2015-03-18T17:43:55.000+0000","iperumal","iperumal","Refactor Deployment related classes in XD DIRT","This is an improvement ticket to address refactoring of XD dirt classes especially XD admin(zk, deployment related) and container.",8.0,0.0001782778361032,0.0001782778361032,0.3434126339559852,True,True,True,1426700635000.0,1426700707000.0,1426704307000.0,1427103599000.0,1427104499000.0,1426700635000.0,-1.0,1426839327000.0,1426842927000.0,1426700707000.0,1426704307000.0,1426700707000.0,1426704307000.0,"iperumal",1426842927000.0,1426700635000.0,45,1426700635000.0,-1.0,"Refactor Deployment related classes in XD DIRT","This is an improvement ticket to address refactoring of XD dirt classes especially XD admin(zk, deployment related) and container.",8.0,False,8.0,0,0.0,-1,0,False
"XD-2830","Story","Sprint 45","2015-03-18T09:43:35.000+0000","mark.pollack","","Create gradle task to check that all projects have descriptions","This keeps coming up as an issue that prevents us from publishing to maven central.",2.0,0.3215998787109245,0.3215998787109245,0.0002065704566344,True,True,True,1426671815000.0,1426841512000.0,1426845112000.0,1427198580000.0,1427199480000.0,1426671815000.0,-1.0,1426671924000.0,1426675524000.0,1426841512000.0,1426845112000.0,1426841512000.0,1426845112000.0,"mark.pollack",1426675524000.0,1426671815000.0,45,1426671815000.0,-1.0,"Create gradle task to check that all projects have descriptions","This keeps coming up as an issue that prevents us from publishing to maven central.",2.0,False,2.0,0,0.0,0,0,False
"XD-2829","Story","Sprint 46","2015-03-18T03:55:24.000+0000","gamars","grussell","Add the Dependencies Required to Use #xpath in Streams","Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the spring-xml.jar to the classpath (otherwise it is missing XPathException class). 

http://stackoverflow.com/questions/29110757/spring-xd-work-with-xml-payload",1.0,0.5868553542512969,0.0,0.1731513960711516,True,True,False,1426650924000.0,1427381198000.0,1427384798000.0,1427894409000.0,1427895309000.0,1426658616000.0,-1.0,1426866391000.0,1426869991000.0,1426650924000.0,1426654524000.0,1427381198000.0,1427384798000.0,"gamars",1426869991000.0,1426658616000.0,45,1426658616000.0,-1.0,"Add the Dependencies Required to Use #xpath in Streams","Thanks to Gary I found this little gem of documentation to be able to use xpath expression in XD. Only hiccup is that I had to also add the spring-xml.jar to the classpath (otherwise it is missing XPathException class). 

http://stackoverflow.com/questions/29110757/spring-xd-work-with-xml-payload",1.0,False,1.0,0,0.0,0,3,False
"XD-2828","Bug","Sprint 45","2015-03-17T18:32:23.000+0000","iperumal","iperumal","XD distributed tests are broken","There are test failures running XD distributed tests. 

It looks like all the test failures are related to NPE on DeploymentProperties format:

java.lang.NullPointerException
	at org.springframework.xd.rest.domain.support.DeploymentPropertiesFormat.formatDeploymentProperties(DeploymentPropertiesFormat.java:72)
	at org.springframework.xd.rest.client.impl.JobTemplate.deploy(JobTemplate.java:71)
	at org.springframework.xd.distributed.test.JobStateTests.testJobStateTransition(JobStateTests.java:83)",1.0,0.0002816712494131,0.0002816712494131,0.6546442223861578,True,True,True,1426617143000.0,1426617164000.0,1426620764000.0,1426690798000.0,1426691698000.0,1426617143000.0,-1.0,1426665950000.0,1426669550000.0,1426617164000.0,1426620764000.0,1426617164000.0,1426620764000.0,"iperumal",1426669550000.0,1426617143000.0,45,1426617143000.0,-1.0,"XD distributed tests are broken","There are test failures running XD distributed tests. 

It looks like all the test failures are related to NPE on DeploymentProperties format:

java.lang.NullPointerException
	at org.springframework.xd.rest.domain.support.DeploymentPropertiesFormat.formatDeploymentProperties(DeploymentPropertiesFormat.java:72)
	at org.springframework.xd.rest.client.impl.JobTemplate.deploy(JobTemplate.java:71)
	at org.springframework.xd.distributed.test.JobStateTests.testJobStateTransition(JobStateTests.java:83)",1.0,False,1.0,0,0.0,-1,0,False
"XD-2825","Story","Sprint 53","2015-03-17T06:58:53.000+0000","dturanski","dturanski","SCS - Verify/Fix AbstractKryoMultitypeCodec implementation","This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http://stackoverflow.com/questions/3403909/get-generic-type-of-class-at-runtime.  We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException... 

{code}
/**
   * Infers the type from this class's generic type argument
   * @param kryo
   * @param input
   * @return
 */
protected T doDeserialize(Kryo kryo, Input input) {
	Class<T> type = (Class<T>) (
				(ParameterizedType) this.getClass().getGenericSuperclass()).getActualTypeArguments()[0];
		return doDeserialize(kryo, input, type);
}
{code}",1.0,0.9888613918541044,0.0,0.9829320190174032,True,True,True,1426575533000.0,1436853608000.0,1436857208000.0,1436968481000.0,1436969381000.0,1426575533000.0,-1.0,1436791979000.0,1436795579000.0,1426575533000.0,1426579133000.0,1436853608000.0,1436857208000.0,"dturanski",1436795579000.0,1426575533000.0,45,1426575533000.0,-1.0,"SCS - Verify/Fix AbstractKryoMultitypeCodec implementation","This apparently is not tested or used internally, but I expect it to fail having tried a similar approach to derive the class of a generic type in a different situation. This method does not always work due to type erasure http://stackoverflow.com/questions/3403909/get-generic-type-of-class-at-runtime.  We need to verify if this is working, if not fix it. The API may require it, so possibly UnsupportedOperationException... 

{code}
/**
   * Infers the type from this class's generic type argument
   * @param kryo
   * @param input
   * @return
 */
protected T doDeserialize(Kryo kryo, Input input) {
	Class<T> type = (Class<T>) (
				(ParameterizedType) this.getClass().getGenericSuperclass()).getActualTypeArguments()[0];
		return doDeserialize(kryo, input, type);
}
{code}",1.0,False,1.0,0,0.0,-1,1,False
"XD-2824","Bug","","2015-03-16T11:55:58.000+0000","thomas.risberg","","hdfs sink loses messages/data when container killed","Scenario running a ""rabbit | hdfs"" stream and killing the xd-container while stream is running.

Looks like the messages get's acked before the data is flushed to hdfs.

This results in some data lost due to data either in tmp file or cached in the dfs client.

Reference: VESC-387",5.0,-1.0,1.7375490663280093e-06,-1.0,False,False,True,1426506958000.0,-1.0,-1.0,1448375944000.0,1448376844000.0,1426506958000.0,-1.0,-1.0,-1.0,1426506996000.0,1426510596000.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1426506958000.0,-1.0,"hdfs sink loses messages/data when container killed","Scenario running a ""rabbit | hdfs"" stream and killing the xd-container while stream is running.

Looks like the messages get's acked before the data is flushed to hdfs.

This results in some data lost due to data either in tmp file or cached in the dfs client.

Reference: VESC-387",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2823","Story","Sprint 45","2015-03-16T10:21:01.000+0000","dturanski","","Composite Modules should inherit ""xd.*"" properties","Currently when modules are composed to a single application context, properties are not inherited.

https://github.com/spring-projects/spring-xd/wiki/Modules#placeholders-available-to-all-modules

",2.0,0.3690125824639143,0.3689919148162233,0.0011243200343909,True,True,True,1426501261000.0,1426590534000.0,1426594134000.0,1426742285000.0,1426743185000.0,1426501261000.0,-1.0,1426501533000.0,1426505133000.0,1426590529000.0,1426594129000.0,1426590534000.0,1426594134000.0,"dturanski",1426505133000.0,1426501261000.0,45,1426501261000.0,-1.0,"Composite Modules should inherit ""xd.*"" properties","Currently when modules are composed to a single application context, properties are not inherited.

https://github.com/spring-projects/spring-xd/wiki/Modules#placeholders-available-to-all-modules

",2.0,False,2.0,0,0.0,-1,0,False
"XD-2818","Story","","2015-03-13T12:27:54.000+0000","dturanski","","upgrade to io.projectreactor breaks generated POMS","./gradlew install fails for spring-xd-extension-batch and spring-xd-extension-reactor. The first case is a simple update to gradle/build-extensions.gradle. The 2nd causes several compilation errors that are not trivial for a Reactor noob.  ",2.0,-1.0,-1.0,-1.0,False,False,False,1426249674000.0,-1.0,-1.0,1436790874000.0,1436791774000.0,1426249674000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"dturanski",-1.0,-1.0,0,1426249674000.0,-1.0,"upgrade to io.projectreactor breaks generated POMS","./gradlew install fails for spring-xd-extension-batch and spring-xd-extension-reactor. The first case is a simple update to gradle/build-extensions.gradle. The 2nd causes several compilation errors that are not trivial for a Reactor noob.  ",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2817","Bug","Sprint 48","2015-03-13T11:07:02.000+0000","fredmelo","dturanski","Classpath issues with gemfire-json-server sink","The GemFire client for SpringXD is throwing java.lang.NoClassDefFoundError for the class com/gemstone/gemfire/cache/client/internal/PingOp after a Stream sinking to gemfire-json-server is destroyed.

Issue starts after destroying a stream, which makes me think we might be unloading the jar files from the classpath while still keeping a connection to the gemfire server.

Steps to reproduce:

1) Create a region in Gemfire to test

e.g.: gfsh>create region --name=Stocks --type=REPLICATE
Member  | Status
------- | -------------------------------------
server1 | Region ""/Stocks"" created on ""server1""


2) Create a simple stream in Spring XD that writes to that region in gemfire-json-server. Deploy it for single node and let it run for a few seconds.


e.g.:

XD$ stream create streamx --definition ""trigger --fixedDelay=3 | http-client --url='''https://query.yahooapis.com/v1/public/yql?q=select * from yahoo.finance.quote where symbol in (\""MSFT\"")&format=json&env=store://datatables.org/alltableswithkeys''' --httpMethod=GET | splitter --expression=#jsonPath(payload,'$.query.results.quote') | gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=Stocks --keyExpression=payload.getField('Symbol')"" --deploy


3)  Destroy the stream

e.g.:
XD$  stream destroy streamx

3)  Wait a few seconds and check the xd-singlenode output.. you'll see the exception as following:

[error 2015/03/13 11:04:52.437 PDT  <poolTimer-client-pool-14> tid=0x15a] Unexpected error in pool task <com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask@635c9341>
java.lang.NoClassDefFoundError: com/gemstone/gemfire/cache/client/internal/PingOp
	at com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask.run2(LiveServerPinger.java:83)
	at com.gemstone.gemfire.cache.client.internal.PoolImpl$PoolTask.run(PoolImpl.java:1197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at com.gemstone.gemfire.internal.ScheduledThreadPoolExecutorWithKeepAlive$DelegatingScheduledFuture.run(ScheduledThreadPoolExecutorWithKeepAlive.java:252)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)",3.0,0.7257156119367743,0.7257156119367743,0.7257305231684578,True,True,True,1426244822000.0,1430381692000.0,1430385292000.0,1431944323000.0,1431945223000.0,1426244822000.0,1431332726000.0,1430381777000.0,1430385377000.0,1430381692000.0,1430385292000.0,1430381692000.0,1430385292000.0,"fredmelo",1430385377000.0,1426244822000.0,45,1431332726000.0,1431332726000.0,"Classpath issues with gemfire-json-server sink","The GemFire client for SpringXD is throwing java.lang.NoClassDefFoundError for the class com/gemstone/gemfire/cache/client/internal/PingOp after a Stream sinking to gemfire-json-server is destroyed.

Issue starts after destroying a stream, which makes me think we might be unloading the jar files from the classpath while still keeping a connection to the gemfire server.

Steps to reproduce:

1) Create a region in Gemfire to test

e.g.: gfsh>create region --name=Stocks --type=REPLICATE
Member  | Status
------- | -------------------------------------
server1 | Region ""/Stocks"" created on ""server1""


2) Create a simple stream in Spring XD that writes to that region in gemfire-json-server. Deploy it for single node and let it run for a few seconds.


e.g.:

XD$ stream create streamx --definition ""trigger --fixedDelay=3 | http-client --url='''https://query.yahooapis.com/v1/public/yql?q=select * from yahoo.finance.quote where symbol in (\""MSFT\"")&format=json&env=store://datatables.org/alltableswithkeys''' --httpMethod=GET | splitter --expression=#jsonPath(payload,'$.query.results.quote') | gemfire-json-server --useLocator=true --host=localhost --port=10334 --regionName=Stocks --keyExpression=payload.getField('Symbol')"" --deploy


3)  Destroy the stream

e.g.:
XD$  stream destroy streamx

3)  Wait a few seconds and check the xd-singlenode output.. you'll see the exception as following:

[error 2015/03/13 11:04:52.437 PDT  <poolTimer-client-pool-14> tid=0x15a] Unexpected error in pool task <com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask@635c9341>
java.lang.NoClassDefFoundError: com/gemstone/gemfire/cache/client/internal/PingOp
	at com.gemstone.gemfire.cache.client.internal.LiveServerPinger$PingTask.run2(LiveServerPinger.java:83)
	at com.gemstone.gemfire.cache.client.internal.PoolImpl$PoolTask.run(PoolImpl.java:1197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at com.gemstone.gemfire.internal.ScheduledThreadPoolExecutorWithKeepAlive$DelegatingScheduledFuture.run(ScheduledThreadPoolExecutorWithKeepAlive.java:252)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)",1.0,True,1.0,1,200.0,0,1,False
"XD-2815","Story","Sprint 46","2015-03-12T17:40:03.000+0000","sabby","","Create jdbchdfs job using ""tasklet"" instead of ""chunk"" processing model","As a user, I'd like to use a _jdbchdfs_ batch job as a passthrough (without chunk processing) so that I don't have to incur the batch read/write overhead.",2.0,0.6216880325002097,0.6910146836685961,0.0,True,True,True,1426182003000.0,1427716740000.0,1427720340000.0,1428649764000.0,1428650664000.0,1426182003000.0,1428306613000.0,1426182003000.0,1426185603000.0,1427887884000.0,1427891484000.0,1427716740000.0,1427720340000.0,"sabby",1426185603000.0,1426182003000.0,45,1428306613000.0,1428306613000.0,"Improve the performance of jdbchdfs batch job","As a user, I'd like to use a _jdbchdfs_ batch job as a passthrough (without chunk processing) so that I don't have to incur the batch read/write overhead.",8.0,True,8.0,1,-75.0,0,0,True
"XD-2814","Story","Sprint 46","2015-03-12T16:50:34.000+0000","jahubba","","RabbitMQ Dead Letter for TAP not deleted","If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains.  The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not.  This problem becomes worse when containers are running in yarn and may not live for long periods of time.  Many dead letter queues for taps can become overwhelming.",3.0,0.645336290132173,0.3442620233839606,0.0460115140181228,True,True,True,1426179034000.0,1428554231000.0,1428557831000.0,1429858691000.0,1429859591000.0,1426179034000.0,-1.0,1426348382000.0,1426351982000.0,1427446110000.0,1427449710000.0,1428554231000.0,1428557831000.0,"jahubba",1426351982000.0,1426179034000.0,45,1426179034000.0,-1.0,"RabbitMQ Dead Letter for TAP not deleted","If automatic binding of dead letter is enabled for rabbit mq and taps are deployed, anytime the tap is undeployed, the dead letter for that tap still remains.  The tap uses a unique name and the queue for that is automatically deleted, but the dead letter queue for it is not.  This problem becomes worse when containers are running in yarn and may not live for long periods of time.  Many dead letter queues for taps can become overwhelming.",3.0,False,3.0,0,0.0,0,0,False
"XD-2812","Bug","","2015-03-12T06:51:14.000+0000","bzeyben","","Batch Job deployment screen only show 10 items...","The Batch Jobs Deplyment screen 
('http://*****************:9393/admin-ui/#/jobs/deployments') UI screen is only showing 10 items with pagination and prohibiting users from launching their deployed jobs from the UI.

The jobs can only be launched via the RESTApi call.",5.0,-1.0,-1.0,-1.0,False,False,False,1426143074000.0,-1.0,-1.0,1433929225000.0,1433930125000.0,1426143074000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"bzeyben",-1.0,-1.0,0,1426143074000.0,-1.0,"Batch Job deployment screen only show 10 items...","The Batch Jobs Deployment screen 
('http://*****************:9393/admin-ui/#/jobs/deployments') UI screen is only showing 10 items without pagination and prohibiting users from launching their deployed jobs from the UI.

The jobs can only be launched via the RESTApi call

In release 1.0.3 the deployment page has not pagination but grows beyond 10 entries.",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-2807","Story","Sprint 45","2015-03-11T08:53:28.000+0000","eric.bottard","eric.bottard","Fix gradle build inconsistencies and leftovers","The build has some inconsistencies that should be taken care of.
Amongst the one I know:

* The UI project is always getting cleaned, for no apparent reason (there might have been one before), thus triggering a rebuild of everything downstream, most notably DIRT
* The exec"" task is not used anymore
* Lots of projects are getting the boot plugin applied to them. I'm not sure 100% what that plugin does, but we don't need the repackage bit for example.",3.0,0.220090949543066,0.220090949543066,0.2200639424672121,True,True,True,1426064008000.0,1426235145000.0,1426238745000.0,1426840682000.0,1426841582000.0,1426064008000.0,-1.0,1426235124000.0,1426238724000.0,1426235145000.0,1426238745000.0,1426235145000.0,1426238745000.0,"eric.bottard",1426238724000.0,1426064008000.0,45,1426064008000.0,-1.0,"Fix gradle build inconsistencies and leftovers","The build has some inconsistencies that should be taken care of.
Amongst the one I know:

* The UI project is always getting cleaned, for no apparent reason (there might have been one before), thus triggering a rebuild of everything downstream, most notably DIRT
* The exec"" task is not used anymore
* Lots of projects are getting the boot plugin applied to them. I'm not sure 100% what that plugin does, but we don't need the repackage bit for example.",3.0,False,3.0,0,0.0,-1,0,False
"XD-2806","Bug","Sprint 45","2015-03-10T15:16:27.000+0000","dturanski","","Module count not respected when label is used","{code}
xd:> stream create test --definition ""http | t1:transform --expression=payload | log""
xd:>stream deploy test --properties module.t1.count=2
Deployed stream 'test'
xd:>runtime modules
  Module Id            Container Id                          Options                                                                                                                                                                                            Deployment Properties                                                                       Unit status
  -------------------  ------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------------------------------------------------------  -----------
  test.processor.t1.1  393d3af0-68e8-49b2-8601-da063cfbf98a  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=1, producer.next.module.count=1, count=1, consumer.count=1, sequence=1}  deployed
  test.sink.log.1      f6bb3189-9c0e-44e8-962b-025e2288ffe3  {name=test, expression=payload, level=INFO}                                                                                                                                                        {consumer.sequence=1, count=1, consumer.count=1, sequence=1}                                deployed
  test.source.http.1   f6bb3189-9c0e-44e8-962b-025e2288ffe3  {sslPropertiesLocation=classpath:httpSSL.properties, maxContentLength=1048576, port=9000, messageConverterClass=org.springframework.integration.x.http.NettyInboundMessageConverter, https=false}  {producer.next.module.count=1, count=1, sequence=1}                                         deployed
{code}

*************************************
Works fine without the label:
*************************************
{code}
xd:>stream destroy test
Destroyed stream 'test'
xd:>stream create test --definition ""http | transform --expression=payload | log""
Created new stream 'test'
xd:>stream deploy test --properties module.transform.count=2
Deployed stream 'test'
xd:>runtime modules
  Module Id                   Container Id                          Options                                                                                                                                                                                            Deployment Properties                                                                       Unit status
  --------------------------  ------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------------------------------------------------------  -----------
  test.processor.transform.1  f6bb3189-9c0e-44e8-962b-025e2288ffe3  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=1, producer.next.module.count=1, count=2, consumer.count=2, sequence=1}  deployed
  test.processor.transform.2  393d3af0-68e8-49b2-8601-da063cfbf98a  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=2, producer.next.module.count=1, count=2, consumer.count=2, sequence=2}  deployed
  test.sink.log.1             393d3af0-68e8-49b2-8601-da063cfbf98a  {name=test, expression=payload, level=INFO}                                                                                                                                                        {consumer.sequence=1, count=1, consumer.count=1, sequence=1}                                deployed
  test.source.http.1          f6bb3189-9c0e-44e8-962b-025e2288ffe3  {sslPropertiesLocation=classpath:httpSSL.properties, maxContentLength=1048576, port=9000, messageConverterClass=org.springframework.integration.x.http.NettyInboundMessageConverter, https=false}  {producer.next.module.count=2, count=1, sequence=1}                                         deployed
{code}",3.0,-1.0,-1.0,0.0090536051586009,False,True,False,1426000587000.0,-1.0,-1.0,1426168570000.0,1426169470000.0,1426000587000.0,-1.0,1426002116000.0,1426005716000.0,-1.0,-1.0,-1.0,-1.0,"dturanski",1426005716000.0,1426000587000.0,45,1426000587000.0,-1.0,"Module count not respected when label is used","{code}
xd:> stream create test --definition ""http | t1:transform --expression=payload | log""
xd:>stream deploy test --properties module.t1.count=2
Deployed stream 'test'
xd:>runtime modules
  Module Id            Container Id                          Options                                                                                                                                                                                            Deployment Properties                                                                       Unit status
  -------------------  ------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------------------------------------------------------  -----------
  test.processor.t1.1  393d3af0-68e8-49b2-8601-da063cfbf98a  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=1, producer.next.module.count=1, count=1, consumer.count=1, sequence=1}  deployed
  test.sink.log.1      f6bb3189-9c0e-44e8-962b-025e2288ffe3  {name=test, expression=payload, level=INFO}                                                                                                                                                        {consumer.sequence=1, count=1, consumer.count=1, sequence=1}                                deployed
  test.source.http.1   f6bb3189-9c0e-44e8-962b-025e2288ffe3  {sslPropertiesLocation=classpath:httpSSL.properties, maxContentLength=1048576, port=9000, messageConverterClass=org.springframework.integration.x.http.NettyInboundMessageConverter, https=false}  {producer.next.module.count=1, count=1, sequence=1}                                         deployed
{code}

*************************************
Works fine without the label:
*************************************
{code}
xd:>stream destroy test
Destroyed stream 'test'
xd:>stream create test --definition ""http | transform --expression=payload | log""
Created new stream 'test'
xd:>stream deploy test --properties module.transform.count=2
Deployed stream 'test'
xd:>runtime modules
  Module Id                   Container Id                          Options                                                                                                                                                                                            Deployment Properties                                                                       Unit status
  --------------------------  ------------------------------------  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------  ------------------------------------------------------------------------------------------  -----------
  test.processor.transform.1  f6bb3189-9c0e-44e8-962b-025e2288ffe3  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=1, producer.next.module.count=1, count=2, consumer.count=2, sequence=1}  deployed
  test.processor.transform.2  393d3af0-68e8-49b2-8601-da063cfbf98a  {valid=true, expression=payload}                                                                                                                                                                   {consumer.sequence=2, producer.next.module.count=1, count=2, consumer.count=2, sequence=2}  deployed
  test.sink.log.1             393d3af0-68e8-49b2-8601-da063cfbf98a  {name=test, expression=payload, level=INFO}                                                                                                                                                        {consumer.sequence=1, count=1, consumer.count=1, sequence=1}                                deployed
  test.source.http.1          f6bb3189-9c0e-44e8-962b-025e2288ffe3  {sslPropertiesLocation=classpath:httpSSL.properties, maxContentLength=1048576, port=9000, messageConverterClass=org.springframework.integration.x.http.NettyInboundMessageConverter, https=false}  {producer.next.module.count=2, count=1, sequence=1}                                         deployed
{code}",3.0,False,3.0,0,0.0,0,0,False
"XD-2804","Bug","Sprint 46","2015-03-10T09:35:53.000+0000","kdowbecki","","Module options are not trimmed","Spring XD 1.1 container will throw following exception: 
{code}
java.lang.IllegalStateException: Can't find class used for type of option 'myField': String 
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.makeSimpleModuleOptions(DefaultModuleOptionsMetadataResolver.java:147)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:202)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:164)
	at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)
	at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)
	at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:174)
	at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:96)
	...
{code}

when module properties have a trailing whitespace character in type property (in example below there is a trailing space in options.myField.type value):

{code}
options.myField.description = this is my field
options.myField.type = String 
{code}

Can the property values be trimmed before comparing to DefaultModuleOptionsMetadataResolver#SHORT_CLASSNAMES map  to avoid this problem?",1.0,-1.0,-1.0,0.0190605134812741,False,True,False,1425980153000.0,-1.0,-1.0,1426575302000.0,1426576202000.0,1425980153000.0,-1.0,1425991514000.0,1425995114000.0,-1.0,-1.0,-1.0,-1.0,"kdowbecki",1425995114000.0,1425980153000.0,45,1425980153000.0,-1.0,"Module options are not trimmed","Spring XD 1.1 container will throw following exception: 
{code}
java.lang.IllegalStateException: Can't find class used for type of option 'myField': String 
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.makeSimpleModuleOptions(DefaultModuleOptionsMetadataResolver.java:147)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolveNormalMetadata(DefaultModuleOptionsMetadataResolver.java:202)
	at org.springframework.xd.module.options.DefaultModuleOptionsMetadataResolver.resolve(DefaultModuleOptionsMetadataResolver.java:164)
	at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:44)
	at org.springframework.xd.module.options.EnvironmentAwareModuleOptionsMetadataResolver.resolve(EnvironmentAwareModuleOptionsMetadataResolver.java:127)
	at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:174)
	at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:96)
	...
{code}

when module properties have a trailing whitespace character in type property (in example below there is a trailing space in options.myField.type value):

{code}
options.myField.description = this is my field
options.myField.type = String 
{code}

Can the property values be trimmed before comparing to DefaultModuleOptionsMetadataResolver#SHORT_CLASSNAMES map  to avoid this problem?",1.0,False,1.0,0,0.0,0,1,False
"XD-2802","Story","Sprint 45","2015-03-09T08:45:17.000+0000","sabby","","Migrate wiki documentation and the chores","As a developer, I'd like to migrate the wiki to project repo so that it can be tagged with the code and versioned etc. ",8.0,-1.0,-1.0,0.0,False,True,False,1425890717000.0,-1.0,-1.0,1428311609000.0,1428312509000.0,1425890717000.0,-1.0,1425890717000.0,1425894317000.0,-1.0,-1.0,-1.0,-1.0,"sabby",1425894317000.0,1425890717000.0,45,1425890717000.0,-1.0,"Migrate wiki documentation and the chores","As a developer, I'd like to migrate the wiki to project repo so that it can be tagged with the code and versioned etc. ",8.0,False,8.0,0,0.0,0,0,False
"XD-2801","Story","Sprint 45","2015-03-09T07:06:39.000+0000","grenfro","","Updated XD-EC2 XD deployment for 1.2","Mask out all properties for XD-EC2",3.0,0.4387818043867321,0.4387818043867321,0.0066149938896193,True,True,True,1425884799000.0,1426490869000.0,1426494469000.0,1427265155000.0,1427266055000.0,1425884799000.0,-1.0,1425893936000.0,1425897536000.0,1426490869000.0,1426494469000.0,1426490869000.0,1426494469000.0,"grenfro",1425897536000.0,1425884799000.0,45,1425884799000.0,-1.0,"Updated XD-EC2 XD deployment for 1.2","Mask out all properties for XD-EC2",3.0,False,3.0,0,0.0,0,1,False
"XD-2797","Story","Sprint 45","2015-03-07T13:42:48.000+0000","sabby","","Lattice Design Spike","",5.0,0.296252080821737,0.296252080821737,0.0,True,True,True,1425735768000.0,1426063581000.0,1426067181000.0,1426841402000.0,1426842302000.0,1425735768000.0,1426842284000.0,1425735768000.0,1425739368000.0,1426063581000.0,1426067181000.0,1426063581000.0,1426067181000.0,"sabby",1425739368000.0,1425735768000.0,45,1426842284000.0,1426842284000.0,"Lattice Design Spike","As a developer, I'd like to continue Lattice/Diego POC so that I can identify the scope, risks, and the overall design for a pluggable SPI in XD runtime.",8.0,True,8.0,1,-37.5,0,0,True
"XD-2795","Story","Sprint 45","2015-03-07T09:04:34.000+0000","sabby","","Placeholder for Perf. Testing #1","",8.0,0.8455269644745883,0.8455269644745883,0.0,True,True,True,1425719074000.0,1430113407000.0,1430117007000.0,1430915327000.0,1430916227000.0,1425719074000.0,-1.0,1425719074000.0,1425722674000.0,1430113407000.0,1430117007000.0,1430113407000.0,1430117007000.0,"sabby",1425722674000.0,1425719074000.0,45,1425719074000.0,-1.0,"Measure performance baseline for a simple stream","As a developer, I'd like to measure performance numbers for a simple stream so that I can characterize the overall throughput.
",8.0,False,8.0,0,0.0,0,0,True
"XD-2794","Improvement","Sprint 45","2015-03-07T01:37:11.000+0000","abhinav","eric.bottard","Add a MongoDB source","As a developer, I'd like to add a mongodb source using an xml and a property file supporting mixing in of parameters so that I can use this module to ingest data from Mongo.",5.0,0.9975504308004384,0.9974039279164832,0.9973803645155672,True,True,True,1425692231000.0,1426665930000.0,1426668321000.0,1426667421000.0,1426668321000.0,1425692231000.0,-1.0,1426665764000.0,1426668321000.0,1426665787000.0,1426668321000.0,1426665930000.0,1426668321000.0,"abhinav",1426668321000.0,1425692231000.0,45,1425692231000.0,-1.0,"Add a MongoDB source","As a developer, I'd like to add a mongodb source using an xml and a property file supporting mixing in of parameters so that I can use this module to ingest data from Mongo.",5.0,False,5.0,0,0.0,0,2,False
"XD-2793","Story","Sprint 45","2015-03-06T20:31:59.000+0000","sabby","","Fix offset management for Kafka source","As a developer, I'd like to fix the offset management with Kafka _source_ module so that I can efficiently perform fetch operation from the given offsets.",8.0,0.5898538088901554,0.5898538088901554,0.0,True,True,True,1425673919000.0,1426576426000.0,1426580026000.0,1427203071000.0,1427203971000.0,1425673919000.0,1425890876000.0,1425673919000.0,1425677519000.0,1426576426000.0,1426580026000.0,1426576426000.0,1426580026000.0,"sabby",1425677519000.0,1425673919000.0,45,1425890876000.0,1425890876000.0,"Fix offset management for Kafka source","As a developer, I'd like to fix the offset management with Kafka _source_ module so that I can efficiently perform fetch operation from the given offsets.",3.0,True,3.0,1,166.66666666666669,-1,0,False
"XD-2791","Story","Sprint 45","2015-03-06T10:20:47.000+0000","sabby","","Complete CI setup for Windows","As a build manager, I'd like to schedule CI builds for windows so that I can verify XD runtime features/functionality.

The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.",1.0,0.9918772168300124,0.9918772168300124,0.0,True,True,True,1425637247000.0,1430284651000.0,1430288251000.0,1430321810000.0,1430322710000.0,1425637247000.0,1429888909000.0,1425637247000.0,1425640847000.0,1430284651000.0,1430288251000.0,1430284651000.0,1430288251000.0,"sabby",1425640847000.0,1425637247000.0,45,1429888909000.0,1429888909000.0,"Complete CI setup for Windows","As a build manager, I'd like to schedule CI builds for windows so that I can verify XD runtime features/functionality.

The scope is to isolate the remaining test failures; perhaps, experiment with new AMI images until we have a solid infrastructure to fix the failing tests.",5.0,True,5.0,1,-80.0,-1,0,False
"XD-2788","Story","Sprint 44","2015-03-05T12:49:00.000+0000","sabby","eric.bottard","Add Kafka load generator sink","As a developer, I'd like to add Kafka load generator _sink_ module so that I could use it for performance testing use-cases. 
",3.0,0.0001989606883155,0.0001989606883155,0.0,True,True,True,1425559740000.0,1425559875000.0,1425563475000.0,1426237366000.0,1426238266000.0,1425559740000.0,-1.0,1425559740000.0,1425563340000.0,1425559875000.0,1425563475000.0,1425559875000.0,1425563475000.0,"sabby",1425563340000.0,1425559740000.0,45,1425559740000.0,-1.0,"Add throughput receiving sink","As a developer, I'd like to add load receiving _sink_ module so that I can measure received throughput",3.0,False,3.0,0,0.0,-1,0,True
"XD-2787","Story","Sprint 44","2015-03-05T12:48:14.000+0000","sabby","Eric Bottard","Add Kafka load generator source","As a developer, I'd like to add Kafka load generator _source_ module so that I could use it for performance testing use-cases. 
",3.0,0.001469339027022,0.999793374199325,0.0,True,True,True,1425559694000.0,1425559758000.0,1425563358000.0,1425602351000.0,1425603251000.0,1425559694000.0,-1.0,1425559694000.0,1425563294000.0,1425603242000.0,1425603251000.0,1425559758000.0,1425563358000.0,"sabby",1425563294000.0,1425559694000.0,45,1425559694000.0,-1.0,"Add load generator source","As a developer, I'd like to add load generator _source_ module so that I could use it for performance testing use-cases. 
",3.0,False,3.0,0,0.0,-1,1,True
"XD-2786","Story","Sprint 44","2015-03-05T12:46:57.000+0000","sabby","mbogoevici","Create EC2 AMI image for performance testing","As a developer, I'd like to create EC2 AMI with the necessary packages so that I can run the Kafka Perf tests.",2.0,0.0020349896731867,0.0050925363463329,0.0,True,True,True,1425559617000.0,1425559818000.0,1425563418000.0,1425657489000.0,1425658389000.0,1425559617000.0,-1.0,1425559617000.0,1425563217000.0,1425560120000.0,1425563720000.0,1425559818000.0,1425563418000.0,"sabby",1425563217000.0,1425559617000.0,45,1425559617000.0,-1.0,"Create EC2 AMI image for performance testing","As a developer, I'd like to create EC2 AMI with the necessary packages so that I can run the Kafka Perf tests.",2.0,False,2.0,0,0.0,-1,0,False
"XD-2785","Story","Sprint 44","2015-03-05T12:43:08.000+0000","sabby","mbogoevici","Identify the Kafka configuration for Kafka performance tests","As a developer, I'd like to identify the Kafka configurations so that I could setup infrastructure to perform performance testing.
",3.0,0.004258196018284,0.004258196018284,0.0,True,True,True,1425559388000.0,1425559810000.0,1425563410000.0,1425657591000.0,1425658491000.0,1425559388000.0,-1.0,1425559388000.0,1425562988000.0,1425559810000.0,1425563410000.0,1425559810000.0,1425563410000.0,"sabby",1425562988000.0,1425559388000.0,45,1425559388000.0,-1.0,"Identify the Kafka configuration for Kafka performance tests","As a developer, I'd like to identify the Kafka configurations so that I could setup infrastructure to perform performance testing.
",3.0,False,3.0,0,0.0,0,0,False
"XD-2784","Story","Sprint 44","2015-03-05T12:42:06.000+0000","sabby","grenfro","Research EC2 infrastructure required for Kafka performance tests","As a developer, I'd like to research and Identify the EC2 infrastructure required  so that I can run performance tests on Kafka.
",3.0,0.5342312008978676,0.5342312008978676,0.0,False,False,False,1425559326000.0,1425559802000.0,1425560217000.0,1425559326000.0,1425560217000.0,1425559326000.0,-1.0,1425559326000.0,1425560217000.0,1425559802000.0,1425560217000.0,1425559802000.0,1425560217000.0,"sabby",1425560217000.0,1425559326000.0,45,1425559326000.0,-1.0,"Research EC2 infrastructure required for Kafka performance tests","As a developer, I'd like to research and Identify the EC2 infrastructure required  so that I can run performance tests on Kafka.
",3.0,False,3.0,0,0.0,-1,0,False
"XD-2779","Bug","Sprint 45","2015-03-04T16:26:28.000+0000","thomas.risberg","","Fix error handling in jdbchdfs job ","The jdbchdfs job keeps the output stream open in case of error writing to HDFS. We should improve this and close it plus throw an exception.

We should also make sure the step is marked as failed instead of complete when an exception is thrown in the writer.",3.0,0.5420683920264217,0.5420683920264217,0.1842628257930562,True,True,True,1425486388000.0,1425900976000.0,1425904576000.0,1426250314000.0,1426251214000.0,1425486388000.0,-1.0,1425627317000.0,1425630917000.0,1425900976000.0,1425904576000.0,1425900976000.0,1425904576000.0,"thomas.risberg",1425630917000.0,1425486388000.0,45,1425486388000.0,-1.0,"Fix error handling in jdbchdfs job ","The jdbchdfs job keeps the output stream open in case of error writing to HDFS. We should improve this and close it plus throw an exception.

We should also make sure the step is marked as failed instead of complete when an exception is thrown in the writer.",3.0,False,3.0,0,0.0,0,0,False
"XD-2778","Story","Sprint 44","2015-03-04T09:29:27.000+0000","sabby","grussell","Document the changes to Message Headers in 1.1","As a developer, I'd like to document the changes to message headers so that users can refer to the troubleshooting section if there are any serialization errors when reusing the 1.0 batch-jobs in 1.1 release.

Perhaps this could be part of [troubleshooting|https://github.com/spring-projects/spring-xd/wiki/Deployment#troubleshooting] section in our wiki.",1.0,0.9652864259028644,0.0,0.0,True,True,True,1425461367000.0,1425554382000.0,1425557727000.0,1425556827000.0,1425557727000.0,1425461367000.0,-1.0,1425461367000.0,1425464967000.0,1425461367000.0,1425464967000.0,1425554382000.0,1425557727000.0,"sabby",1425464967000.0,1425461367000.0,45,1425461367000.0,-1.0,"Document the changes to Message Headers in 1.1","As a developer, I'd like to document the changes to message headers so that users can refer to the troubleshooting section if there are any serialization errors when reusing the 1.0 batch-jobs in 1.1 release.

Perhaps this could be part of [troubleshooting|https://github.com/spring-projects/spring-xd/wiki/Deployment#troubleshooting] section in our wiki.",1.0,False,1.0,0,0.0,0,1,False
"XD-2777","Story","Sprint 46","2015-03-04T08:41:53.000+0000","dturanski","","Document trigger source","",1.0,0.7925320157040184,0.7925320157040184,0.6542890294037522,True,True,True,1425458513000.0,1427445493000.0,1427449093000.0,1427964742000.0,1427965642000.0,1425458513000.0,-1.0,1427098900000.0,1427102500000.0,1427445493000.0,1427449093000.0,1427445493000.0,1427449093000.0,"dturanski",1427102500000.0,1425458513000.0,45,1425458513000.0,-1.0,"Document trigger source","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2776","Story","Sprint 44","2015-03-04T08:13:16.000+0000","sabby","Glenn renfro","Measure baseline numbers for Kafka","As a developer, I'd like to bench Kafka as message bus using in-built perf-testing producer/consumer utilities so that I can use that as a foundation to build XD use-cases and measure performance.",8.0,0.0005641924888685,0.0730785205682183,0.0,True,True,True,1425456796000.0,1425457592000.0,1425461192000.0,1426866762000.0,1426867662000.0,1425456796000.0,1425559553000.0,1425456796000.0,1425460396000.0,1425559900000.0,1425563500000.0,1425457592000.0,1425461192000.0,"sabby",1425460396000.0,1425456796000.0,45,1425559553000.0,1425559553000.0,"Reproduce baseline numbers for Kafka","As a developer, I'd like to bench Kafka as message bus using in-built perf-testing producer/consumer utilities so that I can use that as a foundation to build XD use-cases and measure performance. 

I'd like to reproduce baseline performance metrics as identified by the Kafka [engineering team|https://engineering.linkedin.com/kafka/benchmarking-apache-kafka-2-million-writes-second-three-cheap-machines].",5.0,True,5.0,1,60.0,-1,0,True
"XD-2774","Bug","Sprint 45","2015-03-03T09:20:44.000+0000","thomas.risberg","Janne Valkealahti","hdfs sink doesn't recover after error writing to hdfs","Steps to reproduce -

create a stream using hdfs sink with a small rollover:

{code}
xd:>stream create --name errtest --definition ""time | hdfs --rollover=50"" --deploy 
{code}

stop the datanode(s) and wait for an exception like:

{code}
2015-03-03 10:41:57,832 1.1.0.RELEASE ERROR task-scheduler-3 handler.LoggingHandler - org.springframework.messaging.MessageHandlingException: failed to write Message payload to HDFS; nested exception is org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xd/errtest/errtest-7.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.springframework.xd.integration.hadoop.outbound.HdfsStoreMessageHandler.handleMessageInternal(HdfsStoreMessageHandler.java:129)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:107)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:87)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy136.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy125.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy137.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:130)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:219)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:298)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:292)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xd/errtest/errtest-7.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1468)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy134.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy135.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1532)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1349)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:588)
{code}

start the datanode(s) again, the sink never recovers and has to be undeployed and redeployed.
",5.0,0.6803238443727776,0.3257682646985859,0.1026019701480195,True,True,True,1425374444000.0,1425900982000.0,1425904582000.0,1426147496000.0,1426148396000.0,1425374444000.0,-1.0,1425453853000.0,1425457453000.0,1425626573000.0,1425630173000.0,1425900982000.0,1425904582000.0,"thomas.risberg",1425457453000.0,1425374444000.0,45,1425374444000.0,-1.0,"Update to SHDP 2.1.1 for fixing hdfs store writer to recover after error writing to hdfs","The hdfs sink doesn't recover after error writing to hdfs.

Steps to reproduce -

create a stream using hdfs sink with a small rollover:

{code}
xd:>stream create --name errtest --definition ""time | hdfs --rollover=50"" --deploy 
{code}

stop the datanode(s) and wait for an exception like:

{code}
2015-03-03 10:41:57,832 1.1.0.RELEASE ERROR task-scheduler-3 handler.LoggingHandler - org.springframework.messaging.MessageHandlingException: failed to write Message payload to HDFS; nested exception is org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xd/errtest/errtest-7.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.springframework.xd.integration.hadoop.outbound.HdfsStoreMessageHandler.handleMessageInternal(HdfsStoreMessageHandler.java:129)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:107)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:87)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy136.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy125.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutput(AbstractMessageProducingHandler.java:248)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.produceOutput(AbstractMessageProducingHandler.java:171)
	at org.springframework.integration.handler.AbstractMessageProducingHandler.sendOutputs(AbstractMessageProducingHandler.java:119)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:105)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:114)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:98)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:92)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy137.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:115)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:45)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:95)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:130)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:219)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:298)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:292)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /xd/errtest/errtest-7.txt.tmp could only be replicated to 0 nodes instead of minReplication (=1).  There are 1 datanode(s) running and 1 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:1549)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3200)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:641)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:482)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:962)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2039)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2035)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2033)

	at org.apache.hadoop.ipc.Client.call(Client.java:1468)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy134.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:399)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy135.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1532)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1349)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:588)
{code}

start the datanode(s) again, the sink never recovers and has to be undeployed and redeployed.
",5.0,False,5.0,0,0.0,0,0,True
"XD-2769","Bug","Sprint 45","2015-03-02T10:51:18.000+0000","kdowbecki","","Inconsistent API in AbstractSingleNodeNamedChannelSink ","In [AbstractSingleNodeNamedChannelSink|https://github.com/spring-projects/spring-xd/blob/6bd17162c8a6da0f09f6f8809f694a060c71ecc0/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/test/sink/AbstractSingleNodeNamedChannelSink.java] the receive() and receivePayload() methods  are non-blocking.

Methods without timeout parameter are usually blocking and return the first message delivered to the channel (e.g. org.springframework.integration.channel.AbstractPollableChannel#receive()). 

Integration tests based on spring-xd-test dependency and embedded xd-singlenode are asynchronous. This makes AbstractSingleNodeNamedChannelSink receive method return null in all invocations because test thread is progressing faster than container can process the message in the background.

Would it be possible to make receive methods behave like in AbstractPollableChannel?",1.0,0.9984739186302968,0.9984480966105048,6.64917009642265e-05,True,True,True,1425293478000.0,1428386881000.0,1428390481000.0,1428390709000.0,1428391609000.0,1425293478000.0,-1.0,1425293684000.0,1425297284000.0,1428386801000.0,1428390401000.0,1428386881000.0,1428390481000.0,"kdowbecki",1425297284000.0,1425293478000.0,45,1425293478000.0,-1.0,"Inconsistent API in AbstractSingleNodeNamedChannelSink ","In [AbstractSingleNodeNamedChannelSink|https://github.com/spring-projects/spring-xd/blob/6bd17162c8a6da0f09f6f8809f694a060c71ecc0/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/test/sink/AbstractSingleNodeNamedChannelSink.java] the receive() and receivePayload() methods  are non-blocking.

Methods without timeout parameter are usually blocking and return the first message delivered to the channel (e.g. org.springframework.integration.channel.AbstractPollableChannel#receive()). 

Integration tests based on spring-xd-test dependency and embedded xd-singlenode are asynchronous. This makes AbstractSingleNodeNamedChannelSink receive method return null in all invocations because test thread is progressing faster than container can process the message in the background.

Would it be possible to make receive methods behave like in AbstractPollableChannel?",1.0,False,1.0,0,0.0,0,1,False
"XD-2767","Bug","Sprint 45","2015-03-02T06:17:55.000+0000","grussell","","JMS Source Does Not Expose `acknowledge`","Since the message-driven adapter uses a {{DMLC}}, the default behavior is to lose messages on exceptions (with the DMLC, the message is ack'd before the listener is invoked).

In order to provide recovery of such situations, the source needs to expose {{acknowledge}} so it can be set to {{transacted}}.

Or, perhaps, given that we don't expose complex configuration, the source should use a {{SimpleMessageListenerContainer}} instead (where the ack is sent after the listener is successfully invoked).
",1.0,0.6147176938828336,0.6147176938828336,0.003660330550978,True,True,True,1425277075000.0,1426236519000.0,1426240119000.0,1426836963000.0,1426837863000.0,1425277075000.0,-1.0,1425282788000.0,1425286388000.0,1426236519000.0,1426240119000.0,1426236519000.0,1426240119000.0,"grussell",1425286388000.0,1425277075000.0,45,1425277075000.0,-1.0,"JMS Source Does Not Expose `acknowledge`","Since the message-driven adapter uses a {{DMLC}}, the default behavior is to lose messages on exceptions (with the DMLC, the message is ack'd before the listener is invoked).

In order to provide recovery of such situations, the source needs to expose {{acknowledge}} so it can be set to {{transacted}}.

Or, perhaps, given that we don't expose complex configuration, the source should use a {{SimpleMessageListenerContainer}} instead (where the ack is sent after the listener is successfully invoked).
",1.0,False,1.0,0,0.0,0,1,False
"XD-2766","Story","Sprint 45","2015-03-02T03:06:31.000+0000","eric.bottard","","Add metadata for description of a module (itself)","",8.0,0.5001103208843475,0.4523842981906739,0.1200303938805291,True,True,True,1425265591000.0,1426052107000.0,1426055707000.0,1426837376000.0,1426838276000.0,1425265591000.0,-1.0,1425454361000.0,1425457961000.0,1425977049000.0,1425980649000.0,1426052107000.0,1426055707000.0,"eric.bottard",1425457961000.0,1425265591000.0,45,1425265591000.0,-1.0,"Add metadata for description of a module (itself)","As a user, I'd like to have the description for each of the modules so that I can use it to understand the module purpose and it's capabilities (presumably what is captured in javadoc for the module definition).",8.0,False,8.0,0,0.0,-1,0,True
"XD-2764","Bug","Sprint 44","2015-02-26T17:22:25.000+0000","mbogoevici","mbogoevici","Fix failing KafkaSingleNodeStreamDeploymentIntegrationTests.verifyOnDemandQueues()","Test fails in CI when the topic used by the test had its initial segment removed during cleanup.",1.0,0.0004343851505112,0.0004343851505112,0.0057539824041597,True,True,True,1424971345000.0,1424971479000.0,1424975079000.0,1425278927000.0,1425279827000.0,1424971345000.0,-1.0,1424973120000.0,1424976720000.0,1424971479000.0,1424975079000.0,1424971479000.0,1424975079000.0,"mbogoevici",1424976720000.0,1424971345000.0,45,1424971345000.0,-1.0,"Fix failing KafkaSingleNodeStreamDeploymentIntegrationTests.verifyOnDemandQueues()","Test fails in CI when the topic used by the test had its initial segment removed during cleanup.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2762","Story","Sprint 44","2015-02-26T05:47:33.000+0000","thomas.risberg","thomas.risberg","Update RHEL/CentOS yum/rpm installation instructions","As a build manager, I'd like to have Spring XD RPMs published in spring.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA. 

*Location for 1.1.0 RELEASE:*
http://repo.spring.io/simple/libs-release-local/org/springframework/xd/spring-xd/1.1.0.RELEASE/",2.0,0.0112899605604899,0.0112899605604899,0.0059127200436079,True,True,True,1424929653000.0,1424936695000.0,1424940295000.0,1425552493000.0,1425553393000.0,1424929653000.0,-1.0,1424933341000.0,1424936941000.0,1424936695000.0,1424940295000.0,1424936695000.0,1424940295000.0,"thomas.risberg",1424936941000.0,1424929653000.0,45,1424929653000.0,-1.0,"Update RHEL/CentOS yum/rpm installation instructions","As a build manager, I'd like to have Spring XD RPMs published in spring.io repository so that users can directly download the bits without having to go through appsuite repo or the EULA. 

*Location for 1.1.0 RELEASE:*
http://repo.spring.io/libs-release-local/org/springframework/xd/spring-xd/1.1.0.RELEASE/",2.0,False,2.0,0,0.0,0,0,True
"XD-2761","Story","Sprint 44","2015-02-26T05:23:19.000+0000","dturanski","dturanski","Register only known classes with Kryo in PojoCodec","Currently PojoCodec calls kryo.register(Class<?> type) on every ser/deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize.  See https://github.com/EsotericSoftware/kryo#registration.  The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus (passed between modules)",5.0,1.261790572589091e-05,8.603117540380167e-06,0.0027604536481233,True,True,True,1424928199000.0,1424928221000.0,1424931821000.0,1426670853000.0,1426671753000.0,1424928199000.0,-1.0,1424933012000.0,1424936612000.0,1424928214000.0,1424931814000.0,1424928221000.0,1424931821000.0,"dturanski",1424936612000.0,1424928199000.0,45,1424928199000.0,-1.0,"Register only known classes with Kryo in PojoCodec","Currently PojoCodec calls kryo.register(Class<?> type) on every ser/deser invocation. This fails with 1.1 because instances are pooled and a different instance may be used to serialize and deserialize.  See https://github.com/EsotericSoftware/kryo#registration.  The fix is to not register classes on the fly. Classes serialized by PojoCodec will not be registered by default. This will work but is less efficient. XD should provide an easy way to register types known to be serialized on the MessageBus (passed between modules)",5.0,False,5.0,0,0.0,-1,0,False
"XD-2759","Bug","Sprint 44","2015-02-25T13:06:55.000+0000","grussell","grussell","LocalMessageBus PubSub Needs a Bounded Task Exectutor","Since 1.1, {{PubSub}} channels in the {{LocalMessageBus}} run on a {{CachedThreadPoolExecutor}}.

For high volume environments, where back-pressure might occur on a {{topic:}} thread we could overwhelm the system with threads.

Add a local bus configuration to limit the thread pool used for PubSubs and queue tasks where there are no threads available.

It would be a bus-wide setting.",1.0,0.0141827710448738,0.002303744950322,0.0059395130472759,True,True,True,1424869615000.0,1424870914000.0,1424874514000.0,1424960305000.0,1424961205000.0,1424869615000.0,-1.0,1424870159000.0,1424873759000.0,1424869826000.0,1424873426000.0,1424870914000.0,1424874514000.0,"grussell",1424873759000.0,1424869615000.0,45,1424869615000.0,-1.0,"LocalMessageBus PubSub Needs a Bounded Task Exectutor","Since 1.1, {{PubSub}} channels in the {{LocalMessageBus}} run on a {{CachedThreadPoolExecutor}}.

For high volume environments, where back-pressure might occur on a {{topic:}} thread we could overwhelm the system with threads.

Add a local bus configuration to limit the thread pool used for PubSubs and queue tasks where there are no threads available.

It would be a bus-wide setting.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2755","Bug","Sprint 44","2015-02-24T12:15:08.000+0000","mbogoevici","iperumal","Scala processor module executor trims messages","How to reproduce:

1. Run xd-singlenode (for which setting the Spark master URL to 'local' is a requirement). Use more than 1 worker thread. e.g. {{local[4]}}

2. Deploy the word-count example

3. Create a stream
{{stream create spark-streaming-word-count --definition ""http | word-count | log"" --deploy}}

4. Send data
{{xd:>http post --data ""a b c d e f g""}}

{{xd:>http post --data ""a b c""}}

5.Observe the result

2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (e,1)
2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (d,1)
2015-02-24 15:12:46,019 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1)
2015-02-24 15:12:46,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (g,1)
2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (a,1)
2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1)
2015-02-24 15:13:40,021 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (c,1)

(the last three results are coming from the second invocation))

Note: there seems to be a correlation between the number of values emitted and the number of workers, as, in all the attempts, there aren't more values emitted than the number of workers.",5.0,0.2666566277430218,0.2665334227710167,0.267847609139071,True,True,True,1424780108000.0,1424799587000.0,1424803187000.0,1424852257000.0,1424853157000.0,1424780108000.0,-1.0,1424799674000.0,1424803274000.0,1424799578000.0,1424803178000.0,1424799587000.0,1424803187000.0,"mbogoevici",1424803274000.0,1424780108000.0,45,1424780108000.0,-1.0,"Scala processor module executor trims messages","How to reproduce:

1. Run xd-singlenode (for which setting the Spark master URL to 'local' is a requirement). Use more than 1 worker thread. e.g. {{local[4]}}

2. Deploy the word-count example

3. Create a stream
{{stream create spark-streaming-word-count --definition ""http | word-count | log"" --deploy}}

4. Send data
{{xd:>http post --data ""a b c d e f g""}}

{{xd:>http post --data ""a b c""}}

5.Observe the result

2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (e,1)
2015-02-24 15:12:46,018 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (d,1)
2015-02-24 15:12:46,019 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1)
2015-02-24 15:12:46,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (g,1)
2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-1 sink.spark-streaming-word-count - (a,1)
2015-02-24 15:13:40,020 1.2.0.SNAP  INFO Executor task launch worker-2 sink.spark-streaming-word-count - (b,1)
2015-02-24 15:13:40,021 1.2.0.SNAP  INFO Executor task launch worker-3 sink.spark-streaming-word-count - (c,1)

(the last three results are coming from the second invocation))

Note: there seems to be a correlation between the number of values emitted and the number of workers, as, in all the attempts, there aren't more values emitted than the number of workers.",5.0,False,5.0,0,0.0,0,4,False
"XD-2752","Bug","Sprint 46","2015-02-24T07:31:26.000+0000","hlagos","Janne Valkealahti","SqoopTasklet not using hadoop configuration","Hey Guys,

I'm trying to use a SqoopTasklet but for some reason it is not getting the hadoop configuration. In the attached sqoop job configuration using the sqooprunner class directly works without problems but the SqoopTasklet is not getting the correct configuration throwing kerberos authentication problems (see singlenode.log).  Could please you guys help me to solve this problem?

Thanks in advance.
Regards,",8.0,0.746577004728929,0.5822651813070254,0.1879219102328178,True,True,True,1424763086000.0,1428565334000.0,1428568934000.0,1429855094000.0,1429855994000.0,1424763086000.0,1427098062000.0,1425720155000.0,1425723755000.0,1427728509000.0,1427732109000.0,1428565334000.0,1428568934000.0,"hlagos",1425723755000.0,1424763086000.0,45,1428307758000.0,1427098062000.0,"SqoopTasklet not using hadoop configuration","Hey Guys,

I'm trying to use a SqoopTasklet but for some reason it is not getting the hadoop configuration. In the attached sqoop job configuration using the sqooprunner class directly works without problems but the SqoopTasklet is not getting the correct configuration throwing kerberos authentication problems (see singlenode.log).  Could please you guys help me to solve this problem?

Thanks in advance.
Regards,",3.0,True,3.0,2,166.66666666666669,0,0,False
"XD-2750","Story","Sprint 44","2015-02-23T08:58:36.000+0000","sabby","pperalta","Placeholder for Lattice/Diego POC #2","",8.0,0.1869753417645355,0.0002144096773976,0.0,True,True,True,1424681916000.0,1424848477000.0,1424852077000.0,1425571834000.0,1425572734000.0,1424681916000.0,-1.0,1424681916000.0,1424685516000.0,1424682107000.0,1424685707000.0,1424848477000.0,1424852077000.0,"sabby",1424685516000.0,1424681916000.0,45,1424681916000.0,-1.0,"Placeholder for Lattice/Diego POC #2","",8.0,False,8.0,0,0.0,-1,0,False
"XD-2749","Story","Sprint 44","2015-02-23T08:58:14.000+0000","sabby","mark.fisher","Placeholder for Lattice/Diego POC #1","",8.0,0.1896694050201249,0.0002539096969593,0.0,True,True,True,1424681894000.0,1424848474000.0,1424852074000.0,1425559259000.0,1425560159000.0,1424681894000.0,-1.0,1424681894000.0,1424685494000.0,1424682117000.0,1424685717000.0,1424848474000.0,1424852074000.0,"sabby",1424685494000.0,1424681894000.0,45,1424681894000.0,-1.0,"Placeholder for Lattice/Diego POC #1","",8.0,False,8.0,0,0.0,0,0,False
"XD-2748","Story","Sprint 44","2015-02-23T08:23:12.000+0000","iperumal","iperumal","Implement Reliable spark streaming receiver ","The spark streaming message bus receiver isn't reliable yet. The receiver needs to handle data loss in case of worker node that has it running.

We currently handle the driver failure automatically by re-deploying spark streaming module. But, this is about the data that wasn't replicated when the worker node dies.

Please see the documents here:

https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html

http://spark.apache.org/docs/latest/streaming-custom-receivers.html",3.0,0.0014382364194925,0.0,0.0002571355104139,True,True,True,1424679792000.0,1424683299000.0,1424686899000.0,1427117295000.0,1427118195000.0,1424679792000.0,1425889707000.0,1424680419000.0,1424684019000.0,1424679792000.0,1424683392000.0,1424683299000.0,1424686899000.0,"iperumal",1424684019000.0,1424679792000.0,45,1425889707000.0,1425889707000.0,"Implement Reliable spark streaming receiver ","The spark streaming message bus receiver isn't reliable yet. The receiver needs to handle data loss in case of worker node that has it running.

We currently handle the driver failure automatically by re-deploying spark streaming module. But, this is about the data loss when the worker node dies.

Please see the documents here:

https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html

http://spark.apache.org/docs/latest/streaming-custom-receivers.html",8.0,True,8.0,1,-62.5,-1,0,True
"XD-2747","Story","Sprint 44","2015-02-23T06:58:58.000+0000","sabby","","Placeholder: Performance testing","",8.0,0.0111194665254026,0.8983102104854022,0.0,True,True,True,1424674738000.0,1424685695000.0,1424689295000.0,1425659227000.0,1425660127000.0,1424674738000.0,1424934389000.0,1424674738000.0,1424678338000.0,1425559923000.0,1425563523000.0,1424685695000.0,1424689295000.0,"sabby",1424678338000.0,1424674738000.0,45,1424934428000.0,1424934389000.0,"Document test scenarios for performance testing","As a developer, I'd like to benchmark Rabbit performance so that I can use the results as reference to setup XD cluster.",8.0,True,8.0,2,0.0,-1,0,True
"XD-2744","Story","Sprint 44","2015-02-21T12:58:46.000+0000","sabby","","Placeholder: Spring XD Lab","",8.0,0.3649014962327378,0.1367962619083192,0.0,True,True,True,1424523526000.0,1424933830000.0,1424937430000.0,1425647050000.0,1425647950000.0,1424523526000.0,-1.0,1424523526000.0,1424527126000.0,1424677343000.0,1424680943000.0,1424933830000.0,1424937430000.0,"sabby",1424527126000.0,1424523526000.0,45,1424523526000.0,-1.0,"Placeholder for Spring XD Lab","",8.0,False,8.0,0,0.0,-1,0,True
"XD-2743","Story","Sprint 44","2015-02-21T12:58:16.000+0000","sabby","","Placeholder: Acceptance Testing #1","",5.0,0.9999641098096264,0.9999641098096264,0.0,True,True,True,1424523496000.0,1425359349000.0,1425359379000.0,1425358479000.0,1425359379000.0,1424523496000.0,1424936763000.0,1424523496000.0,1424527096000.0,1425359349000.0,1425359379000.0,1425359349000.0,1425359379000.0,"sabby",1424527096000.0,1424523496000.0,45,1424936763000.0,1424936763000.0,"Improve acceptance testing coverage","The scope is to address the sub-tasks linked with this story.
",8.0,True,8.0,1,-37.5,0,0,True
"XD-2742","Story","Sprint 44","2015-02-21T12:57:55.000+0000","sabby","","Placeholder: Placeholder: DEBS Challenge","",8.0,0.7732515772570996,0.5544998444908236,0.0,True,True,True,1424523475000.0,1427270718000.0,1427274318000.0,1428075420000.0,1428076320000.0,1424523475000.0,-1.0,1424523475000.0,1424527075000.0,1426493527000.0,1426497127000.0,1427270718000.0,1427274318000.0,"sabby",1424527075000.0,1424523475000.0,45,1424523475000.0,-1.0,"Prep for DEBS Challenge","As a developer, I'd like to study the taxi trips based on a stream of trip reports from New York City so that I can evaluate event-based systems in the context of real-time analytics using Spring XD.

[Challenge Details|http://www.debs2015.org/call-grand-challenge.html]",8.0,False,8.0,0,0.0,0,0,True
"XD-2741","Story","Sprint 44","2015-02-20T11:35:48.000+0000","mark.pollack","mark.pollack","Redis sink should default to using spring.redis configuration in servers.yml","The configuration for the redis sink must be provided for explicitly instead of falling back to values defined in servers.yml.  The default behavior configuration should be address in a manner consistent with the default behavior of module config for rabbit source/sink, jdbc sink....",1.0,0.2806106239674673,0.0,0.019566230355403,True,True,True,1424432148000.0,1424697123000.0,1424700723000.0,1425375528000.0,1425376428000.0,1424432148000.0,1424677599000.0,1424450624000.0,1424454224000.0,1424432148000.0,1424435748000.0,1424697123000.0,1424700723000.0,"mark.pollack",1424454224000.0,1424432148000.0,45,1424677599000.0,1424677599000.0,"Redis sink should default to using spring.redis configuration in servers.yml","The configuration for the redis sink must be provided for explicitly instead of falling back to values defined in servers.yml.  The default behavior configuration should be address in a manner consistent with the default behavior of module config for rabbit source/sink, jdbc sink....",2.0,True,2.0,1,-50.0,0,0,False
"XD-2739","Story","Sprint 43","2015-02-20T09:18:02.000+0000","thomas.risberg","thomas.risberg","Add a Sqoop example","",3.0,0.0003391530022413,0.0003214580629939,0.001132476111832,True,True,True,1424423882000.0,1424423997000.0,1424427597000.0,1424762062000.0,1424762962000.0,1424423882000.0,-1.0,1424424266000.0,1424427866000.0,1424423991000.0,1424427591000.0,1424423997000.0,1424427597000.0,"thomas.risberg",1424427866000.0,1424423882000.0,45,1424423882000.0,-1.0,"Add a Sqoop example","",3.0,False,3.0,0,0.0,-1,0,False
"XD-2735","Improvement","Sprint 43","2015-02-18T13:19:20.000+0000","iperumal","iperumal","Make local message bus properties configurable","Currently, the local message bus has couple of properties ""queueSize"" and ""polling"". But these properties can not be configurable via servers.yml.
Also, the property prefix needs to align with other message bus properties.",1.0,3.2811807875927615e-05,0.0,0.0694133795615248,True,True,True,1424265560000.0,1424265566000.0,1424269166000.0,1424447521000.0,1424448421000.0,1424265560000.0,-1.0,1424278253000.0,1424281853000.0,1424265560000.0,1424269160000.0,1424265566000.0,1424269166000.0,"iperumal",1424281853000.0,1424265560000.0,45,1424265560000.0,-1.0,"Make local message bus properties configurable","Currently, the local message bus has couple of properties ""queueSize"" and ""polling"". But these properties can not be configurable via servers.yml.
Also, the property prefix needs to align with other message bus properties.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2734","Story","Sprint 43","2015-02-18T09:35:24.000+0000","sabby","hillert","Build reference architectures for domain specific use-cases","As a field engineer, I'd like to have reference architectures built on Spring XD so that I can use that as reference building POCs.",8.0,0.1222274791958819,0.0,0.0,True,True,True,1424252124000.0,1424424178000.0,1424427778000.0,1425658878000.0,1425659778000.0,1424252124000.0,-1.0,1424252124000.0,1424255724000.0,1424252124000.0,1424255724000.0,1424424178000.0,1424427778000.0,"sabby",1424255724000.0,1424252124000.0,45,1424252124000.0,-1.0,"Create first-cut on reference architectures for domain specific use-cases","As a field engineer, I'd like to have reference architectures built on Spring XD so that I can use that as reference building POCs. The scope is to get the raw domain specific ideas captured as first step.
",8.0,False,8.0,0,0.0,0,0,True
"XD-2730","Story","Sprint 44","2015-02-17T15:20:54.000+0000","sabby","","Add support to include deployment manifest from file","As a user, I'd like to include the deployment manifest from the file so that I don't have spend time typing as ""inline properties"".",3.0,0.4667741876157936,0.3751540249590399,0.0,True,True,True,1424186454000.0,1424770206000.0,1424773806000.0,1425436163000.0,1425437063000.0,1424186454000.0,1424445415000.0,1424186454000.0,1424190054000.0,1424655625000.0,1424659225000.0,1424770206000.0,1424773806000.0,"sabby",1424190054000.0,1424186454000.0,45,1424678210000.0,1424445415000.0,"Add support to include deployment manifest from file","As a user, I'd like to include the deployment manifest from the file so that I don't have spend time typing as ""inline properties"".",8.0,True,8.0,2,-62.5,0,0,False
"XD-2729","Story","Sprint 43","2015-02-17T14:53:48.000+0000","grenfro","grenfro","Update payload-conversion demo to latest module spec","Upload payload conversion demo such that a user can use the module upload feature against the sample.",3.0,0.1355427579292299,1.2227031521287262e-05,0.1355203417047742,True,True,True,1424184828000.0,1424251341000.0,1424254941000.0,1424674644000.0,1424675544000.0,1424184828000.0,-1.0,1424251330000.0,1424254930000.0,1424184834000.0,1424188434000.0,1424251341000.0,1424254941000.0,"grenfro",1424254930000.0,1424184828000.0,45,1424184828000.0,-1.0,"Update payload-conversion demo to latest module spec","Upload payload conversion demo such that a user can use the module upload feature against the sample.",3.0,False,3.0,0,0.0,-1,0,False
"XD-2728","Bug","Sprint 43","2015-02-17T13:51:09.000+0000","iperumal","iperumal","Spark streaming processor module: Dispatcher has no subscribers","The spark streaming processor module emits the following exception when there are more messages in the RDD partitions:

015-02-17 12:45:02,026 1.2.0.SNAP ERROR Executor task launch worker-2 executor.Executor - Exception in task 0.0 in stage 56.0 (TID 142)
org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.send(MessageBusSender.java:105)
	at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:80)
	at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:55)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 17 more
2015-02-17 12:45:02,032 1.2.0.SNAP  WARN task-result-getter-1 scheduler.TaskSetManager - Lost task 0.0 in stage 56.0 (TID 142, localhost): org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.send(MessageBusSender.java:105)
	at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:80)
	at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:55)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 17 more
",3.0,0.0001277710343065,0.0,0.9984156391745992,True,True,True,1424181069000.0,1424181079000.0,1424184679000.0,1424258434000.0,1424259334000.0,1424181069000.0,-1.0,1424259210000.0,1424259334000.0,1424181069000.0,1424184669000.0,1424181079000.0,1424184679000.0,"iperumal",1424259334000.0,1424181069000.0,45,1424181069000.0,-1.0,"Spark streaming processor module: Dispatcher has no subscribers","The spark streaming processor module emits the following exception when there are more messages in the RDD partitions:

015-02-17 12:45:02,026 1.2.0.SNAP ERROR Executor task launch worker-2 executor.Executor - Exception in task 0.0 in stage 56.0 (TID 142)
org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.send(MessageBusSender.java:105)
	at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:80)
	at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:55)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 17 more
2015-02-17 12:45:02,032 1.2.0.SNAP  WARN task-result-getter-1 scheduler.TaskSetManager - Lost task 0.0 in stage 56.0 (TID 142, localhost): org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:239)
	at org.springframework.xd.dirt.plugins.spark.streaming.MessageBusSender.send(MessageBusSender.java:105)
	at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:80)
	at org.springframework.xd.spark.streaming.java.ModuleExecutor$1$1.call(ModuleExecutor.java:55)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
	at org.apache.spark.api.java.JavaRDDLike$$anonfun$foreachPartition$1.apply(JavaRDDLike.scala:195)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:790)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 17 more
",3.0,False,3.0,0,0.0,-1,0,False
"XD-2727","Bug","Sprint 43","2015-02-16T14:22:47.000+0000","iperumal","iperumal","Spark streaming integration with kafka message bus is broken","The property ""xd.messagebus.kafka.offsetStoreTopic"" was added to Kafka message bus which is not updated to Spark streaming message bus properties that will be transferred to spark cluster for streaming module deployment. 
We also need a better approach to re-use the message bus properties so that we don't have to update the properties in Connection Property Names.",3.0,0.0003382940315267,0.0003382940315267,0.4399613378249684,True,True,True,1424096567000.0,1424096686000.0,1424100286000.0,1424447432000.0,1424448332000.0,1424096567000.0,1424276444000.0,1424251330000.0,1424254930000.0,1424096686000.0,1424100286000.0,1424096686000.0,1424100286000.0,"iperumal",1424254930000.0,1424096567000.0,45,1424276444000.0,1424276444000.0,"Spark streaming integration with kafka message does not respect offsetStoreTopic config option","The property ""xd.messagebus.kafka.offsetStoreTopic"" was added to Kafka message bus which is not updated to Spark streaming message bus properties that will be transferred to spark cluster for streaming module deployment. 
We also need a better approach to re-use the message bus properties so that we don't have to update the properties in Connection Property Names.",1.0,True,1.0,1,200.0,-1,0,True
"XD-2723","Story","Sprint 44","2015-02-13T10:38:39.000+0000","thomas.risberg","","Increase the partitionResultsTimeout","The partitionResultsTimeout is set to 300000 as default (5min). This is way to short for long running steps. We should increase this default.",1.0,-1.0,0.9694020379217068,0.4906454428802689,False,True,True,1423823919000.0,-1.0,-1.0,1424681050000.0,1424681950000.0,1423823919000.0,-1.0,1424244908000.0,1424248508000.0,1424655696000.0,1424659296000.0,-1.0,-1.0,"thomas.risberg",1424248508000.0,1423823919000.0,45,1423823919000.0,-1.0,"Increase the partitionResultsTimeout","The partitionResultsTimeout is set to 300000 as default (5min). This is way to short for long running steps. We should increase this default.",1.0,False,1.0,0,0.0,0,0,False
"XD-2719","Story","Sprint 43","2015-02-13T07:38:47.000+0000","sabby","","Invoke Rabbit REST-API to clean-up resources","As a user, I'd like to clean-up stale queues/topics associated with the stream so when the stream gets destroyed I can clean-up resources. ",3.0,0.5947578652133512,0.5947578652133512,0.0,True,True,True,1423813127000.0,1424685522000.0,1424689122000.0,1425279034000.0,1425279934000.0,1423813127000.0,-1.0,1423813127000.0,1423816727000.0,1424685522000.0,1424689122000.0,1424685522000.0,1424689122000.0,"sabby",1423816727000.0,1423813127000.0,45,1423813127000.0,-1.0,"Invoke Rabbit REST-API to clean-up resources","As a user, I'd like to clean-up stale queues/topics associated with the stream so when the stream gets destroyed I can clean-up resources. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-2717","Story","Sprint 50","2015-02-12T06:56:25.000+0000","grussell","hillert","Add nameExpression Property to File Sink","As a stream definer, when defining a stream ending with a file sink, I would like to have more flexibility for naming the file.

Add an alternative {{--nameExpression}} option, allowing complete control over the {{finename-generator-expression}} attribute.

See: http://stackoverflow.com/questions/28466477/issue-with-file-sink-and-filename-expression/28467069#28467069",1.0,0.9904182289039722,0.9904139330841244,0.9904127214426288,True,True,True,1423724185000.0,1432715789000.0,1432719389000.0,1432801878000.0,1432802778000.0,1423724185000.0,-1.0,1432715739000.0,1432719339000.0,1432715750000.0,1432719350000.0,1432715789000.0,1432719389000.0,"grussell",1432719339000.0,1423724185000.0,45,1423724185000.0,-1.0,"Add nameExpression Property to File Sink","As a stream definer, when defining a stream ending with a file sink, I would like to have more flexibility for naming the file.

Add an alternative {{--nameExpression}} option, allowing complete control over the {{finename-generator-expression}} attribute.

See: http://stackoverflow.com/questions/28466477/issue-with-file-sink-and-filename-expression/28467069#28467069",1.0,False,1.0,0,0.0,0,0,False
"XD-2716","Story","Sprint 43","2015-02-11T14:48:09.000+0000","sabby","","Create a Batch example to demonstrate HDBC->HDFS","As a developer, I'd like to create a example to demonstrate JDBC to HDFS data movement and also experiment with Avro or Parquet formats. ",8.0,0.0444096461008456,0.0444096461008456,0.0,True,True,True,1423666089000.0,1424248887000.0,1424252487000.0,1436788419000.0,1436789319000.0,1423666089000.0,-1.0,1423666089000.0,1423669689000.0,1424248887000.0,1424252487000.0,1424248887000.0,1424252487000.0,"sabby",1423669689000.0,1423666089000.0,45,1423666089000.0,-1.0,"Create a Batch example to demonstrate JDBC->HDFS","As a developer, I'd like to create a example to demonstrate JDBC to HDFS data movement.",8.0,False,8.0,0,0.0,-1,0,True
"XD-2715","Story","Sprint 43","2015-02-11T14:45:46.000+0000","sabby","","Add Smart Grid demo to XD samples repo","As a PM, I'd like to have the Smart Grid demo (from s1-2014) ported into Spring XD samples repo.",8.0,0.4315339331334,0.4315339331334,0.0,True,True,True,1423665946000.0,1424248711000.0,1424252311000.0,1425015496000.0,1425016396000.0,1423665946000.0,-1.0,1423665946000.0,1423669546000.0,1424248711000.0,1424252311000.0,1424248711000.0,1424252311000.0,"sabby",1423669546000.0,1423665946000.0,45,1423665946000.0,-1.0,"Add Smart Grid demo to XD samples repo","As a PM, I'd like to have the Smart Grid demo (from s1-2014) ported into Spring XD samples repo.",8.0,False,8.0,0,0.0,-1,0,False
"XD-2713","Story","Sprint 43","2015-02-11T12:45:10.000+0000","sabby","","Replicate Storm examples in XD","As a field engineer, I'd like to have a comparison of Storm examples in Spring XD so that it is easy to relate from implementation standpoint.
",8.0,0.5357211944367609,0.5357211944367609,0.0,True,True,True,1423658710000.0,1424251358000.0,1424254958000.0,1424764072000.0,1424764972000.0,1423658710000.0,-1.0,1423658710000.0,1423662310000.0,1424251358000.0,1424254958000.0,1424251358000.0,1424254958000.0,"sabby",1423662310000.0,1423658710000.0,45,1423658710000.0,-1.0,"Replicate Storm examples in XD","As a field engineer, I'd like to have a comparison of Storm examples in Spring XD so that it is easy to relate from implementation standpoint.
",8.0,False,8.0,0,0.0,-1,2,False
"XD-2712","Story","Sprint 43","2015-02-11T06:56:00.000+0000","dturanski","dturanski","Embed local message bus in DIRT as default for singlenode","Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus-local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application (local is not a valid transport for distributed) eliminates this step.",2.0,0.6898546382033311,6.699261619564767e-06,3.410533188142063e-05,True,True,True,1423637760000.0,1424770482000.0,1424774082000.0,1425278832000.0,1425279732000.0,1423637760000.0,-1.0,1423637816000.0,1423641416000.0,1423637771000.0,1423641371000.0,1424770482000.0,1424774082000.0,"dturanski",1423641416000.0,1423637760000.0,45,1423637760000.0,-1.0,"Embed local message bus in DIRT as default for singlenode","Currently all message bus implementations are removed from the runtime classpath and loaded on demand from the file system according to the transport setting. Custom module projects that include in container testing must install messagebus-local on the local file system. This is currently configured as a task for module build scripts. This is also a dependency for testing in the IDE and developers need to execute the build task or configure the messagebus manually. Embedding the local MB for the singlenode application (local is not a valid transport for distributed) eliminates this step.",2.0,False,2.0,0,0.0,0,0,False
"XD-2710","Bug","Sprint 43","2015-02-10T15:44:12.000+0000","mbogoevici","mbogoevici","Kafka Tests shouldn't assume offset 0 ","In `org.springframework.xd.dirt.stream.KafkaSingleNodeStreamDeploymentIntegrationTests#verifyOnDemandQueues`, when testing the queue partitions for content, the read is assumed to start at offset 0.

This is incorrect, because the topics may exist already, especially in a CI environment",1.0,0.0006469561460259,0.0006423596547575,0.99699504383329,True,True,True,1423583052000.0,1423583615000.0,1423587215000.0,1424452381000.0,1424453281000.0,1423583052000.0,-1.0,1424450666000.0,1424453281000.0,1423583611000.0,1423587211000.0,1423583615000.0,1423587215000.0,"mbogoevici",1424453281000.0,1423583609000.0,45,1423583609000.0,-1.0,"Kafka Tests shouldn't assume offset 0 ","In `org.springframework.xd.dirt.stream.KafkaSingleNodeStreamDeploymentIntegrationTests#verifyOnDemandQueues`, when testing the queue partitions for content, the read is assumed to start at offset 0.

This is incorrect, because the topics may exist already, especially in a CI environment",1.0,False,1.0,0,0.0,-1,0,False
"XD-2706","Story","Sprint 43","2015-02-09T09:56:12.000+0000","grussell","","Improve Redis Bus Error Log Entry","{code}
logger.error(""Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'"",										context.getLastThrowable());
{code}

Should be:

{code}
logger.error(""Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:"" + name + '"",										context.getLastThrowable());
{code}

So the actual name is logged.
",1.0,0.9936658212970848,0.993662436207996,0.000138111634822,True,True,True,1423475772000.0,1424943482000.0,1424947082000.0,1424951938000.0,1424952838000.0,1423475772000.0,-1.0,1423475976000.0,1423479576000.0,1424943477000.0,1424947077000.0,1424943482000.0,1424947082000.0,"grussell",1423479576000.0,1423475772000.0,45,1423475772000.0,-1.0,"Improve Redis Bus Error Log Entry","{code}
logger.error(""Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'"",										context.getLastThrowable());
{code}

Should be:

{code}
logger.error(""Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:"" + name + '"",										context.getLastThrowable());
{code}

So the actual name is logged.
",1.0,False,1.0,0,0.0,-1,0,False
"XD-2704","Story","Sprint 45","2015-02-09T03:00:04.000+0000","eric.bottard","eric.bottard","Move documentation from wiki to main repo","As a consequence, 
* change gradle script regarding generation of documentation
* remove pushGeneratedDocs task, etc
* remove link rewriting that is no longer needed ",8.0,0.7850652535088095,0.0,0.3826198241135598,True,True,True,1423450804000.0,1425976954000.0,1425980554000.0,1426667662000.0,1426668562000.0,1423450804000.0,1425888426000.0,1424681982000.0,1424685582000.0,1423450804000.0,1423454404000.0,1425976954000.0,1425980554000.0,"eric.bottard",1424685582000.0,1423450804000.0,45,1425888426000.0,1425888426000.0,"Move documentation from wiki to main repo","As a consequence, 
* change gradle script regarding generation of documentation
* remove pushGeneratedDocs task, etc
* remove link rewriting that is no longer needed ",5.0,True,5.0,1,60.0,-1,0,False
"XD-2700","Story","Sprint 42","2015-02-06T12:55:21.000+0000","dturanski","dturanski","Remove spark and hadoop dependencies from custom module classpath","remove spark and hadoop requirements from spring-xd-module-parent and gradle module plugin",2.0,0.3188864700837786,0.3188864700837786,0.3175671218418101,True,True,True,1423227321000.0,1423232155000.0,1423235755000.0,1423241580000.0,1423242480000.0,1423227321000.0,-1.0,1423232135000.0,1423235735000.0,1423232155000.0,1423235755000.0,1423232155000.0,1423235755000.0,"dturanski",1423235735000.0,1423227321000.0,45,1423227321000.0,-1.0,"Remove spark and hadoop dependencies from custom module classpath","remove spark and hadoop requirements from spring-xd-module-parent and gradle module plugin",2.0,False,2.0,0,0.0,-1,0,False
"XD-2699","Story","Sprint 42","2015-02-06T11:23:09.000+0000","sabby","grussell","Review and fix Sonar violations","As a developer, I'd like to review the current sonar violations so that I can fix the relevant and update the irrelevant ones as invalid.",3.0,0.8412698412698413,0.5079365079365079,0.4698412698412698,False,False,False,1423221789000.0,1423222054000.0,1423222104000.0,1423221789000.0,1423222104000.0,1423221789000.0,-1.0,1423221937000.0,1423222104000.0,1423221949000.0,1423222104000.0,1423222054000.0,1423222104000.0,"sabby",1423222104000.0,1423221789000.0,45,1423221789000.0,-1.0,"Review and fix Sonar violations","As a developer, I'd like to review the current sonar violations so that I can fix the relevant and update the irrelevant ones as invalid.",3.0,False,3.0,0,0.0,-1,0,False
"XD-2698","Story","Sprint 42","2015-02-06T05:57:48.000+0000","mbogoevici","mbogoevici","Kafka Tests should use an external broker","As a developer, I want to have to run Kafka tests on an external broker, so that I reduce the footprint of the build process. ",2.0,0.198969558719421,0.198969558719421,0.1975930150239911,True,True,True,1423202268000.0,1423207327000.0,1423210927000.0,1423226794000.0,1423227694000.0,1423202268000.0,-1.0,1423207292000.0,1423210892000.0,1423207327000.0,1423210927000.0,1423207327000.0,1423210927000.0,"mbogoevici",1423210892000.0,1423202268000.0,45,1423202268000.0,-1.0,"Kafka Tests should use an external broker","As a developer, I want to have to run Kafka tests on an external broker, so that I reduce the footprint of the build process. ",2.0,False,2.0,0,0.0,-1,0,False
"XD-2696","Story","Sprint 42","2015-02-05T14:35:08.000+0000","mark.pollack","","Downgrade reactor based modules to reactor 1.x GA","Can revert part of the commit that went into upgrading to reactor 2.0

https://github.com/spring-projects/spring-xd/pull/1342/files
",1.0,0.8113128256849891,0.7943231755637202,0.0,True,True,True,1423146908000.0,1423214479000.0,1423218079000.0,1423229294000.0,1423230194000.0,1423146908000.0,-1.0,1423146908000.0,1423150508000.0,1423213064000.0,1423216664000.0,1423214479000.0,1423218079000.0,"mark.pollack",1423150508000.0,1423146908000.0,45,1423146908000.0,-1.0,"Downgrade reactor based modules to reactor 1.x GA","Can revert part of the commit that went into upgrading to reactor 2.0

https://github.com/spring-projects/spring-xd/pull/1342/files
",1.0,False,1.0,0,0.0,-1,0,False
"XD-2694","Story","Sprint 46","2015-02-05T09:53:28.000+0000","mbogoevici","","Spring XD module parent should use dependencies.properties for versioning","As a developer, I'd like the {{publish-maven.gradle}} script to use values for dependencies (e.g. Spring Boot and {{hadoop-common}}) from our central dependency list (in this case {{dependencies.properties}}) so that I don't have to update them manually anymore.",2.0,-1.0,0.219237549983052,0.2186399815800384,False,True,True,1423130008000.0,-1.0,-1.0,1434977124000.0,1434978024000.0,1423130008000.0,-1.0,1425720458000.0,1425724058000.0,1425727538000.0,1425731138000.0,-1.0,-1.0,"mbogoevici",1425724058000.0,1423130008000.0,45,1423130008000.0,-1.0,"Spring XD module parent should use dependencies.properties for versioning","As a developer, I'd like the {{publish-maven.gradle}} script to use values for dependencies (e.g. Spring Boot and {{hadoop-common}}) from our central dependency list (in this case {{dependencies.properties}}) so that I don't have to update them manually anymore.",2.0,False,2.0,0,0.0,0,0,False
"XD-2691","Story","Sprint 49","2015-02-05T08:35:02.000+0000","hillert","dturanski","Upgrade to the latest Gradle 2.x Release","Gradle 2.x is required for the latest Sonar version (sonar.spring.io)

We may need to wait for a fix in Groovy itself (2.4.1) - Please see the following links for details:

* http://forums.gradle.org/gradle/topics/after_upgrade_gradle_to_2_0_version_the_maven_pom_not_support_build_property

* http://jira.codehaus.org/browse/GROOVY-7023

",5.0,0.9675110195806608,0.9675095896656228,0.9675137602511504,True,True,True,1423125302000.0,1431244758000.0,1431248358000.0,1431516509000.0,1431517409000.0,1423125302000.0,1431326653000.0,1431244781000.0,1431248381000.0,1431244746000.0,1431248346000.0,1431244758000.0,1431248358000.0,"hillert",1431248381000.0,1423125302000.0,45,1431326653000.0,1431326653000.0,"Upgrade to the latest Gradle 2.x Release","Gradle 2.x is required for the latest Sonar version (sonar.spring.io)

We may need to wait for a fix in Groovy itself (2.4.1) - Please see the following links for details:

* http://forums.gradle.org/gradle/topics/after_upgrade_gradle_to_2_0_version_the_maven_pom_not_support_build_property

* http://jira.codehaus.org/browse/GROOVY-7023

",4.0,True,4.0,1,25.0,-1,0,False
"XD-2690","Story","Sprint 42","2015-02-05T07:44:31.000+0000","dturanski","dturanski","Restrict entry filter in loading module artifacts","see https://github.com/spring-projects/spring-boot/issues/2454",1.0,0.1357934711726571,0.0,0.1349879166826499,True,True,True,1423122271000.0,1423125811000.0,1423129411000.0,1423147440000.0,1423148340000.0,1423122271000.0,-1.0,1423125790000.0,1423129390000.0,1423122271000.0,1423125871000.0,1423125811000.0,1423129411000.0,"dturanski",1423129390000.0,1423122271000.0,45,1423122271000.0,-1.0,"Restrict entry filter in loading module artifacts","see https://github.com/spring-projects/spring-boot/issues/2454",1.0,False,1.0,0,0.0,-1,0,False
"XD-2687","Story","Sprint 42","2015-02-04T12:54:00.000+0000","eric.bottard","","Fix #jsonPath evaluation following JsonPath version upgrade","",1.0,0.3625887106295339,0.3625887106295339,0.0233566396872663,True,True,True,1423054440000.0,1423109721000.0,1423113321000.0,1423206002000.0,1423206902000.0,1423054440000.0,-1.0,1423058001000.0,1423061601000.0,1423109721000.0,1423113321000.0,1423109721000.0,1423113321000.0,"eric.bottard",1423061601000.0,1423054440000.0,45,1423054440000.0,-1.0,"Fix #jsonPath evaluation following JsonPath version upgrade","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2686","Story","Sprint 42","2015-02-04T12:53:17.000+0000","sabby","","Update log4j properties to include DATE in the logs","As a user, I'd like to see the 'date' in logs so that I can troubleshoot issues that had occurred on a specific day and time.

Property that needs adjusted:
https://github.com/spring-projects/spring-xd/blob/master/config/xd-container-logger.properties#L11",1.0,0.7259168733166431,0.725845682894128,0.0,True,True,True,1423054397000.0,1423115578000.0,1423119178000.0,1423137778000.0,1423138678000.0,1423054397000.0,-1.0,1423054397000.0,1423057997000.0,1423115572000.0,1423119172000.0,1423115578000.0,1423119178000.0,"sabby",1423057997000.0,1423054397000.0,45,1423054397000.0,-1.0,"Update log4j properties to include DATE in the logs","As a user, I'd like to see the 'date' in logs so that I can troubleshoot issues that had occurred on a specific day and time.

Property that needs adjusted:
https://github.com/spring-projects/spring-xd/blob/master/config/xd-container-logger.properties#L11",1.0,False,1.0,0,0.0,-1,0,False
"XD-2684","Story","Sprint 42","2015-02-04T07:25:56.000+0000","mark.pollack","","Set sourceCompatibility to JDK 1.7","Min JDK version for XD 1.1 is 7.  Change the sourceCompatibility to 1.7 but leave targetCompatibility at 1.6.   Changes are also needed in the mdule parent pom and gradle plugins.",1.0,0.1327028353168685,0.1320101213211595,0.0,True,True,True,1423034756000.0,1423048549000.0,1423052149000.0,1423137795000.0,1423138695000.0,1423034756000.0,-1.0,1423034756000.0,1423038356000.0,1423048477000.0,1423052077000.0,1423048549000.0,1423052149000.0,"mark.pollack",1423038356000.0,1423034756000.0,45,1423034756000.0,-1.0,"Set sourceCompatibility to JDK 1.7","Min JDK version for XD 1.1 is 7.  Change the sourceCompatibility to 1.7 but leave targetCompatibility at 1.6.   Changes are also needed in the mdule parent pom and gradle plugins.",1.0,False,1.0,0,0.0,0,0,False
"XD-2683","Bug","Sprint 42","2015-02-03T18:17:59.000+0000","iperumal","iperumal","Message conversion support for spark streaming module","The spark streaming module needs to support inputType (for both processor and sink module) and outputType (for processor module) so that message conversion can happen at the MessageBusReceiver and MessageBusSender.",5.0,0.3274973504920515,0.0,0.4025677517032551,True,True,True,1422987479000.0,1423041557000.0,1423045157000.0,1423151704000.0,1423152604000.0,1422987479000.0,1423058303000.0,1423053953000.0,1423057553000.0,1422987479000.0,1422991079000.0,1423041557000.0,1423045157000.0,"iperumal",1423057553000.0,1422987479000.0,45,1423058303000.0,1423058303000.0,"Message conversion support for spark streaming module","The spark streaming module needs to support inputType (for both processor and sink module) and outputType (for processor module) so that message conversion can happen at the MessageBusReceiver and MessageBusSender.",6.0,True,6.0,1,-16.666666666666664,-1,0,False
"XD-2681","Story","Sprint 42","2015-02-02T12:16:06.000+0000","sabby","","Document recommended 'ulimit' setting for XD","As a developer, I'd like to refer to _ulimit_ section in wiki so that I can configure machines with recommended setting to deploy XD's distributed setup.

*Note:*
Recommended _ulimit_ setting is 10K under ""Troubleshooting"" (new) section

*Exception:* (reason to increase _ulimit_)
8:25:52,266 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module
java.lang.IllegalStateException: java.io.FileNotFoundException: /var/vcap/data/packages/springxd/ee02bd3482eeb620a65862fb54e1f23fcece8022.1-bd
a341640a5de2f922fd3db906ce504b85819c1e/spring-xd-1.1.0.BUILD-SNAPSHOT/xd/config/modules/modules.yml (Too many open files)",1.0,0.9840130911366106,0.9840130911366106,0.0,True,True,True,1422879366000.0,1423124709000.0,1423128309000.0,1423127795000.0,1423128695000.0,1422879366000.0,-1.0,1422879366000.0,1422882966000.0,1423124709000.0,1423128309000.0,1423124709000.0,1423128309000.0,"sabby",1422882966000.0,1422879366000.0,45,1422879366000.0,-1.0,"Document recommended 'ulimit' setting for XD","As a developer, I'd like to refer to wiki so that I can configure machines with recommended _ulimit_ setting for XD's distributed setup.

*Note:*
Recommended _ulimit_ setting is 10K under ""Troubleshooting"" (new) section

*Exception:* (reason to increase _ulimit_)
8:25:52,266 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module
java.lang.IllegalStateException: java.io.FileNotFoundException: /var/vcap/data/packages/springxd/ee02bd3482eeb620a65862fb54e1f23fcece8022.1-bd
a341640a5de2f922fd3db906ce504b85819c1e/spring-xd-1.1.0.BUILD-SNAPSHOT/xd/config/modules/modules.yml (Too many open files)",1.0,False,1.0,0,0.0,0,0,True
"XD-2676","Story","Sprint 42","2015-02-02T10:32:44.000+0000","thomas.risberg","","Resolve classloading issues for custom Hadoop based batch jobs","There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.",8.0,0.9996971401375848,0.9996971401375848,0.0768182408168563,True,True,True,1422873164000.0,1423058012000.0,1423058068000.0,1423057168000.0,1423058068000.0,1422873164000.0,1423306603000.0,1422887368000.0,1422890968000.0,1423058012000.0,1423058068000.0,1423058012000.0,1423058068000.0,"thomas.risberg",1422890968000.0,1422873164000.0,45,1422873164000.0,1423306603000.0,"Resolve classloading issues for custom Hadoop based batch jobs","There are several issues making it hard to impossible to create batch jobs that use Pig, Hive, HBase or other technologies supported by Spring for Apache Hadoop project. We need to make the corresponding dependencies available on the Hadoop classpath.",3.0,True,3.0,1,0.0,0,0,False
"XD-2674","Improvement","Sprint 42","2015-02-02T07:17:42.000+0000","abilan","","Provide more options for the MongoDB Sink","See the SO question on the matter: http://stackoverflow.com/questions/28280206/how-can-i-use-authentication-in-mongo-sink",1.0,0.477595552131181,0.4082788132931357,0.0029584448566818,True,True,True,1422861462000.0,1423039363000.0,1423042963000.0,1423233055000.0,1423233955000.0,1422861462000.0,-1.0,1422862564000.0,1422866164000.0,1423013543000.0,1423017143000.0,1423039363000.0,1423042963000.0,"abilan",1422866164000.0,1422861462000.0,45,1422861462000.0,-1.0,"Provide more options for the MongoDB Sink","See the SO question on the matter: http://stackoverflow.com/questions/28280206/how-can-i-use-authentication-in-mongo-sink",1.0,False,1.0,0,0.0,-1,0,False
"XD-2672","Story","Sprint 42","2015-01-30T15:34:00.000+0000","iperumal","iperumal","Add scala support for spark streaming module","As a scala developer, someone could easily deploy the spark streaming module developed using scala.
  ",8.0,4.039453580737984e-05,0.0,0.0001520735465689,True,True,True,1422632040000.0,1422632057000.0,1422635657000.0,1423051989000.0,1423052889000.0,1422632040000.0,-1.0,1422632104000.0,1422635704000.0,1422632040000.0,1422635640000.0,1422632057000.0,1422635657000.0,"iperumal",1422635704000.0,1422632050000.0,45,1422632050000.0,-1.0,"Add scala support for spark streaming module","As a scala developer, someone could easily deploy the spark streaming module developed using scala.
  ",8.0,False,8.0,0,0.0,0,0,False
"XD-2671","Bug","Sprint 44","2015-01-30T14:04:14.000+0000","grenfro","iperumal","Modules left orphaned after long succession of stream creates and destroys","OS: Ubuntu
Deployment Admin Server.  1 Container
Transport Rabbit

* Using the shell execute the file.  ./bin/shell --cmdfile loop.txt  
** This should run successfully
* Rerun the same script ./bin/shell --cmdfile loop.txt
** it states that the stream foo1 is already deployed. 
** Open shell app and destroy foo1 and run a stream list.  No streams will display
** Exit shell and rerun the file.  
*** Another stream will fail to create stating  stream already exists.

* If you look at the modules deployed on the container some are still present.
* To clear you have to shutdown the cluster and flush the xd directories on Zookeeper.
* Note that if I shutdown the container the modules disappear.

 ",8.0,0.6150257922989124,0.6146336430553071,0.6146300699414702,True,True,True,1422626654000.0,1425380670000.0,1425384270000.0,1427103641000.0,1427104541000.0,1422626654000.0,1425889453000.0,1425378898000.0,1425382498000.0,1425378914000.0,1425382514000.0,1425380670000.0,1425384270000.0,"grenfro",1425382498000.0,1422626654000.0,45,1426075962000.0,1425889453000.0,"Support rapid creation and deletion of streams","OS: Ubuntu
Deployment Admin Server.  1 Container
Transport Rabbit

* Using the shell execute the file.  ./bin/shell --cmdfile loop.txt  
** This should run successfully
* Rerun the same script ./bin/shell --cmdfile loop.txt
** it states that the stream foo1 is already deployed. 
** Open shell app and destroy foo1 and run a stream list.  No streams will display
** Exit shell and rerun the file.  
*** Another stream will fail to create stating  stream already exists.

* If you look at the modules deployed on the container some are still present.
* To clear you have to shutdown the cluster and flush the xd directories on Zookeeper.
* Note that if I shutdown the container the modules disappear.

 ",3.0,True,3.0,2,166.66666666666669,0,1,True
"XD-2670","Story","Sprint 42","2015-01-30T08:36:22.000+0000","grussell","","Bump Spring AMQP to 1.4.3","When available",1.0,0.9998438578016632,0.9998438578016632,0.0,True,True,True,1422606982000.0,1422882329000.0,1422882372000.0,1422881472000.0,1422882372000.0,1422606982000.0,-1.0,1422606982000.0,1422610582000.0,1422882329000.0,1422882372000.0,1422882329000.0,1422882372000.0,"grussell",1422610582000.0,1422606982000.0,45,1422606982000.0,-1.0,"Bump Spring AMQP to 1.4.3","When available",1.0,False,1.0,0,0.0,0,0,False
"XD-2668","Bug","","2015-01-29T12:41:15.000+0000","hlagos","","Can't execute query on Hive","Hey Guys,

We are currently trying to connect Hive by spring-xd to create a table.  Unfortunatly we are not able to do it. We are using the attached job configuration (hive.xml) trying to connect by Hive Thrift Client with the following configuration:

<hadoop:hive-client-factory host=""hive_host"" port=""10000"" />
<hadoop:hive-tasklet id=""hive-script"">
   <hadoop:script>
     USE TESTING;
     DROP TABLE IF EXITS testHiveBatchTable; 
     CREATE TABLE testHiveBatchTable (key int, value string);
   </hadoop:script>
</hadoop:hive-tasklet>

For Some reason spring XD is not finding the TTransportException class. The full log is in the singlenode_hivequery.log  file  attached. After that we tried uploading all the dependencies to the lib folder of the job, but now we are facing problems with log4j (full log is in singlenode_hivequery2.log attached) and we are unable to display the work in any of the two cases. Please,  Could you help us to solve this problem? 

Thanks in advance.",3.0,-1.0,-1.0,-1.0,False,False,False,1422535275000.0,-1.0,-1.0,1424450175000.0,1424451075000.0,1422535275000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hlagos",-1.0,-1.0,0,1422535275000.0,-1.0,"Can't execute query on Hive","Hey Guys,

We are currently trying to connect Hive by spring-xd to create a table.  Unfortunatly we are not able to do it. We are using the attached job configuration (hive.xml) trying to connect by Hive Thrift Client with the following configuration:

<hadoop:hive-client-factory host=""hive_host"" port=""10000"" />
<hadoop:hive-tasklet id=""hive-script"">
   <hadoop:script>
     USE TESTING;
     DROP TABLE IF EXITS testHiveBatchTable; 
     CREATE TABLE testHiveBatchTable (key int, value string);
   </hadoop:script>
</hadoop:hive-tasklet>

For Some reason spring XD is not finding the TTransportException class. The full log is in the singlenode_hivequery.log  file  attached. After that we tried uploading all the dependencies to the lib folder of the job, but now we are facing problems with log4j (full log is in singlenode_hivequery2.log attached) and we are unable to deploy the job in any of the two cases. Please,  Could you help us to solve this problem? 

Thanks in advance.",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-2667","Story","Sprint 42","2015-01-29T11:22:48.000+0000","sabby","","Upgrade to SHDP GA Release","As a developer, I'd like to upgrade to SHDP GA release so that I can sync -up with the latest bits. ",1.0,0.7030459106651274,0.7030459106651274,0.0,True,True,True,1422530568000.0,1422958086000.0,1422961686000.0,1423137762000.0,1423138662000.0,1422530568000.0,-1.0,1422530568000.0,1422534168000.0,1422958086000.0,1422961686000.0,1422958086000.0,1422961686000.0,"sabby",1422534168000.0,1422530568000.0,45,1422530568000.0,-1.0,"Upgrade to SHDP GA Release","As a developer, I'd like to upgrade to SHDP GA release so that I can sync -up with the latest bits. ",1.0,False,1.0,0,0.0,-1,0,False
"XD-2666","Story","Sprint 42","2015-01-29T11:22:15.000+0000","sabby","","Upgrade to SI Kafka GA release","As a developer, I'd like to upgrade to Kafka's SI GA release so that I can sync -up with the latest bits. ",5.0,0.5209128677296205,0.5209128677296205,0.0,True,True,True,1422530535000.0,1422883118000.0,1422886718000.0,1423206491000.0,1423207391000.0,1422530535000.0,1422958342000.0,1422530535000.0,1422534135000.0,1422883118000.0,1422886718000.0,1422883118000.0,1422886718000.0,"sabby",1422534135000.0,1422530535000.0,45,1422958342000.0,1422958342000.0,"Upgrade to SI Kafka GA release","As a developer, I'd like to upgrade to Kafka's SI GA release so that I can sync -up with the latest bits. 

The scope is to backport Kafka XD changes to SI Kafka and then upgrade to the GA release.",1.0,True,1.0,1,400.0,-1,0,True
"XD-2665","Story","Sprint 42","2015-01-29T11:20:37.000+0000","sabby","","Update copyright message in PDF from 2014 to 2015","As a PM, I'd like to have the copyright message in the reference guide (PDF) updated to include 2015 instead of 2014. ",1.0,0.9959515208017714,0.9959515208017714,0.0,True,True,True,1422530437000.0,1422957750000.0,1422959487000.0,1422958587000.0,1422959487000.0,1422530437000.0,-1.0,1422530437000.0,1422534037000.0,1422957750000.0,1422959487000.0,1422957750000.0,1422959487000.0,"sabby",1422534037000.0,1422530437000.0,45,1422530437000.0,-1.0,"Update copyright message in PDF from 2014 to 2015","As a PM, I'd like to have the copyright message in the reference guide (PDF) updated to include 2015 instead of 2014. ",1.0,False,1.0,0,0.0,0,0,False
"XD-2664","Story","Sprint 42","2015-01-29T11:13:00.000+0000","sabby","","Create samples to demonstrate new features and functionalities","As a developer, I'd like to build samples using XD so that we can demonstrate some of the new features included in 1.1 release.

*Areas to consider*
* Spark streaming
* Spark (MLLIb, ..)
* Sqoop
* Reactive-streams
",8.0,0.7080858036191271,0.7080858036191271,0.0,True,True,True,1422529980000.0,1424249087000.0,1424252687000.0,1424956903000.0,1424957803000.0,1422529980000.0,-1.0,1422529980000.0,1422533580000.0,1424249087000.0,1424252687000.0,1424249087000.0,1424252687000.0,"sabby",1422533580000.0,1422529980000.0,45,1422529980000.0,-1.0,"Create Spark Streaming example","As a developer, I'd like to build _Spark Streaming_ as data processors in XD so that we can demonstrate some of the capabilities.

*Implement using:*
* Java / Java Lambdas
* Scala",8.0,False,8.0,0,0.0,0,0,True
"XD-2661","Story","Sprint 42","2015-01-29T09:53:30.000+0000","sabby","","Add build support for XD in Windows","As a user, I'd like to build XD in Windows machine so that I can develop, test,  and contributed to OSS.",5.0,0.0368083730501494,0.0367735812120363,0.0,True,True,True,1422525210000.0,1422604557000.0,1422608157000.0,1424679988000.0,1424680888000.0,1422525210000.0,-1.0,1422525210000.0,1422528810000.0,1422604482000.0,1422608082000.0,1422604557000.0,1422608157000.0,"sabby",1422528810000.0,1422525210000.0,45,1422525210000.0,-1.0,"Add build support for XD in Windows","As a user, I'd like to build XD in Windows machine so that I can develop, test,  and contributed to OSS.",5.0,False,5.0,0,0.0,0,0,False
"XD-2656","Story","Sprint 43","2015-01-27T15:42:29.000+0000","sabby","","Create a sample to invoke Pig script/job from XD","As a user, I'd like to refer to a Pig script/job sample so that I can use that as a reference to integrate Pig jobs in XD.",5.0,0.2681989080328244,0.2681989080328244,0.1428155752443848,True,True,True,1422373349000.0,1422537417000.0,1422541017000.0,1422984189000.0,1422985089000.0,1422373349000.0,1423306588000.0,1422460715000.0,1422464315000.0,1422537417000.0,1422541017000.0,1422537417000.0,1422541017000.0,"sabby",1422464315000.0,1422373349000.0,45,1422373349000.0,1423306588000.0,"Create a sample to invoke Pig script/job from XD","As a user, I'd like to refer to a Pig script/job sample so that I can use that as a reference to integrate Pig jobs in XD.",3.0,True,3.0,1,0.0,0,0,False
"XD-2655","Improvement","Sprint 42","2015-01-27T15:16:23.000+0000","iperumal","iperumal","Make deployment timeout configurable","Currently, ModuleDeploymentWriter uses default timeout 30s which can not be overridden. We need to make this configurable.",1.0,0.9075626933456196,0.0,0.0007371815967879,True,True,True,1422371783000.0,1422440726000.0,1422444326000.0,1422446848000.0,1422447748000.0,1422371783000.0,-1.0,1422371839000.0,1422375439000.0,1422371783000.0,1422375383000.0,1422440726000.0,1422444326000.0,"iperumal",1422375439000.0,1422371783000.0,45,1422371783000.0,-1.0,"Make deployment timeout configurable","Currently, ModuleDeploymentWriter uses default timeout 30s which can not be overridden. We need to make this configurable.",1.0,False,1.0,0,0.0,0,0,False
"XD-2654","Story","Sprint 42","2015-01-27T14:16:21.000+0000","mark.pollack","","Update com.jayway.jsonpath to latest version","Use latest version, my need to exclude version from other dependencies, e.g. SI, in build-common.gradle.",1.0,0.8141022173233884,0.2578458355028705,3.550151811253643e-05,True,True,True,1422368181000.0,1422849742000.0,1422853342000.0,1422958805000.0,1422959705000.0,1422368181000.0,1422521417000.0,1422368202000.0,1422371802000.0,1422520703000.0,1422524303000.0,1422849742000.0,1422853342000.0,"mark.pollack",1422371802000.0,1422368181000.0,45,1422521417000.0,1422521417000.0,"Update com.jayway.jsonpath to latest version","Use latest version, might need to exclude version from other dependencies, e.g. SI, in build-common.gradle.",2.0,True,2.0,1,-50.0,-1,0,True
"XD-2652","Story","Sprint 42","2015-01-27T11:31:31.000+0000","sabby","","Document migration strategy for custom modules (from 1.0 to 1.1)","As a user, I'd like to migrate from 1.0 to 1.1 and be able to port my custom modules so that I can operationalize existing data pipelines and also take advantage of latest XD features.",2.0,0.9168145893831586,0.9168145893831586,0.0,True,True,True,1422358291000.0,1424252275000.0,1424255875000.0,1424423222000.0,1424424122000.0,1422358291000.0,-1.0,1422358291000.0,1422361891000.0,1424252275000.0,1424255875000.0,1424252275000.0,1424255875000.0,"sabby",1422361891000.0,1422358291000.0,45,1422358291000.0,-1.0,"Document migration strategy for custom modules (from 1.0 to 1.1)","As a user, I'd like to migrate from 1.0 to 1.1 and be able to port my custom modules so that I can operationalize existing data pipelines and also take advantage of latest XD features.",2.0,False,2.0,0,0.0,-1,0,False
"XD-2651","Story","Sprint 41","2015-01-27T07:07:07.000+0000","mark.pollack","mbogoevici","Add request/reply support to Kafka Message Bus","The bindRequestor and bindReplier methods of the message bus need to be implemented.",5.0,0.0055664302577681,0.0,0.0,True,True,True,1422342427000.0,1422357439000.0,1422361039000.0,1425038408000.0,1425039308000.0,1422342427000.0,1422457068000.0,1422342427000.0,1422346027000.0,1422342427000.0,1422346027000.0,1422357439000.0,1422361039000.0,"mark.pollack",1422346027000.0,1422342427000.0,45,1422457068000.0,1422457068000.0,"Spike: Research request/reply support to Kafka Message Bus","The scope is to research the available options to provide request/reply support for Kafka. 

* Document findings
* POCs

Previous Desc:
The bindRequestor and bindReplier methods of the message bus need to be implemented.",2.0,True,2.0,1,150.0,0,0,True
"XD-2649","Story","Sprint 42","2015-01-26T22:56:01.000+0000","mbogoevici","mbogoevici","Add Kafka-based implementation for AbstractSingleNodeStreamDeploymentIntegrationTests","",2.0,0.0001820142914925,0.0001011190508291,0.0,True,True,True,1422312961000.0,1422312988000.0,1422316588000.0,1422460401000.0,1422461301000.0,1422312961000.0,-1.0,1422312961000.0,1422316561000.0,1422312976000.0,1422316576000.0,1422312988000.0,1422316588000.0,"mbogoevici",1422316561000.0,1422312961000.0,45,1422312961000.0,-1.0,"Add Kafka-based implementation for AbstractSingleNodeStreamDeploymentIntegrationTests","",2.0,False,2.0,0,0.0,-1,0,False
"XD-2646","Bug","Sprint 41","2015-01-26T09:24:42.000+0000","jvalkeal","","Should use same hadoop security keys than shdp","For kerberos and other security related settings we use keys like 'spring.hadoop.userPrincipal' mentioned in https://github.com/spring-projects/spring-xd/wiki/Hadoop-Kerberos. However when we added boot config props to shdp, we used a sub keys like 'spring.hadoop.security.userPrincipal'.

It'd be good if we'd fix these to be same in both XD and SHDP not to cause confusion.",1.0,0.6922254304352475,0.6922254304352475,0.0631483263822051,True,True,True,1422264282000.0,1422277228000.0,1422280828000.0,1422282084000.0,1422282984000.0,1422264282000.0,-1.0,1422265463000.0,1422269063000.0,1422277228000.0,1422280828000.0,1422277228000.0,1422280828000.0,"jvalkeal",1422269063000.0,1422264282000.0,45,1422264282000.0,-1.0,"XD should use same hadoop security keys as Spring for Apache Hadoop","For kerberos and other security related settings we use keys like 'spring.hadoop.userPrincipal' mentioned in https://github.com/spring-projects/spring-xd/wiki/Hadoop-Kerberos. However when we added boot config props to shdp, we used a sub keys like 'spring.hadoop.security.userPrincipal'.

It'd be good if we'd fix these to be same in both XD and SHDP not to cause confusion.",1.0,False,1.0,0,0.0,0,0,True
"XD-2645","Bug","Sprint 41","2015-01-26T08:09:15.000+0000","steve.mcnamara","thomas.risberg","servers.yaml has a typo that causes XD startup to fail when....","Line 295 has 2 dashes instead of 3.  There is no effect on XD unless the XD extensions block is uncommented.

I have been using the XD extensions in version 1.0.3 and have merged my servers.yaml with all of the new content in 1.1.0.M2.

The Admin JVM crahes without logging any exception and the container JVM crashes and logs the following:

/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
`--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.1.0.M2                         eXtreme Data


Started : ContainerServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki

expected '<document start>', but found BlockMappingStart
in 'reader', line 303, column 1:
    xd:
    ^

        at org.yaml.snakeyaml.parser.ParserImpl$ParseDocumentStart.produce(ParserImpl.java:225)
        at org.yaml.snakeyaml.parser.ParserImpl.peekEvent(ParserImpl.java:158)
        at org.yaml.snakeyaml.parser.ParserImpl.checkEvent(ParserImpl.java:143)
        at org.yaml.snakeyaml.composer.Composer.checkNode(Composer.java:68)
        at org.yaml.snakeyaml.constructor.BaseConstructor.checkData(BaseConstructor.java:93)
        at org.yaml.snakeyaml.Yaml$1.hasNext(Yaml.java:498)
        at org.springframework.beans.factory.config.YamlProcessor.process(YamlProcessor.java:152)
        at org.springframework.beans.factory.config.YamlProcessor.process(YamlProcessor.java:137)
        at org.springframework.boot.env.YamlPropertySourceLoader$Processor.process(YamlPropertySourceLoader.java:78)
        at org.springframework.boot.env.YamlPropertySourceLoader.load(YamlPropertySourceLoader.java:50)
        at org.springframework.boot.env.PropertySourcesLoader.load(PropertySourcesLoader.java:126)
        at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.loadIntoGroup(ConfigFileApplicationListener.java:378)
        at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.load(ConfigFileApplicationListener.java:366)
        at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.load(ConfigFileApplicationListener.java:336)
        at org.springframework.boot.context.config.ConfigFileApplicationListener.addPropertySources(ConfigFileApplicationListener.java:173)
        at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEnvironmentPreparedEvent(ConfigFileApplicationListener.java:144)
        at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEnvironmentPreparedEvent(ConfigFileApplicationListener.java:137)
        at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEvent(ConfigFileApplicationListener.java:126)
        at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:151)
        at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:128)
        at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:100)
        at org.springframework.boot.context.event.EventPublishingRunListener.environmentPrepared(EventPublishingRunListener.java:59)
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:286)
        at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
        at org.springframework.xd.dirt.server.ContainerBootstrapContext.<init>(ContainerBootstrapContext.java:48)
        at org.springframework.xd.dirt.server.ContainerServerApplication.run(ContainerServerApplication.java:83)
        at org.springframework.xd.dirt.server.ContainerServerApplication.main(ContainerServerApplication.java:72)
  

",1.0,0.3621837549933422,0.1804260985352863,0.1413115845539281,True,True,True,1422259755000.0,1422261931000.0,1422265531000.0,1422264863000.0,1422265763000.0,1422259755000.0,-1.0,1422260604000.0,1422264204000.0,1422260839000.0,1422264439000.0,1422261931000.0,1422265531000.0,"steve.mcnamara",1422264204000.0,1422259755000.0,45,1422259755000.0,-1.0,"servers.yaml has a typo that causes XD startup to fail when....","Line 295 has 2 dashes instead of 3.  There is no effect on XD unless the XD extensions block is uncommented.

I have been using the XD extensions in version 1.0.3 and have merged my servers.yaml with all of the new content in 1.1.0.M2.

The Admin JVM crahes without logging any exception and the container JVM crashes and logs the following:

/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
`--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.1.0.M2                         eXtreme Data


Started : ContainerServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki

expected '<document start>', but found BlockMappingStart
in 'reader', line 303, column 1:
    xd:
    ^

        at org.yaml.snakeyaml.parser.ParserImpl$ParseDocumentStart.produce(ParserImpl.java:225)
        at org.yaml.snakeyaml.parser.ParserImpl.peekEvent(ParserImpl.java:158)
        at org.yaml.snakeyaml.parser.ParserImpl.checkEvent(ParserImpl.java:143)
        at org.yaml.snakeyaml.composer.Composer.checkNode(Composer.java:68)
        at org.yaml.snakeyaml.constructor.BaseConstructor.checkData(BaseConstructor.java:93)
        at org.yaml.snakeyaml.Yaml$1.hasNext(Yaml.java:498)
        at org.springframework.beans.factory.config.YamlProcessor.process(YamlProcessor.java:152)
        at org.springframework.beans.factory.config.YamlProcessor.process(YamlProcessor.java:137)
        at org.springframework.boot.env.YamlPropertySourceLoader$Processor.process(YamlPropertySourceLoader.java:78)
        at org.springframework.boot.env.YamlPropertySourceLoader.load(YamlPropertySourceLoader.java:50)
        at org.springframework.boot.env.PropertySourcesLoader.load(PropertySourcesLoader.java:126)
        at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.loadIntoGroup(ConfigFileApplicationListener.java:378)
        at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.load(ConfigFileApplicationListener.java:366)
        at org.springframework.boot.context.config.ConfigFileApplicationListener$Loader.load(ConfigFileApplicationListener.java:336)
        at org.springframework.boot.context.config.ConfigFileApplicationListener.addPropertySources(ConfigFileApplicationListener.java:173)
        at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEnvironmentPreparedEvent(ConfigFileApplicationListener.java:144)
        at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEnvironmentPreparedEvent(ConfigFileApplicationListener.java:137)
        at org.springframework.boot.context.config.ConfigFileApplicationListener.onApplicationEvent(ConfigFileApplicationListener.java:126)
        at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:151)
        at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:128)
        at org.springframework.boot.context.event.EventPublishingRunListener.publishEvent(EventPublishingRunListener.java:100)
        at org.springframework.boot.context.event.EventPublishingRunListener.environmentPrepared(EventPublishingRunListener.java:59)
        at org.springframework.boot.SpringApplication.run(SpringApplication.java:286)
        at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
        at org.springframework.xd.dirt.server.ContainerBootstrapContext.<init>(ContainerBootstrapContext.java:48)
        at org.springframework.xd.dirt.server.ContainerServerApplication.run(ContainerServerApplication.java:83)
        at org.springframework.xd.dirt.server.ContainerServerApplication.main(ContainerServerApplication.java:72)
  

",1.0,False,1.0,0,0.0,0,0,False
"XD-2643","Story","Sprint 41","2015-01-23T14:58:31.000+0000","grenfro","grenfro","Acceptance tests need to create or use an existing topic prior to executing test","",3.0,0.769655102125315,0.0001019182933781,0.7704768183656765,True,True,True,1422025111000.0,1422266765000.0,1422270365000.0,1422338188000.0,1422339088000.0,1422025111000.0,-1.0,1422267023000.0,1422270623000.0,1422025143000.0,1422028743000.0,1422266765000.0,1422270365000.0,"grenfro",1422270623000.0,1422025111000.0,45,1422025111000.0,-1.0,"Acceptance tests need to create or use an existing topic prior to executing test","",3.0,False,3.0,0,0.0,-1,0,False
"XD-2641","Improvement","Sprint 43","2015-01-23T12:13:15.000+0000","mbogoevici","","Validate that topics used for storing offsets are compacted","",1.0,-1.0,-1.0,0.0921986854185089,False,True,False,1422015195000.0,-1.0,-1.0,1424677500000.0,1424678400000.0,1422015195000.0,-1.0,1422260739000.0,1422264339000.0,-1.0,-1.0,-1.0,-1.0,"mbogoevici",1422264339000.0,1422015195000.0,45,1422015195000.0,-1.0,"Validate that topics used for storing offsets are compacted","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2638","Bug","Sprint 43","2015-01-23T07:47:02.000+0000","thomas.risberg","thomas.risberg","The shell distribution zip is missing hadoop26 libraries","The spring-xd-[version]-shell.zip distribution zip doesn't include the lib/hadoop26 directory and libraries, so we get the following exception when starting the shell:

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration
",1.0,0.9512849574698452,0.9512849574698452,0.9470627215678038,True,True,True,1421999222000.0,1422261926000.0,1422265526000.0,1422274479000.0,1422275379000.0,1421999222000.0,-1.0,1422260760000.0,1422264360000.0,1422261926000.0,1422265526000.0,1422261926000.0,1422265526000.0,"thomas.risberg",1422264360000.0,1421999222000.0,45,1421999222000.0,-1.0,"The shell distribution zip is missing hadoop26 libraries","The spring-xd-[version]-shell.zip distribution zip doesn't include the lib/hadoop26 directory and libraries, so we get the following exception when starting the shell:

Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration
",1.0,False,1.0,0,0.0,0,0,False
"XD-2637","Story","Sprint 41","2015-01-22T14:07:41.000+0000","dturanski","dturanski","Shell processor is not thread safe","Multiple threads invoke the shell processor result in I/O errors and/or data corruption. send() and receive() should be synchronized. ",1.0,0.912553776290631,0.000298757170172,0.0017477294455066,True,True,True,1421935661000.0,1421996751000.0,1422000351000.0,1422001705000.0,1422002605000.0,1421935661000.0,-1.0,1421935778000.0,1421939378000.0,1421935681000.0,1421939281000.0,1421996751000.0,1422000351000.0,"dturanski",1421939378000.0,1421935661000.0,45,1421935661000.0,-1.0,"Shell processor is not thread safe","Multiple threads invoke the shell processor result in I/O errors and/or data corruption. send() and receive() should be synchronized. ",1.0,False,1.0,0,0.0,-1,0,False
"XD-2635","Story","Sprint 42","2015-01-22T13:35:56.000+0000","sabby","","Update 'partitionColumn' wiki to include mutual exclusiveness of 'sql' and 'tableName' options","As a user, I'd like to refer to the wiki so that I can create a job with 'partitions' that in turn expects _tableName_ and _columns_ be explicitly included in the job definition. 

It is also beneficial to call-out _sql_ and _tableName_ metadata options are mutually exclusive. Following logic in _JdbcHdfsOptionsMetadata_ needs documented.

{code:java}
@AssertTrue(message = ""Use ('tableName' AND 'columns') when using partition column"")
boolean isPartitionedWithTableName() {
	if (StringUtils.hasText(partitionColumn)) {
		return StringUtils.hasText(tableName) && !StringUtils.hasText(sql);
	}
	else {
		return true;
	}
}
{code}

",1.0,0.9986464501854624,0.9986464501854624,0.0,True,True,True,1421933756000.0,1422010487000.0,1422010591000.0,1422009691000.0,1422010591000.0,1421933756000.0,-1.0,1421933756000.0,1421937356000.0,1422010487000.0,1422010591000.0,1422010487000.0,1422010591000.0,"sabby",1421937356000.0,1421933756000.0,45,1421933756000.0,-1.0,"Update 'partitionColumn' wiki to include mutual exclusiveness of 'sql' and 'tableName' options","As a user, I'd like to refer to the wiki so that I can create a job with 'partitions' that in turn expects _tableName_ and _columns_ be explicitly included in the job definition. 

It is also beneficial to call-out _sql_ and _tableName_ metadata options are mutually exclusive. Following logic in _JdbcHdfsOptionsMetadata_ needs documented.

{code:java}
@AssertTrue(message = ""Use ('tableName' AND 'columns') when using partition column"")
boolean isPartitionedWithTableName() {
	if (StringUtils.hasText(partitionColumn)) {
		return StringUtils.hasText(tableName) && !StringUtils.hasText(sql);
	}
	else {
		return true;
	}
}
{code}

",1.0,False,1.0,0,0.0,-1,0,False
"XD-2631","Story","Sprint 41","2015-01-21T10:16:22.000+0000","hillert","hillert","For time being checkin UI build artifacts","",1.0,0.0003100217635277,0.0003100217635277,0.6368653079446177,True,True,True,1421835382000.0,1421835432000.0,1421839032000.0,1421995761000.0,1421996661000.0,1421835382000.0,-1.0,1421938095000.0,1421941695000.0,1421835432000.0,1421839032000.0,1421835432000.0,1421839032000.0,"hillert",1421941695000.0,1421835382000.0,45,1421835382000.0,-1.0,"For time being checkin UI build artifacts","",1.0,False,1.0,0,0.0,0,0,False
"XD-2630","Story","Sprint 43","2015-01-21T07:49:51.000+0000","sabby","","Spike: Study scope to develop Ambari plugin for XD","As a SysAdmin, I'd like to use Ambari plugin so that I can provision, manage, and monitor Spring XD clusters using the same tool I use for Hadoop clusters.",5.0,0.9533668386953472,0.946693910282798,0.0,True,True,True,1421826591000.0,1432711634000.0,1432715234000.0,1433243167000.0,1433244067000.0,1421826591000.0,1432628666000.0,1421826591000.0,1421830191000.0,1432635446000.0,1432639046000.0,1432711634000.0,1432715234000.0,"sabby",1421830191000.0,1421826591000.0,45,1432628666000.0,1432628666000.0,"Add Ambari plugin (beta) to build and install Spring XD","As a developer, I'd like to use Ambari plugin so that I can provision, manage, and monitor Spring XD cluster using the same tool I use for Hadoop clusters.",8.0,True,8.0,1,-37.5,-1,0,True
"XD-2628","Story","Sprint 42","2015-01-21T07:35:47.000+0000","mark.pollack","","Test Redis Cluster setup and document recommended configuration","Configure Redis Cluster for Analytics (not for message bus) v 3.0 RC2.  Verify fail over, experiment with settings.  Useful reference https://code.flickr.net/2014/07/31/redis-sentinel-at-flickr/

All analytics test cases should be run as well as test that deploy streams that make use of redis analytics.   There might be some minor code changes required as mentioned in the flickr article.",5.0,0.5747372656011167,0.5747372656011167,0.0,True,True,True,1421825747000.0,1422622880000.0,1422626480000.0,1423211799000.0,1423212699000.0,1421825747000.0,-1.0,1421825747000.0,1421829347000.0,1422622880000.0,1422626480000.0,1422622880000.0,1422626480000.0,"mark.pollack",1421829347000.0,1421825747000.0,45,1421825747000.0,-1.0,"Test Redis Sentinel setup and document recommended configuration","Configure Redis Cluster with Sentinal v 2.8.19.   Verify fail over, experiment with settings.  Useful reference https://code.flickr.net/2014/07/31/redis-sentinel-at-flickr/

All analytics test cases should be run as well as test that deploy streams that make use of redis analytics.   There might be some minor code changes required as mentioned in the flickr article.",5.0,False,5.0,0,0.0,0,2,True
"XD-2626","Improvement","Sprint 42","2015-01-20T13:49:09.000+0000","pperalta","","Update deployment guide to include verbose gc","The deployment guide https://github.com/spring-projects/spring-xd/wiki/Deployment should have instructions on turning on verbose gc for production applications.

The gc log tends to be verbose so running it in development is not desirable. However having the gc log in production is very helpful for troubleshooting slow/unresponsive applications.",1.0,0.9952112404290588,0.994588526272398,0.4421752896497891,True,True,True,1421761749000.0,1423123401000.0,1423127001000.0,1423129053000.0,1423129953000.0,1421761749000.0,-1.0,1422366735000.0,1422370335000.0,1423122549000.0,1423126149000.0,1423123401000.0,1423127001000.0,"pperalta",1422370335000.0,1421761749000.0,45,1421761749000.0,-1.0,"Update deployment guide to include verbose gc","The deployment guide https://github.com/spring-projects/spring-xd/wiki/Deployment should have instructions on turning on verbose gc for production applications.

The gc log tends to be verbose so running it in development is not desirable. However having the gc log in production is very helpful for troubleshooting slow/unresponsive applications.",1.0,False,1.0,0,0.0,0,0,False
"XD-2625","Improvement","Sprint 41","2015-01-20T13:44:44.000+0000","pperalta","pperalta","Add ZooKeeper timeout options to configuration file","The following timeout options should be added to our configuration file:

| session timeout | the number of milliseconds before the ZooKeeper session times out |
| connection timeout | the number of milliseconds before a connection to a ZooKeeper server times out |
| retry wait period | number of milliseconds to wait before attempting a connection after a failed connection |
| max retries | number of times to attempt a connection after a failed connection |

",2.0,0.3454826864131589,0.1755293213154886,0.4872406502584646,True,True,True,1421761484000.0,1421932244000.0,1421935844000.0,1422254849000.0,1422255749000.0,1421761484000.0,-1.0,1422002310000.0,1422005910000.0,1421848242000.0,1421851842000.0,1421932244000.0,1421935844000.0,"pperalta",1422005910000.0,1421761484000.0,45,1421761484000.0,-1.0,"Add ZooKeeper timeout options to configuration file","The following timeout options should be added to our configuration file:

| session timeout | the number of milliseconds before the ZooKeeper session times out |
| connection timeout | the number of milliseconds before a connection to a ZooKeeper server times out |
| retry wait period | number of milliseconds to wait before attempting a connection after a failed connection |
| max retries | number of times to attempt a connection after a failed connection |

",2.0,False,2.0,0,0.0,-1,0,False
"XD-2624","Improvement","Sprint 42","2015-01-20T13:40:08.000+0000","mbogoevici","mbogoevici","Add more comprehensive for the simple consumer-based Kafka Message Bus","",3.0,-1.0,0.0,0.155421150985919,False,True,True,1421761208000.0,-1.0,-1.0,1422882459000.0,1422883359000.0,1421761208000.0,-1.0,1421935614000.0,1421939214000.0,1421761208000.0,1421764808000.0,-1.0,-1.0,"mbogoevici",1421939214000.0,1421761208000.0,45,1421761208000.0,-1.0,"Add more comprehensive tests for the simple consumer-based Kafka Message Bus","",3.0,False,3.0,0,0.0,0,0,True
"XD-2622","Improvement","Sprint 42","2015-01-20T12:09:46.000+0000","mbogoevici","mbogoevici","Update documentation for Kafka sources  ","Include new features added by using Spring Integration Kafka M3",1.0,-1.0,0.0051884887766089,0.2038937608713658,False,True,True,1421755786000.0,-1.0,-1.0,1422527558000.0,1422528458000.0,1421755786000.0,-1.0,1421913329000.0,1421916929000.0,1421759795000.0,1421763395000.0,-1.0,-1.0,"mbogoevici",1421916929000.0,1421755786000.0,45,1421755786000.0,-1.0,"Update documentation for Kafka sources  ","Include new features added by using Spring Integration Kafka M3",1.0,False,1.0,0,0.0,0,0,False
"XD-2621","Story","Sprint 41","2015-01-20T12:07:38.000+0000","mbogoevici","mbogoevici","Add Kafka Native Metadata Store","",3.0,0.9607890425222412,0.0,0.6886283246818871,True,True,True,1421755658000.0,1422006643000.0,1422010243000.0,1422015986000.0,1422016886000.0,1421755658000.0,-1.0,1421935547000.0,1421939147000.0,1421755658000.0,1421759258000.0,1422006643000.0,1422010243000.0,"mbogoevici",1421939147000.0,1421755658000.0,45,1421755658000.0,-1.0,"Add Kafka Native Metadata Store","",3.0,False,3.0,0,0.0,-1,0,False
"XD-2618","Story","Sprint 41","2015-01-20T07:38:08.000+0000","eric.bottard","eric.bottard","Update Spring Batch to 3.0.3.RELEASE","",1.0,0.9995236350311468,0.0,0.0071454745327958,True,True,True,1421739488000.0,1421766765000.0,1421766778000.0,1421765878000.0,1421766778000.0,1421739488000.0,-1.0,1421739683000.0,1421743283000.0,1421739488000.0,1421743088000.0,1421766765000.0,1421766778000.0,"eric.bottard",1421743283000.0,1421739488000.0,45,1421739488000.0,-1.0,"Update Spring Batch to 3.0.3.RELEASE","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2617","Story","Sprint 41","2015-01-20T06:36:20.000+0000","mark.pollack","mark.pollack","Create reactor module in spring-xd-modules project","",3.0,0.0735501091967969,0.0037733559815578,0.0,True,True,True,1421735780000.0,1421741842000.0,1421745442000.0,1421817300000.0,1421818200000.0,1421735780000.0,-1.0,1421735780000.0,1421739380000.0,1421736091000.0,1421739691000.0,1421741842000.0,1421745442000.0,"mark.pollack",1421739380000.0,1421735780000.0,45,1421735780000.0,-1.0,"Create reactor module in spring-xd-modules project","",3.0,False,3.0,0,0.0,0,1,False
"XD-2616","Bug","","2015-01-19T21:18:33.000+0000","mbogoevici","","Ensure that metadata for Kafka message bus is propagated before producing/consuming","Currently, `ensureTopicCreated` will invoke the creation of the topic on the brokers, however, the calls is not blocking. So, before proceeding, we should make sure that the metadata is readable (therefore propagated)",3.0,0.0001368330395447,0.0001368330395447,-1.0,True,False,True,1421702313000.0,1421702330000.0,1421705930000.0,1421825652000.0,1421826552000.0,1421702313000.0,-1.0,-1.0,-1.0,1421702330000.0,1421705930000.0,1421702330000.0,1421705930000.0,"mbogoevici",-1.0,-1.0,0,1421702313000.0,-1.0,"Ensure that metadata for Kafka message bus is propagated before producing/consuming","Currently, `ensureTopicCreated` will invoke the creation of the topic on the brokers, however, the calls is not blocking. So, before proceeding, we should make sure that the metadata is readable (therefore propagated)",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2612","Story","Sprint 41","2015-01-19T08:45:11.000+0000","dturanski","dturanski","Use Kryo instance pooling to reduce instantiation overhead","https://github.com/EsotericSoftware/kryo#pooling-kryo-instances",1.0,0.0004783544606553,0.0,0.0,True,True,True,1421657111000.0,1421657121000.0,1421660721000.0,1421677116000.0,1421678016000.0,1421657111000.0,-1.0,1421657111000.0,1421660711000.0,1421657111000.0,1421660711000.0,1421657121000.0,1421660721000.0,"dturanski",1421660711000.0,1421657111000.0,45,1421657111000.0,-1.0,"Use Kryo instance pooling to reduce instantiation overhead","https://github.com/EsotericSoftware/kryo#pooling-kryo-instances",1.0,False,1.0,0,0.0,0,0,False
"XD-2611","Bug","Sprint 41","2015-01-19T07:03:49.000+0000","grussell","grussell","Missing Log Configuration for throughput-sampler","http://stackoverflow.com/questions/28019801/how-to-read-throughput-sampler-sink-values-in-spring-xd/28027544#28027544",1.0,0.0078039927404718,0.0,0.0,True,True,True,1421651029000.0,1421651932000.0,1421655532000.0,1421765839000.0,1421766739000.0,1421651029000.0,-1.0,1421651029000.0,1421654629000.0,1421651029000.0,1421654629000.0,1421651932000.0,1421655532000.0,"grussell",1421654629000.0,1421651029000.0,45,1421651029000.0,-1.0,"Missing Log Configuration for throughput-sampler","http://stackoverflow.com/questions/28019801/how-to-read-throughput-sampler-sink-values-in-spring-xd/28027544#28027544",1.0,False,1.0,0,0.0,-1,0,False
"XD-2609","Bug","Sprint 41","2015-01-17T17:02:56.000+0000","sabby","hillert","Error when listing Streams in admin-ui","As a user, I'm trying to list streams (>20) in admin-ui to use the pagination; however, I ended up with blank page and the server-side errored with _java.lang.IllegalStateException_.

Version: 1.1.0 SNAPSHOT (master build)
Distributed: 1 admin and 2 containers

*Steps to reproduce:*
1) Deploy the following streams.
stream create foo1 --definition ""time | log"" --deploy
stream create foo2 --definition ""time | log"" --deploy
stream create foo3 --definition ""time | log"" --deploy
stream create foo4 --definition ""time | log"" --deploy
stream create foo5 --definition ""time | log"" --deploy
stream create foo6 --definition ""time | log"" --deploy
stream create foo7 --definition ""time | log"" --deploy
stream create foo8 --definition ""time | log"" --deploy
stream create foo9 --definition ""time | log"" --deploy
stream create foo10 --definition ""time | log"" --deploy
stream create foo11 --definition ""time | log"" --deploy
stream create foo12 --definition ""time | log"" --deploy
stream create foo13 --definition ""time | log"" --deploy
stream create foo14 --definition ""time | log"" --deploy
stream create foo15 --definition ""time | log"" --deploy
stream create foo16 --definition ""time | log"" --deploy
stream create foo17 --definition ""time | log"" --deploy
stream create foo18 --definition ""time | log"" --deploy
stream create foo19 --definition ""time | log"" --deploy
stream create foo20 --definition ""time | log"" --deploy
stream create foo21 --definition ""time | log"" --deploy
stream create foo22 --definition ""time | log"" --deploy

2) Go to Streams tab in admin-ui to get a blank page and the following exception in admin logs.

*Error:*
16:55:19,107 1.1.0.SNAP ERROR http-nio-9393-exec-2 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.IllegalStateException: Not all instances were looked at
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:207)
	at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:178)
	at org.springframework.xd.dirt.rest.StreamsController.list(StreamsController.java:63)
",1.0,0.3424756899253082,0.0,0.0,True,True,True,1421514176000.0,1421679426000.0,1421683026000.0,1421995792000.0,1421996692000.0,1421514176000.0,-1.0,1421514176000.0,1421517776000.0,1421514176000.0,1421517776000.0,1421679426000.0,1421683026000.0,"sabby",1421517776000.0,1421514176000.0,45,1421514176000.0,-1.0,"Error when listing Streams in admin-ui","As a user, I'm trying to list streams (>20) in admin-ui to use the pagination; however, I ended up with blank page and the server-side errored with _java.lang.IllegalStateException_.

Version: 1.1.0 SNAPSHOT (master build)
Distributed: 1 admin and 2 containers

*Steps to reproduce:*
1) Deploy the following streams.
stream create foo1 --definition ""time | log"" --deploy
stream create foo2 --definition ""time | log"" --deploy
stream create foo3 --definition ""time | log"" --deploy
stream create foo4 --definition ""time | log"" --deploy
stream create foo5 --definition ""time | log"" --deploy
stream create foo6 --definition ""time | log"" --deploy
stream create foo7 --definition ""time | log"" --deploy
stream create foo8 --definition ""time | log"" --deploy
stream create foo9 --definition ""time | log"" --deploy
stream create foo10 --definition ""time | log"" --deploy
stream create foo11 --definition ""time | log"" --deploy
stream create foo12 --definition ""time | log"" --deploy
stream create foo13 --definition ""time | log"" --deploy
stream create foo14 --definition ""time | log"" --deploy
stream create foo15 --definition ""time | log"" --deploy
stream create foo16 --definition ""time | log"" --deploy
stream create foo17 --definition ""time | log"" --deploy
stream create foo18 --definition ""time | log"" --deploy
stream create foo19 --definition ""time | log"" --deploy
stream create foo20 --definition ""time | log"" --deploy
stream create foo21 --definition ""time | log"" --deploy
stream create foo22 --definition ""time | log"" --deploy

2) Go to Streams tab in admin-ui to get a blank page and the following exception in admin logs.

*Error:*
16:55:19,107 1.1.0.SNAP ERROR http-nio-9393-exec-2 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.IllegalStateException: Not all instances were looked at
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.xd.dirt.rest.XDController.enhanceWithDeployments(XDController.java:207)
	at org.springframework.xd.dirt.rest.XDController.listValues(XDController.java:178)
	at org.springframework.xd.dirt.rest.StreamsController.list(StreamsController.java:63)
",1.0,False,1.0,0,0.0,0,1,False
"XD-2608","Bug","Sprint 42","2015-01-17T12:55:25.000+0000","grenfro","","XD Gemfire modules fail to deploy in  Yarn","1 admin on slave1
1 container on slave2

Gemfire modules fail to deploy.  with the following exception:
Caused by: java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy
This is because the modules require a XD_HOME environment variable and this is not set by the yarn deployment.  
{noformat}
Offending resource: URL [file:null/modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)
	at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:178)
	at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)
	at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)
	at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)
	at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)
	at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)
	at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)
	at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)
	at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: null/modules/common/gemfire-sink.groovy (No such file or directory)
Offending resource: URL [file:null/modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:181)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:217)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:188)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.importBeans(GroovyBeanDefinitionReader.java:337)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:368)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at beans$_run_closure1.doCall(beans:4)
	at beans$_run_closure1.doCall(beans)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:278)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
	at groovy.lang.Closure.call(Closure.java:423)
	at groovy.lang.Closure.call(Closure.java:417)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.invokeBeanDefiningClosure(GroovyBeanDefinitionReader.java:426)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader$1.call(GroovyBeanDefinitionReader.java:223)
	at groovy.lang.Closure.call(Closure.java:439)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1207)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
	at groovy.lang.MetaClassImpl.invokePropertyOrMissing(MetaClassImpl.java:1253)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1209)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at beans.run(beans:1)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:649)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)
	... 29 more
Caused by: java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at java.io.FileInputStream.<init>(FileInputStream.java:101)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at org.springframework.core.io.UrlResource.getInputStream(UrlResource.java:168)
	at org.springframework.core.io.support.EncodedResource.getReader(EncodedResource.java:132)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)
	... 79 more
{noformat}",3.0,0.9274830006606548,0.90950579682838,0.4936927740584477,True,True,True,1421499325000.0,1422277077000.0,1422280677000.0,1422336987000.0,1422337887000.0,1421499325000.0,-1.0,1421913317000.0,1421916917000.0,1422262002000.0,1422265602000.0,1422277077000.0,1422280677000.0,"grenfro",1421916917000.0,1421499325000.0,45,1421499325000.0,-1.0,"XD Gemfire modules fail to deploy in  Yarn","1 admin on slave1
1 container on slave2

Gemfire modules fail to deploy.  with the following exception:
Caused by: java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy
This is because the modules require a XD_HOME environment variable and this is not set by the yarn deployment.  
{noformat}
Offending resource: URL [file:null/modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)
	at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:178)
	at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)
	at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)
	at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)
	at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)
	at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)
	at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)
	at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)
	at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Error evaluating Groovy script: null/modules/common/gemfire-sink.groovy (No such file or directory)
Offending resource: URL [file:null/modules/common/gemfire-sink.groovy]; nested exception is java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:247)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:202)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:181)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:217)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:188)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.importBeans(GroovyBeanDefinitionReader.java:337)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:368)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at beans$_run_closure1.doCall(beans:4)
	at beans$_run_closure1.doCall(beans)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:278)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
	at groovy.lang.Closure.call(Closure.java:423)
	at groovy.lang.Closure.call(Closure.java:417)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.invokeBeanDefiningClosure(GroovyBeanDefinitionReader.java:426)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader$1.call(GroovyBeanDefinitionReader.java:223)
	at groovy.lang.Closure.call(Closure.java:439)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:324)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1207)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
	at groovy.lang.MetaClassImpl.invokePropertyOrMissing(MetaClassImpl.java:1253)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1209)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1016)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:49)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at beans.run(beans:1)
	at groovy.lang.GroovyShell.evaluate(GroovyShell.java:649)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)
	... 29 more
Caused by: java.io.FileNotFoundException: null/modules/common/gemfire-sink.groovy (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at java.io.FileInputStream.<init>(FileInputStream.java:101)
	at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90)
	at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188)
	at org.springframework.core.io.UrlResource.getInputStream(UrlResource.java:168)
	at org.springframework.core.io.support.EncodedResource.getReader(EncodedResource.java:132)
	at org.springframework.beans.factory.groovy.GroovyBeanDefinitionReader.loadBeanDefinitions(GroovyBeanDefinitionReader.java:243)
	... 79 more
{noformat}",3.0,False,3.0,0,0.0,0,0,False
"XD-2607","Bug","Sprint 42","2015-01-17T10:12:36.000+0000","dturanski","","Windows build fails","org.springframework.batch.integration.x.RemoteFileToHadoopTaskletTests > testWrite FAILED
    java.lang.IllegalStateException
        Caused by: org.springframework.beans.factory.BeanCreationException
            Caused by: java.lang.UnsatisfiedLinkError

org.springframework.batch.integration.x.RemoteFileToHadoopTests > testSimple FAILED
    java.lang.IllegalStateException
        Caused by: org.springframework.beans.factory.BeanCreationException
            Caused by: java.lang.UnsatisfiedLinkError
Java HotSpot(TM) Client VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0

3 tests completed, 2 failed
:spring-xd-extension-batch:test FAILED

FAILURE: Build failed with an exception.",1.0,0.6722713953235978,0.6722539100654793,2.1705837664451868e-05,True,True,True,1421489556000.0,1422604545000.0,1422608145000.0,1423147196000.0,1423148096000.0,1421489556000.0,1422520815000.0,1421489592000.0,1421493192000.0,1422604516000.0,1422608116000.0,1422604545000.0,1422608145000.0,"dturanski",1421493192000.0,1421489556000.0,45,1422520815000.0,1422520815000.0,"Windows build fails","This Hadoop scenario will not work in Windows. The scope is to *disable* the test for windows build.

org.springframework.batch.integration.x.RemoteFileToHadoopTaskletTests > testWrite FAILED
    java.lang.IllegalStateException
        Caused by: org.springframework.beans.factory.BeanCreationException
            Caused by: java.lang.UnsatisfiedLinkError

org.springframework.batch.integration.x.RemoteFileToHadoopTests > testSimple FAILED
    java.lang.IllegalStateException
        Caused by: org.springframework.beans.factory.BeanCreationException
            Caused by: java.lang.UnsatisfiedLinkError
Java HotSpot(TM) Client VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0

3 tests completed, 2 failed
:spring-xd-extension-batch:test FAILED

FAILURE: Build failed with an exception.",2.0,True,2.0,1,-50.0,0,0,True
"XD-2606","Story","Sprint 41","2015-01-16T13:16:12.000+0000","sabby","grussell","Add support to 'track history' in message headers","As a user, I'd like to have an option to track history so that I get the visibility of stream name, module name etc. added as part of the message header.",3.0,0.020373082460408,7.757037184133446e-05,5.274785285210743e-05,True,True,True,1421414172000.0,1421420738000.0,1421424338000.0,1421735560000.0,1421736460000.0,1421414172000.0,-1.0,1421414189000.0,1421417789000.0,1421414197000.0,1421417797000.0,1421420738000.0,1421424338000.0,"sabby",1421417789000.0,1421414172000.0,45,1421414172000.0,-1.0,"Add support to 'track history' in message headers","As a user, I'd like to have an option to track history so that I get the visibility of stream name, module name etc. added as part of the message header.",3.0,False,3.0,0,0.0,0,0,False
"XD-2600","Story","Sprint 42","2015-01-15T10:41:12.000+0000","mark.pollack","","Backport XD-2411 to 1.0.x branch","This fix for RichGauge should go into the 1.0.x line.",1.0,0.6677311293312294,0.6677311293312294,0.0,True,True,True,1421318472000.0,1421996786000.0,1422000386000.0,1422333421000.0,1422334321000.0,1421318472000.0,-1.0,1421318472000.0,1421322072000.0,1421996786000.0,1422000386000.0,1421996786000.0,1422000386000.0,"mark.pollack",1421322072000.0,1421318472000.0,45,1421318472000.0,-1.0,"Backport XD-2411 to 1.0.x branch","This fix for RichGauge should go into the 1.0.x line.",1.0,False,1.0,0,0.0,0,0,False
"XD-2599","Story","Sprint 41","2015-01-15T10:37:03.000+0000","mark.pollack","mark.pollack","Create a BroadcasterMessageHandler that uses a 'Serialized' Broadcaster","This is a parallel implementation to the RxJava 

https://github.com/spring-projects/spring-xd/blob/master/spring-xd-rxjava/src/main/java/org/springframework/xd/rxjava/SubjectMessageHandler.java

That will allow multiple threads to broadcast an event but allow processing to occur one at a time on any thread.",2.0,0.0037283408709146,0.0,0.0,True,True,True,1421318223000.0,1421322509000.0,1421326109000.0,1422466896000.0,1422467796000.0,1421318223000.0,-1.0,1421318223000.0,1421321823000.0,1421318223000.0,1421321823000.0,1421322509000.0,1421326109000.0,"mark.pollack",1421321823000.0,1421318223000.0,45,1421318223000.0,-1.0,"Create a BroadcasterMessageHandler that uses a 'Serialized' Broadcaster","This is a parallel implementation to the RxJava 

https://github.com/spring-projects/spring-xd/blob/master/spring-xd-rxjava/src/main/java/org/springframework/xd/rxjava/SubjectMessageHandler.java

That will allow multiple threads to broadcast an event but allow processing to occur one at a time on any thread.",2.0,False,2.0,0,0.0,-1,0,False
"XD-2598","Improvement","Sprint 43","2015-01-15T09:47:13.000+0000","david_geary","","Update PostgreSQL JDBC Driver Version","Update the supplied PostgreSQL JDBC Driver to the latest version (9.3-1102 - 2014-07-10), the current supplied version is from 2012. 

In our particular use case the latest driver allows use of the JDBC connection.unwrap feature which gives access to the underlying connection from a pooled connection which in turn enables use of the postgres copyManager.copyIn functionality which can speed up batch inserts in a batch process. 

See http://jdbc.postgresql.org/documentation/changelog.html
 ",2.0,0.8576050028663664,0.8126705717537404,0.2546886773331414,True,True,True,1421315233000.0,1424865192000.0,1424868792000.0,1425453720000.0,1425454620000.0,1421315233000.0,1424679137000.0,1422369488000.0,1422373088000.0,1424679191000.0,1424682791000.0,1424865192000.0,1424868792000.0,"david_geary",1422373088000.0,1421315233000.0,45,1424679137000.0,1424679137000.0,"Update PostgreSQL JDBC Driver Version","Update the supplied PostgreSQL JDBC Driver to the latest version (9.3-1102 - 2014-07-10), the current supplied version is from 2012. 

In our particular use case the latest driver allows use of the JDBC connection.unwrap feature which gives access to the underlying connection from a pooled connection which in turn enables use of the postgres copyManager.copyIn functionality which can speed up batch inserts in a batch process. 

See http://jdbc.postgresql.org/documentation/changelog.html
 ",5.0,True,5.0,1,-60.0,0,0,False
"XD-2597","Story","Sprint 42","2015-01-14T14:32:29.000+0000","thomas.risberg","","Add an ""xd-yarn info"" command to list admin servers and ports","As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment.

Best way, for now, would be to add an info command to the xd-yarn script.

With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.",5.0,0.872132373709372,0.872132373709372,0.0008865711924513,True,True,True,1421245949000.0,1421912907000.0,1421916507000.0,1422009793000.0,1422010693000.0,1421245949000.0,-1.0,1421246627000.0,1421250227000.0,1421912907000.0,1421916507000.0,1421912907000.0,1421916507000.0,"thomas.risberg",1421250227000.0,1421245949000.0,45,1421245949000.0,-1.0,"Add an ""xd-yarn info"" command to list admin servers and ports","As a user deploying XD on YARN I need a convenient way to get info like the admin port for my current deployment.

Best way, for now, would be to add an info command to the xd-yarn script.

With the latest changes the admin server runs on a random port when we deploy to YARN. In order for the user to connect they would have to query Zookeeper. This is inconvenient.",5.0,False,5.0,0,0.0,0,3,False
"XD-2595","Story","Sprint 42","2015-01-14T09:02:07.000+0000","thomas.risberg","","Test recent Hadoop distro changes","Test basic functionality (hdfs sink, jdbchdfs job) on hadoop26, hdp22, cdh5, phd21

Test XD on YARN on hadoop26, hdp22, cdh5 and phd21
",8.0,0.9005838276182568,0.9005714415944479,0.0001600274276111,True,True,True,1421226127000.0,1423043869000.0,1423047469000.0,1423243631000.0,1423244531000.0,1421226127000.0,1423306570000.0,1421226450000.0,1421230050000.0,1423043844000.0,1423047444000.0,1423043869000.0,1423047469000.0,"thomas.risberg",1421230050000.0,1421226127000.0,45,1421226127000.0,1423306570000.0,"Test recent Hadoop distro changes","Test basic functionality (hdfs sink, jdbchdfs job) on hadoop26, hdp22, cdh5, phd21

Test XD on YARN on hadoop26, hdp22, cdh5 and phd21
",5.0,True,5.0,1,0.0,0,0,False
"XD-2594","Story","Sprint 41","2015-01-14T09:00:52.000+0000","thomas.risberg","","Update spring-data-hadoop version to 2.1.0.RC1","Update spring-data-hadoop version to 2.1.0.RC1. This also includes updating the following:

- adding hadoop26 (Apache Hadoop 2.6.0) as distro
- adding hdp22 (Hortonworks HDP 2.2) as distro
- set default distro to hadoop26
- update cdh5 to version 5.3.0
- remove older distros - hadoop24, hdp21
",3.0,0.86703964290361,0.77593402524095,0.0039532812234154,True,True,True,1421226052000.0,1421312684000.0,1421316284000.0,1421325069000.0,1421325969000.0,1421226052000.0,-1.0,1421226447000.0,1421230047000.0,1421303581000.0,1421307181000.0,1421312684000.0,1421316284000.0,"thomas.risberg",1421230047000.0,1421226052000.0,45,1421226052000.0,-1.0,"Update spring-data-hadoop version to 2.1.0.RC1","Update spring-data-hadoop version to 2.1.0.RC1. This also includes updating the following:

- adding hadoop26 (Apache Hadoop 2.6.0) as distro
- adding hdp22 (Hortonworks HDP 2.2) as distro
- set default distro to hadoop26
- update cdh5 to version 5.3.0
- remove older distros - hadoop24, hdp21
",3.0,False,3.0,0,0.0,0,0,False
"XD-2592","Story","","2015-01-13T13:16:01.000+0000","grenfro","","XD Yarn deployment requires the ability to set permsize","When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default.  
The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source & sink more than 2 times, stream deployment begins to fail.  

The only exception that was captured was the following:
{noformat}
Exception in thread ""ec2Test3_ip-10-146-213-31-1421176704238-e1786039_watcher_executor""
Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""ec2Test3_ip-10-146-213-31-1421176704238-e1786039_watcher_executor
{noformat}

Logs are not available at this time.

",3.0,-1.0,-1.0,-1.0,False,False,False,1421154961000.0,-1.0,-1.0,1421685276000.0,1421686176000.0,1421154961000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1421154961000.0,-1.0,"XD Yarn deployment requires the ability to set permsize","When deploying XD using Java 7 the user must be able to set the permsize to a value larger than the default.  
The reason this is required is that if we deploy a gemfire component more than 2 times or a kafka source & sink more than 2 times, stream deployment begins to fail.  

The only exception that was captured was the following:
{noformat}
Exception in thread ""ec2Test3_ip-10-146-213-31-1421176704238-e1786039_watcher_executor""
Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread ""ec2Test3_ip-10-146-213-31-1421176704238-e1786039_watcher_executor
{noformat}

Logs are not available at this time.

",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2591","Story","Sprint 41","2015-01-13T12:08:56.000+0000","grussell","grussell","Bump Spring Integration to 4.1.2; Spring AMQP to 1.4.2","",1.0,4.146379026430205e-05,0.0,0.0,True,True,True,1421150936000.0,1421150943000.0,1421154543000.0,1421318858000.0,1421319758000.0,1421151000000.0,-1.0,1421150936000.0,1421154536000.0,1421150936000.0,1421154536000.0,1421150943000.0,1421154543000.0,"grussell",1421154536000.0,1421151000000.0,45,1421151000000.0,-1.0,"Bump Spring Integration to 4.1.2; Spring AMQP to 1.4.2","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2590","Story","Sprint 41","2015-01-12T16:09:18.000+0000","sabby","grussell","Create MessageConverter interface to allow user extensions","As a user, I'd like to have the option to extend the default message handling behavior for HTTP source-module so that I can override the settings via _servers.yml_ to control the default message size.

*Notes:*
The adapter currently has that hard-coded (1MB limit) in the HttpChunkAggregator. We will have to expose this property for overrides. [Related PR|https://github.com/spring-projects/spring-xd/pull/1367].",3.0,0.0851106205706129,0.0,0.0,True,True,True,1421078958000.0,1421130238000.0,1421133838000.0,1421680568000.0,1421681468000.0,1421078958000.0,-1.0,1421078958000.0,1421082558000.0,1421078958000.0,1421082558000.0,1421130238000.0,1421133838000.0,"sabby",1421082558000.0,1421078958000.0,45,1421078958000.0,-1.0,"Create MessageConverter interface to allow user extensions","As a user, I'd like to have the option to extend the default message handling behavior for HTTP source-module so that I can override the settings via _servers.yml_ to control the default message size.

*Notes:*
The adapter currently has that hard-coded (1MB limit) in the HttpChunkAggregator. We will have to expose this property for overrides. [Related PR|https://github.com/spring-projects/spring-xd/pull/1367].",3.0,False,3.0,0,0.0,-1,0,False
"XD-2589","Story","Sprint 41","2015-01-12T13:33:57.000+0000","mark.pollack","mark.pollack","Create sample application for RxJava","Create port of https://github.com/spring-projects/spring-xd-samples/tree/master/reactor-moving-average based on RxJava",2.0,0.0026773493037687,0.0,0.0,True,True,True,1421069637000.0,1421070304000.0,1421073904000.0,1421317864000.0,1421318764000.0,1421069637000.0,-1.0,1421069637000.0,1421073237000.0,1421069637000.0,1421073237000.0,1421070304000.0,1421073904000.0,"mark.pollack",1421073237000.0,1421069637000.0,45,1421069637000.0,-1.0,"Create sample application for RxJava","Create port of https://github.com/spring-projects/spring-xd-samples/tree/master/reactor-moving-average based on RxJava",2.0,False,2.0,0,0.0,-1,0,False
"XD-2588","Story","Sprint 42","2015-01-12T13:26:44.000+0000","mark.pollack","","Remove all deprecated compile warnings","Run a clean gradle build to identify all warnings.",3.0,0.767121233635907,0.7233109246547036,0.0,True,True,True,1421069204000.0,1422517041000.0,1422520641000.0,1422955668000.0,1422956568000.0,1421069204000.0,-1.0,1421069204000.0,1421072804000.0,1422434355000.0,1422437955000.0,1422517041000.0,1422520641000.0,"mark.pollack",1421072804000.0,1421069204000.0,45,1421069204000.0,-1.0,"Remove all deprecated compile warnings","Run a clean gradle build to identify all warnings.",3.0,False,3.0,0,0.0,0,0,False
"XD-2584","Story","Sprint 42","2015-01-12T11:59:52.000+0000","mark.pollack","","Upgrade grunt/node plugins","Upgrade in 1.0.x branch what was done in this commit on master.

https://github.com/spring-projects/spring-xd/commit/16062d771e23187a1d9e8d549abc646ff44e435b
",1.0,0.9391312468755322,0.9391312468755322,0.0,True,True,True,1421063992000.0,1422431600000.0,1422435200000.0,1422519340000.0,1422520240000.0,1421063992000.0,-1.0,1421063992000.0,1421067592000.0,1422431600000.0,1422435200000.0,1422431600000.0,1422435200000.0,"mark.pollack",1421067592000.0,1421063992000.0,45,1421063992000.0,-1.0,"Upgrade grunt/node plugins","Upgrade in 1.0.x branch what was done in this commit on master.

https://github.com/spring-projects/spring-xd/commit/16062d771e23187a1d9e8d549abc646ff44e435b
",1.0,False,1.0,0,0.0,0,0,False
"XD-2582","Improvement","","2015-01-12T05:03:35.000+0000","kdowbecki","","Provide missing JARs to enable #xpath() SpEL","Spring XD should package *spring-integration-xml* JAR within distribution so we can invoke *#xpath()* SpEL from processors, e.g. using transformer:

{code}... | transform --expression='#xpath(payload, ""/*[name()=''Datasource'']/*[name()=''row'']/text()"" | ... {code}
",1.0,-1.0,-1.0,-1.0,False,False,False,1421039015000.0,-1.0,-1.0,1424765614000.0,1424766514000.0,1421039015000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"kdowbecki",-1.0,-1.0,0,1421039015000.0,-1.0,"Provide missing JARs to enable #xpath() SpEL","Spring XD should package *spring-integration-xml* JAR within distribution so we can invoke *#xpath()* SpEL from processors, e.g. using transformer:

{code}... | transform --expression='#xpath(payload, ""/*[name()=''Datasource'']/*[name()=''row'']/text()"" | ... {code}
",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2577","Bug","","2015-01-09T12:10:25.000+0000","grenfro","","XD is not logging when deployed using yarn","When deploying XD (admin & container) using Yarn we only get the first 495 characters of the log which is the Ascii Art and Documentation links.
{noformat}
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.1.0.BUILD-SNAPSHOT             eXtreme Data


Started : ContainerServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki
{no format}",5.0,-1.0,-1.0,-1.0,False,False,False,1420805425000.0,-1.0,-1.0,1421685649000.0,1421686549000.0,1420805425000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1420805425000.0,-1.0,"XD is not logging when deployed using yarn","When deploying XD (admin & container) using Yarn we only get the first 495 characters of the log which is the Ascii Art and Documentation links.
{noformat}
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
1.1.0.BUILD-SNAPSHOT             eXtreme Data


Started : ContainerServerApplication
Documentation: https://github.com/spring-projects/spring-xd/wiki
{noformat}",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-2575","Improvement","Sprint 43","2015-01-09T08:17:59.000+0000","lukasz.nowanski@emc.com","","HDFS sink should provide rolloverTime option not only idleTiemout","When using HDFS sink with ildeTimeout and rollover options in stream definition we have noticed that idleTimeout does not give you a flexibility when you would prefer a file to rollover after specific time regardless of the activity/inactivity of the file.

Proposed option:
rolloverTimeout
timeout after file will be automatically closed

",5.0,0.389026785505691,0.389026785505691,0.0238095325694067,True,True,True,1420791479000.0,1425021007000.0,1425024607000.0,1431662653000.0,1431663553000.0,1420791479000.0,1424678988000.0,1421050338000.0,1421053938000.0,1425021007000.0,1425024607000.0,1425021007000.0,1425024607000.0,"lukasz.nowanski@emc.com",1421053938000.0,1420791479000.0,45,1424678988000.0,1424678988000.0,"HDFS sink should provide rolloverTime option not only idleTiemout","When using HDFS sink with ildeTimeout and rollover options in stream definition we have noticed that idleTimeout does not give you a flexibility when you would prefer a file to rollover after specific time regardless of the activity/inactivity of the file.

Proposed option:
rolloverTimeout
timeout after file will be automatically closed

Link: #XD-2413",3.0,True,3.0,1,66.66666666666666,0,0,True
"XD-2574","Improvement","Sprint 41","2015-01-09T08:03:10.000+0000","lukasz.nowanski@emc.com","dturanski","Gemfire sink SpringXD module does not support multiple locators","Gemfire sink module accepts useLocator, host and port properties but this only allows to use one locator at a time.

We have a need to use the Gemfire sink SpringXD Module in our Seamless access project that we want to go live in Q1.
The Version of SpringXD we planned on using is 1.1

However we need HA and we need to connect to a cluster with multiple locators. Problem is this isnt supported yet in SpringXD.

We have used multiple locators in many projects in EMC and we dont want to revert back to a situation where we have to put virtual IPs in front of locators just for SpringXD.

Ref to the SpringXD docs found in 1.0.3 and 1.1.0GA versions:

The locator option is mostly intended for integration with an existing GemFire installation in which the cache servers are configured to use locators in accordance with best practice. While GemFire supports configuration of multiple locators for failover, this is currently not supported in XD. However, using a single virtual IP backed by hardware routers for failover has proven to be an effective and simpler alternative.

",5.0,0.7923997263740539,0.6228030563839347,0.6228030563839347,True,True,True,1420790590000.0,1421145051000.0,1421148651000.0,1421237016000.0,1421237916000.0,1420790590000.0,-1.0,1421069186000.0,1421072786000.0,1421069186000.0,1421072786000.0,1421145051000.0,1421148651000.0,"lukasz.nowanski@emc.com",1421072786000.0,1420790590000.0,45,1420790590000.0,-1.0,"Gemfire sink SpringXD module does not support multiple locators","Gemfire sink module accepts useLocator, host and port properties but this only allows to use one locator at a time.

We have a need to use the Gemfire sink SpringXD Module in our Seamless access project that we want to go live in Q1.
The Version of SpringXD we planned on using is 1.1

However we need HA and we need to connect to a cluster with multiple locators. Problem is this isnt supported yet in SpringXD.

We have used multiple locators in many projects in EMC and we dont want to revert back to a situation where we have to put virtual IPs in front of locators just for SpringXD.

Ref to the SpringXD docs found in 1.0.3 and 1.1.0GA versions:

The locator option is mostly intended for integration with an existing GemFire installation in which the cache servers are configured to use locators in accordance with best practice. While GemFire supports configuration of multiple locators for failover, this is currently not supported in XD. However, using a single virtual IP backed by hardware routers for failover has proven to be an effective and simpler alternative.

",5.0,False,5.0,0,0.0,0,0,False
"XD-2573","Bug","Sprint 41","2015-01-08T13:25:17.000+0000","thomas.risberg","Thomas Risberg","Full build with tests fail on Ubuntu","On Ubuntu 14.04 LTS using OpenJDK version  ""1.7.0_65""

OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)

I see the following failures:

:spring-xd-dirt:test

org.springframework.xd.dirt.security.SingleNodeApplicationWithUserBasedSecurityTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSimpleBindTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithDefaultSecurityTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithSslTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithUsersFileTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSearchAndBindTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

595 tests completed, 6 failed, 2 skipped
:spring-xd-dirt:test FAILED

The test reports has this:

Caused by: java.lang.IllegalStateException: HSQLDB could not be started on 0.0.0.0:7714, state: SHUTDOWN
	at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.startServer(HSQLServerBean.java:162)
	at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.afterPropertiesSet(HSQLServerBean.java:82)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)
	... 42 more
Caused by: java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)
	at java.net.ServerSocket.bind(ServerSocket.java:376)
	at java.net.ServerSocket.<init>(ServerSocket.java:237)
	at java.net.ServerSocket.<init>(ServerSocket.java:128)
	at org.hsqldb.server.HsqlSocketFactory.createServerSocket(Unknown Source)
	at org.hsqldb.server.Server.openServerSocket(Unknown Source)
	at org.hsqldb.server.Server.run(Unknown Source)
	at org.hsqldb.server.Server.access$000(Unknown Source)
	at org.hsqldb.server.Server$ServerThread.run(Unknown Source)

So I assume I see this due HSQL running from another test.",5.0,0.0268821725256584,0.281437605187655,0.0374746674531139,True,True,True,1420723517000.0,1420790331000.0,1420793931000.0,1423208056000.0,1423208956000.0,1420723517000.0,-1.0,1420816658000.0,1420820258000.0,1421423013000.0,1421426613000.0,1420790331000.0,1420793931000.0,"thomas.risberg",1420820258000.0,1420723517000.0,45,1420723517000.0,-1.0,"Full build with tests fail on Ubuntu","On Ubuntu 14.04 LTS using OpenJDK version  ""1.7.0_65""

OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)

I see the following failures:

:spring-xd-dirt:test

org.springframework.xd.dirt.security.SingleNodeApplicationWithUserBasedSecurityTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSimpleBindTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithDefaultSecurityTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithSslTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithUsersFileTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

org.springframework.xd.dirt.security.SingleNodeApplicationWithLdapSearchAndBindTest > classMethod FAILED
    org.springframework.beans.factory.BeanCreationException
        Caused by: java.lang.IllegalStateException
            Caused by: java.net.BindException

595 tests completed, 6 failed, 2 skipped
:spring-xd-dirt:test FAILED

The test reports has this:

Caused by: java.lang.IllegalStateException: HSQLDB could not be started on 0.0.0.0:7714, state: SHUTDOWN
	at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.startServer(HSQLServerBean.java:162)
	at org.springframework.xd.batch.hsqldb.server.HSQLServerBean.afterPropertiesSet(HSQLServerBean.java:82)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1625)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1562)
	... 42 more
Caused by: java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)
	at java.net.ServerSocket.bind(ServerSocket.java:376)
	at java.net.ServerSocket.<init>(ServerSocket.java:237)
	at java.net.ServerSocket.<init>(ServerSocket.java:128)
	at org.hsqldb.server.HsqlSocketFactory.createServerSocket(Unknown Source)
	at org.hsqldb.server.Server.openServerSocket(Unknown Source)
	at org.hsqldb.server.Server.run(Unknown Source)
	at org.hsqldb.server.Server.access$000(Unknown Source)
	at org.hsqldb.server.Server$ServerThread.run(Unknown Source)

So I assume I see this due HSQL running from another test.",5.0,False,5.0,0,0.0,0,0,False
"XD-2572","Story","Sprint 41","2015-01-08T11:50:59.000+0000","hillert","hillert","Set fixed NPM version for Grunt Gradle Plugin","Ensure build works in Windows environments",1.0,0.0282790950011443,0.0282790950011443,0.0739359312350064,True,True,True,1420717859000.0,1420727867000.0,1420731467000.0,1421070860000.0,1421071760000.0,1420717859000.0,-1.0,1420744025000.0,1420747625000.0,1420727867000.0,1420731467000.0,1420727867000.0,1420731467000.0,"hillert",1420747625000.0,1420717859000.0,45,1420717859000.0,-1.0,"Set fixed NPM version for Grunt Gradle Plugin","Ensure build works in Windows environments",1.0,False,1.0,0,0.0,0,0,False
"XD-2570","Story","Sprint 41","2015-01-07T18:55:21.000+0000","mark.pollack","mark.pollack","Create a new Broadcast stream per thread","The data that is entering a broadcast stream can only occur from one thread at a time to prevent race conditions inside the stream implementation.  The current handler shares a single broadcast stream.  Change to create a new one per thread usage.",2.0,0.0046840730693755,0.0,6.312766939859171e-06,True,True,True,1420656921000.0,1420662115000.0,1420665715000.0,1421764885000.0,1421765785000.0,1420656921000.0,-1.0,1420656928000.0,1420660528000.0,1420656921000.0,1420660521000.0,1420662115000.0,1420665715000.0,"mark.pollack",1420660528000.0,1420656921000.0,45,1420656921000.0,-1.0,"Create a new Broadcast stream per thread","The data that is entering a broadcast stream can only occur from one thread at a time to prevent race conditions inside the stream implementation.  The current handler shares a single broadcast stream.  Change to create a new one per thread usage.",2.0,False,2.0,0,0.0,-1,0,False
"XD-2569","Story","Sprint 41","2015-01-07T18:40:35.000+0000","mark.pollack","mark.pollack","Update to Spring Boot 1.2.1.RELEASE","",3.0,0.0405854430379746,0.0,0.0404865506329113,True,True,True,1420656035000.0,1420662191000.0,1420665791000.0,1420806815000.0,1420807715000.0,1420656035000.0,-1.0,1420662176000.0,1420665776000.0,1420656035000.0,1420659635000.0,1420662191000.0,1420665791000.0,"mark.pollack",1420665776000.0,1420656035000.0,45,1420656035000.0,-1.0,"Update to Spring Boot 1.2.1.RELEASE","",3.0,False,3.0,0,0.0,-1,0,False
"XD-2567","Bug","Sprint 41","2015-01-07T12:50:14.000+0000","grussell","grussell","Strip MessageBus DeliveryMode Header","Since the messagebus refactoring, we now see 

{noformat}
15:43:06,379 1.1.0.M2  WARN xdbus.foo.0-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode]
{noformat}

When using a rabbit transport and a rabbit sink (the sink Spring AMQP is in its own classloader).",1.0,0.0056340746388297,0.0,0.0,True,True,True,1420635014000.0,1420640911000.0,1420644511000.0,1421680781000.0,1421681681000.0,1420635014000.0,-1.0,1420635014000.0,1420638614000.0,1420635014000.0,1420638614000.0,1420640911000.0,1420644511000.0,"grussell",1420638614000.0,1420635014000.0,45,1420635014000.0,-1.0,"Strip MessageBus DeliveryMode Header","Since the messagebus refactoring, we now see 

{noformat}
15:43:06,379 1.1.0.M2  WARN xdbus.foo.0-1 support.DefaultAmqpHeaderMapper - skipping header 'amqp_deliveryMode' since it is not of expected type [class org.springframework.amqp.core.MessageDeliveryMode], it is [class org.springframework.amqp.core.MessageDeliveryMode]
{noformat}

When using a rabbit transport and a rabbit sink (the sink Spring AMQP is in its own classloader).",1.0,False,1.0,0,0.0,0,0,False
"XD-2566","Story","Sprint 41","2015-01-07T07:46:00.000+0000","sabby","","Add support to test XD on YARN in EC2","As a developer, I'd like to have acceptance test coverage for XD + YARN on EC2 so that I can verify simple XD features running on YARN on every build cycle.",8.0,0.4209598705020841,0.4209598705020841,0.0807509798682061,True,True,True,1420616760000.0,1421053135000.0,1421056735000.0,1421652479000.0,1421653379000.0,1420616760000.0,-1.0,1420700468000.0,1420704068000.0,1421053135000.0,1421056735000.0,1421053135000.0,1421056735000.0,"sabby",1420704068000.0,1420616760000.0,45,1420616760000.0,-1.0,"Add support to test XD on YARN in EC2","As a developer, I'd like to have acceptance test coverage for XD + YARN on EC2 so that I can verify simple XD features running on YARN on every build cycle.",8.0,False,8.0,0,0.0,0,0,False
"XD-2564","Improvement","Sprint 41","2015-01-06T10:03:21.000+0000","jvalkeal","Janne Valkealahti","Enhance XD on YARN to use SHDP container clustering","Currently yarn runtime needs two yarn appmaster instances(one for admins, one for containers). SHDP's container grouping added functionality to run different type of containers within a same appmaster.

Beyond this, container grouping will also give more functionality like ramping containers up/down on-demand, creating groups with different settings dynamically and restarting failed containers.",2.0,0.6682542256267776,0.9733819475790018,0.6203750367017105,True,True,True,1420538601000.0,1421301049000.0,1421304649000.0,1421678656000.0,1421679556000.0,1420538601000.0,-1.0,1421246421000.0,1421250021000.0,1421649186000.0,1421652786000.0,1421301049000.0,1421304649000.0,"jvalkeal",1421250021000.0,1420538601000.0,45,1420538601000.0,-1.0,"Enhance XD on YARN to use SHDP container clustering","Currently yarn runtime needs two yarn appmaster instances(one for admins, one for containers). SHDP's container grouping added functionality to run different type of containers within a same appmaster.

Beyond this, container grouping will also give more functionality like ramping containers up/down on-demand, creating groups with different settings dynamically and restarting failed containers.",2.0,False,2.0,0,0.0,0,0,False
"XD-2563","Bug","Sprint 41","2015-01-06T09:49:01.000+0000","jvalkeal","Janne Valkealahti","XD on YARN broken due to missing messagebus libs","Admin on YARN simply fails because messagebus libs are not copied in place during a build.

Already tried and simple fix is for gradle/build-dist.gradle:

{code}
task copyYarnMessageBusLibs(type: Copy) {
  from ""$rootDir/lib/messagebus""
  into ""$buildDir/dist/spring-xd-yarn/xd-yarn/lib/messagebus""
}
{code}

and execute it together with copyMessageBusLibs task.",1.0,0.6362316592735056,0.6361199863510872,0.0033998200825138,True,True,True,1420537741000.0,1420640292000.0,1420643892000.0,1420698026000.0,1420698926000.0,1420537741000.0,-1.0,1420538289000.0,1420541889000.0,1420640274000.0,1420643874000.0,1420640292000.0,1420643892000.0,"jvalkeal",1420541889000.0,1420537741000.0,45,1420537741000.0,-1.0,"XD on YARN broken due to missing messagebus libs","Admin on YARN simply fails because messagebus libs are not copied in place during a build.

Already tried and simple fix is for gradle/build-dist.gradle:

{code}
task copyYarnMessageBusLibs(type: Copy) {
  from ""$rootDir/lib/messagebus""
  into ""$buildDir/dist/spring-xd-yarn/xd-yarn/lib/messagebus""
}
{code}

and execute it together with copyMessageBusLibs task.",1.0,False,1.0,0,0.0,0,0,False
"XD-2562","Story","Sprint 41","2015-01-06T09:26:41.000+0000","sabby","","Move the Hadoop test dependencies to a different project","As a developer, I'd like to isolate the Hadoop tests in a different project so that the DIRT project doesn't have to depend upon, thus eliminating the incorrect CP file generation in eclipse. ",3.0,0.8551030731603019,0.8551030731603019,0.0,True,True,True,1420536401000.0,1420700373000.0,1420703973000.0,1420727258000.0,1420728158000.0,1420536401000.0,-1.0,1420536401000.0,1420540001000.0,1420700373000.0,1420703973000.0,1420700373000.0,1420703973000.0,"sabby",1420540001000.0,1420536401000.0,45,1420536401000.0,-1.0,"Move the Hadoop test dependencies to a different project","As a developer, I'd like to isolate the Hadoop tests in a different project so that the DIRT project doesn't have to depend upon, thus eliminating the incorrect CP file generation in eclipse. ",3.0,False,3.0,0,0.0,0,1,False
"XD-2561","Bug","Sprint 42","2015-01-05T12:45:47.000+0000","vicchugu","","Gradle build failed on MAC and Windows","The build failed on two classes from spring-xd-extension-process:
ShellCommandProcessor.java and ShellCommandProcessorTests.java
with error:
FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':spring-xd-extension-process:licenseTest'.
> License violations were found: /Users/victor.chugunov/git/repos/spring/spring-xd/extensions/spring-xd-extension-process/src/test/java/org/springframework/xd/extension/process/ShellCommandProcessorTests.java}


In both classes the license is misplaced:
package org.springframework.xd.extension.process;/*
 *
 *  * Copyright 2014 the original author or authors.
 *  *
 *  * Licensed under the Apache License, Version 2.0 (the ""License"");
 *  * you may not use this file except in compliance with the License.
 *  * You may obtain a copy of the License at
 *  *
 *  * http://www.apache.org/licenses/LICENSE-2.0
 *  *
 *  * Unless required by applicable law or agreed to in writing, software
 *  * distributed under the License is distributed on an ""AS IS"" BASIS,
 *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  * See the License for the specific language governing permissions and
 *  * limitations under the License.
 *
 */",1.0,0.9317996305766668,0.8981036542603676,0.6744002469291546,True,True,True,1420461947000.0,1422958528000.0,1422962128000.0,1423140358000.0,1423141258000.0,1420461947000.0,-1.0,1422268875000.0,1422272475000.0,1422868246000.0,1422871846000.0,1422958528000.0,1422962128000.0,"vicchugu",1422272475000.0,1420461947000.0,45,1420461947000.0,-1.0,"Document minimum memory requirement for Gradle builds","The build failed on two classes from spring-xd-extension-process:
ShellCommandProcessor.java and ShellCommandProcessorTests.java
with error:
FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':spring-xd-extension-process:licenseTest'.
> License violations were found: /Users/victor.chugunov/git/repos/spring/spring-xd/extensions/spring-xd-extension-process/src/test/java/org/springframework/xd/extension/process/ShellCommandProcessorTests.java}


In both classes the license is misplaced:
package org.springframework.xd.extension.process;/*
 *
 *  * Copyright 2014 the original author or authors.
 *  *
 *  * Licensed under the Apache License, Version 2.0 (the ""License"");
 *  * you may not use this file except in compliance with the License.
 *  * You may obtain a copy of the License at
 *  *
 *  * http://www.apache.org/licenses/LICENSE-2.0
 *  *
 *  * Unless required by applicable law or agreed to in writing, software
 *  * distributed under the License is distributed on an ""AS IS"" BASIS,
 *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 *  * See the License for the specific language governing permissions and
 *  * limitations under the License.
 *
 */",1.0,False,1.0,0,0.0,0,0,True
"XD-2556","Story","Sprint 41","2015-01-05T09:35:03.000+0000","sabby","","Reference documentation on RxJava Stream processor/sink","",1.0,0.9912124689773696,0.9912124689773696,0.0,True,True,True,1420450503000.0,1421327166000.0,1421330766000.0,1421334038000.0,1421334938000.0,1420450503000.0,-1.0,1420450503000.0,1420454103000.0,1421327166000.0,1421330766000.0,1421327166000.0,1421330766000.0,"sabby",1420454103000.0,1420450503000.0,45,1420450503000.0,-1.0,"Reference documentation on RxJava Stream processor","",1.0,False,1.0,0,0.0,-1,0,True
"XD-2548","Story","Sprint 41","2015-01-05T09:12:43.000+0000","sabby","","Investigate why CPU startup is high for admin and container servers","As a performance tester, I'd like to investigate why there's high CPU startup time for both admin and container servers. Perhaps profiling would assist isolating the bottlenecks. ",8.0,0.9459468518602718,0.9450865352221608,0.0,True,True,True,1420449163000.0,1432640789000.0,1432644389000.0,1433336541000.0,1433337441000.0,1420449163000.0,1432628216000.0,1420449163000.0,1420452763000.0,1432629701000.0,1432633301000.0,1432640789000.0,1432644389000.0,"sabby",1420452763000.0,1420449163000.0,45,1432628216000.0,1432628216000.0,"Investigate why CPU startup is high for admin and container servers","As a performance tester, I'd like to investigate why there's high CPU startup time for both admin and container servers. Perhaps profiling would assist isolating the bottlenecks. 

*Scope:*
* Identify the bottlenecks
* Document reasons
* List pros/cons",5.0,True,5.0,1,60.0,0,0,True
"XD-2547","Story","Sprint 41","2015-01-05T08:24:53.000+0000","dturanski","","Retire <module-name>.xml and <module-name>.properties","using <module-name> in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name.  The new convention spring-module should be used instead. This requires refactoring all OOTB modules and corresponding documentation changes.",3.0,0.2696731231678813,0.2696731231678813,0.1970898871788822,True,True,True,1420446293000.0,1420702315000.0,1420705915000.0,1421394772000.0,1421395672000.0,1420446293000.0,-1.0,1420633406000.0,1420637006000.0,1420702315000.0,1420705915000.0,1420702315000.0,1420705915000.0,"dturanski",1420637006000.0,1420446293000.0,45,1420446293000.0,-1.0,"Accept any file name for top level module resources","Expecting <module-name> in module configuration is brittle, especially in conjunction with module upload command which permits the module to be registered under a different name.  The convention should be dropped in favor of any file name. This requires at most one foo.xml, foo.groovy, and/or foo.properties in the top level config folder. It is an exception if multiples are found. 
Accepting any file name provides the most flexibility without sacrificing backward compatibility (except in rare cases in which a module developer may have violated the multiple xml or properties files condition). An alternate approach requiring a well known file name such as 'spring-module' were rejected over concerns that it would break any existing custom module implementations.",3.0,False,3.0,0,0.0,-1,0,True
"XD-2546","Story","Sprint 41","2015-01-05T08:11:48.000+0000","grenfro","","Create AMI for Spark Server installed","",2.0,0.396971910485424,0.396971910485424,0.0663390663390663,True,True,True,1420445508000.0,1420451486000.0,1420455086000.0,1420459667000.0,1420460567000.0,1420445508000.0,-1.0,1420446507000.0,1420450107000.0,1420451486000.0,1420455086000.0,1420451486000.0,1420455086000.0,"grenfro",1420450107000.0,1420445508000.0,45,1420445508000.0,-1.0,"Create AMI for Spark Server installed","",2.0,False,2.0,0,0.0,0,0,False
"XD-2544","Story","Sprint 41","2015-01-05T05:10:53.000+0000","grenfro","Sabby Anandan","Create a PerfSource module","Create a Performance source module that  will generate messages and dispatch messages to a XD stream.  

",5.0,0.003362774878956,0.0033617787963497,0.0023615126430773,True,True,True,1420434653000.0,1420451533000.0,1420455133000.0,1425453417000.0,1425454317000.0,1420434653000.0,-1.0,1420446507000.0,1420450107000.0,1420451528000.0,1420455128000.0,1420451533000.0,1420455133000.0,"grenfro",1420450107000.0,1420434653000.0,45,1420434653000.0,-1.0,"Create a loadGenerator source module","Create a load-generator source module that  will generate messages and dispatch messages to a XD stream.  

",5.0,False,5.0,0,0.0,-1,2,True
"XD-2542","Story","Sprint 41","2015-01-04T09:31:46.000+0000","sabby","","Create ReactorMessageHandler for RxJava based XD processor/sink modules","As a user, I'd like to have a flexible RxJava module so that it can act both as a sink as well as a processor. 
",8.0,0.1772813294973854,0.1772813294973854,0.0,True,True,True,1420363906000.0,1420533179000.0,1420536779000.0,1421317833000.0,1421318733000.0,1420363906000.0,1420444318000.0,1420363906000.0,1420367506000.0,1420533179000.0,1420536779000.0,1420533179000.0,1420536779000.0,"sabby",1420367506000.0,1420363906000.0,45,1420533032000.0,1420444318000.0,"Create MessageHandler for RxJava based processor modules","As a user, I'd like to have a flexible RxJava module so that it can as a processor. 
",5.0,True,5.0,3,60.0,-1,0,True
"XD-2541","Story","Sprint 41","2015-01-04T09:29:27.000+0000","sabby","","Define developer facing interfaces for RxJava processors","As a user, I'd like to implement the core interface contract so that I can create a processor module that uses RxJava API.",1.0,0.179169088196026,0.179169088196026,0.0,True,True,True,1420363767000.0,1420534869000.0,1420538469000.0,1421317842000.0,1421318742000.0,1420363767000.0,1420533045000.0,1420363767000.0,1420367367000.0,1420534869000.0,1420538469000.0,1420534869000.0,1420538469000.0,"sabby",1420367367000.0,1420363767000.0,45,1420534795000.0,1420533045000.0,"Define developer facing interfaces for RxJava processors","As a user, I'd like to implement the core interface contract so that I can create a processor module that uses RxJava API.",5.0,True,5.0,3,-80.0,0,0,False
"XD-2539","Bug","Sprint 41","2014-12-31T13:28:00.000+0000","grussell","grussell","XD 1.1.0.M2 Won't Run on Windows","See http://stackoverflow.com/questions/27725905/spring-xd-1-1-0-m2-fails-to-start

With {{XD_HOME}} set with back-whacks, it fails on {{\U...}} with {{XD_HOME}} set with whacks, it fails with {{/xd\...}}. The StackOverflow failure is similar.

1.0.3 works fine.

{noformat}
set XD_HOME=C:\Users\gpr\Documents\spring-xd-1.1.0.M2-dist\spring-xd-1.1.0.M2\xd

Caused by: java.util.regex.PatternSyntaxException: Illegal/unsupported escape sequence near index 5
.*C:\Users\gpr\Documents\spring-xd-1.1.0.M2-dist\spring-xd-1.1.0.M2\xd\lib\messagebus\([^/]*).*

set XD_HOME=C:/Users/gpr/Documents/spring-xd-1.1.0.M2-dist/spring-xd-1.1.0.M2/xd


Caused by: java.util.regex.PatternSyntaxException: Illegal/unsupported escape sequence near index 71
.*C:/Users/gpr/Documents/spring-xd-1.1.0.M2-dist/spring-xd-1.1.0.M2/xd\lib\messagebus\([^/]*).*
{noformat}",5.0,0.9264819253465651,0.9263137238411616,0.9663877325035392,True,True,True,1420032480000.0,1420098578000.0,1420102178000.0,1420102923000.0,1420103823000.0,1420032480000.0,-1.0,1420101425000.0,1420103823000.0,1420098566000.0,1420102166000.0,1420098578000.0,1420102178000.0,"grussell",1420103823000.0,1420032480000.0,45,1420032480000.0,-1.0,"XD 1.1.0.M2 Won't Run on Windows","See http://stackoverflow.com/questions/27725905/spring-xd-1-1-0-m2-fails-to-start

With {{XD_HOME}} set with back-whacks, it fails on {{\U...}} with {{XD_HOME}} set with whacks, it fails with {{/xd\...}}. The StackOverflow failure is similar.

1.0.3 works fine.

{noformat}
set XD_HOME=C:\Users\gpr\Documents\spring-xd-1.1.0.M2-dist\spring-xd-1.1.0.M2\xd

Caused by: java.util.regex.PatternSyntaxException: Illegal/unsupported escape sequence near index 5
.*C:\Users\gpr\Documents\spring-xd-1.1.0.M2-dist\spring-xd-1.1.0.M2\xd\lib\messagebus\([^/]*).*

set XD_HOME=C:/Users/gpr/Documents/spring-xd-1.1.0.M2-dist/spring-xd-1.1.0.M2/xd


Caused by: java.util.regex.PatternSyntaxException: Illegal/unsupported escape sequence near index 71
.*C:/Users/gpr/Documents/spring-xd-1.1.0.M2-dist/spring-xd-1.1.0.M2/xd\lib\messagebus\([^/]*).*
{noformat}",5.0,False,5.0,0,0.0,0,1,False
"XD-2537","Bug","","2014-12-30T07:12:33.000+0000","grussell","","BackPort script.xml Bug Fix","An additional commit (https://github.com/spring-projects/spring-xd/commit/db1f585) for XD-2230 was applied only to master; it needs to be backported to 1.0.x.

{{s/$\{location\}/$\{script\}/}}
",1.0,0.9276807980049876,0.9276807980049876,-1.0,False,False,False,1419923553000.0,1419923925000.0,1419923954000.0,1419923553000.0,1419923954000.0,1419923553000.0,-1.0,-1.0,-1.0,1419923925000.0,1419923954000.0,1419923925000.0,1419923954000.0,"grussell",-1.0,-1.0,0,1419923553000.0,-1.0,"BackPort script.xml Bug Fix","An additional commit (https://github.com/spring-projects/spring-xd/commit/db1f585) for XD-2230 was applied only to master; it needs to be backported to 1.0.x.

{{s/$\{location\}/$\{script\}/}}


https://gopivotal-com.socialcast.com/messages/22909482",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-2533","Story","Sprint 40","2014-12-22T10:13:22.000+0000","mark.pollack","mark.pollack","Upgrade to Reactor 2.0 M2","",1.0,0.0225479143179255,0.0,0.0,True,True,True,1419243202000.0,1419243262000.0,1419245863000.0,1419244963000.0,1419245863000.0,1419243202000.0,-1.0,1419243202000.0,1419245863000.0,1419243202000.0,1419245863000.0,1419243262000.0,1419245863000.0,"mark.pollack",1419245863000.0,1419243202000.0,45,1419243202000.0,-1.0,"Upgrade to Reactor 2.0 M2","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2532","Bug","Sprint 41","2014-12-19T13:33:31.000+0000","grenfro","","Spark Application Job fails when using remote Spark Master","When executing a Spark Application Job on XD against a remote Spark Master we receive a CNF exception for FSDataInputStream.  Running against a local[1] Spark Master works normally.  ",5.0,0.768603525761319,0.768603525761319,0.4953044382029423,True,True,True,1418996011000.0,1421232306000.0,1421235906000.0,1421904667000.0,1421905567000.0,1418996011000.0,-1.0,1420437127000.0,1420440727000.0,1421232306000.0,1421235906000.0,1421232306000.0,1421235906000.0,"grenfro",1420440727000.0,1418996011000.0,44,1418996011000.0,-1.0,"Spark Application Job fails when using remote Spark Master","When executing a Spark Application Job on XD against a remote Spark Master we receive a CNF exception for FSDataInputStream.  Running against a local[1] Spark Master works normally.  ",5.0,False,5.0,0,0.0,0,2,False
"XD-2527","Story","Sprint 42","2014-12-19T09:23:28.000+0000","sabby","","Add support to extend message compression ","As a user, I'd like to have the option to extend compression support so that I can override the defaults and customize as needed.

Follow-up from this PR: https://github.com/spring-projects/spring-xd/pull/1346",3.0,-1.0,-1.0,0.0,False,True,False,1418981008000.0,-1.0,-1.0,1420448348000.0,1420449248000.0,1418981008000.0,-1.0,1418981008000.0,1418984608000.0,-1.0,-1.0,-1.0,-1.0,"sabby",1418984608000.0,1418981008000.0,44,1418981008000.0,-1.0,"Add support to extend message compression ","As a user, I'd like to have the option to extend compression support so that I can override the defaults and customize as needed.

Follow-up from this PR: https://github.com/spring-projects/spring-xd/pull/1346",3.0,False,3.0,0,0.0,0,0,False
"XD-2524","Improvement","Sprint 42","2014-12-18T08:11:20.000+0000","dturanski","","Update deprecated jackson methods in TupleToJsonStringConverter","warning: [options] bootstrap class path not set in conjunction with -source 1.6
/home/dturanski/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleToJsonStringConverter.java:53: warning: [deprecation] put(String,JsonNode) in ObjectNode has been deprecated
					root.put(name, toObjectNode((Tuple) value));
					    ^
/home/dturanski/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleToJsonStringConverter.java:57: warning: [deprecation] put(String,JsonNode) in ObjectNode has been deprecated
					root.put(name, root.pojoNode(value));",1.0,0.9999965163433074,0.9999965163433074,0.9732002290638262,True,True,True,1418890280000.0,1422621978000.0,1422621991000.0,1422621091000.0,1422621991000.0,1418890280000.0,-1.0,1422521982000.0,1422525582000.0,1422621978000.0,1422621991000.0,1422621978000.0,1422621991000.0,"dturanski",1422525582000.0,1418890280000.0,44,1418890280000.0,-1.0,"Update deprecated jackson methods in TupleToJsonStringConverter","warning: [options] bootstrap class path not set in conjunction with -source 1.6
/home/dturanski/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleToJsonStringConverter.java:53: warning: [deprecation] put(String,JsonNode) in ObjectNode has been deprecated
					root.put(name, toObjectNode((Tuple) value));
					    ^
/home/dturanski/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleToJsonStringConverter.java:57: warning: [deprecation] put(String,JsonNode) in ObjectNode has been deprecated
					root.put(name, root.pojoNode(value));",1.0,False,1.0,0,0.0,-1,0,False
"XD-2523","Story","Sprint 41","2014-12-17T11:22:43.000+0000","sabby","","Add gradle build support for custom module projects","As a user, I'd like to have a gradle build option so that I can support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for 'MODULE' layout and other boilerplate build configuration.

This is dependent on Boot's module layout scoping issue: https://github.com/spring-projects/spring-boot/issues/2187",3.0,0.8998945444021712,0.8800073409519238,0.0,True,True,True,1418815363000.0,1422002587000.0,1422006187000.0,1422356238000.0,1422357138000.0,1418815363000.0,-1.0,1418815363000.0,1418818963000.0,1421932151000.0,1421935751000.0,1422002587000.0,1422006187000.0,"sabby",1418818963000.0,1418815363000.0,44,1418815363000.0,-1.0,"Add gradle build support for custom module projects","As a user, I'd like to have a gradle build option so that I can support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for 'MODULE' layout and other boilerplate build configuration.

This is dependent on Boot's module layout scoping issue: https://github.com/spring-projects/spring-boot/issues/2187",3.0,False,3.0,0,0.0,-1,0,False
"XD-2521","Story","Sprint 40","2014-12-16T20:44:26.000+0000","mark.pollack","","Add options for supporting compression on the message bus with RabbitMQ","https://jira.spring.io/browse/AMQP-453 Added support for compression with RabbitMQ.  XD should expose configuration options to enable and configure compression on the message bus.  Note, some options may be specific for brokers or require additional functionality in XD.  

This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq   As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed.  ",2.0,0.3242054143511506,0.2356784669436996,0.0,True,True,True,1418762666000.0,1418834365000.0,1418837965000.0,1418982919000.0,1418983819000.0,1418762666000.0,-1.0,1418762666000.0,1418766266000.0,1418814787000.0,1418818387000.0,1418834365000.0,1418837965000.0,"mark.pollack",1418766266000.0,1418762666000.0,44,1418762666000.0,-1.0,"Add options for supporting compression on the message bus with RabbitMQ","https://jira.spring.io/browse/AMQP-453 Added support for compression with RabbitMQ.  XD should expose configuration options to enable and configure compression on the message bus.  Note, some options may be specific for brokers or require additional functionality in XD.  

This issue should not address adding additional functionality to make the feature set as common as possible across msg bus implementations, but expose what makes sense with the current code base for rabbitmq   As an example, Kafka supports compressed.topics which lets you pick a subset of topics to be compressed.  ",2.0,False,2.0,0,0.0,0,0,False
"XD-2518","Story","","2014-12-16T14:18:42.000+0000","hillert","hillert","Upgrade to Gradle 2.2","",2.0,-1.0,0.0,-1.0,False,False,True,1418739522000.0,-1.0,-1.0,1418813014000.0,1418813914000.0,1418739522000.0,-1.0,-1.0,-1.0,1418739522000.0,1418743122000.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1418739522000.0,-1.0,"Upgrade to Gradle 2.2","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2516","Bug","Sprint 40","2014-12-16T12:03:54.000+0000","grenfro","","spring_rabbitmq_addresses environment variable is ignored","When trying to configure XD to use a RabbitMQ instance other than the default localhost:5672, a user is supposedto updated the ""spring_rabbitmq_addresses"" environment variable or the spring.rabbitmq.addresses setting in the servers.yml file.  In this case XD is ignoring this environment variable.  

h3. Steps to reproduce 

# set the transport by using ""export XD_TRANSPORT=rabbit""
# set the spring_rabbitmq_addresses by ""export spring_rabbitmq_addresses=foo:5672""
# Startup a admin container on your local machine
# deploy ticktock
#* this should fail
#* start up a local rabbitmq
#* deploy a new ticktock and stream will deploy.

",3.0,0.0983758888170652,0.0944704804998922,0.0274321266968325,True,True,True,1418731434000.0,1418738739000.0,1418742339000.0,1418804790000.0,1418805690000.0,1418731434000.0,-1.0,1418733471000.0,1418737071000.0,1418738449000.0,1418742049000.0,1418738739000.0,1418742339000.0,"grenfro",1418737071000.0,1418731434000.0,44,1418731434000.0,-1.0,"spring_rabbitmq_addresses environment variable is ignored","When trying to configure XD to use a RabbitMQ instance other than the default localhost:5672, a user is supposedto updated the ""spring_rabbitmq_addresses"" environment variable or the spring.rabbitmq.addresses setting in the servers.yml file.  In this case XD is ignoring this environment variable.  

h3. Steps to reproduce 

# set the transport by using ""export XD_TRANSPORT=rabbit""
# set the spring_rabbitmq_addresses by ""export spring_rabbitmq_addresses=foo:5672""
# Startup a admin container on your local machine
# deploy ticktock
#* this should fail
#* start up a local rabbitmq
#* deploy a new ticktock and stream will deploy.

",3.0,False,3.0,0,0.0,-1,1,False
"XD-2510","Bug","Sprint 40","2014-12-15T22:21:37.000+0000","mbogoevici","","Fix classpath issues for RabbitMQ source/sink","",1.0,0.6993080243778568,0.6992524758760792,0.0,True,True,True,1418682097000.0,1418770221000.0,1418773821000.0,1418807213000.0,1418808113000.0,1418682097000.0,-1.0,1418682097000.0,1418685697000.0,1418770214000.0,1418773814000.0,1418770221000.0,1418773821000.0,"mbogoevici",1418685697000.0,1418682097000.0,44,1418682097000.0,-1.0,"Fix classpath issues for RabbitMQ source/sink","RabbitMQ Sink is throwing:
{quote}
09:44:16,031 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 boot.SpringApplication - Application startup failed
org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)
    at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)
    at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)
    at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)
    at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)
    at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
    at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)
    at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)
    at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)
    at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)
    at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)
    at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)
    at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
    at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
    at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
    at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
    at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.
    at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)
    at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)
    at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)
    at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)
    at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)
    at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)
    at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)
    at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
    at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)
    at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
    at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
    at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
    at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
    at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)
    at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)
    at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)
    ... 30 more
09:44:16,036 1.1.0.SNAP ERROR DeploymentsPathChildrenCache-0 server.DeploymentListener - Exception deploying module
org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 19 in XML document from class path resource [config/rabbit.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:399)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:336)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:304)
    at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:180)
    at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:138)
    at org.springframework.boot.BeanDefinitionLoader.load(BeanDefinitionLoader.java:127)
    at org.springframework.boot.SpringApplication.load(SpringApplication.java:620)
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:315)
    at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:139)
    at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:211)
    at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:217)
    at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)
    at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)
    at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)
    at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)
    at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
    at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
    at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
    at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
    at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: org.xml.sax.SAXParseException; lineNumber: 19; columnNumber: 53; cvc-complex-type.3.2.2: Attribute 'default-delivery-mode' is not allowed to appear in element 'int-amqp:outbound-channel-adapter'.
    at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)
    at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.error(ErrorHandlerWrapper.java:134)
    at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:437)
    at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)
    at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:325)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator$XSIErrorReporter.reportError(XMLSchemaValidator.java:458)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.reportSchemaError(XMLSchemaValidator.java:3237)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.processAttributes(XMLSchemaValidator.java:2714)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.handleStartElement(XMLSchemaValidator.java:2056)
    at com.sun.org.apache.xerces.internal.impl.xs.XMLSchemaValidator.emptyElement(XMLSchemaValidator.java:766)
    at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:356)
    at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)
    at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
    at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)
    at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
    at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
    at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
    at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
    at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)
    at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)
    at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:429)
    at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)
{quote}",1.0,False,1.0,0,0.0,-1,0,True
"XD-2509","Bug","Sprint 40","2014-12-15T22:21:02.000+0000","mbogoevici","","Solve CP issues for the Rabbit MessageBus","",2.0,-1.0,0.9820324005891016,0.0,False,True,True,1418682062000.0,-1.0,-1.0,1418721902000.0,1418722802000.0,1418682062000.0,-1.0,1418682062000.0,1418685662000.0,1418722070000.0,1418722802000.0,-1.0,-1.0,"mbogoevici",1418685662000.0,1418682062000.0,44,1418682062000.0,-1.0,"Solve CP issues for the Rabbit MessageBus","Rabbit Message Bus is throwing:

{quote}
10:14:04,678 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.StreamDeploymentListener - Deployment status for stream 'foo': DeploymentStatus{state=failed,error(s)=org.springframework.amqp.UncategorizedAmqpException: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader
    at org.springframework.amqp.rabbit.support.RabbitExceptionTranslator.convertRabbitAccessException(RabbitExceptionTranslator.java:66)
    at org.springframework.amqp.rabbit.connection.RabbitAccessor.convertRabbitAccessException(RabbitAccessor.java:110)
    at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:426)
    at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.afterPropertiesSet(AbstractMessageListenerContainer.java:385)
    at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.doRegisterConsumer(RabbitMessageBus.java:367)
    at org.springframework.xd.dirt.integration.rabbit.RabbitMessageBus.bindConsumer(RabbitMessageBus.java:308)
    at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindMessageConsumer(AbstractMessageBusBinderPlugin.java:183)
    at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.bindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:138)
    at org.springframework.xd.dirt.plugins.stream.StreamPlugin.postProcessModule(StreamPlugin.java:73)
    at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:238)
    at org.springframework.xd.dirt.module.ModuleDeployer.doDeploy(ModuleDeployer.java:218)
    at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:200)
    at org.springframework.xd.dirt.server.DeploymentListener.deployModule(DeploymentListener.java:363)
    at org.springframework.xd.dirt.server.DeploymentListener.deployStreamModule(DeploymentListener.java:332)
    at org.springframework.xd.dirt.server.DeploymentListener.onChildAdded(DeploymentListener.java:179)
    at org.springframework.xd.dirt.server.DeploymentListener.childEvent(DeploymentListener.java:147)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
    at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
    at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
    at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
    at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
    at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    at java.util.concurrent.FutureTask.run(FutureTask.java:262)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: interface org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$ContainerDelegate is not visible from class loader
    at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:616)
    at java.lang.reflect.Proxy$ProxyClassFactory.apply(Proxy.java:592)
    at java.lang.reflect.WeakCache$Factory.get(WeakCache.java:244)
    at java.lang.reflect.WeakCache.get(WeakCache.java:141)
    at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:455)
    at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:738)
    at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:121)
    at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:111)
    at org.springframework.aop.framework.ProxyFactory.getProxy(ProxyFactory.java:96)
    at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.initializeProxy(SimpleMessageListenerContainer.java:586)
    at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doInitialize(SimpleMessageListenerContainer.java:612)
    at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.initialize(AbstractMessageListenerContainer.java:424)
    ... 28 more
{quote}",2.0,False,2.0,0,0.0,-1,0,True
"XD-2508","Story","Sprint 41","2014-12-15T15:29:51.000+0000","grussell","","MQTT: Support the New Spring Integration 4.1 Features","HA Configuration, async sends.

http://docs.spring.io/spring-integration/reference/html/whats-new.html#4.1-mqtt",3.0,0.8691178470244404,0.8691178470244404,0.8326234919168419,True,True,True,1418657391000.0,1420523759000.0,1420527359000.0,1420803919000.0,1420804819000.0,1418657391000.0,-1.0,1420445390000.0,1420448990000.0,1420523759000.0,1420527359000.0,1420523759000.0,1420527359000.0,"grussell",1420448990000.0,1418657391000.0,44,1418657391000.0,-1.0,"MQTT: Support the New Spring Integration 4.1 Features","HA Configuration, async sends.

http://docs.spring.io/spring-integration/reference/html/whats-new.html#4.1-mqtt",3.0,False,3.0,0,0.0,0,0,False
"XD-2505","Bug","Sprint 59","2014-12-15T14:17:24.000+0000","jahubba","thomas.risberg","Undeploying HDFS module closes filesystem","When using the hadoop namespace to create a hadoop configuration and filesystem, the FileSystemFactoryBean uses Hadoop FileSystem.get and not newInstance which will return a FileSystem from the cache.  When undeploying the module, the FileSystemFactoryBean destroy method will close the FileSystem which closes for all other deployed Hadoop modules throwing a java.io.IOException: Filesystem closed",3.0,0.997457927549044,0.9974105146522853,0.9974728658589816,True,True,True,1418653044000.0,1444760820000.0,1444764420000.0,1444826457000.0,1444827357000.0,1418653044000.0,-1.0,1444761211000.0,1444764811000.0,1444759579000.0,1444763179000.0,1444760820000.0,1444764420000.0,"jahubba",1444764811000.0,1418653044000.0,44,1418653044000.0,-1.0,"Undeploying HDFS module closes filesystem","When using the hadoop namespace to create a hadoop configuration and filesystem, the FileSystemFactoryBean uses Hadoop FileSystem.get and not newInstance which will return a FileSystem from the cache.  When undeploying the module, the FileSystemFactoryBean destroy method will close the FileSystem which closes for all other deployed Hadoop modules throwing a java.io.IOException: Filesystem closed",3.0,False,3.0,0,0.0,0,0,False
"XD-2504","Story","Sprint 40","2014-12-15T05:52:54.000+0000","grenfro","grenfro","Upgrade CI Acceptance AMI to HVM","Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI
Paravirtual is being phased out by Amazon.  Also so we can utilize VPC and placement groups in the future.
",5.0,0.999917328042328,0.0,0.0443364845938375,True,True,True,1418622774000.0,1418828389000.0,1418828406000.0,1418827506000.0,1418828406000.0,1418622774000.0,-1.0,1418631891000.0,1418635491000.0,1418622774000.0,1418626374000.0,1418828389000.0,1418828406000.0,"grenfro",1418635491000.0,1418622774000.0,44,1418622774000.0,-1.0,"Upgrade CI Acceptance AMI to HVM","Replace the current paravirtual AMI used for CI tests needed to be replaced with a HVM based AMI
Paravirtual is being phased out by Amazon.  Also so we can utilize VPC and placement groups in the future.
",5.0,False,5.0,0,0.0,-1,0,False
"XD-2502","Story","Sprint 40","2014-12-12T08:30:25.000+0000","mark.pollack","mbogoevici","KafkaSourceSinkTests to use embedded Kafka server","Test is failing since Kafka isn't installed on the CI server.  Using an embedded server will make the testing more robust vs. needing an external server.",2.0,0.4947412927435125,0.0,0.0,True,True,True,1418373025000.0,1418382386000.0,1418385986000.0,1418391046000.0,1418391946000.0,1418373025000.0,-1.0,1418373025000.0,1418376625000.0,1418373025000.0,1418376625000.0,1418382386000.0,1418385986000.0,"mark.pollack",1418376625000.0,1418373025000.0,44,1418373025000.0,-1.0,"KafkaSourceSinkTests to use embedded Kafka server","Test is failing since Kafka isn't installed on the CI server.  Using an embedded server will make the testing more robust vs. needing an external server.",2.0,False,2.0,0,0.0,-1,0,False
"XD-2501","Story","Sprint 41","2014-12-12T07:49:10.000+0000","sabby","","Upgrade to Boot 1.2.0 RELEASE and the depdencies","As a XD Admin, I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies so that we can catch up with the latest features, bug-fixes and enhancements. 

*Following XD dependencies needs upgraded to sync-up with Boot 1.2.0 RELEASE:*

<activemq.version>5.10.0</activemq.version>
<aspectj.version>1.8.4</aspectj.version>
<commons-dbcp2.version>2.0.1</commons-dbcp2.version>
<h2.version>1.4.182</h2.version>
<hibernate.version>dd4.3.7.Final</hibernate.version>
<hibernate-validator.version>5.1.3.Final</hibernate-validator.version>
<hikaricp.version>2.2.5</hikaricp.version>
<hornetq.version>2.4.5.Final</hornetq.version>
<httpasyncclient.version>4.0.2</httpasyncclient.version>
<httpclient.version>4.3.6</httpclient.version>
<jackson.version>2.4.4</jackson.version>
<janino.version>2.6.1</janino.version>
<jetty.version>9.2.4.v20141103</jetty.version>
<jetty-jsp.version>2.2.0.v201112011158</jetty-jsp.version>
<joda-time.version>2.5</joda-time.version>
<jolokia.version>1.2.3</jolokia.version>
<junit.version>4.12</junit.version>
<liquibase.version>3.3.0</liquibase.version>
<log4j.version>1.2.17</log4j.version>
<log4j2.version>2.1</log4j2.version>
<mockito.version>1.10.8</mockito.version>
<mongodb.version>2.12.4</mongodb.version>
<mysql.version>5.1.34</mysql.version>
<reactor.version>1.1.5.RELEASE</reactor.version>
<reactor-spring.version>1.1.3.RELEASE</reactor-spring.version>
<servlet-api.version>3.1.0</servlet-api.version>
<spring.version>4.1.3.RELEASE</spring.version>
<spring-batch.version>3.0.2.RELEASE</spring-batch.version>
<spring-data-releasetrain.version>Evans-SR1</spring-data-releasetrain.version>
<spring-hateoas.version>0.16.0.RELEASE</spring-hateoas.version>
<spring-mobile.version>1.1.3.RELEASE</spring-mobile.version>
<spring-security.version>3.2.5.RELEASE</spring-security.version>
<tomcat.version>8.0.15</tomcat.version>
<undertow.version>1.1.1.Final</undertow.version>",5.0,0.6888928250265689,0.2518025996675477,0.0,True,True,True,1418370550000.0,1418623353000.0,1418626953000.0,1418736620000.0,1418737520000.0,1418370550000.0,-1.0,1418370550000.0,1418374150000.0,1418462954000.0,1418466554000.0,1418623353000.0,1418626953000.0,"sabby",1418374150000.0,1418370550000.0,44,1418370550000.0,-1.0,"Upgrade to Boot 1.2.0 RELEASE and the dependencies","As a XD Admin, I'd like to upgrade to Spring Boot 1.2.0 RELEASE and the associated dependencies so that we can catch up with the latest features, bug-fixes and enhancements. 

*Following XD dependencies needs upgraded to sync-up with Boot 1.2.0 RELEASE:*

<activemq.version>5.10.0</activemq.version>
<aspectj.version>1.8.4</aspectj.version>
<commons-dbcp2.version>2.0.1</commons-dbcp2.version>
<h2.version>1.4.182</h2.version>
<hibernate.version>dd4.3.7.Final</hibernate.version>
<hibernate-validator.version>5.1.3.Final</hibernate-validator.version>
<hikaricp.version>2.2.5</hikaricp.version>
<hornetq.version>2.4.5.Final</hornetq.version>
<httpasyncclient.version>4.0.2</httpasyncclient.version>
<httpclient.version>4.3.6</httpclient.version>
<jackson.version>2.4.4</jackson.version>
<janino.version>2.6.1</janino.version>
<jetty.version>9.2.4.v20141103</jetty.version>
<jetty-jsp.version>2.2.0.v201112011158</jetty-jsp.version>
<joda-time.version>2.5</joda-time.version>
<jolokia.version>1.2.3</jolokia.version>
<junit.version>4.12</junit.version>
<liquibase.version>3.3.0</liquibase.version>
<log4j.version>1.2.17</log4j.version>
<log4j2.version>2.1</log4j2.version>
<mockito.version>1.10.8</mockito.version>
<mongodb.version>2.12.4</mongodb.version>
<mysql.version>5.1.34</mysql.version>
<reactor.version>1.1.5.RELEASE</reactor.version>
<reactor-spring.version>1.1.3.RELEASE</reactor-spring.version>
<servlet-api.version>3.1.0</servlet-api.version>
<spring.version>4.1.3.RELEASE</spring.version>
<spring-batch.version>3.0.2.RELEASE</spring-batch.version>
<spring-data-releasetrain.version>Evans-SR1</spring-data-releasetrain.version>
<spring-hateoas.version>0.16.0.RELEASE</spring-hateoas.version>
<spring-mobile.version>1.1.3.RELEASE</spring-mobile.version>
<spring-security.version>3.2.5.RELEASE</spring-security.version>
<tomcat.version>8.0.15</tomcat.version>
<undertow.version>1.1.1.Final</undertow.version>",5.0,False,5.0,0,0.0,-1,0,True
"XD-2499","Story","Sprint 41","2014-12-12T07:32:00.000+0000","sabby","","Document 'partitionResultsTimeout' metadata attribute","As a user, I'd like to use _partitionResultsTimeout_ attribute for jobs that inherit singlestep-partitioning strategy but it is not exposed as a metadata attribute in the wiki. 

*Note:*
The property should be available for all the jobs that import; 3 OOTB jobs have it imported (ref. attachment)",1.0,0.9237780920264472,0.9042527488830944,0.0,True,True,True,1418369520000.0,1422609788000.0,1422613388000.0,1422958757000.0,1422959657000.0,1418369520000.0,-1.0,1418369520000.0,1418373120000.0,1422520164000.0,1422523764000.0,1422609788000.0,1422613388000.0,"sabby",1418373120000.0,1418369520000.0,44,1418369520000.0,-1.0,"Document 'partitionResultsTimeout' metadata attribute","As a user, I'd like to use _partitionResultsTimeout_ attribute for jobs that inherit singlestep-partitioning strategy but it is not exposed as a metadata attribute in the wiki. 

*Note:*
The property should be available for all the jobs that import; 3 OOTB jobs have it imported (ref. attachment)",1.0,False,1.0,0,0.0,0,0,False
"XD-2497","Story","Sprint 46","2014-12-11T16:05:35.000+0000","mark.pollack","","Investigate lack of falling back to origin/master when building docs on a branch","When building on a branch, the docs should be defaulting to build from origin/master, but that doesn't seem to be happening.  Instead an explicit -Pwikibranch=origin/master is required to be specified on the command line.

",2.0,-1.0,-1.0,0.8662018243659003,False,True,False,1418313935000.0,-1.0,-1.0,1426864609000.0,1426865509000.0,1418313935000.0,-1.0,1425721324000.0,1425724924000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1425724924000.0,1418313935000.0,44,1418313935000.0,-1.0,"Investigate lack of falling back to origin/master when building docs on a branch","When building on a branch, the docs should be defaulting to build from origin/master, but that doesn't seem to be happening.  Instead an explicit -Pwikibranch=origin/master is required to be specified on the command line.

",2.0,False,2.0,0,0.0,0,0,False
"XD-2496","Story","Sprint 40","2014-12-11T13:37:52.000+0000","mark.pollack","mark.pollack","Refactor use of UUID and getContainerHostForSource in integration tests","Some cleanup to make the tests a bit easer to read.",1.0,0.8235388452147958,0.0,0.0,True,True,True,1418305072000.0,1418808330000.0,1418811930000.0,1418915264000.0,1418916164000.0,1418305072000.0,-1.0,1418305072000.0,1418308672000.0,1418305072000.0,1418308672000.0,1418808330000.0,1418811930000.0,"mark.pollack",1418308672000.0,1418305072000.0,44,1418305072000.0,-1.0,"Refactor use of getContainerHostForSource in integration tests","Some cleanup to make the tests a bit easer to read.",1.0,False,1.0,0,0.0,-1,0,True
"XD-2492","Improvement","Sprint 41","2014-12-09T20:34:41.000+0000","bzeyben","","Increase Exit Description Text field on Job Execution Process step page","The Step Execution Process (http://localhost:9393/admin-ui/#/jobs/executions/38/52) page should list more lines of text for 'Exit Description' field to make sense of error messages.

",2.0,0.8766077385934196,0.8766077385934196,0.0133288394307975,True,True,True,1418157281000.0,1420789372000.0,1420792972000.0,1421158968000.0,1421159868000.0,1418157281000.0,-1.0,1418197302000.0,1418200902000.0,1420789372000.0,1420792972000.0,1420789372000.0,1420792972000.0,"bzeyben",1418200902000.0,1418157281000.0,44,1418157281000.0,-1.0,"Increase Exit Description Text field on Job Execution Process step page","The Step Execution Process (http://localhost:9393/admin-ui/#/jobs/executions/38/52) page should list more lines of text for 'Exit Description' field to make sense of error messages.

*Scope:*
Investigate how much information can be collected directly from the ExecutionContext. It may be dependent on the error types. Let's have the observation documented to decide next steps. 
",2.0,False,2.0,0,0.0,0,0,True
"XD-2490","Story","Sprint 40","2014-12-09T12:14:20.000+0000","mark.pollack","","Update Reactor Stream processor to use latest snapshots","The code base is changing a bit, so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed.  Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.",2.0,0.3890236395609691,0.3890236395609691,0.0,True,True,True,1418127260000.0,1418392061000.0,1418395661000.0,1418807041000.0,1418807941000.0,1418127260000.0,-1.0,1418127260000.0,1418130860000.0,1418392061000.0,1418395661000.0,1418392061000.0,1418395661000.0,"mark.pollack",1418130860000.0,1418127260000.0,44,1418127260000.0,-1.0,"Update Reactor Stream processor to use latest snapshots","The code base is changing a bit, so using 2.0 M1 for development is stable up until all major JIRA issues have bee completed.  Then we should track snapshots in preparation to move to 2.0. M2 when it gets released.",2.0,False,2.0,0,0.0,-1,0,False
"XD-2489","Story","Sprint 40","2014-12-09T12:10:27.000+0000","mark.pollack","","Reference documentation on creating Reactive Stream processor/sink","",3.0,0.9316123687208324,0.9316123687208324,0.0,True,True,True,1418127027000.0,1419260503000.0,1419264103000.0,1419342809000.0,1419343709000.0,1418127027000.0,-1.0,1418127027000.0,1418130627000.0,1419260503000.0,1419264103000.0,1419260503000.0,1419264103000.0,"mark.pollack",1418130627000.0,1418127027000.0,44,1418127027000.0,-1.0,"Reference documentation on creating Reactive Stream processor/sink","",3.0,False,3.0,0,0.0,0,0,False
"XD-2488","Story","Sprint 40","2014-12-09T12:08:48.000+0000","mark.pollack","","Create sample module in spring-xd-modules for a Reactor Stream processor","A sample, perhaps taken from Pivotal Labs use-case in Denver, that would calculate some time window averages for a many individual senor values .",3.0,0.7719324265663435,0.7719324265663435,0.0,True,True,True,1418126928000.0,1418980500000.0,1418984100000.0,1419231788000.0,1419232688000.0,1418126928000.0,-1.0,1418126928000.0,1418130528000.0,1418980500000.0,1418984100000.0,1418980500000.0,1418984100000.0,"mark.pollack",1418130528000.0,1418126928000.0,44,1418126928000.0,-1.0,"Create sample module in spring-xd-modules for a Reactor Stream processor","A sample, perhaps taken from Pivotal Labs use-case in Denver, that would calculate some time window averages for a many individual senor values .",3.0,False,3.0,0,0.0,-1,0,False
"XD-2487","Story","Sprint 40","2014-12-09T12:06:36.000+0000","mark.pollack","","Create ReactorMessageHandler for Reactor based XD processor/sink modules","The module should be flexible to act as a sink as well as a processor.  ErrorHandling will be considered as part of another JIRA",5.0,0.2830346278383897,0.2830346278383897,0.0,True,True,True,1418126796000.0,1418373819000.0,1418377419000.0,1418998662000.0,1418999562000.0,1418126796000.0,-1.0,1418126796000.0,1418130396000.0,1418373819000.0,1418377419000.0,1418373819000.0,1418377419000.0,"mark.pollack",1418130396000.0,1418126796000.0,44,1418126796000.0,-1.0,"Create ReactorMessageHandler for Reactor based XD processor/sink modules","The module should be flexible to act as a sink as well as a processor.  ErrorHandling will be considered as part of another JIRA",5.0,False,5.0,0,0.0,-1,0,False
"XD-2486","Bug","Sprint 41","2014-12-09T07:55:02.000+0000","jahubba","","Context Deserialize Doesn't Use Parent First Classloader","If a class is added to a batch execution context that is located in an isolated context, an exception will be thrown when that object is deserialized.  It appears the serialize doesn't use the ParentFirstClassloader during deserialization.",8.0,0.9007362984680436,0.4312183466205372,0.0099465347854955,True,True,True,1418111702000.0,1425893062000.0,1425896662000.0,1426749690000.0,1426750590000.0,1418111702000.0,1419409147000.0,1418197629000.0,1418201229000.0,1421836949000.0,1421840549000.0,1425893062000.0,1425896662000.0,"jahubba",1418201229000.0,1418111702000.0,44,1419409147000.0,1419409147000.0,"Context Deserialize Doesn't Use Parent First Classloader","If a class is added to a batch execution context that is located in an isolated context, an exception will be thrown when that object is deserialized.  It appears the serialize doesn't use the ParentFirstClassloader during deserialization.",15.0,True,15.0,1,-46.66666666666666,0,0,False
"XD-2485","Story","Sprint 41","2014-12-09T06:52:23.000+0000","thomas.risberg","","Update spring-data-hadoop version to 2.0.4 for XD 1.0.3","",1.0,0.9876766567643998,0.9876766567643998,0.0257085878298173,True,True,True,1418107943000.0,1419228716000.0,1419232316000.0,1419241800000.0,1419242700000.0,1418107943000.0,-1.0,1418137116000.0,1418140716000.0,1419228716000.0,1419232316000.0,1419228716000.0,1419232316000.0,"thomas.risberg",1418140716000.0,1418107943000.0,44,1418107943000.0,-1.0,"Update spring-data-hadoop version to 2.0.4 for XD 1.0.3","",1.0,False,1.0,0,0.0,0,0,False
"XD-2484","Story","Sprint 40","2014-12-09T06:32:37.000+0000","thomas.risberg","","Update spring-data-hadoop version to 2.1.0.M3","",1.0,0.7822772278101366,0.7822772278101366,0.0075725823458769,True,True,True,1418106757000.0,1418993311000.0,1418996911000.0,1419239156000.0,1419240056000.0,1418106757000.0,-1.0,1418115339000.0,1418118939000.0,1418993311000.0,1418996911000.0,1418993311000.0,1418996911000.0,"thomas.risberg",1418118939000.0,1418106757000.0,44,1418106757000.0,-1.0,"Update spring-data-hadoop version to 2.1.0.M3","",1.0,False,1.0,0,0.0,0,0,False
"XD-2483","Story","Sprint 40","2014-12-09T06:24:13.000+0000","thomas.risberg","","Add codec option to hdfs-dataset sink","As a user, I would like to be able disable snappy compression when using hdfs-dataset sink with Avro files. I'd also like to be able to provide a different codec.",1.0,0.7878993138240638,0.7878993138240638,0.0080391949339401,True,True,True,1418106253000.0,1418999589000.0,1419003189000.0,1419239173000.0,1419240073000.0,1418106253000.0,-1.0,1418115368000.0,1418118968000.0,1418999589000.0,1419003189000.0,1418999589000.0,1419003189000.0,"thomas.risberg",1418118968000.0,1418106253000.0,44,1418106253000.0,-1.0,"Add codec option to hdfs-dataset sink","As a user, I would like to be able disable snappy compression when using hdfs-dataset sink with Avro files. I'd also like to be able to provide a different codec.",1.0,False,1.0,0,0.0,0,0,False
"XD-2482","Improvement","Sprint 41","2014-12-09T06:12:27.000+0000","emedina","","Add ""initialDelay"" to ""source:trigger""","Currently, the {{source:trigger}} module is based on 3 profiles: {{date}}, {{cron}} or {{fixedDelay}}, where the latter has precedence over the former in {{TriggerSourceOptionsMetadata}}:

{code:java}
@Override
public String[] profilesToActivate() {
    if (cron != null) {
        return new String[] { ""use-cron"" };
    }
    else if (fixedDelay != null) {
        return new String[] { ""use-delay"" };
    }
    else {
        return new String[] { ""use-date"" };
    }
}
{code}

Therefore it is not possible to combine {{date}} and {{fixedDelay}} to start off at a specific point in time, and then repeat every X seconds.

This is a request to provide another parameter to {{source:trigger}} such as *{{initialDelay}}* to be able to achieve the desired behaviour.",1.0,0.9891125941948068,0.9890542650482432,0.0276825920270069,True,True,True,1418105547000.0,1421395289000.0,1421398889000.0,1421430600000.0,1421431500000.0,1418105547000.0,1420444735000.0,1418197618000.0,1418201218000.0,1421395095000.0,1421398695000.0,1421395289000.0,1421398889000.0,"emedina",1418201218000.0,1418105547000.0,44,1420444735000.0,-1.0,"Add ""initialDelay"" to ""source:trigger""","Currently, the {{source:trigger}} module is based on 3 profiles: {{date}}, {{cron}} or {{fixedDelay}}, where the latter has precedence over the former in {{TriggerSourceOptionsMetadata}}:

{code:java}
@Override
public String[] profilesToActivate() {
    if (cron != null) {
        return new String[] { ""use-cron"" };
    }
    else if (fixedDelay != null) {
        return new String[] { ""use-delay"" };
    }
    else {
        return new String[] { ""use-date"" };
    }
}
{code}

Therefore it is not possible to combine {{date}} and {{fixedDelay}} to start off at a specific point in time, and then repeat every X seconds.

This is a request to provide another parameter to {{source:trigger}} such as *{{initialDelay}}* to be able to achieve the desired behaviour.",0.0,False,0.0,1,-1.0,0,0,False
"XD-2481","Story","Sprint 40","2014-12-08T16:34:30.000+0000","mark.pollack","mark.pollack","Define developer facing interfaces for Reactor Stream processors","What is the core interface contract users will be exposed to when creating a processor module that uses Reactor's Stream API.   Some consideration for error handling should be considered as it maybe outside normal exception throwing signatures.",2.0,0.9378790630647796,0.0,0.0,True,True,True,1418056470000.0,1418371467000.0,1418375067000.0,1418391431000.0,1418392331000.0,1418056470000.0,-1.0,1418056470000.0,1418060070000.0,1418056470000.0,1418060070000.0,1418371467000.0,1418375067000.0,"mark.pollack",1418060070000.0,1418056470000.0,44,1418056470000.0,-1.0,"Define developer facing interfaces for Reactor Stream processors","What is the core interface contract users will be exposed to when creating a processor module that uses Reactor's Stream API.   Some consideration for error handling should be considered as it maybe outside normal exception throwing signatures.",2.0,False,2.0,0,0.0,0,0,False
"XD-2480","Story","Sprint 41","2014-12-08T15:07:05.000+0000","sabby","","Benchmark: Sqoop vs. jdbchdfs","As a QA, I'd like to benchmark _Sqoop_ vs. _jdbchdfs_ batch job so that I can compare and contrast performance stats. ",5.0,0.972837656916106,0.972837656916106,0.0,True,True,True,1418051225000.0,1424252351000.0,1424255951000.0,1424424591000.0,1424425491000.0,1418051225000.0,-1.0,1418051225000.0,1418054825000.0,1424252351000.0,1424255951000.0,1424252351000.0,1424255951000.0,"sabby",1418054825000.0,1418051225000.0,44,1418051225000.0,-1.0,"Benchmark: Sqoop vs. jdbchdfs","As a QA, I'd like to benchmark _Sqoop_ vs. _jdbchdfs_ batch job so that I can compare and contrast performance stats. ",5.0,False,5.0,0,0.0,0,0,False
"XD-2478","Story","Sprint 41","2014-12-08T15:03:04.000+0000","sabby","","Add support to access Sqoop logs","As a user, I'd like to access Sqoop logs so that I can troubleshoot or evaluate the errors or current state respectively. 

We will have to identify how to capture the Sqoop logs and stream them to our logging mechanism.",5.0,-1.0,-1.0,0.0,False,True,False,1418050984000.0,-1.0,-1.0,1420454823000.0,1420455723000.0,1418050984000.0,-1.0,1418050984000.0,1418054584000.0,-1.0,-1.0,-1.0,-1.0,"sabby",1418054584000.0,1418050984000.0,44,1418050984000.0,-1.0,"Add support to access Sqoop logs","As a user, I'd like to access Sqoop logs so that I can troubleshoot or evaluate the errors or current state respectively. 

We will have to identify how to capture the Sqoop logs and stream them to our logging mechanism.",5.0,False,5.0,0,0.0,0,0,False
"XD-2477","Story","Sprint 41","2014-12-08T15:00:19.000+0000","sabby","","Add support to stop existing Sqoop jobs","As a user, I'd like to have the option to _stop_ an existing Sqoop job so that I can clean-up resources at the time of completion.",8.0,0.7536924348236308,0.7536924348236308,0.0,True,True,True,1418050819000.0,1420456860000.0,1420460460000.0,1421242257000.0,1421243157000.0,1418050819000.0,-1.0,1418050819000.0,1418054419000.0,1420456860000.0,1420460460000.0,1420456860000.0,1420460460000.0,"sabby",1418054419000.0,1418050819000.0,44,1418050819000.0,-1.0,"Add support to stop existing Sqoop jobs","As a user, I'd like to have the option to _stop_ an existing Sqoop job so that I can clean-up resources at the time of completion.",8.0,False,8.0,0,0.0,0,1,False
"XD-2475","Story","Sprint 40","2014-12-08T11:07:16.000+0000","sabby","grussell","Add batching support to Spring AMQP/Rabbit","As a user, I'd like to have the option to setup _batching_ so that I can ingest data in batches as opposed to payload-at-a-time.",8.0,0.3900100326096322,0.0001299712164867,0.0,True,True,True,1418036836000.0,1418303902000.0,1418307502000.0,1418720703000.0,1418721603000.0,1418036836000.0,-1.0,1418036836000.0,1418040436000.0,1418036925000.0,1418040525000.0,1418303902000.0,1418307502000.0,"sabby",1418040436000.0,1418036836000.0,44,1418036836000.0,-1.0,"Add batching support to Spring AMQP/Rabbit","As a user, I'd like to have the option to setup _batching_ so that I can ingest data in batches as opposed to payload-at-a-time.",8.0,False,8.0,0,0.0,-1,0,False
"XD-2473","Story","Sprint 41","2014-12-08T09:51:46.000+0000","sabby","","Add support for ACK mode","As a user, I'd like to have the option to _ACK_ messages so that I can guarantee that the message/request sent is successful. ",3.0,0.8055560835799207,0.2566219535634252,0.0,True,True,True,1418032306000.0,1418795108000.0,1418798708000.0,1418978332000.0,1418979232000.0,1418032306000.0,-1.0,1418032306000.0,1418035906000.0,1418275308000.0,1418278908000.0,1418795108000.0,1418798708000.0,"sabby",1418035906000.0,1418032306000.0,44,1418032306000.0,-1.0,"Kafka Bus: Add support for ACK mode","As a user, I'd like to have the option to _ACK_ messages so that I can guarantee that the message/request sent is successful. ",3.0,False,3.0,0,0.0,0,0,True
"XD-2471","Story","Sprint 40","2014-12-08T09:44:54.000+0000","sabby","","Concurrency and compression support","As a user, I'd like to have concurrency and compression support for Kafka so that I can increase performance throughput and/or increase responsiveness

*Things to consider:*
* make global configuration options be ""defaults"" and allow per-deployment overrides
* add options for 
** concurrency
** compression support",3.0,0.8056400918375426,0.2569403319874383,0.0,True,True,True,1418031894000.0,1418795097000.0,1418798697000.0,1418978319000.0,1418979219000.0,1418031894000.0,-1.0,1418031894000.0,1418035494000.0,1418275300000.0,1418278900000.0,1418795097000.0,1418798697000.0,"sabby",1418035494000.0,1418031894000.0,44,1418031894000.0,-1.0,"Kafka Bus: Concurrency and compression support","As a user, I'd like to have concurrency and compression support for Kafka so that I can increase performance throughput and/or increase responsiveness

*Things to consider:*
* make global configuration options be ""defaults"" and allow per-deployment overrides
* add options for 
** concurrency
** compression support",3.0,False,3.0,0,0.0,0,0,True
"XD-2467","Story","Sprint 40","2014-12-08T05:00:12.000+0000","mark.fisher","","Implement a Spark Streaming Driver application that can be controlled as an XD module instance","This should include lifecycle management, so that when the module's stream is undeployed, the Spark Streaming application should be stopped, etc.

Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata.
",8.0,0.7137917103812113,0.7137917103812113,9.17623092667462e-06,True,True,True,1418014812000.0,1421048506000.0,1421052106000.0,1422264023000.0,1422264923000.0,1418014812000.0,-1.0,1418014851000.0,1418018451000.0,1421048506000.0,1421052106000.0,1421048506000.0,1421052106000.0,"mark.fisher",1418018451000.0,1418014812000.0,44,1418014812000.0,-1.0,"Implement a Spark Streaming Driver application that can be controlled as an XD module instance","This should include lifecycle management, so that when the module's stream is undeployed, the Spark Streaming application should be stopped, etc.

Deploying a number of module instances should result in multiple receiver tasks, and those should bind to the bus using the consumer side partitioning metadata.
",8.0,False,8.0,0,0.0,0,1,False
"XD-2466","Story","Sprint 40","2014-12-08T04:57:12.000+0000","mark.fisher","","Implement a dirt plugin for Spark Streaming support","",5.0,0.7138073929700177,0.7138073929700177,0.0,True,True,True,1418014632000.0,1421048514000.0,1421052114000.0,1422264013000.0,1422264913000.0,1418014632000.0,-1.0,1418014632000.0,1418018232000.0,1421048514000.0,1421052114000.0,1421048514000.0,1421052114000.0,"mark.fisher",1418018232000.0,1418014632000.0,44,1418014632000.0,-1.0,"Implement a dirt plugin for Spark Streaming support","",5.0,False,5.0,0,0.0,0,0,False
"XD-2462","Story","Sprint 40","2014-12-07T15:08:11.000+0000","sabby","","Acceptance test for Kafka as a message bus","As a QA, I'd like to include acceptance test coverage for _Kafka_ as a message bus so that I can validate the functionality as part of every CI build.",5.0,0.0962044651601513,0.0962044651601513,0.0008391560851562,True,True,True,1417964891000.0,1418055460000.0,1418059060000.0,1418905413000.0,1418906313000.0,1417964891000.0,1418023873000.0,1417965681000.0,1417969281000.0,1418055460000.0,1418059060000.0,1418055460000.0,1418059060000.0,"sabby",1417969281000.0,1417964891000.0,44,1418023873000.0,1418023873000.0,"Acceptance test for Kafka as a message bus","As a QA, I'd like to include acceptance test coverage for _Kafka_ as a message bus so that I can validate the functionality as part of every CI build.",3.0,True,3.0,1,66.66666666666666,0,1,False
"XD-2456","Story","Sprint 40","2014-12-07T14:42:16.000+0000","sabby","","Acceptance test for ""spark-app"" batch job","As a QA, I'd like to include acceptance test coverage for _spark-app_ batch job so that I can validate the functionality as part of every CI build. ",5.0,0.8750727967476327,0.8750716423888992,0.0003247595903642,True,True,True,1417963336000.0,1422511694000.0,1422515294000.0,1423160127000.0,1423161027000.0,1417963336000.0,1418023395000.0,1417965024000.0,1417968624000.0,1422511688000.0,1422515288000.0,1422511694000.0,1422515294000.0,"sabby",1417968624000.0,1417963336000.0,44,1418023395000.0,1418023395000.0,"Acceptance test for ""spark-app"" batch job","As a QA, I'd like to include acceptance test coverage for _spark-app_ batch job so that I can validate the functionality as part of every CI build. ",3.0,True,3.0,1,66.66666666666666,0,0,False
"XD-2433","Story","Sprint 40","2014-12-06T11:42:14.000+0000","sabby","","[Placeholder] Spark Integration #2","",8.0,0.7234660424314167,0.7234660424314167,0.0,True,True,True,1417866134000.0,1421048494000.0,1421052094000.0,1422264003000.0,1422264903000.0,1417866134000.0,1418014553000.0,1417866134000.0,1417869734000.0,1421048494000.0,1421052094000.0,1421048494000.0,1421052094000.0,"sabby",1417869734000.0,1417866134000.0,44,1418014553000.0,1418014553000.0,"Implement a Spark Streaming Receiver that binds to the MessageBus","",5.0,True,5.0,1,60.0,0,0,True
"XD-2432","Story","Sprint 40","2014-12-06T11:42:02.000+0000","sabby","","[Placeholder] Spark Integration #1","",3.0,0.7234669517767357,0.7234669517767357,0.0,True,True,True,1417866122000.0,1421048486000.0,1421052086000.0,1422263991000.0,1422264891000.0,1417866122000.0,1418014518000.0,1417866122000.0,1417869722000.0,1421048486000.0,1421052086000.0,1421048486000.0,1421052086000.0,"sabby",1417869722000.0,1417866122000.0,44,1418014518000.0,1418014518000.0,"Define developer-facing interfaces for Spark Streaming modules","",5.0,True,5.0,1,-40.0,0,0,True
"XD-2431","Story","Sprint 39","2014-12-05T15:51:04.000+0000","sabby","","Workaround latest boot snapshot issue","* The workaround explicitly updates spring-core (latest boot needs it)
* merges all application.yml documents that are not profile-specific under on spring: key (the latest boot requires it, at least for now. Boot may go back, see spring-projects/spring-boot#2022",3.0,-1.0,-1.0,0.0,False,False,False,1417794664000.0,-1.0,-1.0,1417794664000.0,1417794687000.0,1417794664000.0,-1.0,1417794664000.0,1417794687000.0,-1.0,-1.0,-1.0,-1.0,"sabby",1417794687000.0,1417794664000.0,44,1417794664000.0,-1.0,"Workaround latest boot snapshot issue","* The workaround explicitly updates spring-core (latest boot needs it)
* merges all application.yml documents that are not profile-specific under on spring: key (the latest boot requires it, at least for now. Boot may go back, see spring-projects/spring-boot#2022",3.0,False,3.0,0,0.0,0,0,False
"XD-2430","Story","Sprint 40","2014-12-05T13:09:25.000+0000","thomas.risberg","","Create a Sqoop job and required batch tasklet integration code","Based on the POC from XD-2124 we should create the actual implementation.

Things to consider to store in step context:
- capture Log output/MapReduce job counters
- capture last-value from incremental imports
",8.0,0.3428972198339996,0.3428972198339996,0.0558438604123412,True,True,True,1417784965000.0,1418283900000.0,1418287500000.0,1419239122000.0,1419240022000.0,1417784965000.0,-1.0,1417866221000.0,1417869821000.0,1418283900000.0,1418287500000.0,1418283900000.0,1418287500000.0,"thomas.risberg",1417869821000.0,1417784965000.0,44,1417784965000.0,-1.0,"Create a Sqoop job and required batch tasklet integration code","Based on the POC from XD-2124 we should create the actual implementation.

Things to consider to store in step context:
- capture Log output/MapReduce job counters
- capture last-value from incremental imports
",8.0,False,8.0,0,0.0,0,0,False
"XD-2429","Bug","Sprint 39","2014-12-05T12:16:59.000+0000","grussell","grussell","Bind Producer Before Consumer","{quote}
		Here is the full exception:
		org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'ResourceConfiguredModule [name=filter, type=processor, group=request-rate, index=0 @58b0f318]:use-expression,default,admin,singlenode,hsqldbServer:9393.output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
		and here is that stream:
		topic:httpstartstop > filter --expression=payload.getHttpStartStop().getPeerType().name().equals('Client') | requestRateAggregator | appMetricsSplitter | router --expression='topic:app-request-rate-'+#jsonPath(payload,'$.appId')
[2:59 PM] Gary Russell: @MarkFisher  @IlayaperumalGopinathan @PatrickPeralta  This looks like another (not fixed by the previous fix) timing problem with taps when using singlenode. The tap is started before the tap stream is deployed. But it's not clear to me how the filter module could be deployed/bound as a consumer before the requestRateAggregator
[3:08 PM] Gary Russell: I see the problem: AbstractMessageBusBinderPlugin.bindConsumerAndProducers() binds the consumer before the producer - this is the wrong order for a passive component such as the filter. /cc @DavidTuranski
{quote}",1.0,0.9265664322126864,0.0,0.9131444795766924,True,True,True,1417781819000.0,1417796178000.0,1417797316000.0,1417796416000.0,1417797316000.0,1417781819000.0,-1.0,1417795970000.0,1417797316000.0,1417781819000.0,1417785419000.0,1417796178000.0,1417797316000.0,"grussell",1417797316000.0,1417781819000.0,44,1417781819000.0,-1.0,"Bind Producer Before Consumer","{quote}
		Here is the full exception:
		org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'ResourceConfiguredModule [name=filter, type=processor, group=request-rate, index=0 @58b0f318]:use-expression,default,admin,singlenode,hsqldbServer:9393.output'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
		and here is that stream:
		topic:httpstartstop > filter --expression=payload.getHttpStartStop().getPeerType().name().equals('Client') | requestRateAggregator | appMetricsSplitter | router --expression='topic:app-request-rate-'+#jsonPath(payload,'$.appId')
[2:59 PM] Gary Russell: @MarkFisher  @IlayaperumalGopinathan @PatrickPeralta  This looks like another (not fixed by the previous fix) timing problem with taps when using singlenode. The tap is started before the tap stream is deployed. But it's not clear to me how the filter module could be deployed/bound as a consumer before the requestRateAggregator
[3:08 PM] Gary Russell: I see the problem: AbstractMessageBusBinderPlugin.bindConsumerAndProducers() binds the consumer before the producer - this is the wrong order for a passive component such as the filter. /cc @DavidTuranski
{quote}",1.0,False,1.0,0,0.0,0,1,False
"XD-2427","Story","Sprint 40","2014-12-04T12:27:32.000+0000","hillert","hillert","Use repo.spring.io as NPM repository","In order to improve the build reliability, we should be using the NPM repo provided by *repo.spring.io* 

See *spring-xd-ui/README.md* for further details.",1.0,0.0029345633859901,0.0,0.2468623042257327,True,True,True,1417696052000.0,1417699093000.0,1417702693000.0,1418731422000.0,1418732322000.0,1417696052000.0,-1.0,1417951868000.0,1417955468000.0,1417696052000.0,1417699652000.0,1417699093000.0,1417702693000.0,"hillert",1417955468000.0,1417696052000.0,44,1417696052000.0,-1.0,"Use repo.spring.io as NPM repository","In order to improve the build reliability, we should be using the NPM repo provided by *repo.spring.io* 

See *spring-xd-ui/README.md* for further details.",1.0,False,1.0,0,0.0,0,0,False
"XD-2426","Story","Sprint 40","2014-12-04T03:23:16.000+0000","eric.bottard","eric.bottard","Travis CI improvements","Travis CI recently introduced docker based builds.
This prevents root access (which we don't need), but allows caching (which we could not use before) and seems to come with beefier machine specs",2.0,-1.0,0.0,0.2866643746442199,False,True,True,1417663396000.0,-1.0,-1.0,1418372204000.0,1418373104000.0,1417663396000.0,-1.0,1417866844000.0,1417870444000.0,1417663396000.0,1417666996000.0,-1.0,-1.0,"eric.bottard",1417870444000.0,1417663396000.0,44,1417663396000.0,-1.0,"Travis CI improvements","Travis CI recently introduced docker based builds.
This prevents root access (which we don't need), but allows caching (which we could not use before) and seems to come with beefier machine specs",2.0,False,2.0,0,0.0,0,0,False
"XD-2425","Bug","Sprint 40","2014-12-04T00:35:08.000+0000","dokeeffe","grussell","SpringXD's syslog source does not fully support syslog RFC5424","SpringXD's syslog source cannot parse rfc5424 messages into a Map.
For the messages we get in RFC 3164, springXD converts these to a Map.

Since the rfc5424 data cannot be interpreted then the map contains just one key called 'UNDECODED'.
The result of this is that we get a string that looks like this (when we convert the message to a String)
{code}
 {UNDECODED=<182>Dec 02 2014 07:56:35: %ASA-6-113008: AAA transaction status ACCEPT : user = jbloggs}
{code}

Should be something like this (note the values below are for illustrative purposes only and should not be used as test data)

{code}
 {FACILITY=22, SEVERITY=6, TIMESTAMP=Tue Dec 02 07:56:35, HOST=the-hostname-that-sent-the-data, TAG=%ASA-6-113008, MESSAGE=........}
{code}

h3. Root Cause
Spring integration does not parse these messages. There is a JIRA for SI here:
https://jira.spring.io/browse/INT-3450
",5.0,0.6234388219052629,0.623432012896263,0.6234056279863888,True,True,True,1417653308000.0,1418385795000.0,1418389395000.0,1418827322000.0,1418828222000.0,1417653308000.0,-1.0,1418385756000.0,1418389356000.0,1418385787000.0,1418389387000.0,1418385795000.0,1418389395000.0,"dokeeffe",1418389356000.0,1417653308000.0,44,1417653308000.0,-1.0,"SpringXD's syslog source does not fully support syslog RFC5424","SpringXD's syslog source cannot parse rfc5424 messages into a Map.
For the messages we get in RFC 3164, springXD converts these to a Map.

Since the rfc5424 data cannot be interpreted then the map contains just one key called 'UNDECODED'.
The result of this is that we get a string that looks like this (when we convert the message to a String)
{code}
 {UNDECODED=<182>Dec 02 2014 07:56:35: %ASA-6-113008: AAA transaction status ACCEPT : user = jbloggs}
{code}

Should be something like this (note the values below are for illustrative purposes only and should not be used as test data)

{code}
 {FACILITY=22, SEVERITY=6, TIMESTAMP=Tue Dec 02 07:56:35, HOST=the-hostname-that-sent-the-data, TAG=%ASA-6-113008, MESSAGE=........}
{code}

h3. Root Cause
Spring integration does not parse these messages. There is a JIRA for SI here:
https://jira.spring.io/browse/INT-3450
",5.0,False,5.0,0,0.0,0,1,False
"XD-2424","Story","Sprint 48","2014-12-03T01:47:46.000+0000","eric.bottard","dturanski","Profile / Improve performance of TupleBuilder","See discussion at https://github.com/spring-projects/spring-xd/pull/1311

1) there seems to be unused SimpleDateFormat in TupleBuilder which hurst perf
2) More generally, should take some time to profile / micro-benchmark TupleBuilder",5.0,0.9050512759089188,0.8945601726199354,0.894560955702783,True,True,True,1417571266000.0,1430284562000.0,1430288162000.0,1431617411000.0,1431618311000.0,1417571266000.0,-1.0,1430137204000.0,1430140804000.0,1430137193000.0,1430140793000.0,1430284562000.0,1430288162000.0,"eric.bottard",1430140804000.0,1417571266000.0,44,1417571266000.0,-1.0,"Profile / Improve performance of TupleBuilder","See discussion at https://github.com/spring-projects/spring-xd/pull/1311

1) there seems to be unused SimpleDateFormat in TupleBuilder which hurst perf
2) More generally, should take some time to profile / micro-benchmark TupleBuilder",5.0,False,5.0,0,0.0,0,0,False
"XD-2423","Bug","Sprint 39","2014-12-02T14:24:13.000+0000","grussell","grussell","WireTap is Applied to OutputChannel Before the Tap Channel has been Bound To The Bus","When establishing the tap, we create the tap channel and add the WireTap before the tap channel has been bound to the bus.

{quote}
17:00:23,918 ERROR task-scheduler-8 handler.LoggingHandler - org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'tap:stream:foo.time.0.tap.bridge'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.interceptor.WireTap.preSend(WireTap.java:129)
{quote}",1.0,0.3857631916386422,0.0,0.0035020443713255,True,True,True,1417530253000.0,1417594032000.0,1417597632000.0,1417694685000.0,1417695585000.0,1417530253000.0,-1.0,1417530832000.0,1417534432000.0,1417530253000.0,1417533853000.0,1417594032000.0,1417597632000.0,"grussell",1417534432000.0,1417530253000.0,44,1417530253000.0,-1.0,"WireTap is Applied to OutputChannel Before the Tap Channel has been Bound To The Bus","When establishing the tap, we create the tap channel and add the WireTap before the tap channel has been bound to the bus.

{quote}
17:00:23,918 ERROR task-scheduler-8 handler.LoggingHandler - org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'tap:stream:foo.time.0.tap.bridge'.; nested exception is org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:277)
	at org.springframework.integration.channel.interceptor.WireTap.preSend(WireTap.java:129)
{quote}",1.0,False,1.0,0,0.0,0,0,False
"XD-2422","Story","Sprint 39","2014-12-02T13:11:12.000+0000","hillert","hillert","UI Provide fixed version numbers for NPM and Bower dependencies","",1.0,0.2677867321162742,0.0,0.7361436180306922,True,True,True,1417525872000.0,1417589372000.0,1417592972000.0,1417762101000.0,1417763001000.0,1417525872000.0,-1.0,1417700433000.0,1417704033000.0,1417525872000.0,1417529472000.0,1417589372000.0,1417592972000.0,"hillert",1417704033000.0,1417525872000.0,44,1417525872000.0,-1.0,"UI Provide fixed version numbers for NPM and Bower dependencies","",1.0,False,1.0,0,0.0,0,0,False
"XD-2416","Bug","Sprint 40","2014-11-29T16:32:58.000+0000","dhunziker","","SpelParseException is thrown when using empty string ("""") inside of an expression","I can only reproduce this when using single quotes around the expression:

{code}
stream create test --definition ""http | transform --expression='payload.replace(\""abc\"", \""\"")' | log"" --deploy true
{code}

The following two alternatives work fine though:
{code}
# Using trim on a single space
stream create test --definition ""http | transform --expression='payload.replace(\""abc\"", \"" \"".trim())' | log"" --deploy true

# Not using single quotes or spaces in the expression
stream create test --definition ""http | transform --expression=payload.replace(\""abc\"",\""\"") | log"" --deploy true
{code}",1.0,0.9391501248834874,0.9160950381437608,0.1959488656416567,True,True,True,1417278778000.0,1420599703000.0,1420603303000.0,1420813974000.0,1420814874000.0,1417278778000.0,-1.0,1417971672000.0,1417975272000.0,1420518178000.0,1420521778000.0,1420599703000.0,1420603303000.0,"dhunziker",1417975272000.0,1417278778000.0,44,1417278778000.0,-1.0,"SpelParseException is thrown when using empty string ("""") inside of an expression","I can only reproduce this when using single quotes around the expression:

{code}
stream create test --definition ""http | transform --expression='payload.replace(\""abc\"", \""\"")' | log"" --deploy true
{code}

The following two alternatives work fine though:
{code}
# Using trim on a single space
stream create test --definition ""http | transform --expression='payload.replace(\""abc\"", \"" \"".trim())' | log"" --deploy true

# Not using single quotes or spaces in the expression
stream create test --definition ""http | transform --expression=payload.replace(\""abc\"",\""\"") | log"" --deploy true
{code}",1.0,False,1.0,0,0.0,0,0,False
"XD-2414","Bug","Sprint 40","2014-11-27T03:33:42.000+0000","kdowbecki","","Incorrect ""directory"" option described in hdfs-datasink docs","Please see [hdfs-dataset 1.0.2.RELEASE docs|http://docs.spring.io/autorepo/docs/spring-xd/1.0.2.RELEASE/reference/html/#hdfs-dataset-avroparquet].

According to docs there should be ""directory"" option in this sink but in code ""basePath"" is used.",1.0,-1.0,0.999991876318575,0.087054182518,False,True,True,1417059222000.0,-1.0,-1.0,1418289291000.0,1418290191000.0,1417059222000.0,-1.0,1417166383000.0,1417169983000.0,1418290181000.0,1418290191000.0,-1.0,-1.0,"kdowbecki",1417169983000.0,1417059222000.0,44,1417059222000.0,-1.0,"Incorrect ""directory"" option described in hdfs-dataset docs","Please see [hdfs-dataset 1.0.2.RELEASE docs|http://docs.spring.io/autorepo/docs/spring-xd/1.0.2.RELEASE/reference/html/#hdfs-dataset-avroparquet].

According to docs there should be ""directory"" option in this sink but in code ""basePath"" is used.",1.0,False,1.0,0,0.0,-1,0,True
"XD-2412","Story","Sprint 41","2014-11-25T09:09:55.000+0000","eric.bottard","","Fix Redis FieldValueCounter repo save() method","That method is actually currently never called, but :
- The case where a mapping already exists is not covered (outstanding TODO comment)
- the semantics of the method should just be to ""save and override""

",2.0,0.9926282822183216,0.9692273641439236,0.9054994908115798,True,True,True,1416906595000.0,1420785965000.0,1420789565000.0,1420813875000.0,1420814775000.0,1416906595000.0,1420445488000.0,1420445450000.0,1420449050000.0,1420694510000.0,1420698110000.0,1420785965000.0,1420789565000.0,"eric.bottard",1420449050000.0,1420445488000.0,45,1420445488000.0,-1.0,"Fix Redis FieldValueCounter repo save() method","That method is actually currently never called, but :
- The case where a mapping already exists is not covered (outstanding TODO comment)
- the semantics of the method should just be to ""save and override""

",2.0,False,2.0,0,0.0,-1,0,False
"XD-2411","Story","Sprint 41","2014-11-25T09:08:26.000+0000","eric.bottard","","Make Redis RichGauge repository ""cluster safe""","Investigate Redis' transactions mechanisms for that, alongside CAS behavior (WATCH command?)",5.0,0.8792263086602203,0.8791891415972183,0.8020223868098626,True,True,True,1416906506000.0,1420786100000.0,1420789700000.0,1421318115000.0,1421319015000.0,1416906506000.0,1420445476000.0,1420445437000.0,1420449037000.0,1420785936000.0,1420789536000.0,1420786100000.0,1420789700000.0,"eric.bottard",1420449037000.0,1420445476000.0,45,1420445476000.0,-1.0,"Make Redis RichGauge repository ""cluster safe""","The current implementation makes individual reads from redis and then writes back the average, so in a cluster environment the reads and writes are not serialized, client reads and writes for specific keys can interfere with each other.  Investigate options, such as use of redis transactions or use of lua scripting to solve this problem.",5.0,False,5.0,0,0.0,0,0,True
"XD-2410","Story","Sprint 41","2014-11-25T09:07:23.000+0000","eric.bottard","","Rename metrics repositories setValue(x, y, z) to something less ""javabean""","",1.0,0.992656728943746,0.9490758224719448,0.9055141057328688,True,True,True,1416906443000.0,1420786083000.0,1420789683000.0,1420813883000.0,1420814783000.0,1416906443000.0,-1.0,1420445500000.0,1420449100000.0,1420615754000.0,1420619354000.0,1420786083000.0,1420789683000.0,"eric.bottard",1420449100000.0,1416906443000.0,44,1416906443000.0,-1.0,"Rename metrics repositories setValue(x, y, z) to something less ""javabean""","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2409","Bug","Sprint 40","2014-11-25T06:11:25.000+0000","jvalkeal","thomas.risberg","hdfs-dataset sink with getName() method in Pojo","Having a pojo:
{code}
public class User{
	private String name;
	public String getName() {
		return user;
	}
	public void setName(String name) {
		this.name = name;
	}
}
{code}

with:
{code}
hdfs-dataset --inputType='application/x-java-object;type=test.User'
{code}

throws exception:
{code}
12:43:27,698 1.1.0.SNAP ERROR task-scheduler-1 handler.LoggingHandler - org.springframework.messaging.MessageHandlingException: Expression evaluation failed: payload.getClass().getName(); nested exception is org.springframework.expression.AccessException: Problem invoking method: public java.lang.String test.User.getName()
{code}

Which I believe is caused by `correlation-strategy-expression` spel in aggregator:
{code}
	<int:aggregator
			input-channel=""input""
			correlation-strategy-expression=""payload.getClass().getName()""
			release-strategy-expression=""size() == ${batchSize}""
			expire-groups-upon-completion=""true""
			send-partial-result-on-expiry=""true""
			message-store=""messageStore""
			output-channel=""objects""/>
{code}

Changing `getName()` method in pojo to something else works.",1.0,0.8263813313197444,0.0,0.8147606447234524,True,True,True,1416895885000.0,1418041728000.0,1418045328000.0,1418281564000.0,1418282464000.0,1416895885000.0,-1.0,1418025615000.0,1418029215000.0,1416895885000.0,1416899485000.0,1418041728000.0,1418045328000.0,"jvalkeal",1418029215000.0,1416895885000.0,44,1416895885000.0,-1.0,"hdfs-dataset sink with getName() method in Pojo","Having a pojo:
{code}
public class User{
	private String name;
	public String getName() {
		return user;
	}
	public void setName(String name) {
		this.name = name;
	}
}
{code}

with:
{code}
hdfs-dataset --inputType='application/x-java-object;type=test.User'
{code}

throws exception:
{code}
12:43:27,698 1.1.0.SNAP ERROR task-scheduler-1 handler.LoggingHandler - org.springframework.messaging.MessageHandlingException: Expression evaluation failed: payload.getClass().getName(); nested exception is org.springframework.expression.AccessException: Problem invoking method: public java.lang.String test.User.getName()
{code}

Which I believe is caused by `correlation-strategy-expression` spel in aggregator:
{code}
	<int:aggregator
			input-channel=""input""
			correlation-strategy-expression=""payload.getClass().getName()""
			release-strategy-expression=""size() == ${batchSize}""
			expire-groups-upon-completion=""true""
			send-partial-result-on-expiry=""true""
			message-store=""messageStore""
			output-channel=""objects""/>
{code}

Changing `getName()` method in pojo to something else works.",1.0,False,1.0,0,0.0,0,0,False
"XD-2408","Bug","Sprint 39","2014-11-24T12:31:13.000+0000","iperumal","iperumal","When a tap is re-deployed after undeploy, it doesn't work","When a tap on a stream is undeployed and re-deployed, it stops working.
To make it work, the main stream associated with the tap needs to be undeployed and re-deployed.
",1.0,0.0007380981670562,0.0,1.0591708697256734,True,True,True,1416832273000.0,1416832279000.0,1416835879000.0,1416839502000.0,1416840402000.0,1416832273000.0,-1.0,1416840883000.0,1416840402000.0,1416832273000.0,1416835873000.0,1416832279000.0,1416835879000.0,"iperumal",1416840402000.0,1416832273000.0,44,1416832273000.0,-1.0,"When a tap is re-deployed after undeploy, it doesn't work","When a tap on a stream is undeployed and re-deployed, it stops working.
To make it work, the main stream associated with the tap needs to be undeployed and re-deployed.
",1.0,False,1.0,0,0.0,0,0,False
"XD-2406","Story","Sprint 40","2014-11-24T07:05:51.000+0000","dturanski","","Create Sample Module projects","Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https://github.com/dturanski/siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using @Configuration or XML configuration (either as separate samples or using build profiles). ",3.0,0.5220135087981526,0.5220135087981526,0.2759008286788806,True,True,True,1416812751000.0,1418806935000.0,1418810535000.0,1420632028000.0,1420632928000.0,1416812751000.0,1418023755000.0,1417866741000.0,1417870341000.0,1418806935000.0,1418810535000.0,1418806935000.0,1418810535000.0,"dturanski",1417870341000.0,1416812751000.0,44,1420445332000.0,1418023755000.0,"Create Sample Module projects","Create one or more Sample module projects in the Spring XD Examples repo to serve as templates for Spring XD module projects. Similar to https://github.com/dturanski/siDslModule, these should include unit and single node integration tests, and demonstrate the use of Spring XD build and packaging tools, and other module development support. This may be split out into separate tasks, but should include a sample for source, processor, sink, and job, using @Configuration or XML configuration (either as separate samples or using build profiles). ",5.0,True,5.0,2,-40.0,0,0,False
"XD-2404","Story","Sprint 39","2014-11-24T06:53:42.000+0000","dturanski","dturanski","Provide an XD Starter POM for module projects","Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for 'MODULE' layout and other boilerplate build configuration. This should include a similar feature for gradle.",3.0,0.0057293840448147,0.0,0.0056870287677993,True,True,True,1416812022000.0,1416824061000.0,1416827661000.0,1418912395000.0,1418913295000.0,1416812022000.0,-1.0,1416823972000.0,1416827572000.0,1416812022000.0,1416815622000.0,1416824061000.0,1416827661000.0,"dturanski",1416827572000.0,1416812022000.0,44,1416812022000.0,-1.0,"Provide an XD Starter POM for module projects","Provide A maven pom to support module projects that will declare the Spring XD dependencies as provided, configure the boot plugin for 'MODULE' layout and other boilerplate build configuration. This should include a similar feature for gradle.",3.0,False,3.0,0,0.0,-1,0,False
"XD-2403","Story","Sprint 40","2014-11-23T14:46:09.000+0000","sabby","dturanski","Create Windows based CI infrastructure","As a build manager, I'd like to setup CI infrastructure so that I can run integration tests in Windows OS automatically as we commit-trigger new builds. ",5.0,0.1620307130450013,0.0,0.144042859537286,True,True,True,1416753969000.0,1418193404000.0,1418197004000.0,1425636786000.0,1425637686000.0,1416753969000.0,1418037493000.0,1418033605000.0,1418037205000.0,1416753969000.0,1416757569000.0,1418193404000.0,1418197004000.0,"sabby",1418037205000.0,1416753969000.0,44,1420445182000.0,1418037493000.0,"Spike: Study elastic instances and scheduling in Bamboo to create Windows CI infrastructure","As a build manager, I'd like to setup CI infrastructure so that I can run integration tests in Windows OS automatically as we commit-trigger new builds. 

*Scope:*
* Use the environment where Bamboo is running
* Gain access to powershell 
* Setup services (redis, rabbit, etc.)
* Kick-off CI task",8.0,True,8.0,2,-37.5,0,0,True
"XD-2401","Story","Sprint 40","2014-11-23T14:21:41.000+0000","sabby","","Include DIRT tests in CI builds","As a developer, I'd like to include DIRT tests as part of the CI builds so that we can automatically evaluate DIRT features and functionalities.",5.0,0.9785310585092956,0.9785310585092956,1.1733961679554716e-05,True,True,True,1416752501000.0,1436516658000.0,1436520258000.0,1436949383000.0,1436950283000.0,1416752501000.0,1420453591000.0,1416752738000.0,1416756338000.0,1436516658000.0,1436520258000.0,1436516658000.0,1436520258000.0,"sabby",1416756338000.0,1416752501000.0,44,1436355970000.0,1420453591000.0,"EC2 CI build improvements","As a developer, I'd like to include the following improvements as part of the EC2 CI infrastructure, so that we can reliably run the CI builds and also assert over feature functionalities.

*Scope:*
* Enable 'distributed jvm test'
* Change from using artifactory gradle task to a command task (that calls ./gradlew)
* Test w/ embedded hadoop off
* Turn on maxParallelForks
",5.0,True,5.0,3,0.0,0,0,True
"XD-2397","Story","Sprint 39","2014-11-21T13:50:13.000+0000","sabby","eric.bottard","TCP-Client source module throws ClassNotFoundException","*Version:*
XD: 1.1 M1

*Problem:*
Trying to use tcp-client source module and observing an exception while deploying the stream.

*Stream Definition:*
{code:xml}
 curl --data name=dummy-firehose --data definition='tcp-client --decoder=LF --port=8080 | log' --data deploy=true http://localhost:9393/streams/definitions
{""name"":""dummy-firehose"",""status"":null,""definition"":""tcp-client --decoder=LF --port=8080 | log"",""_links"":{""self"":{""href"":""http://localhost:9393/streams/dummy-firehose""}}}
{code}

The same curl command works fine against XD 1.0.1 release.
",3.0,0.3724772462188569,0.0018606952331784,0.0,True,True,True,1416577813000.0,1416889496000.0,1416893096000.0,1417413697000.0,1417414597000.0,1416577813000.0,-1.0,1416577813000.0,1416581413000.0,1416579370000.0,1416582970000.0,1416889496000.0,1416893096000.0,"sabby",1416581413000.0,1416577813000.0,44,1416577813000.0,-1.0,"TCP-Client source module throws ClassNotFoundException","*Version:*
XD: 1.1 M1

*Problem:*
Trying to use tcp-client source module and observing an exception while deploying the stream.

*Stream Definition:*
{code:xml}
 curl --data name=dummy-firehose --data definition='tcp-client --decoder=LF --port=8080 | log' --data deploy=true http://localhost:9393/streams/definitions
{""name"":""dummy-firehose"",""status"":null,""definition"":""tcp-client --decoder=LF --port=8080 | log"",""_links"":{""self"":{""href"":""http://localhost:9393/streams/dummy-firehose""}}}
{code}

The same curl command works fine against XD 1.0.1 release.
",3.0,False,3.0,0,0.0,0,0,False
"XD-2389","Improvement","Sprint 45","2014-11-20T15:11:40.000+0000","mark.fisher","","Streams section of doc should explicitly mention that labels are required for ambiguous modules","Currently I believe we only mention labels in this section of the doc:
https://github.com/spring-projects/spring-xd/wiki/DSL-Reference#labels

And it is not even clear there that they are *required* when 2 or more module names would otherwise be ambiguous. It was probably written before we made that a mandatory part of the definition.

We should mention this somewhere in the 'streams' section of the manual. Even if none of the examples there currently have more than one occurrence of the same module, we should add one to illustrate this point.
",2.0,0.9546497842896836,0.9546497842896836,0.804640154248069,True,True,True,1416496300000.0,1427445481000.0,1427449081000.0,1427964717000.0,1427965617000.0,1416496300000.0,-1.0,1425724973000.0,1425728573000.0,1427445481000.0,1427449081000.0,1427445481000.0,1427449081000.0,"mark.fisher",1425728573000.0,1416496300000.0,44,1416496300000.0,-1.0,"Streams section of doc should explicitly mention that labels are required for ambiguous modules","Currently I believe we only mention labels in this section of the doc:
https://github.com/spring-projects/spring-xd/wiki/DSL-Reference#labels

And it is not even clear there that they are *required* when 2 or more module names would otherwise be ambiguous. It was probably written before we made that a mandatory part of the definition.

We should mention this somewhere in the 'streams' section of the manual. Even if none of the examples there currently have more than one occurrence of the same module, we should add one to illustrate this point.
",2.0,False,2.0,0,0.0,-1,1,False
"XD-2388","Story","Sprint 40","2014-11-20T15:05:14.000+0000","sabby","","Add support to host custom module in HDFS","As a user, I'd like to have the custom module (built as uber-jar) hosted in HDFS so that I can deploy the module to newly arriving containers. ",8.0,0.8072501213218787,0.6799331758896987,0.0,True,True,True,1416495914000.0,1424769900000.0,1424773500000.0,1426744608000.0,1426745508000.0,1416495914000.0,-1.0,1416495914000.0,1416499514000.0,1423464953000.0,1423468553000.0,1424769900000.0,1424773500000.0,"sabby",1416499514000.0,1416495914000.0,44,1416495914000.0,-1.0,"Add support to host custom module in HDFS","As a user, I'd like to have the custom module (built as uber-jar) hosted in HDFS so that I can deploy the module to newly arriving containers. ",8.0,False,8.0,0,0.0,-1,0,False
"XD-2387","Story","Sprint 39","2014-11-20T09:12:46.000+0000","grenfro","grenfro","Acceptance test for Kafka source and sink","",5.0,0.5496291980584918,0.0144227679450966,0.0136541928170095,True,True,True,1416474766000.0,1417518137000.0,1417521737000.0,1418372184000.0,1418373084000.0,1416474766000.0,-1.0,1416500686000.0,1416504286000.0,1416502145000.0,1416505745000.0,1417518137000.0,1417521737000.0,"grenfro",1416504286000.0,1416474766000.0,44,1416474766000.0,-1.0,"Acceptance test for Kafka source and sink","",5.0,False,5.0,0,0.0,-1,0,False
"XD-2386","Story","Sprint 39","2014-11-20T09:11:02.000+0000","grenfro","grenfro","Need TCP-Client Source Acceptance test","",3.0,0.4946201732610248,7.874069481838983e-06,0.0136609856130251,True,True,True,1416474662000.0,1417416907000.0,1417420507000.0,1418378749000.0,1418379649000.0,1416474662000.0,-1.0,1416500686000.0,1416504286000.0,1416474677000.0,1416478277000.0,1417416907000.0,1417420507000.0,"grenfro",1416504286000.0,1416474662000.0,44,1416474662000.0,-1.0,"Need TCP-Client Source Acceptance test","",3.0,False,3.0,0,0.0,-1,0,False
"XD-2384","Story","Sprint 39","2014-11-20T07:31:09.000+0000","sabby","","Document custom module install procedures","As a user, I'd like to refer to documentation so that I can build the custom module based on recommended standards and patterns.",3.0,0.4429849979416146,0.199125910856851,0.0,True,True,True,1416468669000.0,1418806925000.0,1418810525000.0,1421746178000.0,1421747078000.0,1416468669000.0,1419253795000.0,1416468669000.0,1416472269000.0,1417519737000.0,1417523337000.0,1418806925000.0,1418810525000.0,"sabby",1416472269000.0,1416468669000.0,44,1419253795000.0,1419253795000.0,"Document custom module install procedures","As a user, I'd like to refer to documentation so that I can build the custom module based on recommended standards and patterns.",1.0,True,1.0,1,200.0,0,0,False
"XD-2382","Story","Sprint 39","2014-11-19T12:56:29.000+0000","sabby","","[PLACEHOLDER] Kafka performance benchmarks","TBD",8.0,0.9500392333487588,0.9500392333487588,0.0,True,True,True,1416401789000.0,1427463244000.0,1427466844000.0,1428044045000.0,1428044945000.0,1416401789000.0,-1.0,1416401789000.0,1416405389000.0,1427463244000.0,1427466844000.0,1427463244000.0,1427466844000.0,"sabby",1416405389000.0,1416401789000.0,44,1416401789000.0,-1.0,"Re-run Kafka baseline tests in new infrastructure","As a developer, I'd like to setup a performance testing infrastructure (rackspace), so I can start benching Kafka baselines and continue with XD use-cases.",8.0,False,8.0,0,0.0,0,0,True
"XD-2381","Story","Sprint 39","2014-11-19T12:32:19.000+0000","sabby","","Decouple and refactor external dependencies","*Refactoring scope:* (_spring-xd-dirt_)
* Message bus dependencies
* Hadoop distribution dependencies 

The goal is to decouple them from startup phase to further enhance initialization time. ",8.0,0.2839297646851404,0.0278503440154801,0.0,True,True,True,1416400339000.0,1417052564000.0,1417056164000.0,1418696574000.0,1418697474000.0,1416400339000.0,-1.0,1416400339000.0,1416403939000.0,1416464315000.0,1416467915000.0,1417052564000.0,1417056164000.0,"sabby",1416403939000.0,1416400339000.0,44,1416400339000.0,-1.0,"Decouple messagebus dependencies","*Refactoring scope:* (_spring-xd-dirt_)
* Message bus dependencies


The goal is to decouple them from startup phase to further enhance initialization time. ",8.0,False,8.0,0,0.0,0,0,True
"XD-2378","Story","Sprint 39","2014-11-19T07:19:37.000+0000","hillert","hillert","Add ability to logout using the Admin UI","While there is a server endpoint to logout, we don't have that ability yet from the UI. As indicated by XD-2122 we will also need a meta-data REST endpoint  so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements: 

* Show a logout button only if a) security is enabled and b) user is logged in
* Show the username and/or full name of the user being logged in
 ",5.0,0.189199454038336,0.0,0.0530352083731705,True,True,True,1416381577000.0,1416811984000.0,1416815584000.0,1418655562000.0,1418656462000.0,1416381577000.0,-1.0,1416502226000.0,1416505826000.0,1416381577000.0,1416385177000.0,1416811984000.0,1416815584000.0,"hillert",1416505826000.0,1416381577000.0,44,1416381577000.0,-1.0,"Add ability to logout using the Admin UI","While there is a server endpoint to logout, we don't have that ability yet from the UI. As indicated by XD-2122 we will also need a meta-data REST endpoint  so we can interrogate whether security is enabled, whether the user is logged etc. So we can fulfill the requirements: 

* Show a logout button only if a) security is enabled and b) user is logged in
* Show the username and/or full name of the user being logged in
 ",5.0,False,5.0,0,0.0,-1,0,False
"XD-2377","Bug","Sprint 39","2014-11-18T18:53:21.000+0000","sabby","","Update ""About"" section in UI with relevant release links","As a user, I'd like to have API and Documentation links in the [""About""|https://github.com/spring-projects/spring-xd/blob/master/spring-xd-ui/app/scripts/shared/views/about.html] section within _admin-ui_.
",1.0,0.3608603241642212,0.0302107615939445,0.0003482982749636,True,True,True,1416336801000.0,1416863123000.0,1416866723000.0,1417794421000.0,1417795321000.0,1416336801000.0,-1.0,1416337309000.0,1416340909000.0,1416380864000.0,1416384464000.0,1416863123000.0,1416866723000.0,"sabby",1416340909000.0,1416336801000.0,44,1416336801000.0,-1.0,"Update ""About"" section in UI with relevant release links","As a user, I'd like to have API and Documentation links in the [""About""|https://github.com/spring-projects/spring-xd/blob/master/spring-xd-ui/app/scripts/shared/views/about.html] section within _admin-ui_. 

It would be ideal to have the version # dynamically replaced for every release.
",1.0,False,1.0,0,0.0,0,1,True
"XD-2375","Story","Sprint 39","2014-11-18T17:11:08.000+0000","sabby","","Add reactor-stream processor module","As a user, I'd like to have a _reactor-stream_ processor module so that I can ingest data using XD source modules and process them as time-window operations. 

*Example:*
http | reactor-stream --timeWindow=10s --field=payload.sensorData --expressions=min,avg

This would give you 10 second time window of the min and avg values.",8.0,0.746117525545022,0.746117525545022,0.0005014051640633,True,True,True,1416330668000.0,1417422899000.0,1417426499000.0,1417793654000.0,1417794554000.0,1416330668000.0,-1.0,1416331402000.0,1416335002000.0,1417422899000.0,1417426499000.0,1417422899000.0,1417426499000.0,"sabby",1416335002000.0,1416330668000.0,44,1416330668000.0,-1.0,"Research reactor-stream integration options","As a user, I'd like to have a _reactor-stream_ processor module so that I can ingest data using XD source modules and process them as time-window operations. 

*Example 1:*
http | reactor-stream --timeWindow=10s --field=payload.sensorData --expressions=min,avg

This would give you 10 second time window of the min and avg values.

*Example 2:*
Reactor as a module

*Example 3:*
Integration with Spark streaming and reactor",8.0,False,8.0,0,0.0,-1,0,True
"XD-2370","Story","Sprint 38","2014-11-18T08:55:38.000+0000","grenfro","grenfro","Remove Test Scripts From XD","The acceptance tests cover the entire suite of script tests.  Thus they are no longer needed.  The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn't get an error.  This test (httpbash) was never called from the scripts CI build.",1.0,0.0116612845353426,0.010704461188853,0.9901327592393254,True,True,True,1416300938000.0,1416301133000.0,1416304733000.0,1416316760000.0,1416317660000.0,1416300938000.0,-1.0,1416317495000.0,1416317660000.0,1416301117000.0,1416304717000.0,1416301133000.0,1416304733000.0,"grenfro",1416317660000.0,1416300938000.0,44,1416300938000.0,-1.0,"Remove Test Scripts From XD","The acceptance tests cover the entire suite of script tests.  Thus they are no longer needed.  The only test that was remaining was posting 10 messages to a http source and writing to a long and making sure we didn't get an error.  This test (httpbash) was never called from the scripts CI build.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2368","Story","Sprint 39","2014-11-17T17:30:43.000+0000","sabby","","Research Spark integration options (phase #2)","As a continuation, we would like to further investigate Spark, develop POC and identify the best appropriate design and implementation for XD.",8.0,0.8661309257746471,0.8661309257746471,0.0,True,True,True,1416245443000.0,1417594092000.0,1417597692000.0,1417801639000.0,1417802539000.0,1416245443000.0,-1.0,1416245443000.0,1416249043000.0,1417594092000.0,1417597692000.0,1417594092000.0,1417597692000.0,"sabby",1416249043000.0,1416245443000.0,44,1416245443000.0,-1.0,"Research Spark integration options (phase #2)","As a continuation, we would like to further investigate Spark, develop POC and identify the best appropriate design and implementation for XD.",8.0,False,8.0,0,0.0,-1,0,False
"XD-2364","Story","Sprint 39","2014-11-17T08:31:47.000+0000","mark.pollack","","Remove usage of <context:property-placeholder location=.../> in module defitions","This doesn't follow the conventions we have with other modules and it also means it isn't easy to override via environment variables etc.  This is in HDFS and some others.",5.0,-1.0,-1.0,7.034690778175895e-06,False,True,False,1416213107000.0,-1.0,-1.0,1422466924000.0,1422467824000.0,1416213107000.0,1418025412000.0,1416213151000.0,1416216751000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1416216751000.0,1416213107000.0,44,1418025412000.0,1418025412000.0,"Remove usage of <context:property-placeholder location=.../> in module defitions","This doesn't follow the conventions we have with other modules and it also means it isn't easy to override via environment variables etc.  This is in HDFS and some others.",8.0,True,8.0,1,-37.5,0,0,False
"XD-2362","Story","Sprint 38","2014-11-17T07:10:10.000+0000","grenfro","grenfro","Created Acceptance CI test environment for 1.0.x","* Create the infrastructure (Mongo, Hadoop, ActiveMQ, Gemfire, Mysql, etc) in EC2 for the 1.0.2 acceptance tests
* Retrofit the 1.0.2 to use the new infrastructure
* Create a 1.0.2 branch for XD-EC2",5.0,-1.0,0.0,18.95,False,False,False,1416208210000.0,-1.0,-1.0,1416208210000.0,1416208230000.0,1416208210000.0,-1.0,1416208589000.0,1416208230000.0,1416208210000.0,1416208230000.0,-1.0,-1.0,"grenfro",1416208230000.0,1416208210000.0,44,1416208210000.0,-1.0,"Created Acceptance CI test environment for 1.0.x","* Create the infrastructure (Mongo, Hadoop, ActiveMQ, Gemfire, Mysql, etc) in EC2 for the 1.0.2 acceptance tests
* Retrofit the 1.0.2 to use the new infrastructure
* Create a 1.0.2 branch for XD-EC2",5.0,False,5.0,0,0.0,-1,0,False
"XD-2361","Story","Sprint 41","2014-11-16T17:37:13.000+0000","sabby","mbogoevici","Pre-allocate partitions for Kafka message bus","As a user, I want Spring XDs message bus to be able to pre-allocate partitions between nodes when a stream is deployed, so that rebalancing doesnt happen when a container crashes and/or its redeployed.",8.0,0.943519362973376,0.9029863366128302,0.9320456418706752,True,True,True,1416159433000.0,1422006698000.0,1422010298000.0,1422355825000.0,1422356725000.0,1416159433000.0,-1.0,1421935592000.0,1421939192000.0,1421755503000.0,1421759103000.0,1422006698000.0,1422010298000.0,"sabby",1421939192000.0,1416159433000.0,44,1416159433000.0,-1.0,"Pre-allocate partitions for Kafka message bus","As a user, I want Spring XDs message bus to be able to pre-allocate partitions between nodes when a stream is deployed, so that rebalancing doesnt happen when a container crashes and/or its redeployed.",8.0,False,8.0,0,0.0,-1,0,False
"XD-2360","Story","Sprint 40","2014-11-16T17:36:30.000+0000","sabby","","Pre-allocate partitions for Kafka source","As a user, I want Spring XD to pre-allocate a set of partitions between the Kafka source modules when a stream is deployed, so that deployment is simpler, and rebalancing doesnt take place. ",8.0,0.8664780243871901,0.8664780243871901,0.0013596455492874,True,True,True,1416159390000.0,1421232157000.0,1421235757000.0,1422012957000.0,1422013857000.0,1416159390000.0,-1.0,1416167350000.0,1416170950000.0,1421232157000.0,1421235757000.0,1421232157000.0,1421235757000.0,"sabby",1416170950000.0,1416159390000.0,44,1416159390000.0,-1.0,"Pre-allocate partitions for Kafka source","As a user, I want Spring XD to pre-allocate a set of partitions between the Kafka source modules when a stream is deployed, so that deployment is simpler, and rebalancing doesnt take place. ",8.0,False,8.0,0,0.0,-1,0,False
"XD-2358","Story","Sprint 39","2014-11-16T17:32:51.000+0000","sabby","","Add replay support for Kafka source","As a user, I want to be able to control the starting offset of the Kafka source when a stream is deployed, so that I can replay a topic if necessary.",8.0,0.8664835876205406,0.8664835876205406,0.00139615137738,True,True,True,1416159171000.0,1421232143000.0,1421235743000.0,1422012937000.0,1422013837000.0,1416159171000.0,-1.0,1416167345000.0,1416170945000.0,1421232143000.0,1421235743000.0,1421232143000.0,1421235743000.0,"sabby",1416170945000.0,1416159171000.0,44,1416159171000.0,-1.0,"Add starting offset support for Kafka source","As a user, I want to be able to control the starting offset of the Kafka source when a stream is deployed, so that I can replay a topic if necessary.

Note:
- starting offset is only considered when the stream is deployed
- progress made by modules must survive their crash for a running stream
- undeploying and redeploying a stream with a specific start offset will cause the stream to read again from the start 

TBD: what happens when streams are undeployed/redeployed - where do they resume from?",8.0,False,8.0,0,0.0,-1,0,True
"XD-2354","Story","Sprint 38","2014-11-15T08:31:56.000+0000","thomas.risberg","grenfro","EC2 Integration Tests fail after Boot 1.2 upgrade","Many of the tests fail with:

{code}
java.lang.IllegalStateException: Cannot find template location: class path resource [templates/] (please add some templates, check your Groovy configuration, or set spring.groovy.template.check-template-location=false)
{code}

Somehow we need to disable this check, using the property suggested.",1.0,-1.0,0.0,0.0127258191537598,False,True,True,1416040316000.0,-1.0,-1.0,1416207028000.0,1416207928000.0,1416040316000.0,-1.0,1416042449000.0,1416046049000.0,1416040316000.0,1416043916000.0,-1.0,-1.0,"thomas.risberg",1416046049000.0,1416040316000.0,44,1416040316000.0,-1.0,"EC2 Integration Tests fail after Boot 1.2 upgrade","Many of the tests fail with:

{code}
java.lang.IllegalStateException: Cannot find template location: class path resource [templates/] (please add some templates, check your Groovy configuration, or set spring.groovy.template.check-template-location=false)
{code}

Somehow we need to disable this check, using the property suggested.",1.0,False,1.0,0,0.0,0,0,False
"XD-2351","Story","Sprint 39","2014-11-14T13:12:48.000+0000","mark.pollack","Marius Bogoevici","POM generation creates the correct dependency list","We are referencing Spring.IO deps when we shouldn't (since we moved to a different version of boot than in in the platform).",8.0,0.4871453153741173,0.4871381949801222,0.2859134872129591,True,True,True,1415970768000.0,1416381261000.0,1416384861000.0,1416812518000.0,1416813418000.0,1415970768000.0,1416471606000.0,1416211693000.0,1416215293000.0,1416381255000.0,1416384855000.0,1416381261000.0,1416384861000.0,"mark.pollack",1416215293000.0,1415970768000.0,44,1416471606000.0,1416471606000.0,"POM generation creates the correct dependency list","We are referencing Spring.IO deps when we shouldn't (since we moved to a different version of boot than in in the platform).",5.0,True,5.0,1,60.0,0,0,False
"XD-2345","Bug","Sprint 38","2014-11-14T08:07:41.000+0000","thomas.risberg","hillert","XD UI not usable with IE 11","Trying to use the XD UI with Internet Explorer (version 11.0.9600.17031) is difficult. The screen doesn't refresh when streams/jobs are created or deployed. Had to erase the browsing history continuously to get state updates to show in the UI.",5.0,0.7099919373827175,0.7099919373827175,0.767425599166625,True,True,True,1415952461000.0,1416151476000.0,1416155076000.0,1416231867000.0,1416232767000.0,1415952461000.0,-1.0,1416167575000.0,1416171175000.0,1416151476000.0,1416155076000.0,1416151476000.0,1416155076000.0,"thomas.risberg",1416171175000.0,1415952461000.0,44,1415952461000.0,-1.0,"XD UI not usable with IE 11","Trying to use the XD UI with Internet Explorer (version 11.0.9600.17031) is difficult. The screen doesn't refresh when streams/jobs are created or deployed. Had to erase the browsing history continuously to get state updates to show in the UI.",5.0,False,5.0,0,0.0,0,0,False
"XD-2344","Bug","Sprint 38","2014-11-14T08:03:04.000+0000","thomas.risberg","hillert","UI should quote parameters containing a space","Trying to deploy the `timestampfile` job using the UI.

Seems the UI doesn't quote string parameters that contains a space so the job creation fails.

Keeping all the defaults I get the following ""Resulting Definition"" in the UI:

timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true

(note: the --format parameter has a space)

which causes:

XD100E:(pos 128): Found unexpected data after stream definition: 'HH' timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true *^
",3.0,0.7034696589399084,0.7034213387380374,0.9836006603760924,True,True,True,1415952184000.0,1416214237000.0,1416217837000.0,1416323799000.0,1416324699000.0,1415952184000.0,-1.0,1416318590000.0,1416322190000.0,1416214219000.0,1416217819000.0,1416214237000.0,1416217837000.0,"thomas.risberg",1416322190000.0,1415952184000.0,44,1415952184000.0,-1.0,"UI should quote parameters containing a space","Trying to deploy the `timestampfile` job using the UI.

Seems the UI doesn't quote string parameters that contains a space so the job creation fails.

Keeping all the defaults I get the following ""Resulting Definition"" in the UI:

timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true

(note: the --format parameter has a space)

which causes:

XD100E:(pos 128): Found unexpected data after stream definition: 'HH' timestampfile --restartable=false --directory=/tmp/xd/output/ --fileExtension=txt --fileName=${xd.job.name} --format=yyyy-MM-dd HH:mm:ss --dateFormat=yyyy-MM-dd --makeUnique=true *^
",3.0,False,3.0,0,0.0,0,0,False
"XD-2342","Bug","Sprint 39","2014-11-13T18:22:02.000+0000","bzeyben","","JDBCHDFS Job Password issue","Password for 'jdbchdfs' job definition is only hashing the initial portion of the password not the entire password (See attached image).

The password has an '_' char but it shouldn't matter. The entire password should be masked with '*' instead.",3.0,0.5828308858095893,0.1853885776730788,0.0452526476297804,True,True,True,1415902922000.0,1417694420000.0,1417698020000.0,1418975809000.0,1418976709000.0,1415902922000.0,-1.0,1416042019000.0,1416045619000.0,1416472767000.0,1416476367000.0,1417694420000.0,1417698020000.0,"bzeyben",1416045619000.0,1415902922000.0,44,1415902922000.0,-1.0,"JDBCHDFS Job Password issue","Password for 'jdbchdfs' job definition is only hashing the initial portion of the password not the entire password (See attached image).

The password has an '_' char but it shouldn't matter. The entire password should be masked with '*' instead.",3.0,False,3.0,0,0.0,0,0,False
"XD-2340","Story","Sprint 38","2014-11-13T13:05:34.000+0000","hillert","hillert","Ensure that branch-specific documentation is pull and generated","",3.0,0.0010815182635518,0.0,0.1259075348907196,True,True,True,1415883934000.0,1415884049000.0,1415887649000.0,1415989366000.0,1415990266000.0,1415883934000.0,-1.0,1415897322000.0,1415900922000.0,1415883934000.0,1415887534000.0,1415884049000.0,1415887649000.0,"hillert",1415900922000.0,1415883934000.0,44,1415883934000.0,-1.0,"Ensure that branch-specific documentation is pulled and generated","",3.0,False,3.0,0,0.0,-1,0,True
"XD-2339","Story","Sprint 38","2014-11-13T11:31:14.000+0000","iperumal","iperumal","Remove external config properties for modules","There are some modules that use external config properties (kafka producer/consumer, hadoop properties etc.,). We need to avoid using such properties and have them configured inside module so that module and its properties are self contained.

",5.0,0.0462733347110284,0.0462733347110284,0.0465890565084613,True,True,True,1415878274000.0,1415894396000.0,1415897996000.0,1416225782000.0,1416226682000.0,1415878274000.0,-1.0,1415894506000.0,1415898106000.0,1415894396000.0,1415897996000.0,1415894396000.0,1415897996000.0,"iperumal",1415898106000.0,1415878274000.0,44,1415878274000.0,-1.0,"Remove external config properties for modules","There are some modules that use external config properties (kafka producer/consumer, hadoop properties etc.,). We need to avoid using such properties and have them configured inside module so that module and its properties are self contained.

",5.0,False,5.0,0,0.0,0,0,False
"XD-2335","Story","Sprint 38","2014-11-12T12:35:16.000+0000","grenfro","grenfro","Update Performance AMI to include Kafka","Create an AMI that will contain the Kafka Executable as well as the Kafka performance test tools.",1.0,0.9963558413719186,0.0,0.6932475884244373,True,True,True,1415795716000.0,1415800364000.0,1415800381000.0,1415799481000.0,1415800381000.0,1415795716000.0,-1.0,1415798950000.0,1415800381000.0,1415795716000.0,1415799316000.0,1415800364000.0,1415800381000.0,"grenfro",1415800381000.0,1415795716000.0,44,1415795716000.0,-1.0,"Update Performance AMI to include Kafka","Create an AMI that will contain the Kafka Executable as well as the Kafka performance test tools.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2334","Story","Sprint 38","2014-11-12T12:32:06.000+0000","grenfro","Chris Schaefer","Create base perf test criteria","Since Kafka and Rabbit have different strategies on how a message system is implemented, we will need to update the tests used on rabbit to work with Kafka.  While they will not be exactly the same as before, they should exercise the same principles.
This story covers: 
* Create the consumer and producer execution configurations for
kafka-producer-perf-test.sh and kafka-consumer-perf-test.sh. 
* Record the tests a spreadsheet much like the Rabbit Base test spreadsheet

",2.0,0.999932419248624,1.00005182884692,0.001725595726864,True,True,True,1415795526000.0,1417763409000.0,1417763542000.0,1417762642000.0,1417763542000.0,1415795526000.0,-1.0,1415798922000.0,1415802522000.0,1417763644000.0,1417763542000.0,1417763409000.0,1417763542000.0,"grenfro",1415802522000.0,1415795526000.0,44,1415795526000.0,-1.0,"Create base perf test criteria","Since Kafka and Rabbit have different strategies on how a message system is implemented, we will need to update the tests used on rabbit to work with Kafka.  While they will not be exactly the same as before, they should exercise the same principles.
This story covers: 
* Create the consumer and producer execution configurations for
kafka-producer-perf-test.sh and kafka-consumer-perf-test.sh. 
* Record the tests a spreadsheet much like the Rabbit Base test spreadsheet

",2.0,False,2.0,0,0.0,-1,0,False
"XD-2333","Story","Sprint 39","2014-11-12T12:29:41.000+0000","iperumal","","Create Integration tests for Kafka source and sink modules","Add integration tests for kafka source and sink modules.",8.0,0.6813935213481184,0.2686418008778962,0.0422743544763976,True,True,True,1415795381000.0,1417512261000.0,1417515861000.0,1418314141000.0,1418315041000.0,1415795381000.0,-1.0,1415901898000.0,1415905498000.0,1416472267000.0,1416475867000.0,1417512261000.0,1417515861000.0,"iperumal",1415905498000.0,1415795381000.0,44,1415795381000.0,-1.0,"Add test coverage for Kafka source and sink modules","As a PM, I'd like to have test coverage for both Kafka source and sink modules so that we can assert its functionality as part of the CI builds. ",8.0,False,8.0,0,0.0,0,0,True
"XD-2332","Story","Sprint 39","2014-11-12T11:41:03.000+0000","hillert","hillert","AdminUI - Provide Server-Side Cron Expression Validation","It is easy to get a cron expression wrong. 

Provide validation of the cron expression on the Schedule Job page using async validation. 

* Submit the cron expression to the server-side - and validate that the expression is valid.
* Send a success message back (we may even send back some meta data  e.g. when is the next execution going to take place)
",5.0,0.3879801248716698,0.3879725126021122,0.6051673100726058,True,True,True,1415792463000.0,1416556979000.0,1416560579000.0,1417762066000.0,1417762966000.0,1415792463000.0,-1.0,1416984947000.0,1416988547000.0,1416556964000.0,1416560564000.0,1416556979000.0,1416560579000.0,"hillert",1416988547000.0,1415792463000.0,44,1415792463000.0,-1.0,"AdminUI - Provide Server-Side Cron Expression Validation","It is easy to get a cron expression wrong. 

Provide validation of the cron expression on the Schedule Job page using async validation. 

* Submit the cron expression to the server-side - and validate that the expression is valid.
* Send a success message back (we may even send back some meta data  e.g. when is the next execution going to take place)
",5.0,False,5.0,0,0.0,0,0,False
"XD-2331","Bug","Sprint 38","2014-11-11T08:18:31.000+0000","hillert","","Job deployment list returns 404 after Laptop wakes up","Step to reproduce:

* System is running in Single Node
* Laptop goes to sleep
* After waking up your laptop from sleep, you cannot retrieve the list of deployed jobs anymore (in AdminUI)

Only getting back a *404* - ""NoSuchBatchJobException"", ""Batch Job with the name abcd doesn't exist""",5.0,0.2881807365873466,0.0416934448061136,0.0005513378342692,True,True,True,1415693911000.0,1416302849000.0,1416306449000.0,1417806053000.0,1417806953000.0,1415693911000.0,1415696600000.0,1415695076000.0,1415698676000.0,1415782011000.0,1415785611000.0,1416302849000.0,1416306449000.0,"hillert",1415698676000.0,1415696600000.0,44,1415696600000.0,-1.0,"Job deployment list returns 404 after Laptop wakes up","*Version:*
XD 1.0.1
Mac OSX 10.9.5

*Problem:*
- Deployed a simple batch job in 'singlenode'
- Laptop put to sleep mode
- After login: notice that ZK is establishing connection 
- Continues to clean-up prior to redeployment, but never goes through successfully
- Listing job both in UI and Shell states it is ""undeployed""

*Gunnar's experiment:*
- System is running in Single Node
- Laptop goes to sleep
- After waking up your laptop from sleep, you cannot retrieve the list of deployed jobs anymore (in AdminUI)

*Error:*
Only getting back a *404* - ""NoSuchBatchJobException"", ""Batch Job with the name abcd doesn't exist""",5.0,False,5.0,0,0.0,-1,1,True
"XD-2330","Story","Sprint 38","2014-11-11T07:56:40.000+0000","grenfro","","Zip created by Publish 1.1 only contains the shell.","XD, gemfire, directories in the zip file are missing.",1.0,-1.0,0.9969119917653114,0.2520844055584148,False,True,True,1415692600000.0,-1.0,-1.0,1415701415000.0,1415702315000.0,1415692600000.0,-1.0,1415695049000.0,1415698649000.0,1415702285000.0,1415702315000.0,-1.0,-1.0,"grenfro",1415698649000.0,1415692600000.0,44,1415692600000.0,-1.0,"Zip created by Publish 1.1 only contains the shell.","XD, gemfire, directories in the zip file are missing.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2326","Bug","Sprint 38","2014-11-10T06:44:19.000+0000","thomas.risberg","dturanski","Can't create stream running on Windows","Trying to test on Windows and getting the following exception when createing a stream - 'stream create --name tictoc --definition ""time | log'

{code}
09:34:20,789 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\..
09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: local
09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//
09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application
09:34:20,793 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//mo
dules/
09:34:20,794 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules
09:34:20,795 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Admin web UI: http://Seattle:9393/admin-ui
09:34:20,797 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:64424
09:34:20,798 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd
09:34:20,799 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: memory
09:34:20,913 1.1.0.SNAP  INFO LeaderSelector-0 server.DeploymentSupervisor - Leader Admin singlenode:default,admin,singlenode,hsqldbServer:9393 is watching for
stream/job deployment requests.
09:34:21,013 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Path cache event: type=INITIALIZED
09:34:21,070 1.1.0.SNAP  INFO main server.AdminServerApplication - Started AdminServerApplication in 6.364 seconds (JVM running for 18.031)
09:34:22,593 1.1.0.SNAP  INFO main server.ContainerRegistrar - Container {ip=192.168.0.120, host=Seattle, groups=, pid=1108, id=08c72e88-66d4-4b47-bd4a-8f5e5849
099f} joined cluster
09:34:22,594 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\..
09:34:22,594 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: local
09:34:22,595 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//mo
dules/
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Container IP address: 192.168.0.120
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Container hostname:   Seattle
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop Distro: hadoop22
09:34:22,597 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Path cache event: path=/containers/08c72e88-66d4-4b47-bd4a-8f5e5849099f, type=CH
ILD_ADDED
09:34:22,600 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: type=INITIALIZED
09:34:22,607 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Container arrived: Container{name='08c72e88-66d4-4b47-bd4a-8f5e5849099f', attrib
utes={ip=192.168.0.120, host=Seattle, groups=, pid=1108, id=08c72e88-66d4-4b47-bd4a-8f5e5849099f}}
09:34:22,609 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Scheduling deployments to new container(s) in 15000 ms
09:34:22,611 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop version detected from classpath: 2.2.0
09:34:22,612 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:64424
09:34:22,613 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd
09:34:22,615 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: memory
09:34:22,616 1.1.0.SNAP  INFO main server.ContainerServerApplication - Started ContainerServerApplication in 0.61 seconds (JVM running for 19.576)
09:36:15,837 1.1.0.SNAP ERROR http-nio-9393-exec-3 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(String.java:1954)
        at org.springframework.xd.dirt.module.ArchiveModuleRegistry.fromResource(ArchiveModuleRegistry.java:140)
        at org.springframework.xd.dirt.module.ArchiveModuleRegistry.findDefinition(ArchiveModuleRegistry.java:68)
        at org.springframework.xd.dirt.module.DelegatingModuleRegistry.findDefinition(DelegatingModuleRegistry.java:48)
        at org.springframework.xd.dirt.module.store.ZooKeeperModuleDefinitionRepository.findByNameAndType(ZooKeeperModuleDefinitionRepository.java:78)
        at org.springframework.xd.dirt.stream.XDStreamParser.resolveModuleType(XDStreamParser.java:317)
        at org.springframework.xd.dirt.stream.XDStreamParser.determineType(XDStreamParser.java:212)
        at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:168)
        at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:96)
        at org.springframework.xd.dirt.rest.XDController.save(XDController.java:223)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.springframework.web.method.support.InvocableHandalerMethod.invoke(InvocableHandlerMethod.java:215)
        at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)
        at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:781)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:721)
        at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)
        at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)
        at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)
        at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)
        at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:868)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)
        at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConf
iguration.java:280)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)
        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:89)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
        at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)
        at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
        at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
        at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1736)
        at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1695)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
        at java.lang.Thread.run(Thread.java:745)
{code}",3.0,0.162316871110999,0.1619290548476244,0.4216382557096486,True,True,True,1415601859000.0,1415704820000.0,1415708420000.0,1416235280000.0,1416236180000.0,1415601859000.0,-1.0,1415869313000.0,1415872913000.0,1415704574000.0,1415708174000.0,1415704820000.0,1415708420000.0,"thomas.risberg",1415872913000.0,1415601859000.0,44,1415601859000.0,-1.0,"Can't create stream running on Windows","Trying to test on Windows and getting the following exception when createing a stream - 'stream create --name tictoc --definition ""time | log'

{code}
09:34:20,789 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\..
09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: local
09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//
09:34:20,790 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application
09:34:20,793 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//mo
dules/
09:34:20,794 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules
09:34:20,795 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Admin web UI: http://Seattle:9393/admin-ui
09:34:20,797 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:64424
09:34:20,798 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd
09:34:20,799 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: memory
09:34:20,913 1.1.0.SNAP  INFO LeaderSelector-0 server.DeploymentSupervisor - Leader Admin singlenode:default,admin,singlenode,hsqldbServer:9393 is watching for
stream/job deployment requests.
09:34:21,013 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Path cache event: type=INITIALIZED
09:34:21,070 1.1.0.SNAP  INFO main server.AdminServerApplication - Started AdminServerApplication in 6.364 seconds (JVM running for 18.031)
09:34:22,593 1.1.0.SNAP  INFO main server.ContainerRegistrar - Container {ip=192.168.0.120, host=Seattle, groups=, pid=1108, id=08c72e88-66d4-4b47-bd4a-8f5e5849
099f} joined cluster
09:34:22,594 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD Home: C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\..
09:34:22,594 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Transport: local
09:34:22,595 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD config names: servers,application
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config location: file:C:\Test\spring-xd-1.1.0.BUILD-SNAPSHOT\xd\bin\../config//mo
dules/
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - XD module config name: modules
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Container IP address: 192.168.0.120
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Container hostname:   Seattle
09:34:22,596 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop Distro: hadoop22
09:34:22,597 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Path cache event: path=/containers/08c72e88-66d4-4b47-bd4a-8f5e5849099f, type=CH
ILD_ADDED
09:34:22,600 1.1.0.SNAP  INFO DeploymentsPathChildrenCache-0 server.DeploymentListener - Path cache event: type=INITIALIZED
09:34:22,607 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Container arrived: Container{name='08c72e88-66d4-4b47-bd4a-8f5e5849099f', attrib
utes={ip=192.168.0.120, host=Seattle, groups=, pid=1108, id=08c72e88-66d4-4b47-bd4a-8f5e5849099f}}
09:34:22,609 1.1.0.SNAP  INFO DeploymentSupervisor-0 server.ContainerListener - Scheduling deployments to new container(s) in 15000 ms
09:34:22,611 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Hadoop version detected from classpath: 2.2.0
09:34:22,612 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper at: localhost:64424
09:34:22,613 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Zookeeper namespace: xd
09:34:22,615 1.1.0.SNAP  INFO main util.XdConfigLoggingInitializer - Analytics: memory
09:34:22,616 1.1.0.SNAP  INFO main server.ContainerServerApplication - Started ContainerServerApplication in 0.61 seconds (JVM running for 19.576)
09:36:15,837 1.1.0.SNAP ERROR http-nio-9393-exec-3 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(String.java:1954)
        at org.springframework.xd.dirt.module.ArchiveModuleRegistry.fromResource(ArchiveModuleRegistry.java:140)
        at org.springframework.xd.dirt.module.ArchiveModuleRegistry.findDefinition(ArchiveModuleRegistry.java:68)
        at org.springframework.xd.dirt.module.DelegatingModuleRegistry.findDefinition(DelegatingModuleRegistry.java:48)
        at org.springframework.xd.dirt.module.store.ZooKeeperModuleDefinitionRepository.findByNameAndType(ZooKeeperModuleDefinitionRepository.java:78)
        at org.springframework.xd.dirt.stream.XDStreamParser.resolveModuleType(XDStreamParser.java:317)
        at org.springframework.xd.dirt.stream.XDStreamParser.determineType(XDStreamParser.java:212)
        at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:168)
        at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:96)
        at org.springframework.xd.dirt.rest.XDController.save(XDController.java:223)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.springframework.web.method.support.InvocableHandalerMethod.invoke(InvocableHandlerMethod.java:215)
        at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)
        at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:781)
        at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:721)
        at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)
        at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:943)
        at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:877)
        at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:966)
        at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:868)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:646)
        at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:842)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:303)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilterInternal(WebRequestTraceFilter.java:110)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextHeaderFilter.doFilterInternal(EndpointWebMvcAutoConf
iguration.java:280)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.security.web.FilterChainProxy.doFilterInternal(FilterChainProxy.java:186)
        at org.springframework.security.web.FilterChainProxy.doFilter(FilterChainProxy.java:160)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:89)
        at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:107)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:241)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:208)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:220)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:122)
        at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:501)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:103)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:116)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:408)
        at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1070)
        at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:611)
        at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun(NioEndpoint.java:1736)
        at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1695)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61)
        at java.lang.Thread.run(Thread.java:745)
{code}",3.0,False,3.0,0,0.0,0,0,False
"XD-2325","Story","Sprint 38","2014-11-07T06:51:46.000+0000","mbogoevici","","Set 'auto-startup' to false in Kafka source","We have to explicitly set it to false, in order to avoid an early start of the poller and the associated DistpatcherHasNoSubscribersException.",1.0,-1.0,1.00083269724516,0.987856498508084,False,True,True,1415343106000.0,-1.0,-1.0,1415356617000.0,1415357517000.0,1415343106000.0,-1.0,1415357342000.0,1415357517000.0,1415357529000.0,1415357517000.0,-1.0,-1.0,"mbogoevici",1415357517000.0,1415343106000.0,44,1415343106000.0,-1.0,"Set 'auto-startup' to false in Kafka source","We have to explicitly set it to false, in order to avoid an early start of the poller and the associated DistpatcherHasNoSubscribersException.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2323","Bug","Sprint 38","2014-11-06T12:51:50.000+0000","grenfro","grussell","Filejdbc jobs status shows ""STARTED"" even when job is complete","SHA: 67473dc71332c0727516b6f3fd11a55561b2472e
Deployment: 1 Admin, 2 Containers
JobStore: HSQLDB
OS: Mac OSX & Ubuntu
Reproducible: Yes
Job: job create foo \-\-definition ""filejdbc \-\-resources=file:filejdbctest/filejdbctest.out \-\-names=data --tableName=filejdbctest \-\-initializeDatabase=true ""\-\-deploy

When using Rabbit as a transport with more than one container and launching the job above.  The Job execution stays as ""STARTED"" status, even though the job is actually finished.   We expect it to reach a state of ""COMPLETED"".  Using Redis as a transport the job execution status does reach ""COMPLETED"".   

The execution step list shows: 
Id  Step Name                Job Exec ID  Start Time               End Time                 Status
  --  -----------------------  -----------  -----------------------  -----------------------  ---------
  8   step1-master             4            2014-11-06 15:28:29,820                           STARTED
  9   step1-master:partition0  4            2014-11-06 15:28:29,854  2014-11-06 15:28:29,890  COMPLETED",3.0,0.0340974992011172,0.0340383227012888,0.769330003668943,True,True,True,1415278310000.0,1415281191000.0,1415284791000.0,1415361903000.0,1415362803000.0,1415278310000.0,-1.0,1415343313000.0,1415346913000.0,1415281186000.0,1415284786000.0,1415281191000.0,1415284791000.0,"grenfro",1415346913000.0,1415278310000.0,44,1415278310000.0,-1.0,"Filejdbc jobs status shows ""STARTED"" even when job is complete","SHA: 67473dc71332c0727516b6f3fd11a55561b2472e
Deployment: 1 Admin, 2 Containers
JobStore: HSQLDB
OS: Mac OSX & Ubuntu
Reproducible: Yes
Job: job create foo \-\-definition ""filejdbc \-\-resources=file:filejdbctest/filejdbctest.out \-\-names=data --tableName=filejdbctest \-\-initializeDatabase=true ""\-\-deploy

When using Rabbit as a transport with more than one container and launching the job above.  The Job execution stays as ""STARTED"" status, even though the job is actually finished.   We expect it to reach a state of ""COMPLETED"".  Using Redis as a transport the job execution status does reach ""COMPLETED"".   

The execution step list shows: 
Id  Step Name                Job Exec ID  Start Time               End Time                 Status
  --  -----------------------  -----------  -----------------------  -----------------------  ---------
  8   step1-master             4            2014-11-06 15:28:29,820                           STARTED
  9   step1-master:partition0  4            2014-11-06 15:28:29,854  2014-11-06 15:28:29,890  COMPLETED",3.0,False,3.0,0,0.0,0,3,False
"XD-2322","Bug","Sprint 38","2014-11-05T21:19:23.000+0000","mbogoevici","mbogoevici","Enable configuration of replication factor on the Kafka message bus","The field exists and it is referred to in application.yml, but it does not have a setter and the bus will always use the configured default, which is 1.",3.0,0.9996322177271056,8.172939397654367e-05,0.9947774917248988,True,True,True,1415222363000.0,1415344673000.0,1415344718000.0,1415343818000.0,1415344718000.0,1415222363000.0,-1.0,1415344079000.0,1415344718000.0,1415222373000.0,1415225973000.0,1415344673000.0,1415344718000.0,"mbogoevici",1415344718000.0,1415222363000.0,44,1415222363000.0,-1.0,"Enable configuration of replication factor on the Kafka message bus","The field exists and it is referred to in application.yml, but it does not have a setter and the bus will always use the configured default, which is 1.",3.0,False,3.0,0,0.0,-1,2,False
"XD-2321","Story","Sprint 39","2014-11-05T14:18:11.000+0000","hillert","hillert","UI: Create a dedicated scheduling page for Jobs","Create a dedicated Scheduling Page for Jobs. Currently we create a form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.

Similar to XD-2320",4.0,0.7133129190486567,0.0001380542336908,0.0034031200256814,True,True,True,1415197091000.0,1415625944000.0,1415629544000.0,1415797404000.0,1415798304000.0,1415197091000.0,-1.0,1415199137000.0,1415202737000.0,1415197174000.0,1415200774000.0,1415625944000.0,1415629544000.0,"hillert",1415202737000.0,1415197091000.0,44,1415197091000.0,-1.0,"UI: Create a dedicated scheduling page for Jobs","Create a dedicated Scheduling Page for Jobs. Currently we create a form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.

Similar to XD-2320",4.0,False,4.0,0,0.0,-1,0,False
"XD-2320","Story","Sprint 39","2014-11-05T14:16:32.000+0000","hillert","hillert","UI: Create a dedicated Launch Page for Jobs","Create a dedicated Launch Page for Jobs. Currently we create a launch form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.",4.0,3.825128432844881e-05,0.0,0.0035673480384575,True,True,True,1415196992000.0,1415197015000.0,1415200615000.0,1415797379000.0,1415798279000.0,1415196992000.0,-1.0,1415199137000.0,1415202737000.0,1415196992000.0,1415200592000.0,1415197015000.0,1415200615000.0,"hillert",1415202737000.0,1415196992000.0,44,1415196992000.0,-1.0,"UI: Create a dedicated Launch Page for Jobs","Create a dedicated Launch Page for Jobs. Currently we create a launch form underneath the deployments table. That is a bit unwieldy when many deployed jobs are shown in the table.",4.0,False,4.0,0,0.0,0,0,False
"XD-2319","Story","Sprint 38","2014-11-05T13:54:09.000+0000","dturanski","dturanski","Add spring-xd-python to the distribution","",1.0,8.30535220625749e-05,1.6314084690862925e-05,6.52563387634517e-05,True,True,True,1415195649000.0,1415195705000.0,1415199305000.0,1415869013000.0,1415869913000.0,1415195649000.0,-1.0,1415195693000.0,1415199293000.0,1415195660000.0,1415199260000.0,1415195705000.0,1415199305000.0,"dturanski",1415199293000.0,1415195649000.0,44,1415195649000.0,-1.0,"Add spring-xd-python to the distribution","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2317","Story","","2014-11-05T12:48:00.000+0000","bzeyben","","Add Greenplum Sink","User should have the option of Greenplum DB sink so they can write data directly to Greenplum DB via the pgfdist/gploader (Greenplum bulk loader).

The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.",3.0,-1.0,-1.0,-1.0,False,False,False,1415191680000.0,-1.0,-1.0,1432112964000.0,1432113864000.0,1415191680000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"bzeyben",-1.0,-1.0,0,1415191680000.0,-1.0,"Add Greenplum Sink","User should have the option of Greenplum DB sink so they can write data directly to Greenplum DB via the pgfdist/gploader (Greenplum bulk loader).

The existing JDBC sinks are not suitable for high volume loads. The JDBC approach utilizes the master segment of Greenplum for loading datasets instead of the bulk loader utility.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2316","Bug","Sprint 39","2014-11-05T10:38:53.000+0000","hillert","iperumal","REST: ""jobs/configurations"" returns 404 if one job has error","There is a bug in the deployments rest end-point. 

*How to reproduce:* 

* Deploy a Batch job (success) that for example does not all necessary libraries in the class-patch and thus causes a java.lang.ClassNotFoundException

*Result:*

You cannot retrieve the list of deployments list anymore using:

* http://localhost:9393/jobs/configurations

The rest endpoint will now report:

[{""links"":[],""logref"":""NoSuchBatchJobException"",""message"":""Batch Job with the name myJob doesn't exist""}]

This message is not entirely wrongbut extremely misleading. I think we should still return the entire list and rather mark the job as having an error.

Also returning an 404 Not Found is misleading as well.
",3.0,0.9759149097331208,0.0,0.0042986512812278,True,True,True,1415183933000.0,1418642026000.0,1418645626000.0,1418726470000.0,1418727370000.0,1415183933000.0,-1.0,1415199165000.0,1415202765000.0,1415183933000.0,1415187533000.0,1418642026000.0,1418645626000.0,"hillert",1415202765000.0,1415183933000.0,44,1415183933000.0,-1.0,"REST: ""jobs/configurations"" returns 404 if one job has error","There is a bug in the deployments rest end-point. 

*How to reproduce:* 

* Deploy a Batch job (success) that for example does not all necessary libraries in the class-patch and thus causes a java.lang.ClassNotFoundException

*Result:*

You cannot retrieve the list of deployments list anymore using:

* http://localhost:9393/jobs/configurations

The rest endpoint will now report:

[{""links"":[],""logref"":""NoSuchBatchJobException"",""message"":""Batch Job with the name myJob doesn't exist""}]

This message is not entirely wrongbut extremely misleading. I think we should still return the entire list and rather mark the job as having an error.

Also returning an 404 Not Found is misleading as well.
",3.0,False,3.0,0,0.0,0,0,False
"XD-2315","Story","Sprint 38","2014-11-05T10:15:15.000+0000","hillert","hillert","UI: Add support for stoppable notifications","* Update Angular Growl to v2
* Allowing for stoppable notifications (in case you want to see it for longer than 5 secs)
",3.0,0.0013542417506346,0.0,0.0033150359478936,True,True,True,1415182515000.0,1415183321000.0,1415186921000.0,1415776782000.0,1415777682000.0,1415182515000.0,-1.0,1415184488000.0,1415188088000.0,1415182515000.0,1415186115000.0,1415183321000.0,1415186921000.0,"hillert",1415188088000.0,1415182515000.0,44,1415182515000.0,-1.0,"UI: Add support for stoppable notifications","* Update Angular Growl to v2
* Allowing for stoppable notifications (in case you want to see it for longer than 5 secs)
",3.0,False,3.0,0,0.0,-1,3,False
"XD-2310","Bug","Sprint 38","2014-11-04T10:22:49.000+0000","mbogoevici","mbogoevici","Parsing issues with kafka-bus.xml","Using Kafka as a transport option yields:

[2014-11-04 12:18:30.528] boot - 24061 ERROR [main] --- SpringApplication: Application startup failed
org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Failed to import bean definitions from URL location [classpath*:/META-INF/spring-xd/transports/kafka-bus.xml]
Offending resource: class path resource [META-INF/spring-xd/bus/message-bus.xml]; nested exception is org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/mbogoevici/.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute ""{1}"" associated with an  element type  ""value"".
	at org.springframework.beans.factory.parsing.FailFastProblemReporter.error(FailFastProblemReporter.java:70)
	at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:85)
	at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:76)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:248)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseDefaultElement(DefaultBeanDefinitionDocumentReader.java:199)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:184)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.doRegisterBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:141)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.registerBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:110)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.registerBeanDefinitions(XmlBeanDefinitionReader.java:508)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsFromImportedResources(ConfigurationClassBeanDefinitionReader.java:313)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:138)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:330)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:254)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:94)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:142)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.SingleNodeApplication.run(SingleNodeApplication.java:63)
	at org.springframework.xd.demo.kafka.KafkaDemo.main(KafkaDemo.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/mbogoevici/.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute ""{1}"" associated with an  element type  ""value"".
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:398)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:242)
	... 31 more
Caused by: org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute ""{1}"" associated with an  element type  ""value"".
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)
	at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)
	at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)
	at com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)
	at com.sun.org.apache.xerces.internal.impl.XMLScanner.scanAttributeValue(XMLScanner.java:829)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanAttribute(XMLNSDocumentScannerImpl.java:439)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:255)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)
	at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:428)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:390)
	... 36 more",1.0,-1.0,0.0101598287469914,0.0177131512106389,False,True,True,1415096569000.0,-1.0,-1.0,1415185828000.0,1415186728000.0,1415096569000.0,-1.0,1415098166000.0,1415101766000.0,1415097485000.0,1415101085000.0,-1.0,-1.0,"mbogoevici",1415101766000.0,1415096569000.0,44,1415096569000.0,-1.0,"Parsing issues with kafka-bus.xml","Using Kafka as a transport option yields:

[2014-11-04 12:18:30.528] boot - 24061 ERROR [main] --- SpringApplication: Application startup failed
org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Failed to import bean definitions from URL location [classpath*:/META-INF/spring-xd/transports/kafka-bus.xml]
Offending resource: class path resource [META-INF/spring-xd/bus/message-bus.xml]; nested exception is org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/mbogoevici/.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute ""{1}"" associated with an  element type  ""value"".
	at org.springframework.beans.factory.parsing.FailFastProblemReporter.error(FailFastProblemReporter.java:70)
	at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:85)
	at org.springframework.beans.factory.parsing.ReaderContext.error(ReaderContext.java:76)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:248)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseDefaultElement(DefaultBeanDefinitionDocumentReader.java:199)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.parseBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:184)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.doRegisterBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:141)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.registerBeanDefinitions(DefaultBeanDefinitionDocumentReader.java:110)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.registerBeanDefinitions(XmlBeanDefinitionReader.java:508)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:391)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsFromImportedResources(ConfigurationClassBeanDefinitionReader.java:313)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:138)
	at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:116)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:330)
	at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:243)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:254)
	at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:94)
	at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:609)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:464)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:691)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:320)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:142)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.xd.dirt.server.SingleNodeApplication.run(SingleNodeApplication.java:63)
	at org.springframework.xd.demo.kafka.KafkaDemo.main(KafkaDemo.java:28)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: org.springframework.beans.factory.xml.XmlBeanDefinitionStoreException: Line 9 in XML document from URL [jar:file:/Users/mbogoevici/.gradle/caches/modules-2/files-2.1/org.springframework.xd/spring-xd-dirt/1.1.0.BUILD-SNAPSHOT/cf6a9a013dbde49d2925e2b5177d01a028379758/spring-xd-dirt-1.1.0.BUILD-SNAPSHOT.jar!/META-INF/spring-xd/transports/kafka-bus.xml] is invalid; nested exception is org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute ""{1}"" associated with an  element type  ""value"".
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:398)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:335)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)
	at org.springframework.beans.factory.xml.DefaultBeanDefinitionDocumentReader.importBeanDefinitionResource(DefaultBeanDefinitionDocumentReader.java:242)
	... 31 more
Caused by: org.xml.sax.SAXParseException; lineNumber: 9; columnNumber: 26; Open quote is expected for attribute ""{1}"" associated with an  element type  ""value"".
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)
	at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)
	at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)
	at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)
	at com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)
	at com.sun.org.apache.xerces.internal.impl.XMLScanner.scanAttributeValue(XMLScanner.java:829)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanAttribute(XMLNSDocumentScannerImpl.java:439)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.scanStartElement(XMLNSDocumentScannerImpl.java:255)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2786)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
	at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:117)
	at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
	at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
	at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
	at com.sun.org.apache.xerces.internal.parsers.DOMParser.parse(DOMParser.java:243)
	at com.sun.org.apache.xerces.internal.jaxp.DocumentBuilderImpl.parse(DocumentBuilderImpl.java:347)
	at org.springframework.beans.factory.xml.DefaultDocumentLoader.loadDocument(DefaultDocumentLoader.java:76)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadDocument(XmlBeanDefinitionReader.java:428)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.doLoadBeanDefinitions(XmlBeanDefinitionReader.java:390)
	... 36 more",1.0,False,1.0,0,0.0,-1,0,False
"XD-2309","Improvement","Sprint 39","2014-11-04T08:38:48.000+0000","dokeeffe","","Incremental data import with jdbchdfs job","Enhance the current jdbchdfs job in spring-xd to have an incremental load / delta load feature similar to sqoop. See sqoop documentation [here|http://sqoop.apache.org/docs/1.4.5/SqoopUserGuide.html#_incremental_imports]. The job will need to maintain some state between executions in order to decide the start point for the next data load. 

The jdbchdfs job definition could take the following 2 new options. 

h5. checkColumn 
optional
Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR). Column should be numeric or timestamp.
h5. lastValue 
optional
If specified this will override any data saved from previous job runs. If not specified will take the saved max-value from the last job run. If no last job run data is available then it will not be an incremental load and all the data which satisfies the query will be used.

Sqoop provides 2 modes of operation for incremental load, 'append' and 'lastModified'. For jdbchdfs the job will always append as it is writing to a hdfs file.

Example: To import data from the database table some_table which has a last update column called lastUpdated, you could use.
{code}
xd:> job create myjob --definition ""jdbchdfs --sql='select col1,col2,col3 from some_table' --checkColumn=lastUpdated"" --deploy
{code}

The batch job should also be capable of being partitioned to run in parallel across multiple containers",5.0,0.8760073115240995,0.8749229072717405,0.0063343615855088,True,True,True,1415090328000.0,1430144928000.0,1430148528000.0,1432274901000.0,1432275801000.0,1415090328000.0,-1.0,1415199187000.0,1415202787000.0,1430126292000.0,1430129892000.0,1430144928000.0,1430148528000.0,"dokeeffe",1415202787000.0,1415090328000.0,44,1415090328000.0,-1.0,"Incremental data import with jdbchdfs job","Enhance the current jdbchdfs job in spring-xd to have an incremental load / delta load feature similar to sqoop. See sqoop documentation [here|http://sqoop.apache.org/docs/1.4.5/SqoopUserGuide.html#_incremental_imports]. The job will need to maintain some state between executions in order to decide the start point for the next data load. 

The jdbchdfs job definition could take the following 2 new options. 

h5. checkColumn 
optional
Specifies the column to be examined when determining which rows to import. (the column should not be of type CHAR/NCHAR/VARCHAR/VARNCHAR/ LONGVARCHAR/LONGNVARCHAR). Column should be numeric or timestamp.
h5. lastValue 
optional
If specified this will override any data saved from previous job runs. If not specified will take the saved max-value from the last job run. If no last job run data is available then it will not be an incremental load and all the data which satisfies the query will be used.

Sqoop provides 2 modes of operation for incremental load, 'append' and 'lastModified'. For jdbchdfs the job will always append as it is writing to a hdfs file.

Example: To import data from the database table some_table which has a last update column called lastUpdated, you could use.
{code}
xd:> job create myjob --definition ""jdbchdfs --sql='select col1,col2,col3 from some_table' --checkColumn=lastUpdated"" --deploy
{code}

The batch job should also be capable of being partitioned to run in parallel across multiple containers",5.0,False,5.0,0,0.0,0,6,False
"XD-2308","Story","Sprint 38","2014-11-03T12:01:44.000+0000","sabby","mbogoevici","Create sample app to demonstrate Kafka integration","As a user, I'd like to have a sample app (GitHub project) so that I can use it as a reference while provisioning Spring XD cluster with Kafka.

Consider:
* Kafka as message bus
* Kafka as source

",8.0,0.0766475186737473,0.0003406183743175,7.336395754532582e-06,True,True,True,1415016104000.0,1415089237000.0,1415092837000.0,1415969351000.0,1415970251000.0,1415016104000.0,-1.0,1415016111000.0,1415019711000.0,1415016429000.0,1415020029000.0,1415089237000.0,1415092837000.0,"sabby",1415019711000.0,1415016104000.0,44,1415016104000.0,-1.0,"Create sample app to demonstrate Kafka integration","As a user, I'd like to have a sample app (GitHub project) so that I can use it as a reference while provisioning Spring XD cluster with Kafka.

Consider:
* Kafka as message bus
* Kafka as source

",8.0,False,8.0,0,0.0,0,0,False
"XD-2307","Story","Sprint 38","2014-11-03T11:28:07.000+0000","sabby","","Add support for PHD 2.1 (XD 1.1 M1 Release)","*XD 1.1 M1 Release + PHD 2.1 Upgrade - Action Items:*

* Update to SHDP 2.1.M2, 
* Add Hadoop 2.5 (hadoop25)
* Remove hadoop22
* Remove PHD 1.0 (phd1) 
* Change PHD 2.x from phd20 to phd21
* Test PHD 2.0 with phd21",3.0,0.1939308729491194,0.1939308729491194,9.311015862867358e-06,True,True,True,1415014087000.0,1415201540000.0,1415205140000.0,1415979784000.0,1415980684000.0,1415014087000.0,-1.0,1415014096000.0,1415017696000.0,1415201540000.0,1415205140000.0,1415201540000.0,1415205140000.0,"sabby",1415017696000.0,1415014087000.0,44,1415014087000.0,-1.0,"Add support for PHD 2.1 (XD 1.1 M1 Release)","*XD 1.1 M1 Release + PHD 2.1 Upgrade - Action Items:*

* Update to SHDP 2.1.M2, 
* Add Hadoop 2.5 (hadoop25)
* Remove hadoop22
* Remove PHD 1.0 (phd1) 
* Change PHD 2.x from phd20 to phd21
* Test PHD 2.0 with phd21",3.0,False,3.0,0,0.0,0,1,False
"XD-2306","Story","Sprint 38","2014-11-03T10:12:24.000+0000","sabby","","Add support to install custom module archive","As a user, I'd like to push the custom module (built as uber-jar) via a REST API so that I can install the custom module in cluster. ",8.0,0.006332738311785,0.6791686537370432,5.654903838360228e-06,True,True,True,1415009544000.0,1415026342000.0,1415029942000.0,1417661209000.0,1417662109000.0,1415009544000.0,1416468858000.0,1415009559000.0,1415013159000.0,1416811083000.0,1416814683000.0,1415026342000.0,1415029942000.0,"sabby",1415013159000.0,1415009544000.0,44,1416468858000.0,1416468858000.0,"Add support to install custom module archive","As a user, I'd like to push the custom module (built as uber-jar) via a REST API so that I can install the custom module in cluster. ",5.0,True,5.0,1,60.0,0,1,False
"XD-2304","Story","Sprint 38","2014-11-03T07:25:41.000+0000","sabby","","Refactor Kafka source to use simple consumer instead of high-level API","As a user, I'd like to use Kafka source through simple consumer API (as opposed to high-level) so that I can gain full control to offsets and partition assignment deterministically.",8.0,0.3916937906107995,0.3916937906107995,7.648242051564448e-06,True,True,True,1414999541000.0,1415358036000.0,1415361636000.0,1415913884000.0,1415914784000.0,1414999541000.0,-1.0,1414999548000.0,1415003148000.0,1415358036000.0,1415361636000.0,1415358036000.0,1415361636000.0,"sabby",1415003148000.0,1414999541000.0,44,1414999541000.0,-1.0,"Research refactoring effort for Kafka source to use simple consumer instead of high-level API","As a user, I'd like to use Kafka source through simple consumer API (as opposed to high-level) so that I can gain full control to offsets and partition assignment deterministically.

*Spike scope*:
- Study simple consumer API functionality
- Document findings, approach and next steps",8.0,False,8.0,0,0.0,-1,0,True
"XD-2302","Story","Sprint 38","2014-10-31T13:04:57.000+0000","hillert","hillert","UI: Update AngularJS to v1.3","
* https://docs.angularjs.org/guide/migration#migrating-from-1-2-to-1-3
* http://angularjs.blogspot.com/2014/10/ng-europe-angular-13-and-beyond.html
* http://angularjs.blogspot.com/2014/10/angularjs-130-superluminal-nudge.html

",4.0,0.9996647393112046,0.0,0.7967352219685468,True,True,True,1414760697000.0,1415184106000.0,1415184248000.0,1415183348000.0,1415184248000.0,1414760697000.0,-1.0,1415098155000.0,1415101755000.0,1414760697000.0,1414764297000.0,1415184106000.0,1415184248000.0,"hillert",1415101755000.0,1414760697000.0,44,1414760697000.0,-1.0,"UI: Update AngularJS to v1.3","
* https://docs.angularjs.org/guide/migration#migrating-from-1-2-to-1-3
* http://angularjs.blogspot.com/2014/10/ng-europe-angular-13-and-beyond.html
* http://angularjs.blogspot.com/2014/10/angularjs-130-superluminal-nudge.html

",4.0,False,4.0,0,0.0,0,0,False
"XD-2301","Story","Sprint 38","2014-10-31T09:44:55.000+0000","sabby","","Research Spark integration options","*Spike scope:*
* Identify options
* POC with the most logical integration option",8.0,0.5750698008944942,0.1682233307776992,1.0026463178482409e-05,True,True,True,1414748695000.0,1415609023000.0,1415612623000.0,1416243836000.0,1416244736000.0,1414748695000.0,-1.0,1414748710000.0,1414752310000.0,1415000364000.0,1415003964000.0,1415609023000.0,1415612623000.0,"sabby",1414752310000.0,1414748695000.0,44,1414748695000.0,-1.0,"Research Spark integration options","*Spike scope:*

* Brainstorm
* Identify options
* Document
",8.0,False,8.0,0,0.0,0,0,True
"XD-2299","Story","Sprint 38","2014-10-30T18:40:58.000+0000","sabby","","Placeholder for 1.0.2 and 1.1M1 release chores","",8.0,0.9400767097104888,0.9400767097104888,4.266391017174662e-06,True,True,True,1414694458000.0,1416236871000.0,1416240471000.0,1416334289000.0,1416335189000.0,1414694458000.0,-1.0,1414694465000.0,1414698065000.0,1416236871000.0,1416240471000.0,1416236871000.0,1416240471000.0,"sabby",1414698065000.0,1414694458000.0,44,1414694458000.0,-1.0,"Placeholder for 1.0.2 and 1.1M1 release testing effort","",8.0,False,8.0,0,0.0,0,0,True
"XD-2298","Story","Sprint 38","2014-10-30T18:35:51.000+0000","sabby","","Investigate options for mass data ingest into HDFS/HAWQ","As a user, I'd like to mass ingest data from databases (and others) into HDFS/HAWQ so that I don't have to write custom code and as well as be able to ingest in an efficient way.

*Spike scope:*
* Identify and document options
**_gpfdist_/_gpload_
** HAWQ native format
** PXF
** Sqoop Tasklet
* Decide whether to handle it via OOTB Batch Job or through other mechanism (perhaps HAWQ sink?)
* Design specs",5.0,0.9774854088375212,0.9774854088375212,9.52793252006366e-07,True,True,True,1414694151000.0,1425979222000.0,1425982822000.0,1426238253000.0,1426239153000.0,1414694151000.0,1425734776000.0,1414694162000.0,1414697762000.0,1425979222000.0,1425982822000.0,1425979222000.0,1425982822000.0,"sabby",1414697762000.0,1414694151000.0,44,1425734776000.0,1425734776000.0,"Review POC and identify scope for gpload as OOTB Batch Job","As a user, I'd like to mass ingest data from databases (and others) into HDFS/HAWQ/GPDB so that I don't have to write custom code and as well as be able to ingest in an efficient way.",8.0,True,8.0,1,-37.5,0,0,True
"XD-2296","Story","Sprint 38","2014-10-30T18:20:49.000+0000","sabby","","Add config parameter to enable/disable message rates in cluster view","As a user, I'd like to have a config parameter preferably in _servers.yml_ file so that I can enable/disable message rates in the cluster view. ",2.0,0.5755546162070766,0.3102498943232961,8.951438446428128e-06,True,True,True,1414693249000.0,1415271926000.0,1415275526000.0,1415697774000.0,1415698674000.0,1414693249000.0,-1.0,1414693258000.0,1414696858000.0,1415005182000.0,1415008782000.0,1415271926000.0,1415275526000.0,"sabby",1414696858000.0,1414693249000.0,44,1414693249000.0,-1.0,"Add config parameter to enable/disable message rates in cluster view","As a user, I'd like to have a config parameter preferably in _servers.yml_ file so that I can enable/disable message rates in the cluster view. ",2.0,False,2.0,0,0.0,0,0,False
"XD-2286","Story","Sprint 37","2014-10-27T10:17:38.000+0000","pperalta","pperalta","Simple OOTB job for testing","Similar to {{time | log}}, we should ship a simple batch job that appends a timestamp to a file. This will make it much easier to validate job functionality, especially in automated tests.",2.0,-1.0,0.0,0.4698516475823486,False,True,True,1414405058000.0,-1.0,-1.0,1414568429000.0,1414569329000.0,1414405058000.0,-1.0,1414482241000.0,1414485841000.0,1414405058000.0,1414408658000.0,-1.0,-1.0,"pperalta",1414485841000.0,1414405058000.0,44,1414405058000.0,-1.0,"Simple OOTB job for testing","Similar to {{time | log}}, we should ship a simple batch job that appends a timestamp to a file. This will make it much easier to validate job functionality, especially in automated tests.",2.0,False,2.0,0,0.0,-1,0,False
"XD-2283","Story","Sprint 38","2014-10-24T12:37:14.000+0000","mark.pollack","","Vary queue number on 32 core machine (ECB-8)","Rerun test XD-2278 on a EC2 32 core machine and see when we max out.",5.0,0.3700037190606475,0.3488646790820048,0.3021287634501129,True,True,True,1414154234000.0,1414594968000.0,1414598568000.0,1415344495000.0,1415345395000.0,1414154234000.0,1414594953000.0,1414514118000.0,1414517718000.0,1414569788000.0,1414573388000.0,1414594968000.0,1414598568000.0,"mark.pollack",1414517718000.0,1414154234000.0,44,1414594953000.0,1414594953000.0,"Vary queue number on 32 core machine (ECB-8)","Rerun test XD-2278 on a EC2 32 core machine and see when we max out.",8.0,True,8.0,1,-37.5,-1,1,False
"XD-2282","Story","Sprint 37","2014-10-24T12:35:58.000+0000","mark.pollack","chrisschaefer","Vary queue number (B-7)","Based on the the results from B-6, select the number of consumer/producers that give a distinct >5% increase in message rate.  Below 5% change in message rate, prefer lower consumer/producer count.    Vary the number of PerfTest instances that are run simultaneously and use their own independent queue from 1 until the overall messages/sec on the broker plateaus.   Note the CPU performance using top for the broker and PerfTest processes.

Test 1 (2 queues)
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 2 (3 queues)
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 3 (4 queues)
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q4 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 4 (5 queues)
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q4 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q5 -p -x 1 -y 1 -s 1000 -z 60 -q 500

etc...

",1.0,-1.0,0.7984582100164692,1.0112580447161637,False,True,True,1414154158000.0,-1.0,-1.0,1414482356000.0,1414483256000.0,1414154158000.0,1414494032000.0,1414486961000.0,1414483256000.0,1414416929000.0,1414420529000.0,-1.0,-1.0,"mark.pollack",1414483256000.0,1414154158000.0,44,1414154158000.0,1414494032000.0,"Vary queue number (B-7)","Based on the the results from B-6, select the number of consumer/producers that give a distinct >5% increase in message rate.  Below 5% change in message rate, prefer lower consumer/producer count.    Vary the number of PerfTest instances that are run simultaneously and use their own independent queue from 1 until the overall messages/sec on the broker plateaus.   Note the CPU performance using top for the broker and PerfTest processes.

Test 1 (2 queues)
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 2 (3 queues)
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 3 (4 queues)
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q4 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 4 (5 queues)
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q4 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q5 -p -x 1 -y 1 -s 1000 -z 60 -q 500

etc...

",8.0,True,8.0,1,0.0,-1,0,False
"XD-2281","Story","Sprint 37","2014-10-24T12:34:19.000+0000","mark.pollack","chrisschaefer","Vary Producer and Consumer in combination using 2 Queues (B-6) ","The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate.  The in-house testing numbers need another plan to try an understand some discrepancies. 

Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue.  Vary the number of consumer and publishers.

Test 1 (one producer / one consumer):
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 2 (one producer / two consumers):
* -a -u q1 -p -x 1 -y 2 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 2 -s 1000 -z 60 -q 500

Test 3 (two producers / one consumer):
* -a -u q1 -p -x 2 -y 1 -s 1000 -z 120 -q 500
* -a -u q2 -p -x 2 -y 1 -s 1000 -z 120 -q 500

Test 4 (two producers / two consumers):
* -a -u q1 -p -x 2 -y 2 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 2 -y 2 -s 1000 -z 60 -q 500
",1.0,-1.0,0.7985789189090666,1.010887461799712,False,True,True,1414154059000.0,-1.0,-1.0,1414482345000.0,1414483245000.0,1414154059000.0,1414494027000.0,1414486829000.0,1414483245000.0,1414416940000.0,1414420540000.0,-1.0,-1.0,"mark.pollack",1414483245000.0,1414154059000.0,44,1414154059000.0,1414494027000.0,"Vary Producer and Consumer in combination using 2 Queues (B-6) ","The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate.  The in-house testing numbers need another plan to try an understand some discrepancies. 

Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue.  Vary the number of consumer and publishers.

Test 1 (one producer / one consumer):
* -a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 2 (one producer / two consumers):
* -a -u q1 -p -x 1 -y 2 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 1 -y 2 -s 1000 -z 60 -q 500

Test 3 (two producers / one consumer):
* -a -u q1 -p -x 2 -y 1 -s 1000 -z 120 -q 500
* -a -u q2 -p -x 2 -y 1 -s 1000 -z 120 -q 500

Test 4 (two producers / two consumers):
* -a -u q1 -p -x 2 -y 2 -s 1000 -z 60 -q 500
* -a -u q2 -p -x 2 -y 2 -s 1000 -z 60 -q 500
",5.0,True,5.0,1,0.0,-1,1,False
"XD-2280","Story","Sprint 38","2014-10-24T12:31:53.000+0000","sabby","","Vary producer size (EC-DB-5)","Use the number of consumers gave a maximum throughput in the previous test (say 10 consumers), message size 100 bytes, Prefetch 100.   Send 1M messages

Vary the number of producers.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of producers:*
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",3.0,0.3702518099733602,0.3489610173787658,0.0001754159203796,True,True,True,1414153913000.0,1414595051000.0,1414598651000.0,1415344467000.0,1415345367000.0,1414153913000.0,1414594898000.0,1414154122000.0,1414157722000.0,1414569684000.0,1414573284000.0,1414595051000.0,1414598651000.0,"sabby",1414157722000.0,1414153913000.0,44,1414594898000.0,1414594898000.0,"Vary producer size (EC-DB-5)","Use the number of consumers gave a maximum throughput in the previous test (say 10 consumers), message size 100 bytes, Prefetch 100.   Send 1M messages

Vary the number of producers.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of producers:*
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",5.0,True,5.0,1,-40.0,-1,1,False
"XD-2279","Story","Sprint 38","2014-10-24T12:31:21.000+0000","sabby","","Vary consumer size (EC-DB-4)","Using a single producer, message size of 1000 bytes, Pretch of 100.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the number of consumers.   Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of consumers:*
* 1
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",2.0,0.3702752746367274,0.348966586983965,0.000202284386667,True,True,True,1414153881000.0,1414595024000.0,1414598624000.0,1415344373000.0,1415345273000.0,1414153881000.0,1414592006000.0,1414154122000.0,1414157722000.0,1414569637000.0,1414573237000.0,1414595024000.0,1414598624000.0,"sabby",1414157722000.0,1414153881000.0,44,1414592006000.0,1414592006000.0,"Vary consumer size (EC-DB-4)","Using a single producer, message size of 1000 bytes, Pretch of 100.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the number of consumers.   Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of consumers:*
* 1
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",5.0,True,5.0,1,-60.0,-1,1,False
"XD-2278","Story","Sprint 38","2014-10-24T12:30:32.000+0000","mark.pollack","","Vary queue number (ECB-7)","Based on the the results from B-6, select the number of consumer/producers that give a distinct >5% increase in message rate.  Below 5% change in message rate, prefer lower consumer/producer count.    Vary the number of PerfTest instances that are run simultaneously and use their own independent queue from 1 until the overall messages/sec on the broker plateaus.   Note the CPU performance using top for the broker and PerfTest processes.


Test 1 (2 queues)
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 2 (3 queues)
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
Test 3 (4 queues)
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q4 -p -x 1 -y 1 -s 1000 -z 60 -q 500
Test 4 (5 queues)
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q4 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q5 -p -x 1 -y 1 -s 1000 -z 60 -q 500
etc.
",3.0,0.3703476147909401,0.3491054679139335,0.3024043041618929,True,True,True,1414153832000.0,1414595066000.0,1414598666000.0,1415344337000.0,1415345237000.0,1414153832000.0,1414594932000.0,1414514118000.0,1414517718000.0,1414569758000.0,1414573358000.0,1414595066000.0,1414598666000.0,"mark.pollack",1414517718000.0,1414153866000.0,44,1414594932000.0,1414594932000.0,"Vary queue number (ECB-7)","Based on the the results from B-6, select the number of consumer/producers that give a distinct >5% increase in message rate.  Below 5% change in message rate, prefer lower consumer/producer count.    Vary the number of PerfTest instances that are run simultaneously and use their own independent queue from 1 until the overall messages/sec on the broker plateaus.   Note the CPU performance using top for the broker and PerfTest processes.


Test 1 (2 queues)
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500

Test 2 (3 queues)
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
Test 3 (4 queues)
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q4 -p -x 1 -y 1 -s 1000 -z 60 -q 500
Test 4 (5 queues)
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q3 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q4 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q5 -p -x 1 -y 1 -s 1000 -z 60 -q 500
etc.
",8.0,True,8.0,1,-62.5,-1,0,False
"XD-2277","Story","Sprint 38","2014-10-24T12:30:21.000+0000","sabby","","Vary prefecth size (EC-DB-3)","Use a single producer, single consumer, message size of 1000 bytes.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the prefetch size.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Prefetch Sizes:*
* 1
* 10
* 50
* 100
* 10000

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",3.0,0.3704008360017962,0.3489677979830197,0.0002526471291815,True,True,True,1414153821000.0,1414595111000.0,1414598711000.0,1415344306000.0,1415345206000.0,1414153821000.0,1414591973000.0,1414154122000.0,1414157722000.0,1414569576000.0,1414573176000.0,1414595111000.0,1414598711000.0,"sabby",1414157722000.0,1414153821000.0,44,1414591973000.0,1414591973000.0,"Vary prefecth size (EC-DB-3)","Use a single producer, single consumer, message size of 1000 bytes.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the prefetch size.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Prefetch Sizes:*
* 1
* 10
* 50
* 100
* 10000

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",5.0,True,5.0,1,-40.0,-1,1,False
"XD-2276","Story","Sprint 38","2014-10-24T12:29:51.000+0000","sabby","","Vary message size (EC-DB-2)","Use a single producer, single consumer, prefetch size = 50.  Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the message size and measure the msg/sec rate and calculate data transfer rate in MB/sec.

*Message Sizes:*
100 bytes
1000
10,000
100,000 

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",3.0,0.3702821241896986,0.3489281026443268,0.000277793630385,True,True,True,1414153791000.0,1414594994000.0,1414598594000.0,1415344423000.0,1415345323000.0,1414153791000.0,1414591931000.0,1414154122000.0,1414157722000.0,1414569550000.0,1414573150000.0,1414594994000.0,1414598594000.0,"sabby",1414157722000.0,1414153791000.0,44,1414591931000.0,1414591931000.0,"Vary message size (EC-DB-2)","Use a single producer, single consumer, prefetch size = 50.  Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the message size and measure the msg/sec rate and calculate data transfer rate in MB/sec.

*Message Sizes:*
100 bytes
1000
10,000
100,000 

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",5.0,True,5.0,1,-40.0,-1,1,False
"XD-2275","Story","Sprint 38","2014-10-24T12:29:36.000+0000","mark.pollack","","Vary Producer and Consumer in combination using 2 Queues (ECB-6)","The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate.  The in-house testing numbers need another plan to try an understand some discrepancies. 

Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue.  Vary the number of consumer and publishers.

Test 1 (one producer / one consumer):
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
Test 2 (one producer / two consumers):
-a -u q1 -p -x 1 -y 2 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 2 -s 1000 -z 60 -q 500
Test 3 (two producers / one consumer):
-a -u q1 -p -x 2 -y 1 -s 1000 -z 120 -q 500
-a -u q2 -p -x 2 -y 1 -s 1000 -z 120 -q 500
Test 4 (two producers / two consumers):
-a -u q1 -p -x 2 -y 2 -s 1000 -z 60 -q 500
-a -u q2 -p -x 2 -y 2 -s 1000 -z 60 -q 500
",3.0,0.3704137363176939,0.349122175303065,0.3024497843313516,True,True,True,1414153776000.0,1414595091000.0,1414598691000.0,1415344287000.0,1415345187000.0,1414153776000.0,1414594862000.0,1414514118000.0,1414517718000.0,1414569724000.0,1414573324000.0,1414595091000.0,1414598691000.0,"mark.pollack",1414517718000.0,1414153776000.0,44,1414594862000.0,1414594862000.0,"Vary Producer and Consumer in combination using 2 Queues (ECB-6)","The results from EC2 testing show that once prefetch and message size are set, varying the number of producers or consumers independently does not impact the message rate.  The in-house testing numbers need another plan to try an understand some discrepancies. 

Using 500 prefetch, 1000 byte message size run two instances of PerfTest at the same time with each instance referencing a different queue.  Vary the number of consumer and publishers.

Test 1 (one producer / one consumer):
-a -u q1 -p -x 1 -y 1 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 1 -s 1000 -z 60 -q 500
Test 2 (one producer / two consumers):
-a -u q1 -p -x 1 -y 2 -s 1000 -z 60 -q 500
-a -u q2 -p -x 1 -y 2 -s 1000 -z 60 -q 500
Test 3 (two producers / one consumer):
-a -u q1 -p -x 2 -y 1 -s 1000 -z 120 -q 500
-a -u q2 -p -x 2 -y 1 -s 1000 -z 120 -q 500
Test 4 (two producers / two consumers):
-a -u q1 -p -x 2 -y 2 -s 1000 -z 60 -q 500
-a -u q2 -p -x 2 -y 2 -s 1000 -z 60 -q 500
",5.0,True,5.0,1,-40.0,-1,0,False
"XD-2274","Bug","Sprint 37","2014-10-24T06:52:03.000+0000","pperalta","pperalta","Intermittent TcpModulesTests.testTcpSink test failure","{noformat}
org.junit.ComparisonFailure: org.junit.ComparisonFailure: expected:<[Hi there!
]> but was:<[]>
org.junit.ComparisonFailure: expected:<[Hi there!
]> but was:<[]>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.springframework.xd.shell.command.TcpModulesTests.testTcpSink(TcpModulesTests.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
(60 more lines...)
{noformat}

https://build.spring.io/browse/XD-JDK8-JOB1-1162/test",1.0,0.9807132459970888,0.0,0.3377001455604075,True,True,True,1414133523000.0,1414144303000.0,1414144515000.0,1414143615000.0,1414144515000.0,1414133523000.0,-1.0,1414137235000.0,1414140835000.0,1414133523000.0,1414137123000.0,1414144303000.0,1414144515000.0,"pperalta",1414140835000.0,1414133523000.0,44,1414133523000.0,-1.0,"Intermittent TcpModulesTests.testTcpSink test failure","{noformat}
org.junit.ComparisonFailure: org.junit.ComparisonFailure: expected:<[Hi there!
]> but was:<[]>
org.junit.ComparisonFailure: expected:<[Hi there!
]> but was:<[]>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.springframework.xd.shell.command.TcpModulesTests.testTcpSink(TcpModulesTests.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
(60 more lines...)
{noformat}

https://build.spring.io/browse/XD-JDK8-JOB1-1162/test",1.0,False,1.0,0,0.0,0,1,False
"XD-2271","Story","Sprint 38","2014-10-23T11:14:34.000+0000","sabby","","Upgrade to Spring Integration 4.1.0","As to prepare for 1.1 release, we would like to upgrade to Spring Integration 4.1.0 so that we can leverage the new features, enhancement and bug fixes. ",3.0,0.8366858057434723,0.8366858057434723,0.0003629573590697,True,True,True,1414062874000.0,1415019528000.0,1415023128000.0,1415205359000.0,1415206259000.0,1414062874000.0,-1.0,1414063289000.0,1414066889000.0,1415019528000.0,1415023128000.0,1415019528000.0,1415023128000.0,"sabby",1414066889000.0,1414062874000.0,44,1414062874000.0,-1.0,"Upgrade to Spring Integration 4.1.0","As to prepare for 1.1 release, we would like to upgrade to Spring Integration 4.1.0 (RC) so that we can leverage the new features, enhancement and bug fixes. 

",3.0,False,3.0,0,0.0,0,0,True
"XD-2270","Story","Sprint 38","2014-10-23T11:13:03.000+0000","sabby","","Upgrade to Spring Boot 1.2.0","As to prepare for 1.1 release, we would like to upgrade to Spring Boot 1.2.0 (RC) so that we can leverage the new features, enhancement and bug fixes. ",3.0,0.9882004330852974,0.9882004330852974,0.0002638347706071,True,True,True,1414062783000.0,1415958020000.0,1415961620000.0,1415979750000.0,1415980650000.0,1414062783000.0,-1.0,1414063289000.0,1414066889000.0,1415958020000.0,1415961620000.0,1415958020000.0,1415961620000.0,"sabby",1414066889000.0,1414063195000.0,44,1414063195000.0,-1.0,"Upgrade to Spring Boot 1.2.0","As to prepare for 1.1 release, we would like to upgrade to Spring Boot 1.2.0 (RC1) (depends on Spring 4.1.2) so that we can leverage the new features, enhancement and bug fixes. 

[Spring Boot Milestones|https://github.com/spring-projects/spring-boot/milestones]",3.0,False,3.0,0,0.0,0,0,True
"XD-2269","Story","Sprint 37","2014-10-21T16:06:00.000+0000","sabby","eric.bottard","Remove deprecated functions ","As a follow-up action from module registry refactoring we would have to clean-up deprecated functions _(ex: download of module definitions)_ within our codebase. 

It may also be necessary to clean-up Shell and Admin-UI modules. ",3.0,0.9549979069869632,0.0,0.0,True,True,True,1413907560000.0,1414733425000.0,1414737025000.0,1414771442000.0,1414772342000.0,1413907560000.0,-1.0,1413907560000.0,1413911160000.0,1413907560000.0,1413911160000.0,1414733425000.0,1414737025000.0,"sabby",1413911160000.0,1413907560000.0,44,1413907560000.0,-1.0,"Remove deprecated functions ","As a follow-up action from module registry refactoring we would have to clean-up deprecated functions _(ex: download of module definitions)_ within our codebase. 

It may also be necessary to clean-up Shell and Admin-UI modules. ",3.0,False,3.0,0,0.0,0,0,False
"XD-2268","Story","Sprint 37","2014-10-21T14:22:12.000+0000","sabby","mark.fisher","Create a maintenance branch","As a developer, I'd like to have a maintenance branch so that I can commit MINOR release _(ex: 1.0.2)_ code changes instead of committing to MASTER.",5.0,0.5780579811386657,0.0,0.0,True,True,True,1413901332000.0,1413984081000.0,1413987681000.0,1414043582000.0,1414044482000.0,1413901332000.0,-1.0,1413901332000.0,1413904932000.0,1413901332000.0,1413904932000.0,1413984081000.0,1413987681000.0,"sabby",1413904932000.0,1413901332000.0,44,1413901332000.0,-1.0,"Create a maintenance branch","As a developer, I'd like to have a maintenance branch so that I can commit MINOR release _(ex: 1.0.2)_ code changes instead of committing to MASTER.",5.0,False,5.0,0,0.0,0,0,False
"XD-2266","Story","Sprint 37","2014-10-20T10:51:07.000+0000","sabby","grenfro","Vary producers size (ECB-5)","Use the number of consumers gave a maximum throughput in the previous test (say 10 consumers), message size 100 bytes, Prefetch 100.   Send 1M messages

Vary the number of producers.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of producers:*
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",3.0,0.0084042793935184,0.0080492239502454,2.731195717485115e-05,True,True,True,1413802267000.0,1413804421000.0,1413808021000.0,1414057665000.0,1414058565000.0,1413802267000.0,-1.0,1413802274000.0,1413805874000.0,1413804330000.0,1413807930000.0,1413804421000.0,1413808021000.0,"sabby",1413805874000.0,1413802267000.0,44,1413802267000.0,-1.0,"Vary producers size (ECB-5)","Use the number of consumers gave a maximum throughput in the previous test (say 10 consumers), message size 100 bytes, Prefetch 100.   Send 1M messages

Vary the number of producers.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of producers:*
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",3.0,False,3.0,0,0.0,-1,1,False
"XD-2265","Story","Sprint 37","2014-10-20T10:50:30.000+0000","sabby","grenfro","Vary consumers size (ECB-4)","Using a single producer, message size of 1000 bytes, Pretch of 100.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the number of consumers.   Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of consumers:*
* 1
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",3.0,0.0083607081828041,0.0081188211518504,0.0001716617639026,True,True,True,1413802230000.0,1413804373000.0,1413807973000.0,1414057648000.0,1414058548000.0,1413802230000.0,-1.0,1413802274000.0,1413805874000.0,1413804311000.0,1413807911000.0,1413804373000.0,1413807973000.0,"sabby",1413805874000.0,1413802230000.0,44,1413802230000.0,-1.0,"Vary consumers size (ECB-4)","Using a single producer, message size of 1000 bytes, Pretch of 100.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the number of consumers.   Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of consumers:*
* 1
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",3.0,False,3.0,0,0.0,-1,1,False
"XD-2264","Story","Sprint 37","2014-10-20T10:49:12.000+0000","sabby","grenfro","Infrastructure for RabbitMQ Cluster (ECB)","Pre-requisite for Rabbit MQ Benchmarks:

* Infrastructure setup
* Configuration changes
* Tool-chain setup",5.0,0.3340506573154019,0.0027694263666063,0.000158922867698,True,True,True,1413802152000.0,1414058592000.0,1414062192000.0,1414568920000.0,1414569820000.0,1413802152000.0,-1.0,1413802274000.0,1413805874000.0,1413804278000.0,1413807878000.0,1414058592000.0,1414062192000.0,"sabby",1413805874000.0,1413802152000.0,44,1413802152000.0,-1.0,"Infrastructure for RabbitMQ Cluster (ECB)","Pre-requisite for Rabbit MQ Benchmarks:

* Infrastructure setup
* Configuration changes
* Tool-chain setup

* IPerf",5.0,False,5.0,0,0.0,-1,2,True
"XD-2259","Story","Sprint 37","2014-10-20T10:00:35.000+0000","sabby","","Infrastructure for RabbitMQ Cluster (Distributed)","Pre-requisite for Rabbit MQ Benchmarks:

* Infrastructure setup
* Configuration changes
* Tool-chain setup",1.0,0.9999652139631152,0.9999652139631152,1.739301844239722e-05,True,True,True,1413799235000.0,1414144189000.0,1414144201000.0,1414143301000.0,1414144201000.0,1413799235000.0,1414494177000.0,1413799241000.0,1413802841000.0,1414144189000.0,1414144201000.0,1414144189000.0,1414144201000.0,"sabby",1413802841000.0,1413799235000.0,44,1413799235000.0,1414494177000.0,"Infrastructure for RabbitMQ Cluster (DB)","Pre-requisite for Rabbit MQ Benchmarks:

* Infrastructure setup
* Configuration changes
* Tool-chain setup",5.0,True,5.0,1,0.0,0,1,True
"XD-2258","Story","Sprint 38","2014-10-16T16:42:35.000+0000","sabby","","Research adding support for 'spring-cloud-config' to configure Modules","Please refer to the GH Issue reported here: https://github.com/spring-projects/spring-xd/issues/1218",8.0,0.2901710107534561,0.2336477383816695,1.07132813441597e-05,True,True,True,1413477755000.0,1413802777000.0,1413806377000.0,1414596960000.0,1414597860000.0,1413477755000.0,1413739453000.0,1413477767000.0,1413481367000.0,1413739465000.0,1413743065000.0,1413802777000.0,1413806377000.0,"sabby",1413481367000.0,1413477755000.0,44,1413740346000.0,1413739453000.0,"Research adding support for 'spring-cloud-config' to configure Modules","Please refer to the GH Issue reported here: https://github.com/spring-projects/spring-xd/issues/1218",2.0,True,2.0,3,300.0,0,0,False
"XD-2257","Story","Sprint 37","2014-10-16T16:10:27.000+0000","sabby","","Vary producers size (DB-5)","Use the number of consumers gave a maximum throughput in the previous test (say 10 consumers), message size 100 bytes, Prefetch 100.   Send 1M messages

Vary the number of producers.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of producers:*
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",1.0,-1.0,0.9885091532204164,0.0008036345540311,False,True,True,1413475827000.0,-1.0,-1.0,1414095856000.0,1414096756000.0,1413475827000.0,1413476357000.0,1413476326000.0,1413479926000.0,1414089621000.0,1414093221000.0,-1.0,-1.0,"sabby",1413479926000.0,1413476357000.0,44,1413476357000.0,1414494022000.0,"Vary producers size (DB-5)","Use the number of consumers gave a maximum throughput in the previous test (say 10 consumers), message size 100 bytes, Prefetch 100.   Send 1M messages

Vary the number of producers.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of producers:*
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",5.0,True,5.0,1,0.0,0,0,False
"XD-2256","Story","Sprint 37","2014-10-16T16:09:33.000+0000","sabby","","Vary consumers size (DB-4)","Using a single producer, message size of 1000 bytes, Pretch of 100.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the number of consumers.   Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of consumers:*
* 1
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",1.0,-1.0,0.9885052103379032,0.0008905321775202,False,True,True,1413475773000.0,-1.0,-1.0,1414095850000.0,1414096750000.0,1413475773000.0,1413476356000.0,1413476326000.0,1413479926000.0,1414089612000.0,1414093212000.0,-1.0,-1.0,"sabby",1413479926000.0,1413476356000.0,44,1413476356000.0,1414494018000.0,"Vary consumers size (DB-4)","Using a single producer, message size of 1000 bytes, Pretch of 100.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the number of consumers.   Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Number of consumers:*
* 1
* 2
* 4
* 6
* 10
* 50

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",5.0,True,5.0,1,0.0,0,0,False
"XD-2255","Story","Sprint 37","2014-10-16T16:08:31.000+0000","sabby","","Vary prefetch size (DB-3)","Use a single producer, single consumer, message size of 1000 bytes.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the prefetch size.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Prefetch Sizes:*
* 1
* 10
* 50
* 100
* 10000

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",1.0,-1.0,0.9885237375225636,0.0009903046756872,False,True,True,1413475711000.0,-1.0,-1.0,1414095832000.0,1414096732000.0,1413475711000.0,1413476356000.0,1413476326000.0,1413479926000.0,1414089605000.0,1414093205000.0,-1.0,-1.0,"sabby",1413479926000.0,1413476356000.0,44,1413476356000.0,1414494012000.0,"Vary prefetch size (DB-3)","Use a single producer, single consumer, message size of 1000 bytes.   Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the prefetch size.  Measure the msg/sec rate and calculate the data transfer rate in MB/sec.

*Prefetch Sizes:*
* 1
* 10
* 50
* 100
* 10000

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",5.0,True,5.0,1,0.0,0,0,False
"XD-2254","Story","Sprint 37","2014-10-16T16:07:26.000+0000","sabby","","Vary message size (DB-2)","Use a single producer, single consumer, prefetch size = 50.  Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the message size and measure the msg/sec rate and calculate data transfer rate in MB/sec.

*Message Sizes:*
100 bytes
1000
10,000
100,000 

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",1.0,-1.0,0.9884925331079176,0.0004170188785573,False,True,True,1413475646000.0,-1.0,-1.0,1414095821000.0,1414096721000.0,1413475646000.0,1413476283000.0,1413475905000.0,1413479505000.0,1414089574000.0,1414093174000.0,-1.0,-1.0,"sabby",1413479505000.0,1413476283000.0,44,1413476283000.0,1414494003000.0,"Vary message size (DB-2)","Use a single producer, single consumer, prefetch size = 50.  Send 1M messages and increase or decrease so that a given test iteration takes about 2 minutes.

Vary the message size and measure the msg/sec rate and calculate data transfer rate in MB/sec.

*Message Sizes:*
100 bytes
1000
10,000
100,000 

During the measurements look at the RabbitMQ Admin UI and see if the queue is backing up.",5.0,True,5.0,1,0.0,0,0,False
"XD-2253","Story","Sprint 37","2014-10-16T16:05:39.000+0000","sabby","","Baseline tcp measurements (DB-1)","Using the [iperf tool|https://iperf.fr/], find out the transfer rate in MB/sec between three machines in a four machine configuration.

*Topology to consider:*
generator -> xd source -> broker -> xd sink 

For each arrow, measure the transfer rate between the two machines on each side.",1.0,-1.0,0.916938276026743,0.0005462034383954,False,True,True,1413475539000.0,-1.0,-1.0,1414144719000.0,1414145619000.0,1413475539000.0,1413476279000.0,1413475905000.0,1413479505000.0,1414089961000.0,1414093561000.0,-1.0,-1.0,"sabby",1413479505000.0,1413476279000.0,44,1413476279000.0,1414494155000.0,"Baseline tcp measurements (DB-1)","Using the [iperf tool|https://iperf.fr/], find out the transfer rate in MB/sec between three machines in a four machine configuration.",5.0,True,5.0,1,0.0,0,0,True
"XD-2251","Improvement","Sprint 36","2014-10-16T10:23:46.000+0000","mbogoevici","mbogoevici","The HTTP Source creates the ChannelPipeline inefficiently","The ChannelPipelineFactory used by the HTTP source should cache expensive objects used by the ChannelPipeline between requests, because creating them every time is inefficient (and in the case of HTTPS it can become even more expensive). ",1.0,0.9733612690811134,0.0011972463334331,0.9483687518706974,True,True,True,1413455026000.0,1413474538000.0,1413475072000.0,1413474172000.0,1413475072000.0,1413455026000.0,-1.0,1413474037000.0,1413475072000.0,1413455050000.0,1413458650000.0,1413474538000.0,1413475072000.0,"mbogoevici",1413475072000.0,1413455026000.0,44,1413455026000.0,-1.0,"The HTTP Source creates the ChannelPipeline inefficiently","The ChannelPipelineFactory used by the HTTP source should cache expensive objects used by the ChannelPipeline between requests, because creating them every time is inefficient (and in the case of HTTPS it can become even more expensive). ",1.0,False,1.0,0,0.0,0,2,False
"XD-2248","Bug","","2014-10-15T08:05:35.000+0000","grenfro","grenfro","Gemfire Source and Sink deployments cause OOME PermGen","If you create more than 4 gemfire modules, the container will throw a OOME Permgen on Java 7.  

The solution is to put a technote into the wiki for the gemfire modules stating that for Java 7 they will need to add the following to environment.  
export JAVA_OPTS=""-XX:PermSize=256m""",2.0,0.0055299539170506,0.0,-1.0,True,False,True,1413360335000.0,1413360371000.0,1413363971000.0,1413365945000.0,1413366845000.0,1413360335000.0,-1.0,-1.0,-1.0,1413360335000.0,1413363935000.0,1413360371000.0,1413363971000.0,"grenfro",-1.0,-1.0,0,1413360335000.0,-1.0,"Gemfire Source and Sink deployments cause OOME PermGen","If you create more than 4 gemfire modules, the container will throw a OOME Permgen on Java 7.  

The solution is to put a technote into the wiki for the gemfire modules stating that for Java 7 they will need to add the following to environment.  
export JAVA_OPTS=""-XX:PermSize=256m""",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2247","Story","Sprint 36","2014-10-15T08:04:50.000+0000","pperalta","pperalta","Log version number in log files","When answering support questions, the first step is to determine what version of the software the customer is using. This question can be easily answered if we log the version as one of the fields in the log file. For example:

{noformat}
10:44:21,212 1.0.2.BUILD-SNAPSHOT  INFO DeploymentSupervisorCacheListener-0 server.ContainerListener - Container arrived: Container{name='431baa56-b23b-48fc-b37d-18b52231e799', attributes={ip=192.168.25.177, host=Patrick-Peralta-MacBook-Pro.local, groups=, pid=38004, id=431baa56-b23b-48fc-b37d-18b52231e799}}
{noformat}

This way when we receive log snippets (initial support inquires rarely include the entire log file) we can immediately determine if the issue has already been fixed in a later release.
",2.0,0.0016983268335639,0.0,0.0637701712050686,True,True,True,1413360290000.0,1413360587000.0,1413364187000.0,1413534268000.0,1413535168000.0,1413360290000.0,-1.0,1413371442000.0,1413375042000.0,1413360290000.0,1413363890000.0,1413360587000.0,1413364187000.0,"pperalta",1413375042000.0,1413360290000.0,44,1413360290000.0,-1.0,"Log version number in log files","When answering support questions, the first step is to determine what version of the software the customer is using. This question can be easily answered if we log the version as one of the fields in the log file. For example:

{noformat}
10:44:21,212 1.0.2.BUILD-SNAPSHOT  INFO DeploymentSupervisorCacheListener-0 server.ContainerListener - Container arrived: Container{name='431baa56-b23b-48fc-b37d-18b52231e799', attributes={ip=192.168.25.177, host=Patrick-Peralta-MacBook-Pro.local, groups=, pid=38004, id=431baa56-b23b-48fc-b37d-18b52231e799}}
{noformat}

This way when we receive log snippets (initial support inquires rarely include the entire log file) we can immediately determine if the issue has already been fixed in a later release.
",2.0,False,2.0,0,0.0,0,0,False
"XD-2245","Bug","Sprint 36","2014-10-15T06:24:54.000+0000","grenfro","dturanski","Gemfire Source throws classnotfound ","* Steps to reproduce.
** Start admin & container.
** Follow the instructions from https://github.com/spring-projects/spring-xd/wiki/Sources#gemfire-source
*** When you post the message is when the stacktrace shows up in the container.

Copying the gemfire-7.0.2.jar to the lib directory will resolve the error.  
The stacktrace is attached.
",1.0,0.0480910954729317,0.0300714800243807,0.0479691915553831,True,True,True,1413354294000.0,1413362973000.0,1413366573000.0,1413533864000.0,1413534764000.0,1413354294000.0,1413362993000.0,1413362951000.0,1413366551000.0,1413359721000.0,1413363321000.0,1413362973000.0,1413366573000.0,"grenfro",1413366551000.0,1413362993000.0,44,1413362993000.0,-1.0,"Gemfire Source throws classnotfound ","* Steps to reproduce.
** Start admin & container.
** Follow the instructions from https://github.com/spring-projects/spring-xd/wiki/Sources#gemfire-source
*** When you post the message is when the stacktrace shows up in the container.

Copying the gemfire-7.0.2.jar to the lib directory will resolve the error.  
The stacktrace is attached.
",1.0,False,1.0,0,0.0,-1,0,False
"XD-2239","Bug","Sprint 37","2014-10-14T10:57:35.000+0000","sabby","hillert","Fix incorrect IP Address associated with containers","The XD Container IP address displayed on both 'singlenode' and Distributed modes are incorrect both on _Shell_ as well as _Admin-UI_. 

*Example:*
Noticed IP address as 10.10.10.*

Following function in [RuntimeUtils|http://docs.spring.io/autorepo/docs/spring-xd/1.0.1.RELEASE/api/org/springframework/xd/dirt/util/RuntimeUtils.html] could be flawed:
{code}
public static String getIpAddress() {
	try {
		for(Enumeration<NetworkInterface> enumNic = NetworkInterface.getNetworkInterfaces();
				enumNic.hasMoreElements();) {
			NetworkInterface ifc = enumNic.nextElement();
			if (ifc.isUp()) {
				for (Enumeration<InetAddress> enumAddr = ifc.getInetAddresses();
						enumAddr.hasMoreElements(); ) {
					InetAddress address = enumAddr.nextElement();
					if (address instanceof Inet4Address && !address.isLoopbackAddress()) {
						return address.getHostAddress();
					}
				}
			}
		}
	}
	catch (IOException e) {
		// ignore
	}

	return ""unknown"";
}
{code}
",5.0,0.0784144335272067,0.0,1.168633867009466e-05,True,True,True,1413284255000.0,1413371484000.0,1413375084000.0,1414395765000.0,1414396665000.0,1413284255000.0,-1.0,1413284268000.0,1413287868000.0,1413284255000.0,1413287855000.0,1413371484000.0,1413375084000.0,"sabby",1413287868000.0,1413284255000.0,44,1413284255000.0,-1.0,"Fix incorrect IP Address associated with containers","The XD Container IP address displayed on both 'singlenode' and Distributed modes are incorrect both on _Shell_ as well as _Admin-UI_. 

*Example:*
Noticed IP address as 10.10.10.*

Following function in [RuntimeUtils|http://docs.spring.io/autorepo/docs/spring-xd/1.0.1.RELEASE/api/org/springframework/xd/dirt/util/RuntimeUtils.html] could be flawed:
{code}
public static String getIpAddress() {
	try {
		for(Enumeration<NetworkInterface> enumNic = NetworkInterface.getNetworkInterfaces();
				enumNic.hasMoreElements();) {
			NetworkInterface ifc = enumNic.nextElement();
			if (ifc.isUp()) {
				for (Enumeration<InetAddress> enumAddr = ifc.getInetAddresses();
						enumAddr.hasMoreElements(); ) {
					InetAddress address = enumAddr.nextElement();
					if (address instanceof Inet4Address && !address.isLoopbackAddress()) {
						return address.getHostAddress();
					}
				}
			}
		}
	}
	catch (IOException e) {
		// ignore
	}

	return ""unknown"";
}
{code}
",5.0,False,5.0,0,0.0,0,0,False
"XD-2238","Improvement","Sprint 37","2014-10-14T10:09:29.000+0000","pperalta","pperalta","Improve module deployment distribution","When a container joins the XD cluster (via ZooKeeper) it triggers stream/job module deployments for modules that need to be deployed. If multiple containers are being started at around the same time, this can result in the first few containers taking all of the deployments while leaving the rest without any deployments.

To solve this, we will introduce a ""quiet period"" where no deployments will be triggered within _n_ seconds of a container joining, where _n_ will have a default value (perhaps 5 to 10 seconds). This value will be configurable.",5.0,0.4625302784266547,0.0,0.3868103846686411,True,True,True,1413281369000.0,1413795785000.0,1413799385000.0,1414392647000.0,1414393547000.0,1413281369000.0,1413795168000.0,1413711571000.0,1413715171000.0,1413281369000.0,1413284969000.0,1413795785000.0,1413799385000.0,"pperalta",1413715171000.0,1413281369000.0,44,1413795168000.0,1413795168000.0,"Improve module deployment distribution","When a container joins the XD cluster (via ZooKeeper) it triggers stream/job module deployments for modules that need to be deployed. If multiple containers are being started at around the same time, this can result in the first few containers taking all of the deployments while leaving the rest without any deployments.

To solve this, we will introduce a ""quiet period"" where no deployments will be triggered within _n_ seconds of a container joining, where _n_ will have a default value (perhaps 5 to 10 seconds). This value will be configurable.",10.0,True,10.0,1,-50.0,0,0,False
"XD-2237","Story","Sprint 37","2014-10-14T07:12:22.000+0000","dturanski","","Provide Python module to handle I/O for implementing a Python shell processor ","",2.0,0.6406562253102198,0.6406562253102198,5.069986944783617e-05,True,True,True,1413270742000.0,1413877282000.0,1413880882000.0,1414216590000.0,1414217490000.0,1413270742000.0,-1.0,1413270790000.0,1413274390000.0,1413877282000.0,1413880882000.0,1413877282000.0,1413880882000.0,"dturanski",1413274390000.0,1413270742000.0,44,1413270742000.0,-1.0,"Provide Python module to handle I/O for implementing a Python shell processor ","",2.0,False,2.0,0,0.0,-1,0,False
"XD-2235","Bug","Sprint 37","2014-10-13T14:06:11.000+0000","mbogoevici","mbogoevici","Basic authentication realm is always 'null'","Besides the Basic authentication realm being always {{null}}, {{security.basic.realm}} is always ignored.",1.0,0.6172479731303053,0.0,0.5377749239703361,True,True,True,1413209171000.0,1413296648000.0,1413300248000.0,1413349992000.0,1413350892000.0,1413209171000.0,-1.0,1413285385000.0,1413288985000.0,1413209171000.0,1413212771000.0,1413296648000.0,1413300248000.0,"mbogoevici",1413288985000.0,1413209171000.0,44,1413209171000.0,-1.0,"Basic authentication realm is always 'null'","Besides the Basic authentication realm being always {{null}}, {{security.basic.realm}} is always ignored.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2230","Bug","Sprint 37","2014-10-10T11:24:29.000+0000","sabby","","Fix the configuration problem with Filter and Trasnform modules","As a user, I'd like to leverage _propertieis-location_ parameter while using *Filter* or *Transform* modules so that I can load the user-defined properties included in the external properties file. 

Attempting to include the _propertieis-location_ attribute errors out - refer to the attachment.

It could also be beneficial to load user-defined properties through stream definition similar to deployment properties.

Example:
--script=myscript.groovy --variables=foo=bar,goo=gaz
",5.0,0.8301674486868732,0.752019391136116,1.7790033133936713e-05,True,True,True,1412940269000.0,1413873564000.0,1413877164000.0,1414063594000.0,1414064494000.0,1412940269000.0,1413794439000.0,1412940289000.0,1412943889000.0,1413785708000.0,1413789308000.0,1413873564000.0,1413877164000.0,"sabby",1412943889000.0,1412940269000.0,44,1413794439000.0,1413794439000.0,"Fix the configuration problem with Filter and Transform modules","As a user, I'd like to leverage _propertieis-location_ parameter while using *Filter* or *Transform* modules so that I can load the user-defined properties included in the external properties file. 

Attempting to include the _propertieis-location_ attribute errors out - refer to the attachment.

It could also be beneficial to load user-defined properties through stream definition similar to deployment properties.

Example:
--script=myscript.groovy --variables=foo=bar,goo=gaz
",8.0,True,8.0,1,-37.5,-1,0,True
"XD-2228","Story","Sprint 37","2014-10-10T10:37:56.000+0000","sabby","","Remove logging of password in Shell","As a user, I'd like to type the _username_ and _password_ to gain access to Admin server so that I don't have to add it in some file; hence I don't have to worry about having the password getting logged somewhere.",5.0,0.8527871558442471,0.6858293317264914,0.0002611944770593,True,True,True,1412937476000.0,1415000925000.0,1415004525000.0,1415356229000.0,1415357129000.0,1412937476000.0,-1.0,1412938108000.0,1412941708000.0,1414596945000.0,1414600545000.0,1415000925000.0,1415004525000.0,"sabby",1412941708000.0,1412937476000.0,44,1412937476000.0,-1.0,"Remove logging of password in Shell","As a user, I'd like to type the _username_ and _password_ to gain access to Admin server so that I don't have to add it in some file; hence I don't have to worry about having the password getting logged somewhere.",5.0,False,5.0,0,0.0,-1,0,False
"XD-2226","Story","Sprint 37","2014-10-09T15:04:50.000+0000","sabby","mark.fisher","Add support for configurable ZK namespace","As a user, I'd like to have the flexibility to change the namespace so that I can isolate ZK _metadata_ based on each _tenant_ profile. ",5.0,0.5688459604523077,0.0,1.6705116275961837e-05,True,True,True,1412867090000.0,1413207612000.0,1413211212000.0,1413464809000.0,1413465709000.0,1412867090000.0,-1.0,1412867100000.0,1412870700000.0,1412867090000.0,1412870690000.0,1413207612000.0,1413211212000.0,"sabby",1412870700000.0,1412867090000.0,44,1412867090000.0,-1.0,"Add support for configurable ZK namespace","As a user, I'd like to have the flexibility to change the namespace so that I can isolate ZK _metadata_ based on each _tenant_ profile. ",5.0,False,5.0,0,0.0,-1,0,False
"XD-2225","Story","Sprint 38","2014-10-09T12:44:19.000+0000","hillert","","REST: Make the Job Execution REST endpoint pagination-aware","Implement pagination for:

http://localhost:9393/jobs/executions",8.0,0.9999897008143795,0.8179029599287296,0.544864683005371,True,True,True,1412858659000.0,1414606352000.0,1414606370000.0,1414605470000.0,1414606370000.0,1412858659000.0,-1.0,1413810925000.0,1413814525000.0,1414288117000.0,1414291717000.0,1414606352000.0,1414606370000.0,"hillert",1413814525000.0,1412858659000.0,44,1412858659000.0,-1.0,"REST: Make the Job Execution REST endpoint pagination-aware","Implement pagination for:

http://localhost:9393/jobs/executions",8.0,False,8.0,0,0.0,0,0,False
"XD-2224","Story","Sprint 38","2014-10-09T12:41:06.000+0000","hillert","","REST: Make the Configurations REST endpoint pagination-aware","Add pagination for:

http://localhost:9393/jobs/configurations

Related to XD-1864",2.0,0.6000625816218946,0.6000607246004734,0.4421841914551945,True,True,True,1412858466000.0,1414150993000.0,1414154593000.0,1415011553000.0,1415012453000.0,1412858466000.0,1414498184000.0,1413810925000.0,1413814525000.0,1414150989000.0,1414154589000.0,1414150993000.0,1414154593000.0,"hillert",1413814525000.0,1412858466000.0,44,1414498184000.0,1414498184000.0,"REST: Make the Configurations REST endpoint pagination-aware","Add pagination for:

http://localhost:9393/jobs/configurations

Related to XD-1864",8.0,True,8.0,1,-75.0,0,1,False
"XD-2216","Story","Sprint 37","2014-10-06T12:03:14.000+0000","sabby","","Upgrade to Spring 4.1","As a user, I'd like to have Spring 'Core' upgraded to 4.1 so that I can benefit from performance improvements associated with 'compiled' SpEL. 

Note:
This would also include upgrading to Spring Boot 1.2.x release.",3.0,0.8205742025654681,0.762052940737919,1.8874376241186848e-06,True,True,True,1412596994000.0,1415205528000.0,1415209128000.0,1415775007000.0,1415775907000.0,1412596994000.0,1414063623000.0,1412597000000.0,1412600600000.0,1415019494000.0,1415023094000.0,1415205528000.0,1415209128000.0,"sabby",1412600600000.0,1412596994000.0,44,1414063623000.0,1414063623000.0,"Upgrade to Spring 4.1.2, SI 4.1.0, SA 1.4.0","As a user, I'd like to have Spring 'Core' upgraded to 4.1.1 (_milestone_ ) so that I can benefit from performance improvements associated with 'compiled' SpEL and other enhancements. 
",8.0,True,8.0,1,-62.5,0,0,True
"XD-2215","Bug","Sprint 36","2014-10-06T10:46:11.000+0000","pperalta","pperalta","NPE in ContainerRedeploymentTests","Running the distributed tests ({{-Drun_distributed_tests=true}}) against d109a3a and got the following:

{noformat}
   java.lang.NullPointerException
   	at org.springframework.xd.dirt.module.store.ZooKeeperModuleMetadataRepository.updateDeploymentStatus(ZooKeeperModuleMetadataRepository.java:209)
   	at org.springframework.xd.dirt.module.store.ZooKeeperModuleMetadataRepository.findAllByContainerId(ZooKeeperModuleMetadataRepository.java:313)
   	at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findAllRuntimeContainers(ZooKeeperContainerRepository.java:339)
   	at org.springframework.xd.dirt.rest.ContainersController.list(ContainersController.java:97)
   	at sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)
   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   	at java.lang.reflect.Method.invoke(Method.java:483)
   	at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)
   	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)
   	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
   	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)
   	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)
   	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)
   	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)
   	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)
   	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)
   	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)
   	at javax.servlet.http.HttpServlet.service(HttpServlet.java:620)
   	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)
   	at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
   	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:
{noformat}",2.0,-1.0,0.0,0.0062715109218474,False,True,True,1412592371000.0,-1.0,-1.0,1413534306000.0,1413535206000.0,1412592371000.0,-1.0,1412598284000.0,1412601884000.0,1412592371000.0,1412595971000.0,-1.0,-1.0,"pperalta",1412601884000.0,1412592371000.0,44,1412592371000.0,-1.0,"NPE in ContainerRedeploymentTests","Running the distributed tests ({{-Drun_distributed_tests=true}}) against d109a3a and got the following:

{noformat}
   java.lang.NullPointerException
   	at org.springframework.xd.dirt.module.store.ZooKeeperModuleMetadataRepository.updateDeploymentStatus(ZooKeeperModuleMetadataRepository.java:209)
   	at org.springframework.xd.dirt.module.store.ZooKeeperModuleMetadataRepository.findAllByContainerId(ZooKeeperModuleMetadataRepository.java:313)
   	at org.springframework.xd.dirt.container.store.ZooKeeperContainerRepository.findAllRuntimeContainers(ZooKeeperContainerRepository.java:339)
   	at org.springframework.xd.dirt.rest.ContainersController.list(ContainersController.java:97)
   	at sun.reflect.GeneratedMethodAccessor100.invoke(Unknown Source)
   	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
   	at java.lang.reflect.Method.invoke(Method.java:483)
   	at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)
   	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)
   	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
   	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)
   	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)
   	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)
   	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)
   	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)
   	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)
   	at org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)
   	at javax.servlet.http.HttpServlet.service(HttpServlet.java:620)
   	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)
   	at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)
   	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:
{noformat}",2.0,False,2.0,0,0.0,0,1,False
"XD-2214","Bug","","2014-10-06T07:14:11.000+0000","mbogoevici","","Typo in httpSSL.properties","The sample {{httpSSLproperties}} file that is included in the distribution contains the line:

{quote}
keystore.passPhrase=secret
{quote}

The correct key value is {{keyStore.passPhrase}}. This issue causes HTTPS sources to deploy, but not bind to the port.",1.0,0.0015345268542199,0.0003978402955385,-1.0,True,False,True,1412579651000.0,1412579678000.0,1412583278000.0,1412596346000.0,1412597246000.0,1412579651000.0,-1.0,-1.0,-1.0,1412579658000.0,1412583258000.0,1412579678000.0,1412583278000.0,"mbogoevici",-1.0,-1.0,0,1412579651000.0,-1.0,"HTTPS Source Configuration issues","1. The sample {{httpSSLproperties}} file that is included in the distribution contains the line:

{quote}
keystore.passPhrase=secret
{quote}

The correct key value is {{keyStore.passPhrase}}. This issue causes HTTPS sources to deploy, but not bind to the port.

2. The password is always defaulting to ""secret""",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-2202","Story","","2014-10-02T11:31:04.000+0000","mark.pollack","mark.pollack","Release 1.0.1","",5.0,-1.0,0.0,-1.0,False,False,True,1412249464000.0,-1.0,-1.0,1412251114000.0,1412252014000.0,1412249464000.0,-1.0,-1.0,-1.0,1412249464000.0,1412252014000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1412249464000.0,-1.0,"Release 1.0.1","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2201","Bug","","2014-10-02T08:38:02.000+0000","dokeeffe","","Exception in a tap will stop the tapped stream from sinking data","Exception in a tap will stop the tapped stream from sinking data.

h2. Background
We experienced this when streaming data from a rabbit queue to hdfs. The stream was tapped and we had a groovy processor on the tap stream. Any exceptions in the processor stopped the main stream from writing data to the hdfs sink.

h2. Steps to reproduce.
1: Create a groovy script that throws an exception in  modules/processor/scripts/exceptionthrower.groovy. Code below
{code}
/**
 * Custom processor to be wired into a tap to throw an exception.
 */
 throw new RuntimeException(""Error from processor"")
{code}
2: Create a sample main stream
{code}
xd:>stream create --name ticktock --definition ""time | log"" --deploy
{code}
3: Tail the log to confirm the data is going to the sink. We see  'sink.ticktock' appearing in the log as expected.
4: Add a tap to the stream that will throw an exception.
{code}
xd:>stream create --name exTap --definition ""tap:stream:ticktock > script --location=exceptionthrower.groovy | log"" --deploy
{code}
5: Tail the log and we see that there are no more 'sink.ticktock' strings being logged. Looks like the main stream is no longer sending messages to the sink.
",5.0,-1.0,0.8547377792646856,-1.0,False,False,True,1412239082000.0,-1.0,-1.0,1416578285000.0,1416579185000.0,1412239082000.0,-1.0,-1.0,-1.0,1415948732000.0,1415952332000.0,-1.0,-1.0,"dokeeffe",-1.0,-1.0,0,1412239082000.0,-1.0,"Exception in a tap will stop the tapped stream from sinking data","Exception in a tap will stop the tapped stream from sinking data.

h2. Background
Running xd-singlenode.
We experienced this when streaming data from a rabbit queue to hdfs. The stream was tapped and we had a groovy processor on the tap stream. Any exceptions in the processor stopped the main stream from writing data to the hdfs sink.

h2. Steps to reproduce.
1: Create a groovy script that throws an exception in  modules/processor/scripts/exceptionthrower.groovy. Code below
{code}
/**
 * Custom processor to be wired into a tap to throw an exception.
 */
 throw new RuntimeException(""Error from processor"")
{code}
2: Create a sample main stream
{code}
xd:>stream create --name ticktock --definition ""time | log"" --deploy
{code}
3: Tail the log to confirm the data is going to the sink. We see  'sink.ticktock' appearing in the log as expected.
4: Add a tap to the stream that will throw an exception.
{code}
xd:>stream create --name exTap --definition ""tap:stream:ticktock > script --location=exceptionthrower.groovy | log"" --deploy
{code}
5: Tail the log and we see that there are no more 'sink.ticktock' strings being logged. Looks like the main stream is no longer sending messages to the sink.
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-2199","Improvement","Sprint 37","2014-10-01T06:59:27.000+0000","dturanski","David Turanski","Simplify ModuleRegistry","MR is responsible for looking up existing module definitions by name and type.  ModuleDefinition should contain name, type, and resource; where resource is a springframework.core.io.Resource containing the root path of the module definition. This could be file, classpath, http, hdfs or something else but the contents under this path will not be inspected or processed by MR. The exception is for composite modules which should contain a list of corresponding Resources.

Simplify ModuleDefinition - remove classpath. Provide support for CompositeModuleDefinition - requires a list of resources (possibly a subclass)

Also retire RedisModuleRegistry
",8.0,0.7232624250566718,0.3137235951071372,0.0816225300452255,True,True,True,1412146767000.0,1413957738000.0,1413961338000.0,1414649759000.0,1414650659000.0,1412146767000.0,1413878143000.0,1412351141000.0,1412354741000.0,1412932297000.0,1412935897000.0,1413957738000.0,1413961338000.0,"dturanski",1412354741000.0,1412146767000.0,44,1413878143000.0,1413878143000.0,"Simplify ModuleRegistry","MR is responsible for looking up existing module definitions by name and type.  ModuleDefinition should contain name, type, and resource; where resource is a springframework.core.io.Resource containing the root path of the module definition. This could be file, classpath, http, hdfs or something else but the contents under this path will not be inspected or processed by MR. The exception is for composite modules which should contain a list of corresponding Resources.

Simplify ModuleDefinition - remove classpath. Provide support for CompositeModuleDefinition - requires a list of resources (possibly a subclass)

Also retire RedisModuleRegistry
",3.0,True,3.0,1,166.66666666666669,0,0,False
"XD-2198","Improvement","Sprint 37","2014-10-01T06:53:30.000+0000","dturanski","David Turanski","Create CompositeModuleRegistry","Currently there is no ModuleRegistry contract for composite modules. The composed module definition is maintained in ZooKeeperModuleDefinitionRepository which delegates to the ModuleRegistry to define and validate component modules. The repository should be converted to a ModuleRegistry.",5.0,0.4566076028372073,0.2064383024039348,0.0537797470974629,True,True,True,1412146410000.0,1413884643000.0,1413888243000.0,1415952352000.0,1415953252000.0,1412146410000.0,1414998811000.0,1412351141000.0,1412354741000.0,1412932288000.0,1412935888000.0,1413884643000.0,1413888243000.0,"dturanski",1412354741000.0,1412146410000.0,44,1414998811000.0,1414998811000.0,"Create CompositeModuleRegistry","Currently there is no ModuleRegistry contract for composite modules. The composed module definition is maintained in ZooKeeperModuleDefinitionRepository which delegates to the ModuleRegistry to define and validate component modules. The repository should be converted to a ModuleRegistry.",3.0,True,3.0,1,66.66666666666666,0,0,False
"XD-2197","Improvement","Sprint 36","2014-10-01T06:46:10.000+0000","dturanski","dturanski","Create ModuleFactory","ModuleFactory is responsible for determining how the Module application context is created from available sources at the resource location exposed via the ModuleDefinition. The factory creates the application context and creates a SimpleModule or CompositeModule as appropropriate. For example, if an XML file is present, it is assumed to be the bean definition file used to create a CXMLAC. If no XML file is present, inspect the properties file for the existence of well-known properties such as base-package-name for component scanning for an @Configuration, or a module-class-name for an Annotated POJO based module (see XD-2100).  The MF is also responsible for creating composite modules.   Also includes Module refactoring, add getApplicationContext() and probably setApplicationContext().  Also refactor CompositeModule code to use boot SpringApplicationBuilder  
",5.0,0.780714412390259,0.0,0.2617670587424739,True,True,True,1412145970000.0,1412757876000.0,1412761476000.0,1412928847000.0,1412929747000.0,1412145970000.0,-1.0,1412351137000.0,1412354737000.0,1412145970000.0,1412149570000.0,1412757876000.0,1412761476000.0,"dturanski",1412354737000.0,1412145970000.0,44,1412145970000.0,-1.0,"Create ModuleFactory","ModuleFactory is responsible for determining how the Module application context is created from available sources at the resource location exposed via the ModuleDefinition. The factory creates the application context and creates a SimpleModule or CompositeModule as appropropriate. For example, if an XML file is present, it is assumed to be the bean definition file used to create a CXMLAC. If no XML file is present, inspect the properties file for the existence of well-known properties such as base-package-name for component scanning for an @Configuration, or a module-class-name for an Annotated POJO based module (see XD-2100).  The MF is also responsible for creating composite modules.   Also includes Module refactoring, add getApplicationContext() and probably setApplicationContext().  Also refactor CompositeModule code to use boot SpringApplicationBuilder  
",5.0,False,5.0,0,0.0,-1,2,False
"XD-2196","Improvement","Sprint 36","2014-10-01T06:31:43.000+0000","dturanski","dturanski","Refactor ContainerRegistrar","Decouple from DeploymentListener. Make DL a public class to handle deployment related events.  Remove createSimpleModule() and createComposedModule() from DL.  This will be delegated to ModuleDeployer which will eventually be further refactored to use the proposed ModuleFactory.",3.0,0.7809261763270279,7.90144099344053e-05,0.2625750796197623,True,True,True,1412145103000.0,1412757870000.0,1412761470000.0,1412928870000.0,1412929770000.0,1412145103000.0,-1.0,1412351137000.0,1412354737000.0,1412145165000.0,1412148765000.0,1412757870000.0,1412761470000.0,"dturanski",1412354737000.0,1412145103000.0,44,1412145103000.0,-1.0,"Refactor ContainerRegistrar","Decouple from DeploymentListener. Make DL a public class to handle deployment related events.  Remove createSimpleModule() and createComposedModule() from DL.  This will be delegated to ModuleDeployer which will eventually be further refactored to use the proposed ModuleFactory.",3.0,False,3.0,0,0.0,-1,0,False
"XD-2192","Story","Sprint 35","2014-09-30T12:48:04.000+0000","mark.pollack","thomas.risberg","Fix failing script integration test","See https://build.spring.io/browse/XD-SCRIPTS-723",3.0,0.0363870059522898,0.0267011460835768,0.0,True,True,True,1412081284000.0,1412083662000.0,1412087262000.0,1412145737000.0,1412146637000.0,1412081284000.0,-1.0,1412081284000.0,1412084884000.0,1412083029000.0,1412086629000.0,1412083662000.0,1412087262000.0,"mark.pollack",1412084884000.0,1412081284000.0,44,1412081284000.0,-1.0,"Fix failing script integration test","See https://build.spring.io/browse/XD-SCRIPTS-723",3.0,False,3.0,0,0.0,0,0,False
"XD-2190","Bug","Sprint 36","2014-09-29T12:05:47.000+0000","kparikh","iperumal","xd-shell from 1.0.1 doesn't work with 1.0.0 GA admin","Targeting xd-shell from 1.0.1 to 1.0.0 GA admin server fails

server-unknown:>admin config info
  -------------  -------------------------------------------------------------
  Result         Unable to contact XD Admin Server at 'http://localhost:9393'.
  Target         http://localhost:9393
  Timezone used  Pacific Standard Time (UTC -8:00)
  -------------  -------------------------------------------------------------
-------------------------------------------------------------------------------
An exception ocurred during targeting:
java.lang.NullPointerException
    at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:110)
    at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:137)
    at org.springframework.xd.shell.command.ConfigCommands.target(ConfigCommands.java:106)
    at org.springframework.xd.shell.command.ConfigCommands.afterPropertiesSet(ConfigCommands.java:191)
",1.0,0.43220737289521,0.4321767206146804,0.7164153179663234,True,True,True,1411992347000.0,1412034648000.0,1412038248000.0,1412089319000.0,1412090219000.0,1411992347000.0,-1.0,1412062464000.0,1412066064000.0,1412034645000.0,1412038245000.0,1412034648000.0,1412038248000.0,"kparikh",1412066064000.0,1411992347000.0,44,1411992347000.0,-1.0,"xd-shell from 1.0.1 doesn't work with 1.0.0 GA admin","Targeting xd-shell from 1.0.1 to 1.0.0 GA admin server fails

server-unknown:>admin config info
  -------------  -------------------------------------------------------------
  Result         Unable to contact XD Admin Server at 'http://localhost:9393'.
  Target         http://localhost:9393
  Timezone used  Pacific Standard Time (UTC -8:00)
  -------------  -------------------------------------------------------------
-------------------------------------------------------------------------------
An exception ocurred during targeting:
java.lang.NullPointerException
    at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:110)
    at org.springframework.xd.rest.client.impl.SpringXDTemplate.<init>(SpringXDTemplate.java:137)
    at org.springframework.xd.shell.command.ConfigCommands.target(ConfigCommands.java:106)
    at org.springframework.xd.shell.command.ConfigCommands.afterPropertiesSet(ConfigCommands.java:191)
",1.0,False,1.0,0,0.0,0,0,False
"XD-2181","Story","Sprint 36","2014-09-25T21:07:29.000+0000","mbogoevici","mbogoevici","Document how to enable SSL and Basic authentication ","As a user, I want to know my configuration options are for enabling SSL/HTTPS and Basic authentication for administration endpoints, so that I can secure my application.",1.0,0.9632243805862374,0.0,0.8415392047175313,True,True,True,1411679249000.0,1412064087000.0,1412067687000.0,1412077880000.0,1412078780000.0,1411679249000.0,-1.0,1412015470000.0,1412019070000.0,1411679249000.0,1411682849000.0,1412064087000.0,1412067687000.0,"mbogoevici",1412019070000.0,1411679249000.0,44,1411679249000.0,-1.0,"Document how to enable SSL and Basic authentication ","As a user, I want to know my configuration options are for enabling SSL/HTTPS and Basic authentication for administration endpoints, so that I can secure my application.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2180","Story","Sprint 36","2014-09-25T20:29:16.000+0000","sabby","","Expose property to change ""commit-interval""","As a user, I'd like to override the default ""commit-interval"" so that I can configure commit interval depending on data volume.

*Note:*
This would apply for all OOTB jobs that has partition support. The property could be part of _servers.yml_ file.",3.0,0.8483392560855394,0.8483392560855394,6.687189783232776e-05,True,True,True,1411676956000.0,1412755269000.0,1412758869000.0,1412947143000.0,1412948043000.0,1411676956000.0,1412583530000.0,1411677041000.0,1411680641000.0,1412755269000.0,1412758869000.0,1412755269000.0,1412758869000.0,"sabby",1411680641000.0,1411676956000.0,44,1412583530000.0,1412583530000.0,"Expose property to change ""commit-interval""","As a user, I'd like to override the default ""commit-interval"" so that I can configure commit interval depending on data volume.

*Note:*
This would apply for all OOTB jobs that has partition support. The property could be part of _servers.yml_ file.",5.0,True,5.0,1,-40.0,0,0,False
"XD-2178","Story","Sprint 36","2014-09-25T20:16:01.000+0000","sabby","","Add remote partitioning on 'jdbchdfs' job","As a user, I'd like to have the option to supply data partitioning strategy so that I can parallelize ingest of data from RDBMS to HDFS.",8.0,0.7678073128100372,0.7155233078816785,1.4939185720025508e-05,True,True,True,1411676161000.0,1412652676000.0,1412656276000.0,1412947084000.0,1412947984000.0,1411676161000.0,-1.0,1411676180000.0,1411679780000.0,1412586180000.0,1412589780000.0,1412652676000.0,1412656276000.0,"sabby",1411679780000.0,1411676161000.0,44,1411676161000.0,-1.0,"Add remote partitioning on 'jdbchdfs' job","As a user, I'd like to have the option to supply data partitioning strategy so that I can parallelize ingest of data from RDBMS to HDFS.",8.0,False,8.0,0,0.0,0,0,False
"XD-2177","Story","Sprint 38","2014-09-25T09:47:22.000+0000","thomas.risberg","","Add support for Pivotal HD 2.1","Pivotal HD 2.1 is now available and we should consider adding it as a supported version.

We currently support:
 - Hadoop 2.2.0 (hadoop22)
 - Hadoop 2.4.0 (hadoop24)
 - Pivotal HD 1.0 (phd1)
 - Pivotal HD 2.0 (phd20)
 - Hortonwork HDP 2.1 (hdp21)
 - Cloudera CDH5 (cdh5)

Do we replace PHD 2.0 with 2.1 or do we need to support both?
",3.0,0.7973356952058571,0.7973356952058571,0.6362451763952405,True,True,True,1411638442000.0,1415094425000.0,1415098025000.0,1415971956000.0,1415972856000.0,1411638442000.0,-1.0,1414396192000.0,1414399792000.0,1415094425000.0,1415098025000.0,1415094425000.0,1415098025000.0,"thomas.risberg",1414399792000.0,1411638442000.0,44,1411638442000.0,-1.0,"Add support for Pivotal HD 2.1 (XD 1.0.2 Release)","*XD 1.0.2 Release + PHD 2.1 Upgrade - Action Items:*

* Update to SHDP 2.0.3
* Add Hadoop 2.5 (hadoop25)
* Change PHD 2.x from phd20 to phd21
* Test PHD 2.0 with phd21 
* Document that both PHD 2.1 and PHD 2.0 is supported with phd21

",3.0,False,3.0,0,0.0,0,0,True
"XD-2172","Improvement","","2014-09-23T04:02:15.000+0000","smollet","","Provide a way to customize the isolation level of the JobRepository","The Gemfire XD database cannot be used to store the Spring XD metadata because the former doesn't support the default Spring Batch transaction isolation level ISOLATION_SERIALIZABLE.

There looks to be no way to configure the Spring XD's internal Spring Batch JobRepository with another isolation level.

The JobRepository instance is getting created with default settings by the Spring Batch'es {{SimpleBatchConfiguration}} and there are no custom {{BatchConfigurer}}s available to change the default settings of the JobRepository.",1.0,0.9999923650892176,0.9999923650892176,-1.0,True,False,True,1411444935000.0,1414064461000.0,1414064481000.0,1414063581000.0,1414064481000.0,1411444935000.0,-1.0,-1.0,-1.0,1414064461000.0,1414064481000.0,1414064461000.0,1414064481000.0,"smollet",-1.0,-1.0,0,1411444935000.0,-1.0,"Provide a way to customize the isolation level of the JobRepository","The Gemfire XD database cannot be used to store the Spring XD metadata because the former doesn't support the default Spring Batch transaction isolation level ISOLATION_SERIALIZABLE.

There looks to be no way to configure the Spring XD's internal Spring Batch JobRepository with another isolation level.

The JobRepository instance is getting created with default settings by the Spring Batch'es {{SimpleBatchConfiguration}} and there are no custom {{BatchConfigurer}}s available to change the default settings of the JobRepository.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2171","Story","Sprint 36","2014-09-23T02:39:09.000+0000","eric.bottard","","Document Kafka source","",1.0,-1.0,0.996408072035472,0.1003736157820281,False,True,True,1411439949000.0,-1.0,-1.0,1412596923000.0,1412597823000.0,1411439949000.0,-1.0,1411556169000.0,1411559769000.0,1412593664000.0,1412597264000.0,-1.0,-1.0,"eric.bottard",1411559769000.0,1411439949000.0,44,1411439949000.0,-1.0,"Document Kafka source","",1.0,False,1.0,0,0.0,0,0,False
"XD-2170","Bug","Sprint 35","2014-09-22T11:01:47.000+0000","pperalta","pperalta","NoNodeException for job creation","The following exception was encountered by a few parties: for example https://gopivotal-com.socialcast.com/messages/21678398

{noformat}
ERROR LeaderSelector-0 leader.LeaderSelector - The leader threw an exception
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/jobs/NNNN/modules
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1586)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
	at org.springframework.xd.dirt.server.JobDeploymentListener.recalculateJobStates(JobDeploymentListener.java:197)
	at org.springframework.xd.dirt.server.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:389)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

No specific details on reproducing yet; although the Socialcast thread indicates:

{quote}
I only hit this when I have tried to deploy a job that fails deployment the first time
{quote}",5.0,0.1564645270497199,0.0001209076131493,0.2768246973951127,True,True,True,1411383707000.0,1411476881000.0,1411480481000.0,1411978303000.0,1411979203000.0,1411383707000.0,-1.0,1411548555000.0,1411552155000.0,1411383779000.0,1411387379000.0,1411476881000.0,1411480481000.0,"pperalta",1411552155000.0,1411383707000.0,44,1411383707000.0,-1.0,"NoNodeException for job creation","The following exception was encountered by a few parties: for example https://gopivotal-com.socialcast.com/messages/21678398

{noformat}
ERROR LeaderSelector-0 leader.LeaderSelector - The leader threw an exception
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/jobs/NNNN/modules
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1586)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
	at org.springframework.xd.dirt.server.JobDeploymentListener.recalculateJobStates(JobDeploymentListener.java:197)
	at org.springframework.xd.dirt.server.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:389)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{noformat}

No specific details on reproducing yet; although the Socialcast thread indicates:

{quote}
I only hit this when I have tried to deploy a job that fails deployment the first time
{quote}",5.0,False,5.0,0,0.0,0,1,False
"XD-2169","Story","Sprint 35","2014-09-22T10:12:36.000+0000","sabby","mbogoevici","Evaluate Spring Boot dependency upgrade","As a user, I'd like to evaluate Spring Boot dependency upgrades so that I can make sure there aren't any side effects or impacts to existing functionalities. ",3.0,0.2052302234485477,0.0,0.0,True,True,True,1411380756000.0,1411397417000.0,1411401017000.0,1411461038000.0,1411461938000.0,1411380756000.0,-1.0,1411380756000.0,1411384356000.0,1411380756000.0,1411384356000.0,1411397417000.0,1411401017000.0,"sabby",1411384356000.0,1411380756000.0,44,1411380756000.0,-1.0,"Evaluate Spring Boot dependency upgrade","As a user, I'd like to evaluate Spring Boot dependency upgrades so that I can make sure there aren't any side effects or impacts to existing functionalities. ",3.0,False,3.0,0,0.0,0,0,False
"XD-2158","Story","Sprint 35","2014-09-20T17:57:20.000+0000","sabby","","Provide Docker image for developers","As a user, I need a 'sandbox' Docker Image so that I can get started to experiment XD deployment with the following setup:

* Ubuntu OS
* Full XD Jar
* Java 7.x
* Redis
* RabbitMQ",5.0,0.8335748742445094,0.1835938812255229,0.0389622483048425,True,True,True,1411235840000.0,1412650204000.0,1412653804000.0,1412931685000.0,1412932585000.0,1411235840000.0,1412584488000.0,1411301949000.0,1411305549000.0,1411547352000.0,1411550952000.0,1412650204000.0,1412653804000.0,"sabby",1411305549000.0,1411235840000.0,44,1412584488000.0,1412584488000.0,"Provide Docker image for developers","As a user, I need a 'sandbox' Docker Image so that I can get started to experiment XD deployment with the following setup:

* Ubuntu OS
* Full XD Jar
* Java 7.x
* Redis
* RabbitMQ",8.0,True,8.0,1,-37.5,0,0,False
"XD-2155","Story","Sprint 36","2014-09-19T20:35:30.000+0000","hillert","hillert","UI: Add Tooltip Directive","Small follow up story to XD-2094 to improve tooltip handling.",1.0,4.289606283973931e-05,0.0,0.396268840599228,True,True,True,1411158930000.0,1411158973000.0,1411162573000.0,1412160453000.0,1412161353000.0,1411158930000.0,-1.0,1411556159000.0,1411559759000.0,1411158930000.0,1411162530000.0,1411158973000.0,1411162573000.0,"hillert",1411559759000.0,1411158930000.0,44,1411158930000.0,-1.0,"UI: Add Tooltip Directive","Small follow up story to XD-2094 to improve tooltip handling.",1.0,False,1.0,0,0.0,0,0,False
"XD-2154","Story","Sprint 35","2014-09-19T15:04:57.000+0000","sabby","","Research REST endpoint approach to push custom module","As a user, I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations so that I don't have to manually move and set it up.",8.0,-1.0,0.09203671684165,0.0698599302015454,False,True,True,1411139097000.0,-1.0,-1.0,1413464323000.0,1413465223000.0,1411139097000.0,-1.0,1411301600000.0,1411305200000.0,1411353186000.0,1411356786000.0,-1.0,-1.0,"sabby",1411305200000.0,1411139097000.0,44,1411139097000.0,-1.0,"Research REST endpoint approach to push custom module","As a user, I'd like to have a REST API to point and push an archive that includes custom module definitions and configurations so that I don't have to manually move and set it up.

*Scope of this spike:*
* Assess customer requirement, brainstorm, and document options
* Socialize with the team to collect feedback
* Identify phases
* Create new stories",8.0,False,8.0,0,0.0,0,0,True
"XD-2153","Improvement","","2014-09-19T14:19:20.000+0000","grenfro","","Update Wiki to reflect the change from runtime x to cluster x","Update the wiki to reflect the change from:
runtime containers to cluster containers and runtime modules to cluster modules.  ",1.0,-1.0,-1.0,-1.0,False,False,False,1411136360000.0,-1.0,-1.0,1412140328000.0,1412141228000.0,1411136360000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1411136360000.0,-1.0,"Update Wiki to reflect the change from runtime x to cluster x","Update the wiki to reflect the change from:
runtime containers to cluster containers and runtime modules to cluster modules.  ",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2151","Bug","Sprint 36","2014-09-19T08:35:07.000+0000","thomas.risberg","","HDFS sink should honor --fileExtension parameter for bzip2 compressed files","Looks like the --fileExtension isn't used when compressing files with bzip2, some use cases requirer bz2 instead of bzip2 as the extension. Also, '.bz2' should be the default extension. At the same time we should change the default gzip extension to '.gz'.",3.0,0.8561710180413751,0.8561710180413751,0.485375190289454,True,True,True,1411115707000.0,1415275318000.0,1415278918000.0,1415973195000.0,1415974095000.0,1411115707000.0,-1.0,1413473848000.0,1413477448000.0,1415275318000.0,1415278918000.0,1415275318000.0,1415278918000.0,"thomas.risberg",1413477448000.0,1411115707000.0,44,1411115707000.0,-1.0,"HDFS sink should honor --fileExtension parameter for bzip2 compressed files","Looks like the --fileExtension isn't used when compressing files with bzip2, some use cases requirer bz2 instead of bzip2 as the extension. Also, '.bz2' should be the default extension. At the same time we should change the default gzip extension to '.gz'.",3.0,False,3.0,0,0.0,0,1,False
"XD-2148","Improvement","Sprint 35","2014-09-18T17:09:48.000+0000","kparikh","kparikh","Create separate distribution for shell","Create zip distribution for shell",1.0,0.0936615540274384,0.0,0.0531757222900208,True,True,True,1411060188000.0,1411486754000.0,1411490354000.0,1415613622000.0,1415614522000.0,1411060188000.0,-1.0,1411302368000.0,1411305968000.0,1411060188000.0,1411063788000.0,1411486754000.0,1411490354000.0,"kparikh",1411305968000.0,1411060188000.0,44,1411060188000.0,-1.0,"Create separate distribution for shell","Create zip distribution for shell",1.0,False,1.0,0,0.0,-1,0,False
"XD-2147","Story","Sprint 35","2014-09-18T15:44:49.000+0000","sabby","","Upgrade to spring boot 1.1.7.RELEASE","As a user, I'd like to have latest Spring Boot RELEASE pulled as a dependency so that I can inherit and implement the OOTB security features.",1.0,0.7138931758669863,0.2938907213382177,0.2677293545349591,True,True,True,1411055089000.0,1411712404000.0,1411716004000.0,1411974936000.0,1411975836000.0,1411055089000.0,-1.0,1411301600000.0,1411305200000.0,1411325688000.0,1411329288000.0,1411712404000.0,1411716004000.0,"sabby",1411305200000.0,1411139306000.0,44,1411139306000.0,-1.0,"Upgrade to spring boot 1.1.7.RELEASE","As a user, I'd like to have latest Spring Boot RELEASE pulled as a dependency so that I can inherit and implement the OOTB security features.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2146","Story","Sprint 35","2014-09-18T15:44:06.000+0000","sabby","","Upgrade to spring boot 1.1.7.SNAPSHOT","As a user, I'd like to have latest Spring Boot snapshot pulled as a dependency so that I can inherit and implement the OOTB security features.",1.0,0.8408442333479049,0.6647257027500946,0.6055070655670873,True,True,True,1411055046000.0,1411397426000.0,1411401026000.0,1411461332000.0,1411462232000.0,1411055046000.0,-1.0,1411301600000.0,1411305200000.0,1411325713000.0,1411329313000.0,1411397426000.0,1411401026000.0,"sabby",1411305200000.0,1411139301000.0,44,1411139301000.0,-1.0,"Upgrade to spring boot 1.1.7.SNAPSHOT","As a user, I'd like to have latest Spring Boot snapshot pulled as a dependency so that I can inherit and implement the OOTB security features.",1.0,False,1.0,0,0.0,0,0,False
"XD-2145","Story","Sprint 35","2014-09-18T10:41:22.000+0000","sabby","mbogoevici","XD Shell needs to be be able to authenticate using basic auth to admin server","As a user, I want to be able to provide security credentials to the XD Shell so that I can interact with an xd admin server that is secured via basic auth

Technical implementation:

Add ---password and --username to the admin config command.


",5.0,0.8554912221568415,0.1043629279662711,0.3780756728346749,True,True,True,1411036882000.0,1411635873000.0,1411639473000.0,1411736154000.0,1411737054000.0,1411036882000.0,-1.0,1411301600000.0,1411305200000.0,1411109954000.0,1411113554000.0,1411635873000.0,1411639473000.0,"sabby",1411305200000.0,1411036882000.0,44,1411036882000.0,-1.0,"XD Shell needs to be be able to authenticate using basic auth to admin server","As a user, I want to be able to provide security credentials to the XD Shell so that I can interact with an xd admin server that is secured via basic auth

Technical implementation:

Add ---password and --username to the admin config command.


",5.0,False,5.0,0,0.0,-1,2,False
"XD-2144","Story","Sprint 35","2014-09-17T20:45:12.000+0000","sabby","mbogoevici","Support Spring Boot's single-user security configurations","As a user, I'd like to have the option to provide single-user security configurations so that I can override them as needed.

*Reference:*
[Spring Boot - Security|http://docs.spring.io/spring-boot/docs/1.1.x-SNAPSHOT/reference/html/boot-features-security.html]

*Scope:*
Configurations can be provided through _servers.yml_ file.",1.0,0.8651527711511345,0.1642406488809043,0.4196615391996673,True,True,True,1410986712000.0,1411635869000.0,1411639469000.0,1411736150000.0,1411737050000.0,1410986712000.0,1411372162000.0,1411301600000.0,1411305200000.0,1411109948000.0,1411113548000.0,1411635869000.0,1411639469000.0,"sabby",1411305200000.0,1410986712000.0,44,1411372162000.0,1411372162000.0,"Support Spring Boot's single-user security configurations","As a user, I'd like to have the option to provide single-user security configurations so that I can override them as needed.

*Reference:*
[Spring Boot - Security|http://docs.spring.io/spring-boot/docs/1.1.x-SNAPSHOT/reference/html/boot-features-security.html]

*Scope:*
Configurations can be provided through _servers.yml_ file.",5.0,True,5.0,1,-80.0,-1,0,False
"XD-2143","Story","Sprint 35","2014-09-17T20:33:37.000+0000","sabby","mbogoevici","Add Basic Auth support","As a user, I'd like to have the option of _Basic Auth_ so that I'm challenged to provide _user name_ and _password_ when making a request.

Technical Implementation:

This functionality is provided in Spring Boot 1.1.x, it should be a matter of adding the spring boot security starter dependency to the spring-xd-dirt project.  

It will be controlled using the spring boot property server.basic.enabled = true/false.  Our default in application.yml for this property should be false.

",1.0,0.8652658293243785,0.164998062652874,0.420201936814522,True,True,True,1410986017000.0,1411635855000.0,1411639455000.0,1411736144000.0,1411737044000.0,1410986017000.0,1411371877000.0,1411301600000.0,1411305200000.0,1411109935000.0,1411113535000.0,1411635855000.0,1411639455000.0,"sabby",1411305200000.0,1410986017000.0,44,1411371877000.0,1411371877000.0,"Add Basic Auth support","As a user, I'd like to have the option of _Basic Auth_ so that I'm challenged to provide _user name_ and _password_ when making a request.

Technical Implementation:

This functionality is provided in Spring Boot 1.1.x, it should be a matter of adding the spring boot security starter dependency to the spring-xd-dirt project.  

It will be controlled using the spring boot property server.basic.enabled = true/false.  Our default in application.yml for this property should be false.

",8.0,True,8.0,1,-87.5,-1,2,False
"XD-2140","Story","Sprint 45","2014-09-17T07:04:29.000+0000","eric.bottard","mark.fisher","Deployment properties should use label instead of name","stream create foo --definition ""label: bar | xxxx""
stream deploy foo --properties ""module.label.yyy=zzz"" seems to work but it does not. The pre-validation is correct, but downstream, deployment logic still looks for module.bar (instead of module.label)",3.0,0.0060003408620606,0.992978057727012,0.9928631677797642,True,True,True,1410937469000.0,1411028866000.0,1411032466000.0,1426168537000.0,1426169437000.0,1410937469000.0,-1.0,1426060729000.0,1426064329000.0,1426062479000.0,1426066079000.0,1411028866000.0,1411032466000.0,"eric.bottard",1426064329000.0,1410937469000.0,44,1410937469000.0,-1.0,"Deployment properties should use label instead of name","stream create foo --definition ""label: bar | xxxx""
stream deploy foo --properties ""module.label.yyy=zzz"" seems to work but it does not. The pre-validation is correct, but downstream, deployment logic still looks for module.bar (instead of module.label)",3.0,False,3.0,0,0.0,0,2,False
"XD-2139","Improvement","Sprint 50","2014-09-17T03:02:13.000+0000","fmarchand","hillert","Add ftp sink to default sink modules","It would be nice to have a simple ftp sink. I had to do it for one of my projects. Therefore, the sink already exists. I would like to contribute but I don't know how you do the 'testing' part for that kind of module.",2.0,0.998653536705823,0.9986475336873588,0.9986510621791278,True,True,True,1410922933000.0,1432715905000.0,1432719505000.0,1432744388000.0,1432745288000.0,1410922933000.0,-1.0,1432715851000.0,1432719451000.0,1432715774000.0,1432719374000.0,1432715905000.0,1432719505000.0,"fmarchand",1432719451000.0,1410922933000.0,44,1410922933000.0,-1.0,"Add ftp sink to default sink modules","It would be nice to have a simple ftp sink. I had to do it for one of my projects. Therefore, the sink already exists. I would like to contribute but I don't know how you do the 'testing' part for that kind of module.",2.0,False,2.0,0,0.0,0,0,False
"XD-2130","Story","Sprint 48","2014-09-16T15:36:42.000+0000","sabby","","Add Cassandra sink","As a user, I'd like to have the option of _Cassandra_ sink, so I can leverage the NoSQL database to write high volumes of variable data segments in high velocity.",3.0,0.9694448539536632,0.9694443454512702,0.8202204196848226,True,True,True,1410881802000.0,1433759449000.0,1433763049000.0,1434479611000.0,1434480511000.0,1410881802000.0,-1.0,1430237945000.0,1430241545000.0,1433759437000.0,1433763037000.0,1433759449000.0,1433763049000.0,"sabby",1430241545000.0,1410881802000.0,44,1410881802000.0,-1.0,"Add Cassandra sink","As a user, I'd like to have the option of _Cassandra_ sink, so I can leverage the NoSQL database to write high volumes of variable data segments in high velocity.",3.0,False,3.0,0,0.0,-1,1,False
"XD-2124","Story","Sprint 37","2014-09-16T14:58:14.000+0000","sabby","","Provide a Sqoop 'tasklet' for mass data ingest","As a user, I'd like to have the ability to mass-ingest data from various database systems so that I'm not restricted with the current approach (_jdbchdfs_) that is dependent on JDBC drivers. 

",5.0,0.9478332984824516,0.9478332984824516,0.2132181767524732,True,True,True,1410879494000.0,1417424863000.0,1417428463000.0,1417784206000.0,1417785106000.0,1410879494000.0,-1.0,1412351896000.0,1412355496000.0,1417424863000.0,1417428463000.0,1417424863000.0,1417428463000.0,"sabby",1412355496000.0,1410879494000.0,44,1410879494000.0,-1.0,"Research integration options for Sqoop 'tasklet'","As a user, I'd like to have the ability to mass-ingest data from various database systems so that I'm not restricted with the current approach (_jdbchdfs_) that is dependent on JDBC drivers. 

*Spike Scope:*
* Identify integration options
* Collaborate to determine the design
* Document outcome (design specs)
",5.0,False,5.0,0,0.0,0,0,True
"XD-2123","Story","Sprint 36","2014-09-16T14:39:28.000+0000","sabby","","Provide kerberos support for HDFS sink","As a user, I'd like to have the option of _kerberized_ HDFS sink so that I can leverage Kerberos (open source distributed authentication system) for secured data writes into Hadoop.",3.0,0.9979295988696142,0.9979249163991728,0.1494417933337354,True,True,True,1410878368000.0,1416206376000.0,1416209976000.0,1416216530000.0,1416217430000.0,1410878368000.0,1416132224000.0,1411676247000.0,1411679847000.0,1416206351000.0,1416209951000.0,1416206376000.0,1416209976000.0,"sabby",1411679847000.0,1410878368000.0,44,1416132224000.0,1416132224000.0,"Provide kerberos support for HDFS sink","As a user, I'd like to have the option of _kerberized_ HDFS sink so that I can leverage Kerberos (open source distributed authentication system) for secured data writes into Hadoop.",8.0,True,8.0,1,-62.5,0,0,False
"XD-2122","Story","Sprint 35","2014-09-16T11:53:12.000+0000","sabby","","Add support to configure security by access control policies","As a user, I'd like to have the option to configure access control for all endpoints so that I can grant access by role, group, or setup privileges. 

Ideally, all the listed *endpoints* (http://localhost:9393/) needs to have the flexibility of access control policies. 

*Examples:*
* Who can create streams?
* Who can destroy the streams?
* Who can view the streams? ??(defaults to all)??",8.0,0.9126808619993484,0.9126808619993484,0.0539378981068172,True,True,True,1410868392000.0,1418198686000.0,1418202286000.0,1418899099000.0,1418899999000.0,1410868392000.0,-1.0,1411301600000.0,1411305200000.0,1418198686000.0,1418202286000.0,1418198686000.0,1418202286000.0,"sabby",1411305200000.0,1410868392000.0,44,1410868392000.0,-1.0,"Secure endpoints using either ROLE_VIEWER and ROLE_ADMIN","As a user, I'd like to have the option to configure default access control for endpoints so that I can grant access by _Admin_ or _Viewer_ roles.",8.0,False,8.0,0,0.0,0,0,True
"XD-2121","Story","Sprint 35","2014-09-16T11:41:45.000+0000","sabby","mbogoevici","Secure all endpoints using LDAP based security configurations","As a user, I'd like to have the option to provide LDAP based security configurations so that I can access the endpoints in a secured manner.

Ideally, all the listed *endpoints* (http://localhost:9393/) needs to be encapsulated within LDAP based security layer.

Reference:
[Authentication using LDAP|https://spring.io/guides/gs/authenticating-ldap/]",8.0,0.2299282592453005,0.0640723622875052,0.1147574369045021,True,True,True,1410867705000.0,1411737058000.0,1411740658000.0,1414647780000.0,1414648680000.0,1410867705000.0,1411372281000.0,1411301600000.0,1411305200000.0,1411109961000.0,1411113561000.0,1411737058000.0,1411740658000.0,"sabby",1411305200000.0,1410867705000.0,44,1411372281000.0,1411372281000.0,"Secure all endpoints using LDAP based security configurations","As a user, I'd like to have the option to provide LDAP based security configurations so that I can access the endpoints in a secured manner.

Ideally, all the listed *endpoints* (http://localhost:9393/) needs to be encapsulated within LDAP based security layer.

Reference:
[Authentication using LDAP|https://spring.io/guides/gs/authenticating-ldap/]",5.0,True,5.0,1,60.0,-1,0,False
"XD-2120","Story","Sprint 35","2014-09-16T11:34:07.000+0000","sabby","","Secure all endpoints using file based security configurations","As a user, I'd like to have the option to provide file based security configurations so that I can access the endpoints in a secured manner.

Ideally, all the listed *endpoints* (http://localhost:9393/) needs to be encapsulated within file based security layer.

Reference:
[Securing Web App|https://spring.io/guides/gs/securing-web/]",5.0,0.9949936636233108,0.9949936636233108,0.0626716207229457,True,True,True,1410867247000.0,1417763167000.0,1417766767000.0,1417796964000.0,1417797864000.0,1410867247000.0,-1.0,1411301600000.0,1411305200000.0,1417763167000.0,1417766767000.0,1417763167000.0,1417766767000.0,"sabby",1411305200000.0,1410867247000.0,44,1410867247000.0,-1.0,"Provide file based storage for users, groups (and roles)","As a user, I'd like to have the option to provide file based security configurations so that I can access the endpoints in a secured manner.

Ideally, all the listed *endpoints* (http://localhost:9393/) needs to be encapsulated within file based security layer.

Reference:
[Securing Web App|https://spring.io/guides/gs/securing-web/]",5.0,False,5.0,0,0.0,-1,2,True
"XD-2119","Story","Sprint 35","2014-09-16T11:25:06.000+0000","sabby","mbogoevici","Support accessing admin server endpoints over HTTPS","As a user, I'd like to have the option to enable HTTPS so that I can access XD's Admin server [endpoints|https://github.com/spring-projects/spring-xd/wiki/REST-API#xd-resources] over secured communication.

Technical Implementation:

This functionality is available in Spring Boot 1.2.0 M1 and has been backported into the 1.1.x branch to be released under Spring 1.1.7.  We can test against 1.1.7 SNAPSHOT.

Working through the way to update the build file to pick up a new version of boot is a bit tricky :(",1.0,0.8837480783239537,0.1899684488943325,0.4996863273180181,True,True,True,1410866706000.0,1411635862000.0,1411639462000.0,1411736140000.0,1411737040000.0,1410866706000.0,1411371893000.0,1411301600000.0,1411305200000.0,1411032042000.0,1411035642000.0,1411635862000.0,1411639462000.0,"sabby",1411305200000.0,1410869067000.0,44,1411371893000.0,1411371893000.0,"Support accessing admin server endpoints over HTTPS","As a user, I'd like to have the option to enable HTTPS so that I can access XD's Admin server [endpoints|https://github.com/spring-projects/spring-xd/wiki/REST-API#xd-resources] over secured communication.

Technical Implementation:

This functionality is available in Spring Boot 1.2.0 M1 and has been backported into the 1.1.x branch to be released under Spring 1.1.7.  We can test against 1.1.7 SNAPSHOT.

Working through the way to update the build file to pick up a new version of boot is a bit tricky :(",8.0,True,8.0,1,-87.5,-1,5,False
"XD-2118","Story","Sprint 36","2014-09-16T08:07:47.000+0000","dturanski","dturanski","Create a shell command processor","Create processor and sink modules that can execute a shell command using stdin and stdout to stream data.",5.0,0.1219732597177147,0.1219732597177147,0.831926694111875,True,True,True,1410854867000.0,1411107952000.0,1411111552000.0,1412928889000.0,1412929789000.0,1410854867000.0,1412584209000.0,1412581050000.0,1412584650000.0,1411107952000.0,1411111552000.0,1411107952000.0,1411111552000.0,"dturanski",1412584650000.0,1412584209000.0,44,1412584209000.0,-1.0,"Create a shell command processor and sink","Create processor and sink modules that can execute a shell command using stdin and stdout to stream data.",5.0,False,5.0,0,0.0,-1,1,True
"XD-2114","Bug","Sprint 36","2014-09-09T16:13:48.000+0000","leochu","","Job stuck in ""deploying"" state when no containers are available","A job gets stuck in ""deploying"" state when a job is deployed when there are no containers available.  When a container is started after this event, the job doesn't automatically start because of the job is stuck in the ""deploying"" state instead of the ""failed"" state.  

Refer to https://github.com/spring-projects/spring-xd/blob/193088dc164c73e07d7b4509de22241b28bf42b3/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/JobDeploymentListener.java

Update of the status in Zookeeper is inside the NoContainerException catch block.

This works correctly for streams.",5.0,0.848030105262781,0.516030461569864,0.285842881109816,True,True,True,1410279228000.0,1414068274000.0,1414071874000.0,1414746384000.0,1414747284000.0,1410279228000.0,1412584868000.0,1411556390000.0,1411559990000.0,1412584881000.0,1412588481000.0,1414068274000.0,1414071874000.0,"leochu",1411559990000.0,1410279228000.0,44,1413793839000.0,1412584868000.0,"Job stuck in ""deploying"" state when no containers are available","A job gets stuck in ""deploying"" state when a job is deployed when there are no containers available.  When a container is started after this event, the job doesn't automatically start because of the job is stuck in the ""deploying"" state instead of the ""failed"" state.  

Refer to https://github.com/spring-projects/spring-xd/blob/193088dc164c73e07d7b4509de22241b28bf42b3/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/JobDeploymentListener.java

Update of the status in Zookeeper is inside the NoContainerException catch block.

This works correctly for streams.",1.0,True,1.0,2,400.0,0,0,False
"XD-2113","Bug","Sprint 36","2014-09-05T08:32:20.000+0000","eric.bottard","","Redis aggregate-counter fails when end of interval is on the hour/day/month/etc","curl http://localhost:9393/metrics/aggregate-counters/smartgrid_h_28_load_predicted\?resolution\=minute\&from\=2013-09-01T12:55:00.000Z\&to\=2013-09-01T13:00:00.000Z 

would return 0 as the last bucket, while

curl http://localhost:9393/metrics/aggregate-counters/smartgrid_h_28_load_predicted\?resolution\=minute\&from\=2013-09-01T12:55:00.000Z\&to\=2013-09-01T13:05:00.000Z

would correctly return the value for 13:00:00",2.0,0.5883830311201046,0.4616833162131699,0.3538665947984291,True,True,True,1409905940000.0,1412650212000.0,1412653812000.0,1414569131000.0,1414570031000.0,1409905940000.0,1412584929000.0,1411556406000.0,1411560006000.0,1412059273000.0,1412062873000.0,1412650212000.0,1412653812000.0,"eric.bottard",1411560006000.0,1409905940000.0,44,1412584929000.0,1412584929000.0,"Redis aggregate-counter fails when end of interval is on the hour","curl http://localhost:9393/metrics/aggregate-counters/smartgrid_h_28_load_predicted\?resolution\=minute\&from\=2013-09-01T12:55:00.000Z\&to\=2013-09-01T13:00:00.000Z 

would return 0 as the last bucket, while

curl http://localhost:9393/metrics/aggregate-counters/smartgrid_h_28_load_predicted\?resolution\=minute\&from\=2013-09-01T12:55:00.000Z\&to\=2013-09-01T13:05:00.000Z

would correctly return the value for 13:00:00",3.0,True,3.0,1,-33.33333333333333,0,0,True
"XD-2112","Story","","2014-09-03T14:52:23.000+0000","grenfro","grenfro","User wants to select the ec2 zone when deploying XD","Currently the application allows AWS select which zone in the region to create an instance.",3.0,5.370768785270846e-06,0.0,-1.0,True,False,True,1409755943000.0,1409755955000.0,1409759555000.0,1411989360000.0,1411990260000.0,1409755943000.0,-1.0,-1.0,-1.0,1409755943000.0,1409759543000.0,1409755955000.0,1409759555000.0,"grenfro",-1.0,-1.0,0,1409755943000.0,-1.0,"User wants to select the ec2 zone when deploying XD","Currently the application allows AWS select which zone in the region to create an instance.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2110","Story","","2014-09-02T09:44:59.000+0000","grenfro","grenfro","XD-EC2 needs to provide ability to use a preinstalled zip vs downloading from s3","[Current Behavior]
Currently XD-EC2 downloads an XD zip file from the location specified by the xd-dist-url after verifying that the file is accessible..  
",3.0,7.731336071514858e-05,0.0,-1.0,True,False,True,1409651099000.0,1409651107000.0,1409654707000.0,1409753674000.0,1409754574000.0,1409651099000.0,-1.0,-1.0,-1.0,1409651099000.0,1409654699000.0,1409651107000.0,1409654707000.0,"grenfro",-1.0,-1.0,0,1409651099000.0,-1.0,"XD-EC2 needs to provide ability to use a preinstalled zip vs downloading from s3","[Current Behavior]
Currently XD-EC2 downloads an XD zip file from the location specified by the xd-dist-url after verifying that the file is accessible..  
",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2109","Bug","Sprint 36","2014-09-02T09:35:25.000+0000","thomas.risberg","","No way to set 'makeUnique' false when creating job in UI","The resulting definition starts with --makeUnique=true even if the MakeUnique checkbox is unchecked. I can check and uncheck the box and the --make unique parameter isn't included. Since the default for this parameter is true the end result is the same. There doesn't seem to be a way to set --makeUnique=false.",3.0,0.9233744172606364,0.9196893912647982,0.3138971486376973,True,True,True,1409650525000.0,1414758491000.0,1414762091000.0,1415181472000.0,1415182372000.0,1409650525000.0,-1.0,1411386956000.0,1411390556000.0,1414738106000.0,1414741706000.0,1414758491000.0,1414762091000.0,"thomas.risberg",1411390556000.0,1409650525000.0,44,1409650525000.0,-1.0,"No way to set 'makeUnique' false when creating job in UI","The resulting definition starts with --makeUnique=true even if the MakeUnique checkbox is unchecked. I can check and uncheck the box and the --make unique parameter isn't included. Since the default for this parameter is true the end result is the same. There doesn't seem to be a way to set --makeUnique=false.",3.0,False,3.0,0,0.0,0,0,False
"XD-2107","Story","Sprint 36","2014-08-28T08:00:13.000+0000","eric.bottard","eric.bottard","Allow aggregate-counter to increment by some value of the message","Currently, the aggregate counter only adds +1 to the individual values, even though support is there to add any increment.

This ticket is about surfacing a SpEL expression on the message to choose the increment",3.0,0.4185692758705718,0.0,0.2853825523378114,True,True,True,1409212813000.0,1412650219000.0,1412653819000.0,1417424188000.0,1417425088000.0,1409212813000.0,-1.0,1411556453000.0,1411560053000.0,1409212813000.0,1409216413000.0,1412650219000.0,1412653819000.0,"eric.bottard",1411560053000.0,1409212813000.0,44,1409212813000.0,-1.0,"Allow aggregate-counter to increment by some value of the message","Currently, the aggregate counter only adds +1 to the individual values, even though support is there to add any increment.

This ticket is about surfacing a SpEL expression on the message to choose the increment",3.0,False,3.0,0,0.0,0,0,False
"XD-2105","Story","Sprint 34","2014-08-26T04:00:59.000+0000","eric.bottard","","MessageBus should see custom SpEL property accessors","Currently, custom SI property accessors are registered by a plugin (org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin) and are not visible by the bus.

I believe they should be.

This may just be a matter of moving them around.",5.0,0.8561773796348675,0.2577311306214471,0.0526299998596649,True,True,True,1409025659000.0,1410794935000.0,1410798535000.0,1411091242000.0,1411092142000.0,1409025659000.0,1409134567000.0,1409134418000.0,1409138018000.0,1409558256000.0,1409561856000.0,1410794935000.0,1410798535000.0,"eric.bottard",1409138018000.0,1409134567000.0,44,1409134567000.0,-1.0,"MessageBus should see custom SpEL property accessors","Currently, custom SI property accessors are registered by a plugin (org.springframework.xd.dirt.plugins.SpelPropertyAccessorPlugin) and are not visible by the bus.

I believe they should be.

This may just be a matter of moving them around.",5.0,False,5.0,0,0.0,0,0,False
"XD-2103","Story","Sprint 38","2014-08-25T17:32:10.000+0000","iperumal","iperumal","Add Kafka sink","As a user, I'd like to have the option to write into _Kafka_ sink so that I can publish mass data into Kafka broker.",8.0,0.933182025536084,0.0,0.8095865643598209,True,True,True,1408987930000.0,1415358075000.0,1415361675000.0,1415813292000.0,1415814192000.0,1408987930000.0,-1.0,1414514380000.0,1414517980000.0,1408987930000.0,1408991530000.0,1415358075000.0,1415361675000.0,"iperumal",1414517980000.0,1408987930000.0,44,1408987930000.0,-1.0,"Add Kafka sink","As a user, I'd like to have the option to write into _Kafka_ sink so that I can publish mass data into Kafka broker.",8.0,False,8.0,0,0.0,0,2,False
"XD-2098","Story","Sprint 37","2014-08-25T02:02:39.000+0000","eric.bottard","eric.bottard","Implement KafkaMessageBus","Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning.

Implement KafkaMessageBus and supporting classes and UT/IT.",3.0,0.6408957670585052,0.0,0.5896269533387861,True,True,True,1408932159000.0,1412650178000.0,1412653778000.0,1414732544000.0,1414733444000.0,1408932159000.0,1412585098000.0,1412352753000.0,1412356353000.0,1408932159000.0,1408935759000.0,1412650178000.0,1412653778000.0,"eric.bottard",1412356353000.0,1408932159000.0,44,1412585098000.0,1412585098000.0,"Implement KafkaMessageBus","Kafka lends itself well as a message bus, and kafka partitioning maps to MB partitioning.

Implement KafkaMessageBus and supporting classes and UT/IT.",12.0,True,12.0,1,-75.0,-1,0,False
"XD-2097","Story","Sprint 34","2014-08-22T16:03:46.000+0000","iperumal","iperumal","Enable shutdown containers from admin server","When the container starts up, it has a random http port for management configurations. If the container has management port enabled, then we can store it as container attribute. If the admin can reach out to the container on that port, then we can provide an option to shutdown container from the admin server.",5.0,0.1580125175462782,0.0,0.1536912161901409,True,True,True,1408723426000.0,1409063270000.0,1409066870000.0,1410873267000.0,1410874167000.0,1408723426000.0,1409131505000.0,1409053976000.0,1409057576000.0,1408723426000.0,1408727026000.0,1409063270000.0,1409066870000.0,"iperumal",1409057576000.0,1408723426000.0,44,1409131505000.0,1409131505000.0,"Enable shutdown containers from admin server","When the container starts up, it has a random http port for management configurations. If the container has management port enabled, then we can store it as container attribute. If the admin can reach out to the container on that port, then we can provide an option to shutdown container from the admin server.",4.0,True,4.0,1,25.0,0,0,False
"XD-2096","Story","Sprint 34","2014-08-22T16:01:12.000+0000","iperumal","iperumal","UI: Visual representation of Stream/Job with deployed modules","For a given stream/job, we need a visual representation of the stream/job with any deployed modules.",5.0,0.9668346364242428,0.0,0.0619474414068024,True,True,True,1408723272000.0,1413884681000.0,1413888281000.0,1414060833000.0,1414061733000.0,1408723272000.0,-1.0,1409053976000.0,1409057576000.0,1408723272000.0,1408726872000.0,1413884681000.0,1413888281000.0,"iperumal",1409057576000.0,1408723272000.0,44,1408723272000.0,-1.0,"UI: Visual representation of Stream/Job with deployed modules","For a given stream/job, we need a visual representation of the stream/job with any deployed modules.",5.0,False,5.0,0,0.0,0,0,False
"XD-2095","Story","Sprint 34","2014-08-22T15:58:58.000+0000","iperumal","iperumal","UI: Ability to deploy stream with deployment properties","Admin UI currently allows job to be deployed with deployment properties, we need similar way to deploy stream with the deployment properties (module count, container matching criteria).",2.0,0.0974554580958317,2.6347247691228324e-06,0.1245238675952942,True,True,True,1408723138000.0,1408982060000.0,1408985660000.0,1411379062000.0,1411379962000.0,1408723138000.0,-1.0,1409053976000.0,1409057576000.0,1408723145000.0,1408726745000.0,1408982060000.0,1408985660000.0,"iperumal",1409057576000.0,1408723138000.0,44,1408723138000.0,-1.0,"UI: Ability to deploy stream with deployment properties","Admin UI currently allows job to be deployed with deployment properties, we need similar way to deploy stream with the deployment properties (module count, container matching criteria).",2.0,False,2.0,0,0.0,0,0,False
"XD-2094","Story","Sprint 34","2014-08-22T15:57:37.000+0000","iperumal","iperumal","UI: Cluster view of a container","We need a visual representation of the XD cluster with runtime container and deployed modules.",5.0,0.8934733423933314,0.0,0.1379082573089471,True,True,True,1408723057000.0,1410866999000.0,1410870599000.0,1411121716000.0,1411122616000.0,1408723057000.0,-1.0,1409053976000.0,1409057576000.0,1408723057000.0,1408726657000.0,1410866999000.0,1410870599000.0,"iperumal",1409057576000.0,1408723057000.0,44,1408723057000.0,-1.0,"UI: Cluster view of a container","We need a visual representation of the XD cluster with runtime container and deployed modules.",5.0,False,5.0,0,0.0,0,0,False
"XD-2093","Story","Sprint 34","2014-08-22T15:55:18.000+0000","iperumal","iperumal","List Streams/Jobs based with deployed modules","Currently, there is a ""stream list""/""job list"" which shows the status of a given stream/job along with the DSL. and, there is ""runtime modules"" which shows all the deployed modules with their container info.

We need a better REST endpoint that gives all the deployed modules for a given stream/job along with the status.",3.0,0.1566391410981344,0.0,0.1538937120097879,True,True,True,1408722918000.0,1409059882000.0,1409063482000.0,1410873230000.0,1410874130000.0,1408722918000.0,1409060754000.0,1409053976000.0,1409057576000.0,1408722918000.0,1408726518000.0,1409059882000.0,1409063482000.0,"iperumal",1409057576000.0,1408722918000.0,44,1409131435000.0,1409060754000.0,"List Streams/Jobs based with deployed modules","Currently, there is a ""stream list""/""job list"" which shows the status of a given stream/job along with the DSL. and, there is ""runtime modules"" which shows all the deployed modules with their container info.

We need a better REST endpoint that gives all the deployed modules for a given stream/job along with the status.",2.0,True,2.0,3,50.0,0,0,False
"XD-2092","Improvement","Sprint 34","2014-08-22T15:49:41.000+0000","iperumal","iperumal","Enhance Container domain object","Currently, org.springframework.xd.dirt.cluster.Container has name and attributes (container attributes). This can be enhanced to include all the deployed modules, number of deployed modules and any more useful info. and can subsequently be used to get a detailed runtime container info.",2.0,0.118947614508296,0.0,0.1540248070129287,True,True,True,1408722581000.0,1408978505000.0,1408982105000.0,1410873250000.0,1410874150000.0,1408722581000.0,-1.0,1409053976000.0,1409057576000.0,1408722581000.0,1408726181000.0,1408978505000.0,1408982105000.0,"iperumal",1409057576000.0,1408722581000.0,44,1408722581000.0,-1.0,"Enhance Container domain object","Currently, org.springframework.xd.dirt.cluster.Container has name and attributes (container attributes). This can be enhanced to include all the deployed modules, number of deployed modules and any more useful info. and can subsequently be used to get a detailed runtime container info.",2.0,False,2.0,0,0.0,0,0,False
"XD-2091","Story","Sprint 34","2014-08-22T15:11:35.000+0000","sabby","Patrick Peralta","Research approach to bootstrap custom modules","Design Spike: Investigate various approaches to bootstrap custom modules. Can Spring Boot be leveraged? Starter POM?",5.0,0.995617305236866,0.9554116144906196,0.1929601946000354,True,True,True,1408720295000.0,1410856830000.0,1410860430000.0,1410865335000.0,1410866235000.0,1408720295000.0,-1.0,1409134376000.0,1409137976000.0,1410770551000.0,1410774151000.0,1410856830000.0,1410860430000.0,"sabby",1409137976000.0,1409132226000.0,44,1409132226000.0,-1.0,"Research approach to bootstrap custom modules","Design Spike: Investigate various approaches to bootstrap custom modules. Can Spring Boot be leveraged? Starter POM?",5.0,False,5.0,0,0.0,0,0,False
"XD-2090","Story","Sprint 37","2014-08-22T15:08:45.000+0000","sabby","","Custom module packaging strategy","As a user, I'd like to have guidance to create custom modules so that I can align the development practices with recommended approach. ",1.0,0.7793559385418654,0.7793521073879848,0.5863651457933251,True,True,True,1408720125000.0,1415026328000.0,1415029928000.0,1416810782000.0,1416811682000.0,1408720125000.0,1413466814000.0,1413464732000.0,1413468332000.0,1415026297000.0,1415029897000.0,1415026328000.0,1415029928000.0,"sabby",1413468332000.0,1413466824000.0,44,1416471588000.0,1415003502000.0,"Custom module packaging strategy","As a user, I'd like to have guidance to create custom modules so that I can align the development practices with recommended approach. 

11/20: Update: Scope of this task is to create an example to demonstrate and document the capability.",5.0,True,5.0,2,-80.0,0,2,True
"XD-2085","Story","Sprint 34","2014-08-22T08:11:40.000+0000","grenfro","","Test Recommended XD Cluster Strategy on slow/bad network","h1. Run Acceptance tests on the following  deployments.  

h2. Slow Network
Simulate slow network by deploying a XD cluster where the ZK Ensemble is only available via WAN.  

h2. Network packet loss
Simulate cases where a network packets can be lost.  
",5.0,0.999847267344433,0.2859754978388826,0.1189154589964172,True,True,True,1408695100000.0,1411712985000.0,1411713446000.0,1411712546000.0,1411713446000.0,1408695100000.0,1409131103000.0,1409054028000.0,1409057628000.0,1409558273000.0,1409561873000.0,1411712985000.0,1411713446000.0,"grenfro",1409057628000.0,1408695100000.0,44,1409131103000.0,1409131103000.0,"Test Recommended XD Cluster Strategy on slow/bad network","h1. Run Acceptance tests on the following  deployments.  

h2. Slow Network
Simulate slow network by deploying a XD cluster where the ZK Ensemble is only available via WAN.  

h2. Network packet loss
Simulate cases where a network packets can be lost.  
",12.0,True,12.0,1,-58.333333333333336,0,1,False
"XD-2084","Story","","2014-08-21T08:06:18.000+0000","grenfro","grenfro","Spring XD EC2 needs to setup cluster that uses static resources.",".h1 Summary 
User wants the ability to deploy an ec2 cluster where the admin & containers use a pre existing ZK ensemble, Rabbit and redis instance that are deployed on different machines. 

.h2 Current functionality
Currently spring-xd-ec2 sets up its containers & admin server to use a ZK, rabbit and redis that are provisioned and collocated with the admin server.  

.h2 Detail
The following properties will be added to the spring-xd-ec2.properties
# *spring_zk_client_connect* - contains a comma delimited list of zk hosts:ports for a ensemble.  The application will check to see if the port is open on at least of the servers in the list, if not deployment will fail.  Default is adminServer_host:2181.
# *spring_rabbitmq_addresses* - contains a comma delimited list of hosts:ports for a rabbit cluster.  The application will check to see if the port is open on at least of the servers in the list, if not deployment will fail.  Default is adminServer_host:5672.
# *spring_redis* - contains a host:port for a redis instance. The application will check to see if the port is open , if not deployment will fail.  Default is adminServer_host:5672.
# *ec2_zone* - user can specify the zone to which the containers and admin will be deployed.  If not present AWS will decide which zone to deploy the cluster.",8.0,0.9999491415321808,0.0,-1.0,True,False,True,1408608378000.0,1409040929000.0,1409040951000.0,1409040051000.0,1409040951000.0,1408608378000.0,-1.0,-1.0,-1.0,1408608378000.0,1408611978000.0,1409040929000.0,1409040951000.0,"grenfro",-1.0,-1.0,0,1408608378000.0,-1.0,"Spring XD EC2 needs to setup cluster that uses static resources.","h1. Summary 
User wants the ability to deploy an ec2 cluster where the admin & containers use a pre existing ZK ensemble, Rabbit and redis instance that are deployed on different machines. 

h2. Current functionality
Currently spring-xd-ec2 sets up its containers & admin server to use a ZK, rabbit and redis that are provisioned and collocated with the admin server.  

h2. Detail
The following properties will be added to the spring-xd-ec2.properties
# *spring_zk_client_connect* - contains a comma delimited list of zk hosts:ports for a ensemble.  The application will check to see if the port is open on at least of the servers in the list, if not deployment will fail.  Default is adminServer_host:2181.
# *spring_rabbitmq_addresses* - contains a comma delimited list of hosts:ports for a rabbit cluster.  The application will check to see if the port is open on at least of the servers in the list, if not deployment will fail.  Default is adminServer_host:5672.
# *spring_redis* - contains a host:port for a redis instance. The application will check to see if the port is open , if not deployment will fail.  Default is adminServer_host:5672.
# *ec2_zone* - user can specify the zone to which the containers and admin will be deployed.  If not present AWS will decide which zone to deploy the cluster.",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-2080","Bug","","2014-08-18T11:29:40.000+0000","grenfro","","Modules do not redeploy properly when Zookeeper node is lost.","* SHA baddfc24b08286a78392d5f565742c9bab5adfea
* EC2 Environment
** Look at Zookeeper Ec2 Deployment Test Topology.png for a view of the topology

h2. The test scenario 
# Bring up a up a 5 container 2 admin XD Cluster up using 3 ZK Server ensemble.
# Create ticktock  stream ""time|log""
# Deploy with --properties ""module.log.count=5""
# Kill one of the ZK Servers in the ensemble

h2. Observed Behavior. 
# In this particular scenario 3 containers were affected by killing (sudo kill <pid>) Zookeeper 2
# 2 Containers did not come back online even though they did show up in the runtime containers
 
h2. Timeline
# 14:08:21 deployed stream
# 14:09:10 kill server in ZK Ensemble
# After waiting a few seconds ran runtime Modules (*Note:* log2 is undeployed and log5 is then deployed)  :

xd:>runtime modules
  Module             Container Id                          Options                                     Deployment Properties
  -----------------  ------------------------------------  ------------------------------------------  ---------------------
  foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}
  foo.sink.log.2     9a3a1846-bac4-4504-81fd-151665d851dc  {name=foo, expression=payload, level=INFO}  {count=5, sequence=2}
  foo.source.time.1  9a3a1846-bac4-4504-81fd-151665d851dc  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}

xd:>runtime modules
  Module             Container Id                          Options                                     Deployment Properties
  -----------------  ------------------------------------  ------------------------------------------  ---------------------
  foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}
  foo.sink.log.5     5c454a39-fc4c-4bd3-b828-08cd837dc4ba  {name=foo, expression=payload, level=INFO}  {count=5, sequence=5}
  foo.source.time.1  9a3a1846-bac4-4504-81fd-151665d851dc  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}

xd:>runtime modules
  Module             Container Id                          Options                                     Deployment Properties
  -----------------  ------------------------------------  ------------------------------------------  ---------------------
  foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}
  foo.sink.log.5     5c454a39-fc4c-4bd3-b828-08cd837dc4ba  {name=foo, expression=payload, level=INFO}  {count=5, sequence=5}
  foo.source.time.1  9a3a1846-bac4-4504-81fd-151665d851dc  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}

xd:>runtime containers
  Container Id                          Host              IP Address     PID   Groups  Custom Attributes
  ------------------------------------  ----------------  -------------  ----  ------  -----------------
  0ba5e6ce-aedf-429c-b846-1cd4e32836c7  ip-10-2-209-174   10.2.209.174   1045
  5c454a39-fc4c-4bd3-b828-08cd837dc4ba  ip-10-70-9-57     10.70.9.57     1099
  707a968b-15a5-451f-9034-1e7f05cdcf97  ip-10-70-11-185   10.70.11.185   1055
  98a32c62-302a-484b-af9c-d670f2a3cfc2  ip-10-110-186-48  10.110.186.48  1056  GROUPA
  9a3a1846-bac4-4504-81fd-151665d851dc  ip-10-70-9-153    10.70.9.153    1020  GROUP0

h2. Undeploy and redeploy stream 
# 14:16:42 Undeploy and redploy with module.log.count=5
xd:>runtime modules
  Module             Container Id                          Options                                     Deployment Properties
  -----------------  ------------------------------------  ------------------------------------------  ---------------------
  foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}
  foo.sink.log.2     5c454a39-fc4c-4bd3-b828-08cd837dc4ba  {name=foo, expression=payload, level=INFO}  {count=5, sequence=2}
  foo.sink.log.5     9a3a1846-bac4-4504-81fd-151665d851dc  {name=foo, expression=payload, level=INFO}  {count=5, sequence=5}
  foo.source.time.1  98a32c62-302a-484b-af9c-d670f2a3cfc2  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}

xd:>runtime containers
  Container Id                          Host              IP Address     PID   Groups  Custom Attributes
  ------------------------------------  ----------------  -------------  ----  ------  -----------------
  0ba5e6ce-aedf-429c-b846-1cd4e32836c7  ip-10-2-209-174   10.2.209.174   1045
  5c454a39-fc4c-4bd3-b828-08cd837dc4ba  ip-10-70-9-57     10.70.9.57     1099
  707a968b-15a5-451f-9034-1e7f05cdcf97  ip-10-70-11-185   10.70.11.185   1055
  98a32c62-302a-484b-af9c-d670f2a3cfc2  ip-10-110-186-48  10.110.186.48  1056  GROUPA
  9a3a1846-bac4-4504-81fd-151665d851dc  ip-10-70-9-153    10.70.9.153    1020  GROUP0
  
# 14:21:06 undeploy foo

",5.0,-1.0,-1.0,-1.0,False,False,False,1408361380000.0,-1.0,-1.0,1408693604000.0,1408694504000.0,1408361380000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1408361380000.0,-1.0,"Modules do not redeploy properly when Zookeeper node is lost.","* SHA baddfc24b08286a78392d5f565742c9bab5adfea
* EC2 Environment
** Look at Zookeeper Ec2 Deployment Test Topology.png for a view of the topology

h2. The test scenario 
# Bring up a up a 5 container 2 admin XD Cluster up using 3 ZK Server ensemble.
# Create ticktock  stream ""time|log""
# Deploy with --properties ""module.log.count=5""
# Kill one of the ZK Servers in the ensemble

h2. Observed Behavior. 
# In this particular scenario 3 containers were affected by killing (sudo kill <pid>) Zookeeper 2
# 2 Containers did not come back online even though they did show up in the runtime containers
 
h2. Timeline
# 14:08:21 deployed stream
# 14:09:10 kill server in ZK Ensemble
# After waiting a few seconds ran runtime Modules (*Note:* log2 is undeployed and log5 is then deployed)  :

xd:>runtime modules
  Module             Container Id                          Options                                     Deployment Properties
  -----------------  ------------------------------------  ------------------------------------------  ---------------------
  foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}
  foo.sink.log.2     9a3a1846-bac4-4504-81fd-151665d851dc  {name=foo, expression=payload, level=INFO}  {count=5, sequence=2}
  foo.source.time.1  9a3a1846-bac4-4504-81fd-151665d851dc  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}

xd:>runtime modules
  Module             Container Id                          Options                                     Deployment Properties
  -----------------  ------------------------------------  ------------------------------------------  ---------------------
  foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}
  foo.sink.log.5     5c454a39-fc4c-4bd3-b828-08cd837dc4ba  {name=foo, expression=payload, level=INFO}  {count=5, sequence=5}
  foo.source.time.1  9a3a1846-bac4-4504-81fd-151665d851dc  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}

xd:>runtime modules
  Module             Container Id                          Options                                     Deployment Properties
  -----------------  ------------------------------------  ------------------------------------------  ---------------------
  foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}
  foo.sink.log.5     5c454a39-fc4c-4bd3-b828-08cd837dc4ba  {name=foo, expression=payload, level=INFO}  {count=5, sequence=5}
  foo.source.time.1  9a3a1846-bac4-4504-81fd-151665d851dc  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}

xd:>runtime containers
  Container Id                          Host              IP Address     PID   Groups  Custom Attributes
  ------------------------------------  ----------------  -------------  ----  ------  -----------------
  0ba5e6ce-aedf-429c-b846-1cd4e32836c7  ip-10-2-209-174   10.2.209.174   1045
  5c454a39-fc4c-4bd3-b828-08cd837dc4ba  ip-10-70-9-57     10.70.9.57     1099
  707a968b-15a5-451f-9034-1e7f05cdcf97  ip-10-70-11-185   10.70.11.185   1055
  98a32c62-302a-484b-af9c-d670f2a3cfc2  ip-10-110-186-48  10.110.186.48  1056  GROUPA
  9a3a1846-bac4-4504-81fd-151665d851dc  ip-10-70-9-153    10.70.9.153    1020  GROUP0


h2.  Undeploy and redeploy stream 

# 14:16:42 Undeploy and redploy with module.log.count=5
xd:>runtime modules
  Module             Container Id                          Options                                     Deployment Properties
  -----------------  ------------------------------------  ------------------------------------------  ---------------------
  foo.sink.log.1     98a32c62-302a-484b-af9c-d670f2a3cfc2  {name=foo, expression=payload, level=INFO}  {count=5, sequence=1}
  foo.sink.log.2     5c454a39-fc4c-4bd3-b828-08cd837dc4ba  {name=foo, expression=payload, level=INFO}  {count=5, sequence=2}
  foo.sink.log.5     9a3a1846-bac4-4504-81fd-151665d851dc  {name=foo, expression=payload, level=INFO}  {count=5, sequence=5}
  foo.source.time.1  98a32c62-302a-484b-af9c-d670f2a3cfc2  {fixedDelay=1, format=yyyy-MM-dd HH:mm:ss}  {count=1, sequence=1}

xd:>runtime containers
  Container Id                          Host              IP Address     PID   Groups  Custom Attributes
  ------------------------------------  ----------------  -------------  ----  ------  -----------------
  0ba5e6ce-aedf-429c-b846-1cd4e32836c7  ip-10-2-209-174   10.2.209.174   1045
  5c454a39-fc4c-4bd3-b828-08cd837dc4ba  ip-10-70-9-57     10.70.9.57     1099
  707a968b-15a5-451f-9034-1e7f05cdcf97  ip-10-70-11-185   10.70.11.185   1055
  98a32c62-302a-484b-af9c-d670f2a3cfc2  ip-10-110-186-48  10.110.186.48  1056  GROUPA
  9a3a1846-bac4-4504-81fd-151665d851dc  ip-10-70-9-153    10.70.9.153    1020  GROUP0
  
# 14:21:06 undeploy foo

",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-2079","Story","Sprint 38","2014-08-17T13:59:35.000+0000","grussell","","Add a Retry/Dead Letter Interceptor to the RabbitMQ Source","Provide for retry and/or dead-lettering for the rabbit source (similar to the rabbit message bus).",2.0,0.6351433050669147,0.985167566533811,0.5847640360204771,True,True,True,1408283975000.0,1415244997000.0,1415248597000.0,1419242841000.0,1419243741000.0,1408283975000.0,-1.0,1414692852000.0,1414696452000.0,1419081181000.0,1419084781000.0,1415244997000.0,1415248597000.0,"grussell",1414696452000.0,1408283975000.0,44,1408283975000.0,-1.0,"Add a Retry/Dead Letter Interceptor to the RabbitMQ Source","Provide for retry and/or dead-lettering for the rabbit source (similar to the rabbit message bus).",2.0,False,2.0,0,0.0,0,2,False
"XD-2075","Bug","Sprint 41","2014-08-10T10:00:42.000+0000","grussell","","Add --binary Option to MQTT Source","See http://stackoverflow.com/questions/25226527/mqtt-source-spring-xd/25227531#25227531",1.0,0.9743169727058516,0.9743169727058516,0.9727423849448464,True,True,True,1407664842000.0,1420466078000.0,1420469678000.0,1420802619000.0,1420803519000.0,1407664842000.0,-1.0,1420445390000.0,1420448990000.0,1420466078000.0,1420469678000.0,1420466078000.0,1420469678000.0,"grussell",1420448990000.0,1407664842000.0,44,1407664842000.0,-1.0,"Add --binary Option to MQTT Source","See http://stackoverflow.com/questions/25226527/mqtt-source-spring-xd/25227531#25227531",1.0,False,1.0,0,0.0,0,0,False
"XD-2072","Improvement","Sprint 36","2014-08-06T10:41:34.000+0000","rmeneses","","xd-shell should provide a command line argument ""--adminUri"" to specify the location of the admin node","xd-shell by default will try to connect to the admin node in the same host. You need to run ""admin config server"" to specify the URI where the admin node is - every time you run xd-shell. 

xd-shell should have a command line argument ""--adminUri"" so that the user can invoke the xd-shell passing the Admin Node Uri as an argument when running the shell and then be able to use xd-shell in interactive mode without having to run ""admin config server"" every time. 



",2.0,-1.0,-1.0,0.2224859604529598,False,True,False,1407321694000.0,-1.0,-1.0,1426355240000.0,1426356140000.0,1407321694000.0,-1.0,1411556591000.0,1411560191000.0,-1.0,-1.0,-1.0,-1.0,"rmeneses",1411560191000.0,1407321694000.0,44,1407321694000.0,-1.0,"xd-shell should provide a command line argument ""--adminUri"" to specify the location of the admin node","xd-shell by default will try to connect to the admin node in the same host. You need to run ""admin config server"" to specify the URI where the admin node is - every time you run xd-shell. 

xd-shell should have a command line argument ""--adminUri"" so that the user can invoke the xd-shell passing the Admin Node Uri as an argument when running the shell and then be able to use xd-shell in interactive mode without having to run ""admin config server"" every time. 



",2.0,False,2.0,0,0.0,0,0,False
"XD-2071","Improvement","Sprint 33","2014-08-06T10:13:36.000+0000","dturanski","dturanski","Modularize gradle build","split out build.gradle into multiple files.",2.0,0.0012993794552321,0.0003400245303411,0.0004736055958322,True,True,True,1407320016000.0,1407320123000.0,1407323723000.0,1407401463000.0,1407402363000.0,1407320016000.0,-1.0,1407320055000.0,1407323655000.0,1407320044000.0,1407323644000.0,1407320123000.0,1407323723000.0,"dturanski",1407323655000.0,1407320016000.0,44,1407320016000.0,-1.0,"Modularize gradle build","split out build.gradle into multiple files.",2.0,False,2.0,0,0.0,0,0,False
"XD-2069","Story","Sprint 36","2014-08-06T06:27:54.000+0000","mark.pollack","","Provide a way to debug the http source","The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned.

I updated the log4j config for org.jboss.netty but it had no effect.  I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory()).

",5.0,0.970204880435646,0.970204880435646,0.6904500085206426,True,True,True,1407306474000.0,1413278705000.0,1413282305000.0,1413461213000.0,1413462113000.0,1407306474000.0,1412584086000.0,1411556635000.0,1411560235000.0,1413278705000.0,1413282305000.0,1413278705000.0,1413282305000.0,"mark.pollack",1411560235000.0,1407306474000.0,44,1412584100000.0,1412584086000.0,"Provide a way to debug the http source","The http source does not provide debug logging to see information such as http headers and requests, in particular if a non OK response is returned.

I updated the log4j config for org.jboss.netty but it had no effect.  I suspect this is due to the need to configure the netty logging system via InternalLoggerFactory.setDefaultFactory(new Log4JLoggerFactory()).

",4.0,True,4.0,2,25.0,0,0,False
"XD-2067","Improvement","Sprint 33","2014-08-06T01:33:45.000+0000","eric.bottard","","Upgrade asciidoctor toolchain","This will in turn allow us to get rid of the custom logic for handling crossref links between documents",3.0,0.9991222117537284,0.2381790173012896,0.088872462442175,True,True,True,1407288825000.0,1408608030000.0,1408609189000.0,1408608289000.0,1408609189000.0,1407288825000.0,-1.0,1407406169000.0,1407409769000.0,1407603308000.0,1407606908000.0,1408608030000.0,1408609189000.0,"eric.bottard",1407409769000.0,1407288825000.0,44,1407288825000.0,-1.0,"Upgrade asciidoctor toolchain","This will in turn allow us to get rid of the custom logic for handling crossref links between documents",3.0,False,3.0,0,0.0,0,0,False
"XD-2066","Story","","2014-08-05T08:47:07.000+0000","grenfro","grenfro","Tests sporadically fail when checking send counts with rabbit as transport","Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail.  This is because, sometimes it takes 2 or more sends to get the data transmitted between modules.  With the current test structure this is considered a failure.  Is this the correct behavior?",5.0,-1.0,0.0,-1.0,False,False,True,1407228427000.0,-1.0,-1.0,1412008837000.0,1412009737000.0,1407228427000.0,-1.0,-1.0,-1.0,1407228427000.0,1407232027000.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1407228427000.0,-1.0,"Tests sporadically fail when checking send counts with rabbit as transport","Tests that use verifySendCounts to validate whether data was sent to all the modules in a stream occasionally fail.  This is because, sometimes it takes 2 or more sends to get the data transmitted between modules.  With the current test structure this is considered a failure.  Is this the correct behavior?",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2063","Story","Sprint 33","2014-08-04T17:25:39.000+0000","sabby","","Measure DIRT performance with linear payload variations","Use a baseline DIRT infrastructure to measure throughput, HA and scalability for various payload sizes.

Depends on testing infrastructure setup, configuration and availability.",8.0,0.0985128555638236,0.0985128555638236,0.0365481745026868,True,True,True,1407173139000.0,1407322603000.0,1407326203000.0,1408689442000.0,1408690342000.0,1407173139000.0,-1.0,1407228590000.0,1407232190000.0,1407322603000.0,1407326203000.0,1407322603000.0,1407326203000.0,"sabby",1407232190000.0,1407173139000.0,44,1407173139000.0,-1.0,"Investigate setting up performance test environment on cloud providers","Use a baseline DIRT infrastructure to measure throughput, HA and scalability for various payload sizes.

Depends on testing infrastructure setup, configuration and availability.",8.0,False,8.0,0,0.0,0,0,True
"XD-2061","Story","Sprint 33","2014-08-04T17:09:04.000+0000","sabby","","Add Redis sink","As an user, I'd like to have the ability to ingest data into Redis sink.",3.0,0.6100470489676367,0.6100465783210782,0.0088553718813782,True,True,True,1407172144000.0,1411060712000.0,1411064312000.0,1413545454000.0,1413546354000.0,1407172144000.0,1411060724000.0,1407228590000.0,1407232190000.0,1411060709000.0,1411064309000.0,1411060712000.0,1411064312000.0,"sabby",1407232190000.0,1407172144000.0,44,1412583981000.0,1411060724000.0,"Add Redis sink","As an user, I'd like to have the ability to ingest data into _Redis_ sink.",8.0,True,8.0,2,-62.5,0,0,True
"XD-2060","Story","Sprint 33","2014-08-04T17:06:46.000+0000","sabby","","Add JDBC source","As an user, I'd like to have a native JDBC source module to ingest data directly from various databases. ",3.0,0.8616776920900235,0.7824522510848863,0.0089004006084706,True,True,True,1407172006000.0,1412650190000.0,1412653790000.0,1413528685000.0,1413529585000.0,1407172006000.0,1412351483000.0,1407228591000.0,1407232191000.0,1412146508000.0,1412150108000.0,1412650190000.0,1412653790000.0,"sabby",1407232191000.0,1407172006000.0,44,1412351483000.0,1412351483000.0,"Add JDBC source","As an user, I'd like to have a native _JDBC_ source module to ingest data directly from various databases. ",8.0,True,8.0,1,-62.5,0,0,True
"XD-2058","Story","Sprint 33","2014-08-04T16:42:01.000+0000","sabby","","Investigate long running tests","The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ",4.0,0.3261995560033371,0.1345338756751607,0.033527810415157,True,True,True,1407170521000.0,1407735497000.0,1407739097000.0,1408901616000.0,1408902516000.0,1407170521000.0,-1.0,1407228591000.0,1407232191000.0,1407403533000.0,1407407133000.0,1407735497000.0,1407739097000.0,"sabby",1407232191000.0,1407170521000.0,44,1407170521000.0,-1.0,"Investigate long running tests","The goal is to optimize the build process and at the same time validate the feature capabilities as quickly as we can. Investigate the long running tests. Look for long timeout window declarations. ",4.0,False,4.0,0,0.0,0,0,False
"XD-2049","Improvement","","2014-08-04T06:22:52.000+0000","dturanski","","Singlenode fails to start from external module","Singlenode fails to start using spring-xd-dirt-1.0.0.RELEASE.jar see https://github.com/dturanski/xd-test

The root cause of the error:

java.lang.NoSuchMethodError: javax.servlet.ServletContext.addServlet(Ljava/lang/String;Ljavax/servlet/Servlet;)Ljavax/servlet/ServletRegistration$Dynamic;

Indicating an incompatible servlet version is being pulled in by default.",4.0,-1.0,-1.0,-1.0,False,False,False,1407133372000.0,-1.0,-1.0,1436364917000.0,1436365817000.0,1407133372000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"dturanski",-1.0,-1.0,0,1407133372000.0,-1.0,"Singlenode fails to start from external module","Singlenode fails to start using spring-xd-dirt-1.0.0.RELEASE.jar see https://github.com/dturanski/xd-test

The root cause of the error:

java.lang.NoSuchMethodError: javax.servlet.ServletContext.addServlet(Ljava/lang/String;Ljavax/servlet/Servlet;)Ljavax/servlet/ServletRegistration$Dynamic;

Indicating an incompatible servlet version is being pulled in by default.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2048","Improvement","","2014-08-02T03:00:27.000+0000","zouweixing","","Do you have plan to support Spark?","Do you have plans to support Spark?
In version 1.0 GA, Spring XD has supported Hadoop, But it has not supported the brand new big data calculation platform Spark. do you have plans to support Spark in the future?",20.0,-1.0,-1.0,-1.0,False,False,False,1406948427000.0,-1.0,-1.0,1425728504000.0,1425729404000.0,1406948427000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"zouweixing",-1.0,-1.0,0,1406948427000.0,-1.0,"Do you have plan to support Spark?","Do you have plans to support Spark?
In version 1.0 GA, Spring XD has supported Hadoop, But it has not supported the brand new big data calculation platform Spark. do you have plans to support Spark in the future?",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-2047","Improvement","Sprint 33","2014-08-01T07:22:35.000+0000","grussell","grussell","Easier Customization of Headers Passed by RedisMessageBus","http://stackoverflow.com/questions/25072967/spring-xd-redis-message-bus-removing-headers-from-the-message/25081538#25081538

Currently, you have to modify {{redis-bus.xml}} in the dirt jar.",1.0,0.0063077034378506,0.0053829797534469,0.1914765498232961,True,True,True,1406877755000.0,1406889351000.0,1406892951000.0,1408715242000.0,1408716142000.0,1406877755000.0,-1.0,1407229763000.0,1407233363000.0,1406887651000.0,1406891251000.0,1406889351000.0,1406892951000.0,"grussell",1407233363000.0,1406889346000.0,44,1406889346000.0,-1.0,"Easier Customization of Headers Passed by RedisMessageBus","http://stackoverflow.com/questions/25072967/spring-xd-redis-message-bus-removing-headers-from-the-message/25081538#25081538

Currently, you have to modify {{redis-bus.xml}} in the dirt jar.",1.0,False,1.0,0,0.0,0,0,False
"XD-2044","Story","Sprint 33","2014-07-31T12:31:20.000+0000","mark.pollack","","Create a SFTP source","Use the SFTP adapter in SI as the basis.

Need to consider the infrastructure for testing.",2.0,0.953770902058924,0.7212865504902146,0.0622778941326653,True,True,True,1406809880000.0,1413224099000.0,1413227699000.0,1413534095000.0,1413534995000.0,1406809880000.0,-1.0,1407228706000.0,1407232306000.0,1411660615000.0,1411664215000.0,1413224099000.0,1413227699000.0,"mark.pollack",1407232306000.0,1406809880000.0,44,1406809880000.0,-1.0,"Add SFTP source","As a user, I'd like to have the option to use the _SFTP_ source module so that I can access, transfer, and mange files over any reliable data streams.

*Reference:*
[Spring Integration SFTP Adapter|http://docs.spring.io/spring-integration/reference/html/sftp.html]

Need to consider the infrastructure for testing.",2.0,False,2.0,0,0.0,0,0,True
"XD-2042","Story","","2014-07-30T06:21:43.000+0000","grenfro","grenfro","Update XD-EC2 & Acceptance Test Configs to use 1.0.1 repo","* Update XD-EC2 configs to Pull from 1.0.1 Repo
* Update XD-EC2 Configs to use spring-xd-1.0.1.BUILD-SNAPSHOT dir 
* Update test configs XD_HOME to spring-xd-1.0.1.BUILD-SNAPSHOT instead of spring-xd-1.0.0.BUILD-SNAPSHOT",3.0,0.0096932593584592,0.0,-1.0,True,False,True,1406701303000.0,1406701455000.0,1406705055000.0,1406716084000.0,1406716984000.0,1406701303000.0,-1.0,-1.0,-1.0,1406701303000.0,1406704903000.0,1406701455000.0,1406705055000.0,"grenfro",-1.0,-1.0,0,1406701303000.0,-1.0,"Update XD-EC2 & Acceptance Test Configs to use 1.0.1 repo","* Update XD-EC2 configs to Pull from 1.0.1 Repo
* Update XD-EC2 Configs to use spring-xd-1.0.1.BUILD-SNAPSHOT dir 
* Update test configs XD_HOME to spring-xd-1.0.1.BUILD-SNAPSHOT instead of spring-xd-1.0.0.BUILD-SNAPSHOT",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2041","Improvement","Sprint 33","2014-07-29T16:55:00.000+0000","dturanski","eric.bottard","Fix anchor links so that they work in both the wiki and generated docs","Part of the issue is likely with the build script https://github.com/spring-projects/spring-xd/blob/master/build.gradle#L1859  We should also add a link checker to the CI build.",8.0,0.3833851042075322,0.0720156473695059,0.2944576947647908,True,True,True,1406652900000.0,1407402452000.0,1407406052000.0,1408607089000.0,1408607989000.0,1406652900000.0,-1.0,1407228591000.0,1407232191000.0,1406793697000.0,1406797297000.0,1407402452000.0,1407406052000.0,"dturanski",1407232191000.0,1406652900000.0,44,1406652900000.0,-1.0,"Fix anchor links so that they work in both the wiki and generated docs","Part of the issue is likely with the build script https://github.com/spring-projects/spring-xd/blob/master/build.gradle#L1859  We should also add a link checker to the CI build.",8.0,False,8.0,0,0.0,0,0,False
"XD-2040","Story","","2014-07-29T10:39:08.000+0000","mark.pollack","mark.pollack","Release 1.0 GA","",2.0,0.9996686876718682,0.0,-1.0,True,False,True,1406630348000.0,1406660521000.0,1406660531000.0,1406659631000.0,1406660531000.0,1406630348000.0,-1.0,-1.0,-1.0,1406630348000.0,1406633948000.0,1406660521000.0,1406660531000.0,"mark.pollack",-1.0,-1.0,0,1406630348000.0,-1.0,"Release 1.0 GA","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-2038","Bug","Sprint 32","2014-07-29T06:48:57.000+0000","mark.fisher","mark.fisher","Restore JDK6 compatibility","",2.0,0.0043305243445692,0.0038623595505617,0.0021067415730337,True,True,True,1406616537000.0,1406616574000.0,1406620174000.0,1406624181000.0,1406625081000.0,1406616537000.0,-1.0,1406616555000.0,1406620155000.0,1406616570000.0,1406620170000.0,1406616574000.0,1406620174000.0,"mark.fisher",1406620155000.0,1406616537000.0,44,1406616537000.0,-1.0,"Restore JDK6 compatibility","",2.0,False,2.0,0,0.0,0,0,False
"XD-2037","Improvement","Sprint 32","2014-07-28T22:27:00.000+0000","iperumal","iperumal","Log the servers/modules config locations and names","It would be good to log the server, module config locations and names when the admin, container, singlenode servers startup.",1.0,0.1366188396756082,0.0,0.4391765439800374,True,True,True,1406586420000.0,1406586639000.0,1406588023000.0,1406587123000.0,1406588023000.0,1406586420000.0,-1.0,1406587124000.0,1406588023000.0,1406586420000.0,1406588023000.0,1406586639000.0,1406588023000.0,"iperumal",1406588023000.0,1406586420000.0,44,1406586420000.0,-1.0,"Log the servers/modules config locations and names","It would be good to log the server, module config locations and names when the admin, container, singlenode servers startup.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2035","Bug","Sprint 32","2014-07-28T20:24:38.000+0000","iperumal","iperumal","Exclude HealthIndicatorAutoConfiguration","With HealthIndicatorAutoConfiguration, the health endpoint shows up:

{""status"":""DOWN"",""healthIndicator"":{""status"":""UP""},""rabbit"":{""status"":""DOWN"",""error"":""org.springframework.amqp.AmqpConnectException: java.net.ConnectException: Connection refused""},""redis"":{""status"":""UP"",""version"":""2.8.9""},""mongo"":{""status"":""DOWN"",""error"":""org.springframework.dao.InvalidDataAccessResourceUsageException: Timed out while waiting to connect after 3894 ms; nested exception is com.mongodb.MongoTimeoutException: Timed out while waiting to connect after 3894 ms""},""db"":{""status"":""UP"",""database"":""HSQL Database Engine"",""hello"":1}}

We can only use vanilla health indicator, since other status may not be relevant.",1.0,0.9982096677939128,0.0,0.3302168291227372,True,True,True,1406579078000.0,1406584096000.0,1406584105000.0,1406583205000.0,1406584105000.0,1406579078000.0,-1.0,1406580738000.0,1406584105000.0,1406579078000.0,1406582678000.0,1406584096000.0,1406584105000.0,"iperumal",1406584105000.0,1406579078000.0,44,1406579078000.0,-1.0,"Exclude HealthIndicatorAutoConfiguration","With HealthIndicatorAutoConfiguration, the health endpoint shows up:

{""status"":""DOWN"",""healthIndicator"":{""status"":""UP""},""rabbit"":{""status"":""DOWN"",""error"":""org.springframework.amqp.AmqpConnectException: java.net.ConnectException: Connection refused""},""redis"":{""status"":""UP"",""version"":""2.8.9""},""mongo"":{""status"":""DOWN"",""error"":""org.springframework.dao.InvalidDataAccessResourceUsageException: Timed out while waiting to connect after 3894 ms; nested exception is com.mongodb.MongoTimeoutException: Timed out while waiting to connect after 3894 ms""},""db"":{""status"":""UP"",""database"":""HSQL Database Engine"",""hello"":1}}

We can only use vanilla health indicator, since other status may not be relevant.",1.0,False,1.0,0,0.0,-1,0,False
"XD-2033","Bug","Sprint 32","2014-07-28T16:09:26.000+0000","thomas.risberg","thomas.risberg","Connection pool settings need to be in their own section in server.yml","The 
#ConnectionPoolSettings

define this in the beginning - 

{code}
#spring:
#  datasource:
{code}

uncommenting this will override/invalidate any changes made earlier in the section since it defines spring:datasource again

should either be removed or in separate section",1.0,0.9999005552332808,0.0020386177177426,0.9997182398276292,True,True,True,1406563766000.0,1406624095000.0,1406624101000.0,1406623201000.0,1406624101000.0,1406563766000.0,-1.0,1406624084000.0,1406624101000.0,1406563889000.0,1406567489000.0,1406624095000.0,1406624101000.0,"thomas.risberg",1406624101000.0,1406563766000.0,44,1406563766000.0,-1.0,"Connection pool settings need to be in their own section in server.yml","The 
#ConnectionPoolSettings

define this in the beginning - 

{code}
#spring:
#  datasource:
{code}

uncommenting this will override/invalidate any changes made earlier in the section since it defines spring:datasource again

should either be removed or in separate section",1.0,False,1.0,0,0.0,0,2,False
"XD-2032","Story","Sprint 32","2014-07-28T14:58:44.000+0000","mark.pollack","hillert","Fix misc doc formatting issues","Noticed a few issues while reviewing the documentation

* The sidebar for TOC is no longer there :(  That was really nice.
* Somehow the 'Using-MQTT-on-XD' section is giving an error.

{quote}
asciidoctor: WARNING: index.adoc: line 167: invalid style for paragraph: appendix
asciidoctor: WARNING: index.adoc: line 169: include file not found: /data/projects/spring-xd/build/asciidoc/guide/Using-MQTT-on-XD.asciidoc
:distZip
{quote}

but I don't notice anything different between that appendix and the others in index.adoc.


",2.0,0.99967908064245,0.0,0.0,True,True,True,1406559524000.0,1406624940000.0,1406624961000.0,1406624061000.0,1406624961000.0,1406559524000.0,-1.0,1406559524000.0,1406563124000.0,1406559524000.0,1406563124000.0,1406624940000.0,1406624961000.0,"mark.pollack",1406563124000.0,1406559524000.0,44,1406559524000.0,-1.0,"Fix misc doc formatting issues","Noticed a few issues while reviewing the documentation

* The sidebar for TOC is no longer there :(  That was really nice.
* Somehow the 'Using-MQTT-on-XD' section is giving an error.

{quote}
asciidoctor: WARNING: index.adoc: line 167: invalid style for paragraph: appendix
asciidoctor: WARNING: index.adoc: line 169: include file not found: /data/projects/spring-xd/build/asciidoc/guide/Using-MQTT-on-XD.asciidoc
:distZip
{quote}

but I don't notice anything different between that appendix and the others in index.adoc.


",2.0,False,2.0,0,0.0,-1,5,False
"XD-2031","Story","Sprint 49","2014-07-28T11:43:53.000+0000","mark.pollack","","Improvements to Tuple project","Removed  various TODO comments in code and put here for proper triage.

DefaultTuple
  * Error handling.  When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure.  Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in.  (see method convert)
 * Ctor visibility.  Consider making ctor final and package protect the ctor so as to always use TupleBuilder
 * check for no duplicate values when initializing names/values list

tuple.
* top level methods to add.  
  String getComponentName... somethign that would indicate which stream or job this tuple is being processed in....

* TupleFieldSetMapper

Only one date format?

* JsonStringtoTupleConverter/JsonNodetoTupleConverter
 * do we want to not map id and timestamp (believe the answer is don't map, preserve original)

",1.0,0.979276966781696,0.979276966781696,0.9768974087694192,True,True,True,1406547833000.0,1431678772000.0,1431682372000.0,1432209682000.0,1432210582000.0,1406547833000.0,-1.0,1431617706000.0,1431621306000.0,1431678772000.0,1431682372000.0,1431678772000.0,1431682372000.0,"mark.pollack",1431621306000.0,1431617706000.0,45,1431617706000.0,-1.0,"Improvements to Tuple project","Removed  various TODO comments in code and put here for proper triage.

DefaultTuple
  * Error handling.  When delegating to the conversion service, the ConversionFailedException does not have the context of which key caused the failure.  Need to wrap ConversionFailedException with IllegalArgumentException and add that context back in.  (see method convert)
 * Ctor visibility.  Consider making ctor final and package protect the ctor so as to always use TupleBuilder
 * check for no duplicate values when initializing names/values list

tuple.
* top level methods to add.  
  String getComponentName... somethign that would indicate which stream or job this tuple is being processed in....

* TupleFieldSetMapper

Only one date format?

* JsonStringtoTupleConverter/JsonNodetoTupleConverter
 * do we want to not map id and timestamp (believe the answer is don't map, preserve original)

",1.0,False,1.0,0,0.0,0,1,False
"XD-2030","Bug","Sprint 32","2014-07-28T11:31:23.000+0000","iperumal","iperumal","Make producible media type to `application/json` for Job executions GET request endpoints ","As a temporary work around to fix XD-1935, make producible media type to 'application/json' for Job executions GET request endpoints.",1.0,0.0003257232536791,0.0,0.9981641052974446,True,True,True,1406547083000.0,1406547094000.0,1406550694000.0,1406579954000.0,1406580854000.0,1406547083000.0,-1.0,1406580792000.0,1406580854000.0,1406547083000.0,1406550683000.0,1406547094000.0,1406550694000.0,"iperumal",1406580854000.0,1406547083000.0,44,1406547083000.0,-1.0,"Make producible media type to `application/json` for Job executions GET request endpoints ","As a temporary work around to fix XD-1935, make producible media type to 'application/json' for Job executions GET request endpoints.",1.0,False,1.0,0,0.0,0,0,False
"XD-2029","Story","Sprint 32","2014-07-28T11:27:22.000+0000","mark.pollack","mark.pollack","Use AlternativeJdkIdGenerator  instead of 3rd party library","Spring 4.0 provides a UUID generator (used by default in SI) that should be used instead of the com.eaoi.uuid library in the xd-tuple library",1.0,0.000912041632018,0.000912041632018,0.0,True,True,True,1406546842000.0,1406546876000.0,1406550476000.0,1406583221000.0,1406584121000.0,1406546842000.0,-1.0,1406546842000.0,1406550442000.0,1406546876000.0,1406550476000.0,1406546876000.0,1406550476000.0,"mark.pollack",1406550442000.0,1406546842000.0,44,1406546842000.0,-1.0,"Use AlternativeJdkIdGenerator  instead of 3rd party library","Spring 4.0 provides a UUID generator (used by default in SI) that should be used instead of the com.eaoi.uuid library in the xd-tuple library",1.0,False,1.0,0,0.0,-1,1,False
"XD-2028","Story","Sprint 32","2014-07-28T11:16:00.000+0000","mark.pollack","","Add docs for creating a Job Module","There are a few places in the doc we can reference regarding overall lifecycle of jobs but this should provide a basic recipe for a single step job.

The focus should be on creating a job item processor.

In particular how List<Message<Tuple>> as the payload.

this should link back to a new section in the aggregator that also mentions List<Message<Tuple>>

Change title from  Creating a Job Item Processor to Creating a Job Module",2.0,0.1313513513513513,0.1313513513513513,0.0,True,True,True,1406546160000.0,1406556366000.0,1406559966000.0,1406622960000.0,1406623860000.0,1406546160000.0,-1.0,1406546160000.0,1406549760000.0,1406556366000.0,1406559966000.0,1406556366000.0,1406559966000.0,"mark.pollack",1406549760000.0,1406546160000.0,44,1406546160000.0,-1.0,"Add docs for creating a Job Module","There are a few places in the doc we can reference regarding overall lifecycle of jobs but this should provide a basic recipe for a single step job.

The focus should be on creating a job item processor.

In particular how List<Message<Tuple>> as the payload.

this should link back to a new section in the aggregator that also mentions List<Message<Tuple>>

Change title from  Creating a Job Item Processor to Creating a Job Module",2.0,False,2.0,0,0.0,0,0,False
"XD-2026","Story","Sprint 37","2014-07-28T10:11:17.000+0000","iperumal","jvalkeal","Handle random available http port for admin server","If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to '0', the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port.

We also need to persist the admin servers' ports into ZK so that this repo can be accessed by the client.",5.0,0.9243928397290392,0.7798708085667618,0.858426278260626,True,True,True,1406542277000.0,1413803392000.0,1413806992000.0,1414396387000.0,1414397287000.0,1406542277000.0,-1.0,1413285224000.0,1413288824000.0,1412668170000.0,1412671770000.0,1413803392000.0,1413806992000.0,"iperumal",1413288824000.0,1406542277000.0,44,1406542277000.0,-1.0,"Handle random available http port for admin server","If the XD admin server lets tomcat chooses random http port by setting PORT or server.port to '0', the XD config logging and admin server context id still points to port zero as these are set before tomcat assigns the available random port.

We also need to persist the admin servers' ports into ZK so that this repo can be accessed by the client.",5.0,False,5.0,0,0.0,0,0,False
"XD-2025","Improvement","Sprint 32","2014-07-28T07:02:01.000+0000","thomas.risberg","thomas.risberg","Update rabbitmq setting in server.yml for spring-xd-yarn","",3.0,0.2139544470782232,0.2138926105328249,0.2026486653612285,True,True,True,1406530921000.0,1406551681000.0,1406555281000.0,1406627051000.0,1406627951000.0,1406530921000.0,-1.0,1406550584000.0,1406554184000.0,1406551675000.0,1406555275000.0,1406551681000.0,1406555281000.0,"thomas.risberg",1406554184000.0,1406530921000.0,44,1406530921000.0,-1.0,"Update spring-xd-yarn configuration options","Need a re-write of the configuration files for YARN deployments",3.0,False,3.0,0,0.0,0,0,True
"XD-2024","Story","Sprint 32","2014-07-28T06:18:26.000+0000","mark.pollack","","Fix failing script integration tests","issue seems to be

error	25-Jul-2014 18:36:18	cat: gemfire.pid: No such file or directory
error	25-Jul-2014 18:36:18	Usage:
error	25-Jul-2014 18:36:18	  kill pid ...              Send SIGTERM to every process listed.
error	25-Jul-2014 18:36:18	  kill signal pid ...       Send a signal to every process listed.
error	25-Jul-2014 18:36:18	  kill -s signal pid ...    Send a signal to every process listed.
error	25-Jul-2014 18:36:18	  kill -l                   List all signal names.
error	25-Jul-2014 18:36:18	  kill -L                   List all signal names in a nice table.
error	25-Jul-2014 18:36:18	  kill -l signal            Convert between signal numbers and names.
error	25-Jul-2014 18:36:18	rm: cannot remove `gemfire.pid': No such file or directory",2.0,-1.0,-1.0,0.0,False,True,False,1406528306000.0,-1.0,-1.0,1406546028000.0,1406546928000.0,1406528306000.0,-1.0,1406528306000.0,1406531906000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1406531906000.0,1406528306000.0,44,1406528306000.0,-1.0,"Fix failing script integration tests","issue seems to be

error	25-Jul-2014 18:36:18	cat: gemfire.pid: No such file or directory
error	25-Jul-2014 18:36:18	Usage:
error	25-Jul-2014 18:36:18	  kill pid ...              Send SIGTERM to every process listed.
error	25-Jul-2014 18:36:18	  kill signal pid ...       Send a signal to every process listed.
error	25-Jul-2014 18:36:18	  kill -s signal pid ...    Send a signal to every process listed.
error	25-Jul-2014 18:36:18	  kill -l                   List all signal names.
error	25-Jul-2014 18:36:18	  kill -L                   List all signal names in a nice table.
error	25-Jul-2014 18:36:18	  kill -l signal            Convert between signal numbers and names.
error	25-Jul-2014 18:36:18	rm: cannot remove `gemfire.pid': No such file or directory",2.0,False,2.0,0,0.0,0,0,False
"XD-2023","Improvement","Sprint 32","2014-07-26T06:56:51.000+0000","mark.pollack","","Update to Spring Hadoop 2.0.2","A bug in the HDFS Store was discovered that should be fixed. ",1.0,0.9458524881038792,0.9458524881038792,0.0,True,True,True,1406357811000.0,1406545453000.0,1406549053000.0,1406555295000.0,1406556195000.0,1406357811000.0,-1.0,1406357811000.0,1406361411000.0,1406545453000.0,1406549053000.0,1406545453000.0,1406549053000.0,"mark.pollack",1406361411000.0,1406357811000.0,44,1406357811000.0,-1.0,"Update to Spring Hadoop 2.0.2","A bug in the HDFS Store was discovered that should be fixed. ",1.0,False,1.0,0,0.0,0,0,False
"XD-2022","Story","Sprint 33","2014-07-26T06:55:00.000+0000","mark.pollack","","0xData - investigate embedding","0xData is a rich JVM based machine learning and scoring engine.",24.0,0.4980714601168309,0.4980714601168309,0.4198870068482451,True,True,True,1406357700000.0,1407390754000.0,1407394354000.0,1408430908000.0,1408431808000.0,1406357700000.0,1407390746000.0,1407228591000.0,1407232191000.0,1407390754000.0,1407394354000.0,1407390754000.0,1407394354000.0,"mark.pollack",1407232191000.0,1406357700000.0,44,1407390746000.0,1407390746000.0,"0xData - investigate embedding","0xData is a rich JVM based machine learning and scoring engine.",8.0,True,8.0,1,200.0,0,0,False
"XD-2021","Bug","Sprint 32","2014-07-25T14:24:29.000+0000","iperumal","iperumal","Admin UI: Deployment Status tooltip should close when the controller scope is lost","Please refer to: https://github.com/spring-projects/spring-xd/issues/1119",1.0,0.0004596162607394,0.0004596162607394,0.0003588232211036,True,True,True,1406298269000.0,1406298383000.0,1406301983000.0,1406545402000.0,1406546302000.0,1406298269000.0,-1.0,1406298358000.0,1406301958000.0,1406298383000.0,1406301983000.0,1406298383000.0,1406301983000.0,"iperumal",1406301958000.0,1406298269000.0,44,1406298269000.0,-1.0,"Admin UI: Deployment Status tooltip should close when the controller scope is lost","Please refer to: https://github.com/spring-projects/spring-xd/issues/1119",1.0,False,1.0,0,0.0,0,0,False
"XD-2018","Bug","Sprint 32","2014-07-25T08:58:40.000+0000","iperumal","iperumal","Fix images alignment in reference pdf doc","The reference pdf doc has some of the images not aligned well within the document.

For the latest doc from snapshot build, please refer here:

http://repo.spring.io/libs-snapshot-local/org/springframework/xd/spring-xd/1.0.0.BUILD-SNAPSHOT/",3.0,0.5599341422384481,0.5599341422384481,0.5767565052435664,True,True,True,1406278720000.0,1406294364000.0,1406297964000.0,1406305759000.0,1406306659000.0,1406278720000.0,-1.0,1406294834000.0,1406298434000.0,1406294364000.0,1406297964000.0,1406294364000.0,1406297964000.0,"iperumal",1406298434000.0,1406278720000.0,44,1406278720000.0,-1.0,"Fix images alignment in reference pdf doc","The reference pdf doc has some of the images not aligned well within the document.

For the latest doc from snapshot build, please refer here:

http://repo.spring.io/libs-snapshot-local/org/springframework/xd/spring-xd/1.0.0.BUILD-SNAPSHOT/",3.0,False,3.0,0,0.0,0,0,False
"XD-2017","Improvement","Sprint 32","2014-07-25T07:30:04.000+0000","fbiville","","Spring XD should log the address of the admin UI","When I start xd-singlenode for instance, I would expect to see http://localhost:9393/admin-ui listed in the logs.",2.0,0.8745574760516451,0.8745574760516451,0.0714806330695543,True,True,True,1406273404000.0,1406307001000.0,1406310601000.0,1406310920000.0,1406311820000.0,1406273404000.0,-1.0,1406276150000.0,1406279750000.0,1406307001000.0,1406310601000.0,1406307001000.0,1406310601000.0,"fbiville",1406279750000.0,1406273404000.0,44,1406273404000.0,-1.0,"Spring XD should log the address of the admin UI","When I start xd-singlenode for instance, I would expect to see http://localhost:9393/admin-ui listed in the logs.",2.0,False,2.0,0,0.0,0,0,False
"XD-2016","Story","Sprint 32","2014-07-25T05:27:20.000+0000","mark.pollack","","Reoganize TOC for manual","Here is a strawman
{{monospaced}}
Getting Started  (rather meaty compared to other top level sections, maybe have a section - running in SingleNode)
* Running in Disributed Mode
* Running on YARN
*Application Configuration
Message Bus Configuration
Monitoring and Management

Technical Documenation  

Architecture
Distributed Runtime  (remove 'XD' prefix)
Interactive Shell
Batch Jobs
Streams
Modules
Tuples
Sources
Processors
Analytics
Sinks
Taps
Type Conversion
Deployment  (better name?)
Best Practices (new section)

Admin UI
DSL Reference
REST API

Samples

{{monospaced}}",2.0,0.8910688419348916,0.9114976844138268,0.3112542248396771,True,True,True,1406266040000.0,1406528098000.0,1406531698000.0,1406559234000.0,1406560134000.0,1406266040000.0,-1.0,1406357578000.0,1406361178000.0,1406534106000.0,1406537706000.0,1406528098000.0,1406531698000.0,"mark.pollack",1406361178000.0,1406266040000.0,44,1406266040000.0,-1.0,"Reorganize TOC for manual","Here is a strawman
{noformat}
Getting Started  (rather meaty compared to other top level sections, maybe have a section - running in SingleNode)
* Running in Distributed Mode
* Running on YARN
*Application Configuration
Message Bus Configuration
Monitoring and Management

Technical Documentation  

Architecture
Distributed Runtime  (remove 'XD' prefix)
Interactive Shell
Batch Jobs
Streams
Modules
Tuples
Sources
Processors
Analytics
Sinks
Taps
Type Conversion
Deployment  (better name?)
Best Practices (new section)

Admin UI
DSL Reference
REST API

Samples

{noformat}",2.0,False,2.0,0,0.0,0,0,True
"XD-2015","Bug","Sprint 33","2014-07-25T05:06:43.000+0000","fbiville","","Spring XD UI: end-to-end tests do not work","Currently, end-to-end tests of Spring XD UI will not run, as protractor relies on a non-existing chromedriver.exe file.

Either the configuration has to be removed from the Gruntfile or the necessary dependencies should be there.",2.0,0.4069718607138054,0.4069718607138054,0.3465913054359184,True,True,True,1406264803000.0,1407396630000.0,1407400230000.0,1409044997000.0,1409045897000.0,1406264803000.0,-1.0,1407228706000.0,1407232306000.0,1407396630000.0,1407400230000.0,1407396630000.0,1407400230000.0,"fbiville",1407232306000.0,1406264803000.0,44,1406264803000.0,-1.0,"Spring XD UI: end-to-end tests do not work","Currently, end-to-end tests of Spring XD UI will not run, as protractor relies on a non-existing chromedriver.exe file.

Either the configuration has to be removed from the Gruntfile or the necessary dependencies should be there.",2.0,False,2.0,0,0.0,0,2,False
"XD-2012","Story","Sprint 32","2014-07-24T12:36:59.000+0000","mark.pollack","mark.pollack","Upgrade to Spring Shell 1.1 GA","",1.0,0.660083974147285,0.0,0.0,True,True,True,1406205419000.0,1406275378000.0,1406278978000.0,1406310504000.0,1406311404000.0,1406205419000.0,-1.0,1406205419000.0,1406209019000.0,1406205419000.0,1406209019000.0,1406275378000.0,1406278978000.0,"mark.pollack",1406209019000.0,1406205419000.0,44,1406205419000.0,-1.0,"Upgrade to Spring Shell 1.1 GA","",1.0,False,1.0,0,0.0,-1,0,False
"XD-2011","Story","Sprint 32","2014-07-24T12:30:38.000+0000","grenfro","grenfro","Replace xd MongoItemWriter in hdfsmongodb with spring batch MongoItemWriter","When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available, so Mark wrote one for XD.  The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.",3.0,0.999923774677948,0.0,0.9998584386876176,True,True,True,1406205038000.0,1406296864000.0,1406296871000.0,1406295971000.0,1406296871000.0,1406205038000.0,-1.0,1406296858000.0,1406296871000.0,1406205038000.0,1406208638000.0,1406296864000.0,1406296871000.0,"grenfro",1406296871000.0,1406205038000.0,44,1406205038000.0,-1.0,"Replace xd MongoItemWriter in hdfsmongodb with spring batch MongoItemWriter","When hdfsmongodb was written Spring Batch did not have a MongoItemWriter available, so Mark wrote one for XD.  The hdfsmongodb module now needs to be use Spring Batches MongoItemWriter.",3.0,False,3.0,0,0.0,-1,1,False
"XD-2010","Story","Sprint 32","2014-07-24T04:58:52.000+0000","eric.bottard","eric.bottard","Fix package-info.java warnings","",2.0,0.4524699464592383,0.0008081624406505,0.4490352560864734,True,True,True,1406177932000.0,1406182411000.0,1406186011000.0,1406186931000.0,1406187831000.0,1406177932000.0,-1.0,1406182377000.0,1406185977000.0,1406177940000.0,1406181540000.0,1406182411000.0,1406186011000.0,"eric.bottard",1406185977000.0,1406177932000.0,44,1406177932000.0,-1.0,"Fix package-info.java warnings","",2.0,False,2.0,0,0.0,0,0,False
"XD-2009","Story","Sprint 32","2014-07-23T18:25:55.000+0000","iperumal","iperumal","Cleanup Module Deployer","Container's module deployer (org.springframework.xd.dirt.module.ModuleDeployer) has some unused code and container-server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.",1.0,0.0131048167288109,0.0131048167288109,0.9249253178081596,True,True,True,1406139955000.0,1406140556000.0,1406144156000.0,1406184916000.0,1406185816000.0,1406139955000.0,-1.0,1406182373000.0,1406185816000.0,1406140556000.0,1406144156000.0,1406140556000.0,1406144156000.0,"iperumal",1406185816000.0,1406139955000.0,44,1406139955000.0,-1.0,"Cleanup Module Deployer","Container's module deployer (org.springframework.xd.dirt.module.ModuleDeployer) has some unused code and container-server.xml has listeners.xml which is no longer used. Also, all the extension code is moved to SharedContextConfiguration.",1.0,False,1.0,0,0.0,0,0,False
"XD-2008","Story","Sprint 32","2014-07-23T14:57:04.000+0000","mark.pollack","","Verify we meet all requirements to publish to maven central","https://docs.sonatype.org/display/Repository/Central+Sync+Requirements

has a list of requirements.  This also means that https://jira.spring.io/browse/XD-1509 is critical to fix.",3.0,0.0681107649782939,0.0681107649782939,0.0,True,True,True,1406127424000.0,1406139646000.0,1406143246000.0,1406305967000.0,1406306867000.0,1406127424000.0,-1.0,1406127424000.0,1406131024000.0,1406139646000.0,1406143246000.0,1406139646000.0,1406143246000.0,"mark.pollack",1406131024000.0,1406127424000.0,44,1406127424000.0,-1.0,"Verify we meet all requirements to publish to maven central","https://docs.sonatype.org/display/Repository/Central+Sync+Requirements

has a list of requirements.  This also means that https://jira.spring.io/browse/XD-1509 is critical to fix.",3.0,False,3.0,0,0.0,0,0,False
"XD-2007","Story","Sprint 32","2014-07-23T12:30:51.000+0000","grenfro","grenfro","Acceptance test must be able to handle log names with PID suffix","Introduced by XD-2006, admin and container logs will have a pid suffix appended to their filename.
The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd_container_log_dir.",4.0,0.4804211542454975,0.3580877442094097,0.3577341632197197,True,True,True,1406118651000.0,1406204251000.0,1406207851000.0,1406295928000.0,1406296828000.0,1406118651000.0,-1.0,1406182391000.0,1406185991000.0,1406182454000.0,1406186054000.0,1406204251000.0,1406207851000.0,"grenfro",1406185991000.0,1406118651000.0,44,1406118651000.0,-1.0,"Acceptance test must be able to handle log names with PID suffix","Introduced by XD-2006, admin and container logs will have a pid suffix appended to their filename.
The acceptance tests will have to identify the PID for the admin server and the container servers deployed in the cluster and then append the pid value to the filename contained in the xd_container_log_dir.",4.0,False,4.0,0,0.0,-1,0,False
"XD-2006","Improvement","Sprint 32","2014-07-23T11:25:13.000+0000","pperalta","pperalta","Logging improvements","Propose the following changes to our logging:
* Create unique file names by including the pid in the file name - this allows each process (in particular containers) to maintain its own log file
* Use DailyRollingFileAppender to roll files over on a daily basis ",2.0,0.9194100605741374,0.0,0.9170397682380826,True,True,True,1406114713000.0,1406118204000.0,1406118510000.0,1406117610000.0,1406118510000.0,1406114713000.0,-1.0,1406118195000.0,1406118510000.0,1406114713000.0,1406118313000.0,1406118204000.0,1406118510000.0,"pperalta",1406118510000.0,1406114713000.0,44,1406114713000.0,-1.0,"Logging improvements","Propose the following changes to our logging:
* Create unique file names by including the pid in the file name - this allows each process (in particular containers) to maintain its own log file
* Use DailyRollingFileAppender to roll files over on a daily basis ",2.0,False,2.0,0,0.0,0,0,False
"XD-2005","Bug","Sprint 32","2014-07-23T10:45:15.000+0000","pperalta","pperalta","IllegalStateException when shutting down container","{noformat}
13:23:57,643  INFO main-EventThread server.ContainerRegistrar - Undeploying module [ModuleDescriptor@1c736092 moduleName = 'log', moduleLabel = 'log', group = 'paymenttap', sourceChannelName = 'tap:job:payment', sinkChannelName = [null], sinkChannelName = [null], index = 0, type = sink, parameters = map[[empty]], children = list[[empty]]]
13:23:57,643 ERROR main-EventThread imps.CuratorFrameworkImpl - Watcher exception
java.lang.IllegalStateException: instance must be started before calling this method
at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
at org.apache.curator.framework.imps.CuratorFrameworkImpl.delete(CuratorFrameworkImpl.java:344)
at org.springframework.xd.dirt.server.ContainerRegistrar.unregisterTap(ContainerRegistrar.java:292)
at org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:257)
at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:711)
at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67)
at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
{noformat}

Sequence of events:
* Stream module ZK path is removed
* Event is raised
* ZK connection is closed
* Event handler causes module undeployment which includes unregistration of tap
* Since connection is closed, exception is thrown
",3.0,0.8504200240013715,0.0023144179667409,0.8495628321618378,True,True,True,1406112315000.0,1406122236000.0,1406123981000.0,1406123081000.0,1406123981000.0,1406112315000.0,-1.0,1406122226000.0,1406123981000.0,1406112342000.0,1406115942000.0,1406122236000.0,1406123981000.0,"pperalta",1406123981000.0,1406112315000.0,44,1406112315000.0,-1.0,"IllegalStateException when shutting down container","{noformat}
13:23:57,643  INFO main-EventThread server.ContainerRegistrar - Undeploying module [ModuleDescriptor@1c736092 moduleName = 'log', moduleLabel = 'log', group = 'paymenttap', sourceChannelName = 'tap:job:payment', sinkChannelName = [null], sinkChannelName = [null], index = 0, type = sink, parameters = map[[empty]], children = list[[empty]]]
13:23:57,643 ERROR main-EventThread imps.CuratorFrameworkImpl - Watcher exception
java.lang.IllegalStateException: instance must be started before calling this method
at com.google.common.base.Preconditions.checkState(Preconditions.java:176)
at org.apache.curator.framework.imps.CuratorFrameworkImpl.delete(CuratorFrameworkImpl.java:344)
at org.springframework.xd.dirt.server.ContainerRegistrar.unregisterTap(ContainerRegistrar.java:292)
at org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:257)
at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:711)
at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67)
at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
{noformat}

Sequence of events:
* Stream module ZK path is removed
* Event is raised
* ZK connection is closed
* Event handler causes module undeployment which includes unregistration of tap
* Since connection is closed, exception is thrown
",3.0,False,3.0,0,0.0,0,0,False
"XD-2004","Bug","Sprint 33","2014-07-23T06:29:55.000+0000","grenfro","pperalta","Containers stopped responding to Admin","SHA = a205d43f0b59e1984bf55c3368b031a373a03712
Environment: Rabbit Transport Test 1 admin 2 containers.

[Initial Event]
During the run of FileJdbcTest.testPartitionedFileJdbcJob the containers quit responding to the admin server.   After the initial failure at 12:26:46 no other streams can be deployed.  

[Secondary Event]
When shutting down one of the container 1 the following exception occurs on the admin server:
12:51:12,004  INFO DeploymentSupervisorCacheListener-0 server.DepartingContainerModuleRedeployer - Container departed: Container{name='5353dc4b-6068-49a0-8981-fa175869edf0', attributes={id=5353dc4b-6068-49a0-8981-fa175869edf0, host=domU-12-31-39-07-81-02, pid=1270, groups=, ip=10.209.130.240}}
12:51:12,004 ERROR DeploymentSupervisorCacheListener-0 cache.PathChildrenCache - 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/allocated/5353dc4b-6068-49a0-8981-fa175869edf0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
	at org.springframework.xd.dirt.server.DepartingContainerModuleRedeployer.deployModules(DepartingContainerModuleRedeployer.java:101)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:104)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

The attached container logs are only partial, because they have rolled over.  The attached admin log is fairly complete.
",12.0,0.4664798073868916,0.299968771846807,0.4325733718263959,True,True,True,1406096995000.0,1407317413000.0,1407321013000.0,1408712324000.0,1408713224000.0,1406096995000.0,1407317437000.0,1407228706000.0,1407232306000.0,1406881782000.0,1406885382000.0,1407317413000.0,1407321013000.0,"grenfro",1407232306000.0,1406096995000.0,44,1407317437000.0,1407317437000.0,"Containers stopped responding to Admin","SHA = a205d43f0b59e1984bf55c3368b031a373a03712
Environment: Rabbit Transport Test 1 admin 2 containers.

[Initial Event]
During the run of FileJdbcTest.testPartitionedFileJdbcJob the containers quit responding to the admin server.   After the initial failure at 12:26:46 no other streams can be deployed.  

[Secondary Event]
When shutting down one of the container 1 the following exception occurs on the admin server:
12:51:12,004  INFO DeploymentSupervisorCacheListener-0 server.DepartingContainerModuleRedeployer - Container departed: Container{name='5353dc4b-6068-49a0-8981-fa175869edf0', attributes={id=5353dc4b-6068-49a0-8981-fa175869edf0, host=domU-12-31-39-07-81-02, pid=1270, groups=, ip=10.209.130.240}}
12:51:12,004 ERROR DeploymentSupervisorCacheListener-0 cache.PathChildrenCache - 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/allocated/5353dc4b-6068-49a0-8981-fa175869edf0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
	at org.springframework.xd.dirt.server.DepartingContainerModuleRedeployer.deployModules(DepartingContainerModuleRedeployer.java:101)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:104)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

The attached container logs are only partial, because they have rolled over.  The attached admin log is fairly complete.
",3.0,True,3.0,1,300.0,0,1,False
"XD-2003","Story","Sprint 32","2014-07-22T12:29:35.000+0000","grenfro","grenfro","Allow users to set download jars into the lib/xd directory ","In cases where the deployment requires jars that can not be included with the distribution, the user should be able to pull a jar from a http site and place it in lib/xd.  

The use case is that when we removed the mysql jar from the distribution, the CI tests could not start the XD instances on EC2 without it.  It was suggested that we use the postgresql instead, but decided to continue the use of mysql for acceptance tests.",5.0,0.0119694011287345,0.0,2.117647845507797,True,True,True,1406032175000.0,1406033070000.0,1406036670000.0,1406106049000.0,1406106949000.0,1406032175000.0,-1.0,1406190520000.0,1406106949000.0,1406032175000.0,1406035775000.0,1406033070000.0,1406036670000.0,"grenfro",1406106949000.0,1406032175000.0,44,1406032175000.0,-1.0,"In EC2 deployment, Allow users to set download jars into the lib/xd directory ","In cases where the deployment requires jars that can not be included with the distribution, the user should be able to pull a jar from a http site and place it in lib/xd.  

The use case is that when we removed the mysql jar from the distribution, the CI tests could not start the XD instances on EC2 without it.  It was suggested that we use the postgresql instead, but decided to continue the use of mysql for acceptance tests.",5.0,False,5.0,0,0.0,0,0,True
"XD-1999","Story","Sprint 32","2014-07-21T20:37:24.000+0000","iperumal","iperumal","Remove unused post module references","",1.0,0.0177777777777777,0.0,0.0074074074074074,False,False,False,1405975044000.0,1405975056000.0,1405975719000.0,1405975044000.0,1405975719000.0,1405975044000.0,-1.0,1405975049000.0,1405975719000.0,1405975044000.0,1405975719000.0,1405975056000.0,1405975719000.0,"iperumal",1405975719000.0,1405975044000.0,44,1405975044000.0,-1.0,"Remove unused post module references","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1998","Story","Sprint 32","2014-07-21T19:56:34.000+0000","mark.pollack","","Remove jersey test framework for xd/lib distribution","The jars

 jersey-test-framework-core-1.9.jar
 jersey-test-framework-grizzly2-1.9.jar

are incorrectly classified as compile time deps in hadoop vs. testCompile.

",1.0,0.6348520499108734,0.6128532382650029,0.0001045751633986,True,True,True,1405972594000.0,1406106151000.0,1406109751000.0,1406182069000.0,1406182969000.0,1405972594000.0,-1.0,1405972616000.0,1405976216000.0,1406101523000.0,1406105123000.0,1406106151000.0,1406109751000.0,"mark.pollack",1405976216000.0,1405972594000.0,44,1405972594000.0,-1.0,"Remove jersey test framework for xd/lib distribution","The jars

 jersey-test-framework-core-1.9.jar
 jersey-test-framework-grizzly2-1.9.jar

are incorrectly classified as compile time deps in hadoop vs. testCompile.

",1.0,False,1.0,0,0.0,0,0,False
"XD-1997","Story","Sprint 42","2014-07-21T11:01:18.000+0000","mark.fisher","","Add comprehensive tests for AggregateCounterRepository","The AggregateCounterTests were created to satisfy XD-1462, but currently they only have a couple tests to validate the time field processing. More comprehensive tests need to be added (including the testing of the Redis-based implementation in addition to in-memory).

For more info, see the comment here:
https://github.com/spring-projects/spring-xd/pull/1087#issuecomment-49638189",2.0,-1.0,-1.0,0.8874094363653799,False,True,False,1405940478000.0,-1.0,-1.0,1424449353000.0,1424450253000.0,1405940478000.0,1422521329000.0,1422366227000.0,1422369827000.0,-1.0,-1.0,-1.0,-1.0,"mark.fisher",1422369827000.0,1405940478000.0,44,1422521329000.0,1422521329000.0,"Add comprehensive tests for AggregateCounterRepository","The AggregateCounterTests were created to satisfy XD-1462, but currently they only have a couple tests to validate the time field processing. More comprehensive tests need to be added (including the testing of the Redis-based implementation in addition to in-memory).

For more info, see the comment here:
https://github.com/spring-projects/spring-xd/pull/1087#issuecomment-49638189",4.0,True,4.0,1,-50.0,0,0,False
"XD-1996","Bug","Sprint 32","2014-07-18T16:02:39.000+0000","iperumal","","Inconsistent failure while deploying job from admin UI","After clicking 'deploy' on the definitions page, the 'deploy' button is deactivated and message says:

""An error occurred. We were unable to retrieve the module name from the provided definition ....""
and web console says:

TypeError: Cannot read property '0' of null
    at Object.getModuleNameFromJobDefinition (http://localhost:9393/admin-ui/scripts/shared/services.js:43:26)
    at http://localhost:9393/admin-ui/scripts/job/controllers/definition-deploy.js:35:36
    at wrappedCallback (http://localhost:9393/admin-ui/lib/angular/angular.js:11319:81)
    at http://localhost:9393/admin-ui/lib/angular/angular.js:11405:26
    at Scope.$eval (http://localhost:9393/admin-ui/lib/angular/angular.js:12412:28)
    at Scope.$digest (http://localhost:9393/admin-ui/lib/angular/angular.js:12224:31)
    at Scope.$apply (http://localhost:9393/admin-ui/lib/angular/angular.js:12516:24)
    at done (http://localhost:9393/admin-ui/lib/angular/angular.js:8204:45)
    at completeRequest (http://localhost:9393/admin-ui/lib/angular/angular.js:8412:7)
    at XMLHttpRequest.xhr.onreadystatechange (http://localhost:9393/admin-ui/lib/angular/angular.js:8351:11) ",3.0,0.5589495615938883,0.5588731660734438,0.4026755794773852,True,True,True,1405699359000.0,1406021286000.0,1406024886000.0,1406274409000.0,1406275309000.0,1405699359000.0,-1.0,1405931280000.0,1405934880000.0,1406021242000.0,1406024842000.0,1406021286000.0,1406024886000.0,"iperumal",1405934880000.0,1405699359000.0,44,1405699359000.0,-1.0,"Inconsistent failure while deploying job from admin UI","After clicking 'deploy' on the definitions page, the 'deploy' button is deactivated and message says:

""An error occurred. We were unable to retrieve the module name from the provided definition ....""
and web console says:

TypeError: Cannot read property '0' of null
    at Object.getModuleNameFromJobDefinition (http://localhost:9393/admin-ui/scripts/shared/services.js:43:26)
    at http://localhost:9393/admin-ui/scripts/job/controllers/definition-deploy.js:35:36
    at wrappedCallback (http://localhost:9393/admin-ui/lib/angular/angular.js:11319:81)
    at http://localhost:9393/admin-ui/lib/angular/angular.js:11405:26
    at Scope.$eval (http://localhost:9393/admin-ui/lib/angular/angular.js:12412:28)
    at Scope.$digest (http://localhost:9393/admin-ui/lib/angular/angular.js:12224:31)
    at Scope.$apply (http://localhost:9393/admin-ui/lib/angular/angular.js:12516:24)
    at done (http://localhost:9393/admin-ui/lib/angular/angular.js:8204:45)
    at completeRequest (http://localhost:9393/admin-ui/lib/angular/angular.js:8412:7)
    at XMLHttpRequest.xhr.onreadystatechange (http://localhost:9393/admin-ui/lib/angular/angular.js:8351:11) ",3.0,False,3.0,0,0.0,0,0,False
"XD-1995","Bug","Sprint 32","2014-07-18T15:18:56.000+0000","iperumal","iperumal","Step execution count is zero for the job execution list result","For the Job execution list, the step execution count for each job execution is always set to zero.

For a single job execution display command, the step execution count is set correctly.",4.0,0.0034532362171103,0.0,0.9723119271379594,True,True,True,1405696736000.0,1405697569000.0,1405701169000.0,1405937059000.0,1405937959000.0,1405696736000.0,-1.0,1405931280000.0,1405934880000.0,1405696736000.0,1405700336000.0,1405697569000.0,1405701169000.0,"iperumal",1405934880000.0,1405696736000.0,44,1405696736000.0,-1.0,"Step execution count is zero for the job execution list result","For the Job execution list, the step execution count for each job execution is always set to zero.

For a single job execution display command, the step execution count is set correctly.",4.0,False,4.0,0,0.0,0,0,False
"XD-1994","Bug","Sprint 32","2014-07-18T11:04:17.000+0000","hillert","","ParentLastURLClassLoader should set itself as context ClassLoader","I am converting a Spring XD Sample (Batch notifications) from copying jars to (old way)

{code}
$XD_HOME/lib
{code}

To rather copy the module jar to (new preferred way)

{code}
$XD_HOME/modules/job/payment-import/lib
{code}

By doing so, I hit a classloader issue. Custom classes and resources are loaded in Spring XD using *org.springframework.xd.module.support.ParentLastURLClassLoader*.

However, the sample is initializing custom bean definitions and one of those creates a new *DataSource* using the *EmbeddedDatabaseBuilder*. This class however, under the hood, uses the *Default* class loader to load SQL scripts:

{code}
	public DefaultResourceLoader() {
		this.classLoader = ClassUtils.getDefaultClassLoader();
	}
{code}

Therefore, the SQL scripts are NOT FOUND.

*Possible Solution*

A possible solution seems to be for *ParentLastURLClassLoader* to set itself as the context ClassLoader for the current thread:

{code}
	public ParentLastURLClassLoader(URL[] classpath, ClassLoader parent) {
		...
		Thread.currentThread().setContextClassLoader(this);
		...
	}
{code}",4.0,0.5017387735706417,0.5017387735706417,0.4905540708508503,True,True,True,1405681457000.0,1405936976000.0,1405940576000.0,1406189824000.0,1406190724000.0,1405681457000.0,-1.0,1405931280000.0,1405934880000.0,1405936976000.0,1405940576000.0,1405936976000.0,1405940576000.0,"hillert",1405934880000.0,1405681457000.0,44,1405681457000.0,-1.0,"ParentLastURLClassLoader should set itself as context ClassLoader","I am converting a Spring XD Sample (Batch notifications) from copying jars to (old way)

{code}
$XD_HOME/lib
{code}

To rather copy the module jar to (new preferred way)

{code}
$XD_HOME/modules/job/payment-import/lib
{code}

By doing so, I hit a classloader issue. Custom classes and resources are loaded in Spring XD using *org.springframework.xd.module.support.ParentLastURLClassLoader*.

However, the sample is initializing custom bean definitions and one of those creates a new *DataSource* using the *EmbeddedDatabaseBuilder*. This class however, under the hood, uses the *Default* class loader to load SQL scripts:

{code}
	public DefaultResourceLoader() {
		this.classLoader = ClassUtils.getDefaultClassLoader();
	}
{code}

Therefore, the SQL scripts are NOT FOUND.

*Possible Solution*

A possible solution seems to be for *ParentLastURLClassLoader* to set itself as the context ClassLoader for the current thread:

{code}
	public ParentLastURLClassLoader(URL[] classpath, ClassLoader parent) {
		...
		Thread.currentThread().setContextClassLoader(this);
		...
	}
{code}",4.0,False,4.0,0,0.0,0,7,False
"XD-1993","Improvement","Sprint 32","2014-07-18T09:28:29.000+0000","mark.pollack","","Update rpm and brew recipes","",2.0,0.9999926565775036,0.998969822729781,0.2681094044064731,True,True,True,1405675709000.0,1406628936000.0,1406628943000.0,1406628043000.0,1406628943000.0,1405675709000.0,-1.0,1405931280000.0,1405934880000.0,1406627961000.0,1406628943000.0,1406628936000.0,1406628943000.0,"mark.pollack",1405934880000.0,1405675709000.0,44,1405675709000.0,-1.0,"Update rpm and brew recipes","",2.0,False,2.0,0,0.0,0,0,False
"XD-1992","Story","","2014-07-18T09:16:44.000+0000","mark.pollack","","Release 1.0 RC1","",2.0,-1.0,-1.0,-1.0,False,False,False,1405675004000.0,-1.0,-1.0,1405689109000.0,1405690009000.0,1405675004000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1405675004000.0,-1.0,"Release 1.0 RC1","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1991","Story","Sprint 32","2014-07-18T06:43:16.000+0000","dturanski","","Error message about memory leak when ctrl-c xd-container and xd-admin","e.g., 

^C09:42:00,882 ERROR localhost-startStop-2 loader.WebappClassLoader - The web application [] appears to have started a thread named [Abandoned connection cleanup thread] but has failed to stop it. This is very likely to create a memory leak.

The thread name may be different...",4.0,0.7890105588190661,0.7890105588190661,0.5838756430191053,True,True,True,1405665796000.0,1406024552000.0,1406028152000.0,1406119587000.0,1406120487000.0,1405665796000.0,-1.0,1405931279000.0,1405934879000.0,1406024552000.0,1406028152000.0,1406024552000.0,1406028152000.0,"dturanski",1405934879000.0,1405665796000.0,44,1405665796000.0,-1.0,"Error message about memory leak when ctrl-c xd-container and xd-admin","e.g., 

^C09:42:00,882 ERROR localhost-startStop-2 loader.WebappClassLoader - The web application [] appears to have started a thread named [Abandoned connection cleanup thread] but has failed to stop it. This is very likely to create a memory leak.

The thread name may be different...",4.0,False,4.0,0,0.0,0,1,False
"XD-1990","Story","Sprint 32","2014-07-18T06:03:43.000+0000","grussell","","Add Docs (or Reference) For Standard Shell Commands (e.g. script)","http://stackoverflow.com/questions/24819401/how-to-get-spring-xd-to-deploy-a-predefined-set-of-streams-and-taps-on-startup

It is documented here https://github.com/spring-projects/spring-xd/wiki/Shell#executing-a-script

But maybe it should also be at the top of the appendix?

https://github.com/spring-projects/spring-xd/wiki/ShellReference",1.0,0.9896641936037738,0.9896461200625344,0.6051428261596435,True,True,True,1405663423000.0,1406101484000.0,1406105084000.0,1406105159000.0,1406106059000.0,1405663423000.0,-1.0,1405931281000.0,1405934881000.0,1406101476000.0,1406105076000.0,1406101484000.0,1406105084000.0,"grussell",1405934881000.0,1405663423000.0,44,1405663423000.0,-1.0,"Add Docs (or Reference) For Standard Shell Commands (e.g. script)","http://stackoverflow.com/questions/24819401/how-to-get-spring-xd-to-deploy-a-predefined-set-of-streams-and-taps-on-startup

It is documented here https://github.com/spring-projects/spring-xd/wiki/Shell#executing-a-script

But maybe it should also be at the top of the appendix?

https://github.com/spring-projects/spring-xd/wiki/ShellReference",1.0,False,1.0,0,0.0,0,1,False
"XD-1989","Story","Sprint 31","2014-07-17T10:08:59.000+0000","thomas.risberg","iperumal","Remove warnings from Shell hadoop commands","Some hadoop commands generate warnings/deprecation messages. We should try to get rid of most of them.

{code}
xd:>hadoop fs ls /xd --recursive 
Hadoop configuration changed, re-initializing shell...
lsr: DEPRECATED: Please use 'ls -R' instead.
13:01:07,120  WARN Spring Shell util.NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - trisberg supergroup          0 2014-07-17 11:19 /xd/hashtagcount
drwxr-xr-x   - trisberg supergroup          0 2014-07-17 11:19 /xd/hashtagcount/output
-rw-r--r--   3 trisberg supergroup          0 2014-07-17 11:19 /xd/hashtagcount/output/_SUCCESS
-rw-r--r--   3 trisberg supergroup        833 2014-07-17 11:19 /xd/hashtagcount/output/part-r-00000
drwxr-xr-x   - trisberg supergroup          0 2014-07-16 18:28 /xd/tweets
-rw-r--r--   3 trisberg supergroup     982993 2014-07-16 18:28 /xd/tweets/tweets-0.txt
{code}",5.0,0.3619088353904436,0.3619088353904436,0.6179885127703776,True,True,True,1405591739000.0,1405597662000.0,1405601262000.0,1405607205000.0,1405608105000.0,1405591739000.0,-1.0,1405601853000.0,1405605453000.0,1405597662000.0,1405601262000.0,1405597662000.0,1405601262000.0,"thomas.risberg",1405605453000.0,1405591739000.0,44,1405591739000.0,-1.0,"Remove warnings from Shell hadoop commands","Some hadoop commands generate warnings/deprecation messages. We should try to get rid of most of them.

{code}
xd:>hadoop fs ls /xd --recursive 
Hadoop configuration changed, re-initializing shell...
lsr: DEPRECATED: Please use 'ls -R' instead.
13:01:07,120  WARN Spring Shell util.NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
drwxr-xr-x   - trisberg supergroup          0 2014-07-17 11:19 /xd/hashtagcount
drwxr-xr-x   - trisberg supergroup          0 2014-07-17 11:19 /xd/hashtagcount/output
-rw-r--r--   3 trisberg supergroup          0 2014-07-17 11:19 /xd/hashtagcount/output/_SUCCESS
-rw-r--r--   3 trisberg supergroup        833 2014-07-17 11:19 /xd/hashtagcount/output/part-r-00000
drwxr-xr-x   - trisberg supergroup          0 2014-07-16 18:28 /xd/tweets
-rw-r--r--   3 trisberg supergroup     982993 2014-07-16 18:28 /xd/tweets/tweets-0.txt
{code}",5.0,False,5.0,0,0.0,0,0,False
"XD-1986","Bug","Sprint 31","2014-07-16T23:33:40.000+0000","iperumal","iperumal","Fix package tangle","Fix package tangle issue reported here:

https://build.spring.io/browse/XD-SONAR-490",1.0,0.0099185340449899,0.0,0.6325388421894246,True,True,True,1405553620000.0,1405554079000.0,1405557679000.0,1405598997000.0,1405599897000.0,1405553620000.0,-1.0,1405582892000.0,1405586492000.0,1405553620000.0,1405557220000.0,1405554079000.0,1405557679000.0,"iperumal",1405586492000.0,1405553620000.0,44,1405553620000.0,-1.0,"Fix package tangle","Fix package tangle issue reported here:

https://build.spring.io/browse/XD-SONAR-490",1.0,False,1.0,0,0.0,0,0,False
"XD-1985","Story","Sprint 31","2014-07-16T13:46:07.000+0000","thomas.risberg","thomas.risberg","Packaging of Guava 17 results in failure to deploy mapreduce job to Hadoop 2.4 based distros","Trying to deploy the hashtagcount batch sample [1] to Hadoop 2.4.1 or Hortonworks HDP 2.1 fails with an IllegalAccessError exception.

Looks like a Guava versioning issue - Swapping out guava-17.0.jar for guava-11.0.2.jar in the xd/lib directory solves it.

Mark P suggested we try 16.0.1 which is what Curator uses and that seems to work as well. 

Looking into changing the build to not force 17.0 which is the IO platform version.

http://upstream-tracker.org/java/compat_reports/guava/16.0.1_to_17.0/src_compat_report.html 


I get the following exception:

{code}
16:42:22,214  INFO Deployer server.JobDeploymentListener - Deployment status for job 'hashtagCountJob': DeploymentStatus{state=deployed}
16:42:27,315  WARN task-scheduler-2 mapreduce.JobSubmitter - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
16:42:27,325 ERROR task-scheduler-2 step.AbstractStep - Encountered an error executing step hashtagcount in job hashtagCountJob
java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.<init>()V from class org.apache.hadoop.mapreduce.lib.input.FileInputFormat
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:369)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:493)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.Job$$FastClassBySpringCGLIB$$a048cbfe.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:708)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:133)
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:121)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:644)
	at org.apache.hadoop.mapreduce.Job$$EnhancerBySpringCGLIB$$875ec891.waitForCompletion(<generated>)
	at org.springframework.data.hadoop.mapreduce.JobExecutor$2.run(JobExecutor.java:199)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.data.hadoop.mapreduce.JobExecutor.startJobs(JobExecutor.java:170)
	at org.springframework.data.hadoop.batch.mapreduce.JobTasklet.execute(JobTasklet.java:90)
	at org.springframework.batch.core.step.tasklet.TaskletStep$ChunkTransactionCallback.doInTransaction(TaskletStep.java:406)
	at org.springframework.batch.core.step.tasklet.TaskletStep$ChunkTransactionCallback.doInTransaction(TaskletStep.java:330)
	at org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:133)
	at org.springframework.batch.core.step.tasklet.TaskletStep$2.doInChunkContext(TaskletStep.java:271)
	at org.springframework.batch.core.scope.context.StepContextRepeatCallback.doInIteration(StepContextRepeatCallback.java:77)
	at org.springframework.batch.repeat.support.RepeatTemplate.getNextResult(RepeatTemplate.java:368)
	at org.springframework.batch.repeat.support.RepeatTemplate.executeInternal(RepeatTemplate.java:215)
	at org.springframework.batch.repeat.support.RepeatTemplate.iterate(RepeatTemplate.java:144)
	at org.springframework.batch.core.step.tasklet.TaskletStep.doExecute(TaskletStep.java:257)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:162)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:141)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy44.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)
	at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy125.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor97.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy123.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy125.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor97.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy123.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.endpoint.PollingConsumer.handleMessage(PollingConsumer.java:74)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:205)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:284)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:278)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

[1] https://github.com/spring-projects/spring-xd-samples",5.0,0.7268352992281217,0.7269912342255043,0.7270914781523931,True,True,True,1405518367000.0,1405583623000.0,1405587223000.0,1405607248000.0,1405608148000.0,1405518367000.0,-1.0,1405583646000.0,1405587246000.0,1405583637000.0,1405587237000.0,1405583623000.0,1405587223000.0,"thomas.risberg",1405587246000.0,1405518367000.0,44,1405518367000.0,-1.0,"Packaging of Guava 17 results in failure to deploy mapreduce job to Hadoop 2.4 based distros","Trying to deploy the hashtagcount batch sample [1] to Hadoop 2.4.1 or Hortonworks HDP 2.1 fails with an IllegalAccessError exception.

Looks like a Guava versioning issue - Swapping out guava-17.0.jar for guava-11.0.2.jar in the xd/lib directory solves it.

Mark P suggested we try 16.0.1 which is what Curator uses and that seems to work as well. 

Looking into changing the build to not force 17.0 which is the IO platform version.

http://upstream-tracker.org/java/compat_reports/guava/16.0.1_to_17.0/src_compat_report.html 


I get the following exception:

{code}
16:42:22,214  INFO Deployer server.JobDeploymentListener - Deployment status for job 'hashtagCountJob': DeploymentStatus{state=deployed}
16:42:27,315  WARN task-scheduler-2 mapreduce.JobSubmitter - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
16:42:27,325 ERROR task-scheduler-2 step.AbstractStep - Encountered an error executing step hashtagcount in job hashtagCountJob
java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.<init>()V from class org.apache.hadoop.mapreduce.lib.input.FileInputFormat
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:369)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:493)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)
	at org.apache.hadoop.mapreduce.Job$$FastClassBySpringCGLIB$$a048cbfe.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)
	at org.springframework.aop.framework.CglibAopProxy$CglibMethodInvocation.invokeJoinpoint(CglibAopProxy.java:708)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.doProceed(DelegatingIntroductionInterceptor.java:133)
	at org.springframework.aop.support.DelegatingIntroductionInterceptor.invoke(DelegatingIntroductionInterceptor.java:121)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.CglibAopProxy$DynamicAdvisedInterceptor.intercept(CglibAopProxy.java:644)
	at org.apache.hadoop.mapreduce.Job$$EnhancerBySpringCGLIB$$875ec891.waitForCompletion(<generated>)
	at org.springframework.data.hadoop.mapreduce.JobExecutor$2.run(JobExecutor.java:199)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.data.hadoop.mapreduce.JobExecutor.startJobs(JobExecutor.java:170)
	at org.springframework.data.hadoop.batch.mapreduce.JobTasklet.execute(JobTasklet.java:90)
	at org.springframework.batch.core.step.tasklet.TaskletStep$ChunkTransactionCallback.doInTransaction(TaskletStep.java:406)
	at org.springframework.batch.core.step.tasklet.TaskletStep$ChunkTransactionCallback.doInTransaction(TaskletStep.java:330)
	at org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:133)
	at org.springframework.batch.core.step.tasklet.TaskletStep$2.doInChunkContext(TaskletStep.java:271)
	at org.springframework.batch.core.scope.context.StepContextRepeatCallback.doInIteration(StepContextRepeatCallback.java:77)
	at org.springframework.batch.repeat.support.RepeatTemplate.getNextResult(RepeatTemplate.java:368)
	at org.springframework.batch.repeat.support.RepeatTemplate.executeInternal(RepeatTemplate.java:215)
	at org.springframework.batch.repeat.support.RepeatTemplate.iterate(RepeatTemplate.java:144)
	at org.springframework.batch.core.step.tasklet.TaskletStep.doExecute(TaskletStep.java:257)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:162)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:141)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy44.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)
	at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy125.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor97.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy123.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.GeneratedMethodAccessor98.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy125.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor97.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy123.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.endpoint.PollingConsumer.handleMessage(PollingConsumer.java:74)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:205)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:284)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:278)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

[1] https://github.com/spring-projects/spring-xd-samples",5.0,False,5.0,0,0.0,0,4,False
"XD-1984","Story","Sprint 31","2014-07-16T12:27:48.000+0000","dturanski","pperalta","Avoid all modules deploying to the first container instance upon system restart","A possible approach is to set a configurable wait period when the first container arrives. If another container arrives during the wait period, reset the clock. When the wait period expires, start deploying modules. ",8.0,0.8124348229114364,0.6231702135877332,0.6248406782279556,True,True,True,1405513668000.0,1405597807000.0,1405601407000.0,1405616332000.0,1405617232000.0,1405513668000.0,-1.0,1405578379000.0,1405581979000.0,1405578206000.0,1405581806000.0,1405597807000.0,1405601407000.0,"dturanski",1405581979000.0,1405513668000.0,44,1405513668000.0,-1.0,"Avoid all modules deploying to the first container instance upon system restart","A possible approach is to set a configurable wait period when the first container arrives. If another container arrives during the wait period, reset the clock. When the wait period expires, start deploying modules. ",8.0,False,8.0,0,0.0,0,0,False
"XD-1983","Story","Sprint 31","2014-07-16T09:15:46.000+0000","iperumal","iperumal","NodeExists Exception upon container disconnect/reconnect without admin leader","When the container which has modules deployed disconnects/reconnects to the cluster while the admin leader isn't available, following exception is thrown:
This is more likely to happen in single-node scenario as there is no admin leader re-election there. In distributed mode, we can always setup HA on admins so that the leadership re-election happens.

20:03:16,307 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache - 
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/deployments/modules/allocated/53f41042-8abd-443b-abfb-ba42a24fb9fb/foo.sink.log.1/metadata
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)
	at org.springframework.xd.dirt.server.ContainerRegistrar.writeModuleMetadata(ContainerRegistrar.java:486)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:461)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$8(ContainerRegistrar.java:426)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:807)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)",3.0,5.21996951537803e-05,0.0,0.1057774622596204,True,True,True,1405502146000.0,1405502151000.0,1405505751000.0,1405597032000.0,1405597932000.0,1405502146000.0,-1.0,1405512278000.0,1405515878000.0,1405502146000.0,1405505746000.0,1405502151000.0,1405505751000.0,"iperumal",1405515878000.0,1405502146000.0,44,1405502146000.0,-1.0,"NodeExists Exception upon container disconnect/reconnect without admin leader","When the container which has modules deployed disconnects/reconnects to the cluster while the admin leader isn't available, following exception is thrown:
This is more likely to happen in single-node scenario as there is no admin leader re-election there. In distributed mode, we can always setup HA on admins so that the leadership re-election happens.

20:03:16,307 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache - 
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/deployments/modules/allocated/53f41042-8abd-443b-abfb-ba42a24fb9fb/foo.sink.log.1/metadata
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)
	at org.springframework.xd.dirt.server.ContainerRegistrar.writeModuleMetadata(ContainerRegistrar.java:486)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:461)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$8(ContainerRegistrar.java:426)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:807)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)",3.0,False,3.0,0,0.0,0,0,False
"XD-1982","Story","Sprint 31","2014-07-16T08:23:30.000+0000","grussell","grussell","Add Https Support to the HTTP Source","",7.0,0.1339655713952805,0.0,0.0,True,True,True,1405499010000.0,1405512232000.0,1405515832000.0,1405596807000.0,1405597707000.0,1405499010000.0,1405505901000.0,1405499010000.0,1405502610000.0,1405499010000.0,1405502610000.0,1405512232000.0,1405515832000.0,"grussell",1405502610000.0,1405499010000.0,44,1405524521000.0,1405505901000.0,"Add Https Support to the HTTP Source","",3.0,True,3.0,2,133.33333333333331,0,0,False
"XD-1980","Bug","Sprint 31","2014-07-15T20:12:41.000+0000","iperumal","iperumal","Remove duplicate logger info on application started","When the admin, container and singlenode servers start, the ""Started ..Application"" log message is displayed everytime the spring application is created.
We should only log when the server is started eventually.",1.0,0.0081291289884271,0.0,0.9566134095487984,True,True,True,1405455161000.0,1405455508000.0,1405459108000.0,1405496947000.0,1405497847000.0,1405455161000.0,-1.0,1405495995000.0,1405497847000.0,1405455161000.0,1405458761000.0,1405455508000.0,1405459108000.0,"iperumal",1405497847000.0,1405455161000.0,44,1405455161000.0,-1.0,"Remove duplicate logger info on application started","When the admin, container and singlenode servers start, the ""Started ..Application"" log message is displayed everytime the spring application is created.
We should only log when the server is started eventually.",1.0,False,1.0,0,0.0,0,0,False
"XD-1979","Bug","Sprint 31","2014-07-15T17:29:31.000+0000","grenfro","eric.bottard","Log Sink is not writing to log.","The log sink is not writing information to the log.
Not the solution but, when log4j.rootLogger is set to INFO, the log sink information is written to the log.  ",2.0,0.0005413859480269,0.9407483156881616,1.004752165543792,True,True,True,1405445371000.0,1405445398000.0,1405448998000.0,1405494343000.0,1405495243000.0,1405445371000.0,-1.0,1405495480000.0,1405495243000.0,1405492288000.0,1405495243000.0,1405445398000.0,1405448998000.0,"grenfro",1405495243000.0,1405445371000.0,44,1405445371000.0,-1.0,"Change xd.sink logging level to INFO","The log sink is not writing information to the log.
Not the solution but, when log4j.rootLogger is set to INFO, the log sink information is written to the log.  ",2.0,False,2.0,0,0.0,0,2,True
"XD-1978","Story","Sprint 31","2014-07-15T15:58:54.000+0000","grussell","grussell","SSL Support For RabbitMQ (Bus and Modules)","",3.0,0.4177617024265196,0.0,0.0,True,True,True,1405439934000.0,1405499417000.0,1405503017000.0,1405581419000.0,1405582319000.0,1405439934000.0,-1.0,1405439934000.0,1405443534000.0,1405439934000.0,1405443534000.0,1405499417000.0,1405503017000.0,"grussell",1405443534000.0,1405439934000.0,44,1405439934000.0,-1.0,"SSL Support For RabbitMQ (Bus and Modules)","",3.0,False,3.0,0,0.0,0,0,False
"XD-1976","Bug","Sprint 31","2014-07-15T07:09:08.000+0000","thomas.risberg","hillert","Unable to deploy job in UI","This only happens when creating jobs via the CLI and deploying using the UI

On the job page:
http://localhost:9393/admin-ui/#/jobs/definitions

I click [Deploy] for a Job and get a screen asking for Container Match Criteria and Job Module Count - clicking on the [Deploy] button on that screen does nothing - I see this error reported:

Deploying Job Definition undefined angular.js:9778
TypeError: Cannot read property 'jobDefinition' of undefined
    at Scope.$scope.deployDefinition (http://localhost:9393/admin-ui/scripts/job/controllers/definition-deploy.js:52:78)
    at http://localhost:9393/admin-ui/lib/angular/angular.js:10567:21
    at http://localhost:9393/admin-ui/lib/angular/angular.js:18627:17
    at Scope.$eval (http://localhost:9393/admin-ui/lib/angular/angular.js:12412:28)
    at Scope.$apply (http://localhost:9393/admin-ui/lib/angular/angular.js:12510:23)
    at HTMLButtonElement.<anonymous> (http://localhost:9393/admin-ui/lib/angular/angular.js:18626:21)
    at HTMLButtonElement.jQuery.event.dispatch (http://localhost:9393/admin-ui/lib/jquery/jquery.js:5095:9)
    at HTMLButtonElement.elemData.handle (http://localhost:9393/admin-ui/lib/jquery/jquery.js:4766:46) angular.js:9778",5.0,0.1660814163757937,0.0003889986963286,0.0689368770764119,True,True,True,1405408148000.0,1405423945000.0,1405427545000.0,1405502364000.0,1405503264000.0,1405408148000.0,-1.0,1405414705000.0,1405418305000.0,1405408185000.0,1405411785000.0,1405423945000.0,1405427545000.0,"thomas.risberg",1405418305000.0,1405408148000.0,44,1405408148000.0,-1.0,"Unable to deploy job in UI","This only happens when creating jobs via the CLI and deploying using the UI

On the job page:
http://localhost:9393/admin-ui/#/jobs/definitions

I click [Deploy] for a Job and get a screen asking for Container Match Criteria and Job Module Count - clicking on the [Deploy] button on that screen does nothing - I see this error reported:

Deploying Job Definition undefined angular.js:9778
TypeError: Cannot read property 'jobDefinition' of undefined
    at Scope.$scope.deployDefinition (http://localhost:9393/admin-ui/scripts/job/controllers/definition-deploy.js:52:78)
    at http://localhost:9393/admin-ui/lib/angular/angular.js:10567:21
    at http://localhost:9393/admin-ui/lib/angular/angular.js:18627:17
    at Scope.$eval (http://localhost:9393/admin-ui/lib/angular/angular.js:12412:28)
    at Scope.$apply (http://localhost:9393/admin-ui/lib/angular/angular.js:12510:23)
    at HTMLButtonElement.<anonymous> (http://localhost:9393/admin-ui/lib/angular/angular.js:18626:21)
    at HTMLButtonElement.jQuery.event.dispatch (http://localhost:9393/admin-ui/lib/jquery/jquery.js:5095:9)
    at HTMLButtonElement.elemData.handle (http://localhost:9393/admin-ui/lib/jquery/jquery.js:4766:46) angular.js:9778",5.0,False,5.0,0,0.0,0,1,False
"XD-1975","Bug","Sprint 32","2014-07-15T05:41:28.000+0000","thomas.risberg","","Undeploying twitterstream logs warning - MessageDeliveryException","To reproduce - 

Download recent snapshot - http://repo.spring.io/libs-snapshot-local/org/springframework/xd/spring-xd/1.0.0.BUILD-SNAPSHOT/spring-xd-1.0.0.BUILD-20140715.101224-1-dist.zip

Start XD and shell -
xd:>stream create --name tweets --definition ""twitterstream | file"" --deploy 
xd:>stream undeploy --name tweets 

(Note: the IllegalStateException has been fixed for RC1, still need to fix the MessageDeliveryException)

There is an error logged in the logs:

{code}
08:37:57,022  INFO DeploymentsPathChildrenCache-0 module.ModuleDeployer - deployed SimpleModule [name=twitterstream, type=source, group=tweets, index=0 @581a12b9]
08:38:02,685  INFO main-EventThread server.ContainerRegistrar - Undeploying module [ModuleDescriptor@4807f3e2 moduleName = 'twitterstream', moduleLabel = 'twitterstream', group = 'tweets', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map[[empty]], children = list[[empty]]]
08:38:02,687  INFO main-EventThread module.ModuleDeployer - removed SimpleModule [name=twitterstream, type=source, group=tweets, index=0 @581a12b9]
08:38:02,705  INFO DeploymentsPathChildrenCache-0 server.ContainerRegistrar - Path cache event: /deployments/modules/allocated/fa40cb45-3c16-4b19-81e9-eb6d357d186d/tweets.source.twitterstream.1, type: CHILD_REMOVED
08:38:02,779  WARN task-scheduler-4 twitter.TwitterStreamChannelAdapter - Exception while reading stream.
org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=twitterstream, type=source, group=tweets, index=0 @581a12b9]:default,container:0.to.discardDeletes'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy81.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.x.twitter.TwitterStreamChannelAdapter.doSendLine(TwitterStreamChannelAdapter.java:154)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:553)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:521)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 33 more
08:38:02,780  WARN task-scheduler-4 twitter.TwitterStreamChannelAdapter - Exception while reading stream, waiting for 250 ms before restarting
08:38:02,781 ERROR task-scheduler-4 handler.LoggingHandler - java.lang.IllegalStateException: java.lang.InterruptedException: sleep interrupted
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.wait(AbstractTwitterInboundChannelAdapter.java:258)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.waitLinearBackoff(AbstractTwitterInboundChannelAdapter.java:232)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.access$600(AbstractTwitterInboundChannelAdapter.java:54)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:174)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.wait(AbstractTwitterInboundChannelAdapter.java:254)
	... 11 more
{code}
",3.0,0.8814780326033244,0.8814423271929549,0.7546581278367949,True,True,True,1405402888000.0,1406020076000.0,1406023676000.0,1406102162000.0,1406103062000.0,1405402888000.0,-1.0,1405931280000.0,1405934880000.0,1406020051000.0,1406023651000.0,1406020076000.0,1406023676000.0,"thomas.risberg",1405934880000.0,1405402888000.0,44,1405402888000.0,-1.0,"Undeploying twitterstream logs warning - MessageDeliveryException","To reproduce - 

Download recent snapshot - http://repo.spring.io/libs-snapshot-local/org/springframework/xd/spring-xd/1.0.0.BUILD-SNAPSHOT/spring-xd-1.0.0.BUILD-20140715.101224-1-dist.zip

Start XD and shell -
xd:>stream create --name tweets --definition ""twitterstream | file"" --deploy 
xd:>stream undeploy --name tweets 

(Note: the IllegalStateException has been fixed for RC1, still need to fix the MessageDeliveryException)

There is an error logged in the logs:

{code}
08:37:57,022  INFO DeploymentsPathChildrenCache-0 module.ModuleDeployer - deployed SimpleModule [name=twitterstream, type=source, group=tweets, index=0 @581a12b9]
08:38:02,685  INFO main-EventThread server.ContainerRegistrar - Undeploying module [ModuleDescriptor@4807f3e2 moduleName = 'twitterstream', moduleLabel = 'twitterstream', group = 'tweets', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map[[empty]], children = list[[empty]]]
08:38:02,687  INFO main-EventThread module.ModuleDeployer - removed SimpleModule [name=twitterstream, type=source, group=tweets, index=0 @581a12b9]
08:38:02,705  INFO DeploymentsPathChildrenCache-0 server.ContainerRegistrar - Path cache event: /deployments/modules/allocated/fa40cb45-3c16-4b19-81e9-eb6d357d186d/tweets.source.twitterstream.1, type: CHILD_REMOVED
08:38:02,779  WARN task-scheduler-4 twitter.TwitterStreamChannelAdapter - Exception while reading stream.
org.springframework.messaging.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=twitterstream, type=source, group=tweets, index=0 @581a12b9]:default,container:0.to.discardDeletes'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor86.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy81.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.x.twitter.TwitterStreamChannelAdapter.doSendLine(TwitterStreamChannelAdapter.java:154)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:553)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:521)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:107)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 33 more
08:38:02,780  WARN task-scheduler-4 twitter.TwitterStreamChannelAdapter - Exception while reading stream, waiting for 250 ms before restarting
08:38:02,781 ERROR task-scheduler-4 handler.LoggingHandler - java.lang.IllegalStateException: java.lang.InterruptedException: sleep interrupted
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.wait(AbstractTwitterInboundChannelAdapter.java:258)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.waitLinearBackoff(AbstractTwitterInboundChannelAdapter.java:232)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.access$600(AbstractTwitterInboundChannelAdapter.java:54)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:174)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter.wait(AbstractTwitterInboundChannelAdapter.java:254)
	... 11 more
{code}
",3.0,False,3.0,0,0.0,0,3,False
"XD-1974","Story","Sprint 31","2014-07-15T04:27:06.000+0000","thomas.risberg","iperumal","Move [Back] button to top right","The [Back] button is at lower left of the page which requires scrolling all the way to the bottom - could we move it to top right? Would make clicking back and forth for job executions much easier.",3.0,0.3734882502982281,0.3734882502982281,0.4600956269578795,True,True,True,1405398426000.0,1405436936000.0,1405440536000.0,1405500635000.0,1405501535000.0,1405398426000.0,-1.0,1405445866000.0,1405449466000.0,1405436936000.0,1405440536000.0,1405436936000.0,1405440536000.0,"thomas.risberg",1405449466000.0,1405398426000.0,44,1405398426000.0,-1.0,"Move [Back] button to top right","The [Back] button is at lower left of the page which requires scrolling all the way to the bottom - could we move it to top right? Would make clicking back and forth for job executions much easier.",3.0,False,3.0,0,0.0,0,0,False
"XD-1972","Improvement","Sprint 36","2014-07-14T19:52:32.000+0000","mminella","liujiong","Add ability to define nested jobs","h3.  Narrative
As a developer, I need to be able to create a Spring XD job module that consists of a job orchestrating the execution of other Spring Batch jobs using the Spring Batch Job Step (see section 5.3.6 here: http://docs.spring.io/spring-batch/reference/html/configureStep.html) within the same module definition.

h3.  Acceptance Criteria
# Define the ""contract"" for a job module
## Currently the contract consists of a single job definition within the assembled {{ApplicationContext}} ({{context.getBean(Job.class)}}).
## The new version will need to document what job definition within the assembled {{ApplicationContext}} should be run as the entry point.  I'm assuming it would be by id ({{context.getBean(""job"")}} for example) of the job but am open to other options.
# A custom job module that orchestrates multiple Spring Batch jobs via Job steps should be able to be deployed and executed as a single Spring XD module.
## Spring XD launches the job that conforms to the previously defined ""contract"".
## Spring Batch manages the execution of the child jobs.
# The existing OOTB jobs should work under the new ""contract"".

h3.  Assumptions
# The UI should ""just work"" in that child jobs update the job repository independently so no updates should be needed for an MVP of this functionality.
# *This will be a breaking change for users that have developed custom job modules.*

h3.  Out of Scope
# Execution of child jobs that are remote (deployed on another node / {{ApplicationContext}}).
# Dynamically assembling jobs via the shell's DSL or the UI.
",3.0,0.3013144708060407,0.1714099286566588,0.901846895369434,True,True,True,1405367552000.0,1407777617000.0,1407781217000.0,1413365156000.0,1413366056000.0,1405367552000.0,1412593825000.0,1412580978000.0,1412584578000.0,1406738575000.0,1406742175000.0,1407777617000.0,1407781217000.0,"mminella",1412584578000.0,1405367552000.0,44,1412593825000.0,1412593825000.0,"Add ability to define nested jobs","h3.  Narrative
As a developer, I need to be able to create a Spring XD job module that consists of a job orchestrating the execution of other Spring Batch jobs using the Spring Batch Job Step (see section 5.3.6 here: http://docs.spring.io/spring-batch/reference/html/configureStep.html) within the same module definition.

h3.  Acceptance Criteria
# Define the ""contract"" for a job module
## Currently the contract consists of a single job definition within the assembled {{ApplicationContext}} ({{context.getBean(Job.class)}}).
## The new version will need to document what job definition within the assembled {{ApplicationContext}} should be run as the entry point.  I'm assuming it would be by id ({{context.getBean(""job"")}} for example) of the job but am open to other options.
# A custom job module that orchestrates multiple Spring Batch jobs via Job steps should be able to be deployed and executed as a single Spring XD module.
## Spring XD launches the job that conforms to the previously defined ""contract"".
## Spring Batch manages the execution of the child jobs.
# The existing OOTB jobs should work under the new ""contract"".

h3.  Assumptions
# The UI should ""just work"" in that child jobs update the job repository independently so no updates should be needed for an MVP of this functionality.
# *This will be a breaking change for users that have developed custom job modules.*

h3.  Out of Scope
# Execution of child jobs that are remote (deployed on another node / {{ApplicationContext}}).
# Dynamically assembling jobs via the shell's DSL or the UI.
",16.0,True,16.0,1,-81.25,0,4,False
"XD-1970","Story","Sprint 31","2014-07-14T13:41:21.000+0000","grussell","grussell","Name the TaskExecutors in the RabbitMessageBus","The Rabbit {{SMLC}} uses a {{SimpleAsyncTaskExecutor}} by default.

This makes it difficult to debug when multiple modules are in the same container because all threads are named {{SimpleAsyncTaskExecutor-1}}.",0.2,0.0199276857798543,0.019267281942908,0.0196470141491522,True,True,True,1405345281000.0,1405346488000.0,1405350088000.0,1405404950000.0,1405405850000.0,1405345281000.0,-1.0,1405346471000.0,1405350071000.0,1405346448000.0,1405350048000.0,1405346488000.0,1405350088000.0,"grussell",1405350071000.0,1405346406000.0,44,1405346406000.0,-1.0,"Name the TaskExecutors in the RabbitMessageBus","The Rabbit {{SMLC}} uses a {{SimpleAsyncTaskExecutor}} by default.

This makes it difficult to debug when multiple modules are in the same container because all threads are named {{SimpleAsyncTaskExecutor-1}}.",0.2,False,0.2,0,0.0,0,0,False
"XD-1967","Story","","2014-07-14T13:01:04.000+0000","thomas.risberg","","Dependendcies for Hadoop distros are completely broken ","We used to have distro specific jars i the lib/[distro] directory. That is no longer working and all distros seem to contain mostly the same version (hadoop 2.2.0 dependencies)

This is the list for phd1 now:

avro-1.7.5.jar
hadoop-annotations-2.2.0.jar
hadoop-auth-2.2.0.jar
hadoop-client-2.0.5-alpha-gphd-2.1.0.0.jar
hadoop-common-2.2.0.jar
hadoop-distcp-2.2.0.jar
hadoop-hdfs-2.2.0.jar
hadoop-mapreduce-client-app-2.2.0.jar
hadoop-mapreduce-client-common-2.2.0.jar
hadoop-mapreduce-client-core-2.2.0.jar
hadoop-mapreduce-client-jobclient-2.2.0.jar
hadoop-mapreduce-client-shuffle-2.2.0.jar
hadoop-streaming-2.2.0.jar
hadoop-yarn-api-2.2.0.jar
hadoop-yarn-client-2.2.0.jar
hadoop-yarn-common-2.2.0.jar
hadoop-yarn-server-common-2.2.0.jar
hadoop-yarn-server-nodemanager-2.2.0.jar
jersey-core-1.9.jar
jersey-server-1.9.jar
jetty-util-6.1.26.jar
protobuf-java-2.5.0.jar
spring-data-hadoop-2.0.1.RELEASE.jar
spring-data-hadoop-batch-2.0.1.RELEASE.jar
spring-data-hadoop-core-2.0.1.RELEASE.jar
spring-data-hadoop-store-2.0.1.RELEASE.jar
",5.0,0.9974774017237756,0.4059281059491276,-1.0,True,False,True,1405342864000.0,1405352354000.0,1405352378000.0,1405351478000.0,1405352378000.0,1405342864000.0,-1.0,-1.0,-1.0,1405346726000.0,1405350326000.0,1405352354000.0,1405352378000.0,"thomas.risberg",-1.0,-1.0,0,1405342864000.0,-1.0,"Dependendcies for Hadoop distros are broken ","We used to have distro specific jars i the lib/[distro] directory. That is no longer working and all distros seem to contain mostly the same version (hadoop 2.2.0 dependencies)

This is the list for phd1 now:

avro-1.7.5.jar
hadoop-annotations-2.2.0.jar
hadoop-auth-2.2.0.jar
hadoop-client-2.0.5-alpha-gphd-2.1.0.0.jar
hadoop-common-2.2.0.jar
hadoop-distcp-2.2.0.jar
hadoop-hdfs-2.2.0.jar
hadoop-mapreduce-client-app-2.2.0.jar
hadoop-mapreduce-client-common-2.2.0.jar
hadoop-mapreduce-client-core-2.2.0.jar
hadoop-mapreduce-client-jobclient-2.2.0.jar
hadoop-mapreduce-client-shuffle-2.2.0.jar
hadoop-streaming-2.2.0.jar
hadoop-yarn-api-2.2.0.jar
hadoop-yarn-client-2.2.0.jar
hadoop-yarn-common-2.2.0.jar
hadoop-yarn-server-common-2.2.0.jar
hadoop-yarn-server-nodemanager-2.2.0.jar
jersey-core-1.9.jar
jersey-server-1.9.jar
jetty-util-6.1.26.jar
protobuf-java-2.5.0.jar
spring-data-hadoop-2.0.1.RELEASE.jar
spring-data-hadoop-batch-2.0.1.RELEASE.jar
spring-data-hadoop-core-2.0.1.RELEASE.jar
spring-data-hadoop-store-2.0.1.RELEASE.jar
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1965","Story","Sprint 31","2014-07-14T11:49:47.000+0000","iperumal","Ilayaperumal Gopinathan","StepExecutionInfo can not be retrieved in distributed mode","When constructing StepExecutionInfo, the TaskletType class could not be loaded as the spring-data-hadoop-batch jar is missing from admin classpath in distributed mode.

Following exception is thrown:

SEVERE: Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Handler processing failed; nested exception is java.lang.NoClassDefFoundError: org/springframework/data/hadoop/batch/hive/HiveTasklet] with root cause
java.lang.ClassNotFoundException: org.springframework.data.hadoop.batch.hive.HiveTasklet
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at org.springframework.xd.dirt.job.TaskletType.<clinit>(TaskletType.java:57)
	at org.springframework.xd.dirt.job.StepExecutionInfo.<init>(StepExecutionInfo.java:94)
	at org.springframework.xd.dirt.rest.BatchStepExecutionsController.details(BatchStepExecutionsController.java:98)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)",3.0,0.0005360780529645,0.1902719702655373,0.0016797112326221,True,True,True,1405338587000.0,1405338602000.0,1405342202000.0,1405365668000.0,1405366568000.0,1405338587000.0,-1.0,1405338634000.0,1405342234000.0,1405343911000.0,1405347511000.0,1405338602000.0,1405342202000.0,"iperumal",1405342234000.0,1405338587000.0,44,1405338587000.0,-1.0,"StepExecutionInfo can not be retrieved in distributed mode","When constructing StepExecutionInfo, the TaskletType class could not be loaded as the spring-data-hadoop-batch jar is missing from admin classpath in distributed mode.

Following exception is thrown:

SEVERE: Servlet.service() for servlet [dispatcherServlet] in context with path [] threw exception [Handler processing failed; nested exception is java.lang.NoClassDefFoundError: org/springframework/data/hadoop/batch/hive/HiveTasklet] with root cause
java.lang.ClassNotFoundException: org.springframework.data.hadoop.batch.hive.HiveTasklet
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at org.springframework.xd.dirt.job.TaskletType.<clinit>(TaskletType.java:57)
	at org.springframework.xd.dirt.job.StepExecutionInfo.<init>(StepExecutionInfo.java:94)
	at org.springframework.xd.dirt.rest.BatchStepExecutionsController.details(BatchStepExecutionsController.java:98)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)",3.0,False,3.0,0,0.0,-1,0,False
"XD-1964","Story","Sprint 31","2014-07-14T09:32:19.000+0000","mark.pollack","","Servers not finding logging file","12:30:43,930  WARN main logging.LoggingApplicationListener - Logging environment value 'file:/data/projects/spring-xd/build/dist/spring-xd/xd/config///xd-container-logger.properties' cannot be opened and will be ignored

There are extra slashes in there.... probably due to some recent changes related to xd/config location in the scripts.",2.0,0.2769744160177975,0.2769744160177975,0.0,True,True,True,1405330339000.0,1405362709000.0,1405366309000.0,1405446309000.0,1405447209000.0,1405330339000.0,-1.0,1405330339000.0,1405333939000.0,1405362709000.0,1405366309000.0,1405362709000.0,1405366309000.0,"mark.pollack",1405333939000.0,1405330339000.0,44,1405330339000.0,-1.0,"Servers not finding logging file","12:30:43,930  WARN main logging.LoggingApplicationListener - Logging environment value 'file:/data/projects/spring-xd/build/dist/spring-xd/xd/config///xd-container-logger.properties' cannot be opened and will be ignored

There are extra slashes in there.... probably due to some recent changes related to xd/config location in the scripts.",2.0,False,2.0,0,0.0,0,0,False
"XD-1963","Improvement","Sprint 31","2014-07-14T08:50:59.000+0000","mark.fisher","mark.fisher","Upgrade Curator to 2.6.0","Curator 2.6.0 was released on July 11:
https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12314425&version=12327098",1.0,0.0217052404365031,0.0,0.0,True,True,True,1405327859000.0,1405328040000.0,1405331640000.0,1405335298000.0,1405336198000.0,1405327859000.0,-1.0,1405327859000.0,1405331459000.0,1405327859000.0,1405331459000.0,1405328040000.0,1405331640000.0,"mark.fisher",1405331459000.0,1405327859000.0,44,1405327859000.0,-1.0,"Upgrade Curator to 2.6.0","Curator 2.6.0 was released on July 11:
https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12314425&version=12327098",1.0,False,1.0,0,0.0,0,0,False
"XD-1962","Bug","","2014-07-14T05:06:44.000+0000","grenfro","grenfro","Acceptance Tests fail to map some EC2 internal IPs to External IPs","The acceptance tests interrogate the XD-Admin for the containers that are available.  When on EC2 the admin only returns the internal EC2 addresses without the associated suffix of .ec2.internal or .compute-1.internal.   

[Defect]
The acceptance tests only handled the most common suffix of .ec2.internal.  Thus some CI Acceptance tests will fail because, because the container's IPs were not properly mapped.  Thus the acceptance tests should map internal to external IP without regard to the suffixes EC2 issues.

FYI
EC2 issues addresses in 2 different formats: ip-XXX-XXX-XXX-XXX.ec2.internal or domU-XX-XX-XX-XX-XX-XX.compute-1.internal.  The code only able to handle ip-XXX-XXX-XXX-XXX.ec2.internal.  ",3.0,0.0001865193164067,0.0,-1.0,True,False,True,1405314404000.0,1405314412000.0,1405318012000.0,1405356395000.0,1405357295000.0,1405314404000.0,-1.0,-1.0,-1.0,1405314404000.0,1405318004000.0,1405314412000.0,1405318012000.0,"grenfro",-1.0,-1.0,0,1405314404000.0,-1.0,"Acceptance Tests fail to map some EC2 internal IPs to External IPs","The acceptance tests interrogate the XD-Admin for the containers that are available.  When on EC2 the admin only returns the internal EC2 addresses without the associated suffix of .ec2.internal or .compute-1.internal.   

[Defect]
The acceptance tests only handled the most common suffix of .ec2.internal.  Thus some CI Acceptance tests will fail because, because the container's IPs were not properly mapped.  Thus the acceptance tests should map internal to external IP without regard to the suffixes EC2 issues.

FYI
EC2 issues addresses in 2 different formats: ip-XXX-XXX-XXX-XXX.ec2.internal or domU-XX-XX-XX-XX-XX-XX.compute-1.internal.  The code only able to handle ip-XXX-XXX-XXX-XXX.ec2.internal.  ",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1961","Story","Sprint 32","2014-07-12T10:18:41.000+0000","thomas.risberg","eric.bottard","Module info for jdbc sink and jobs are unreadable","The 'module info' command renders text that is pretty much unreadable on a reasonably sized screen. See attached screen shot. Also all the jdbc pool settings are mixed in with module settings making for a confusing list of options. What the heck does 'fairQueue' have to do with filejdbc jobs?",3.0,0.879396405797837,0.535123139011272,0.8151397598437726,True,True,True,1405160321000.0,1405992055000.0,1405995655000.0,1406105222000.0,1406106122000.0,1405160321000.0,-1.0,1405931281000.0,1405934881000.0,1405666441000.0,1405670041000.0,1405992055000.0,1405995655000.0,"thomas.risberg",1405934881000.0,1405160321000.0,44,1405160321000.0,-1.0,"Module info for jdbc sink and jobs are unreadable","The 'module info' command renders text that is pretty much unreadable on a reasonably sized screen. See attached screen shot. Also all the jdbc pool settings are mixed in with module settings making for a confusing list of options. What the heck does 'fairQueue' have to do with filejdbc jobs?",3.0,False,3.0,0,0.0,0,1,False
"XD-1960","Bug","","2014-07-11T16:58:23.000+0000","iperumal","iperumal","Prevent deploying modules of same type on a given stream/job when new leadership election happens","When the leadership election happens, the new deployment supervisor's container listener tries to deploy unallocated modules (via ArrivingContainerModuleRedeployer) into existing container that has the modules of the same type on a given stream/job already deployed.

Currently, on a given stream/job we don't allow more than one deployment of the same module type and there by avoiding any conflicting properties for the given module type.
",2.0,0.0008846646780614,0.0,-1.0,True,False,True,1405097903000.0,1405098137000.0,1405101737000.0,1405361510000.0,1405362410000.0,1405097903000.0,-1.0,-1.0,-1.0,1405097903000.0,1405101503000.0,1405098137000.0,1405101737000.0,"iperumal",-1.0,-1.0,0,1405097903000.0,-1.0,"Prevent deploying modules of same type on a given stream/job when new leadership election happens","When the leadership election happens, the new deployment supervisor's container listener tries to deploy unallocated modules (via ArrivingContainerModuleRedeployer) into existing container that has the modules of the same type on a given stream/job already deployed.

Currently, on a given stream/job we don't allow more than one deployment of the same module type and there by avoiding any conflicting properties for the given module type.
",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1959","Bug","Sprint 31","2014-07-11T13:44:24.000+0000","pperalta","pperalta","NoNodeException after bouncing admin server","Steps to reproduce:

h6. 1. Clear out ZK
{code}
[zk: localhost:2181(CONNECTED) 0] rmr /xd
{code}

h6. 2. Start admin

h6. 3. Deploy stream
{code}
xd:>stream create --name tt --definition ""time|log"" --deploy 
{code}

Admin log:
{code}
16:38:10,537  INFO Deployer server.StreamDeploymentListener - Deploying stream Stream{name='tt'}
16:38:10,545  WARN Deployer server.StreamDeploymentListener - No containers available for deployment of module 'log' for stream 'tt'
16:38:10,547  WARN Deployer server.StreamDeploymentListener - No containers available for deployment of module 'time' for stream 'tt'
16:38:10,547  INFO Deployer server.StreamDeploymentListener - Deployment status for stream 'tt': DeploymentStatus{state=failed}
16:38:10,550  INFO Deployer server.StreamDeploymentListener - Stream Stream{name='tt'} deployment attempt complete
{code}

h6. 4. Shut down and restart admin. The following is logged:
{code}
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/tt/modules
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
	at org.springframework.xd.dirt.server.StreamDeploymentListener.recalculateStreamStates(StreamDeploymentListener.java:207)
	at org.springframework.xd.dirt.server.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:352)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)

{code}
",1.0,0.2142394822006472,0.0,0.3902912621359223,True,True,True,1405086264000.0,1405086595000.0,1405087809000.0,1405086909000.0,1405087809000.0,1405086264000.0,1405087336000.0,1405086867000.0,1405087809000.0,1405086264000.0,1405087809000.0,1405086595000.0,1405087809000.0,"pperalta",1405087809000.0,1405087336000.0,44,1405087336000.0,-1.0,"NoNodeException after bouncing admin server","Steps to reproduce:

h6. 1. Clear out ZK
{code}
[zk: localhost:2181(CONNECTED) 0] rmr /xd
{code}

h6. 2. Start admin

h6. 3. Deploy stream
{code}
xd:>stream create --name tt --definition ""time|log"" --deploy 
{code}

Admin log:
{code}
16:38:10,537  INFO Deployer server.StreamDeploymentListener - Deploying stream Stream{name='tt'}
16:38:10,545  WARN Deployer server.StreamDeploymentListener - No containers available for deployment of module 'log' for stream 'tt'
16:38:10,547  WARN Deployer server.StreamDeploymentListener - No containers available for deployment of module 'time' for stream 'tt'
16:38:10,547  INFO Deployer server.StreamDeploymentListener - Deployment status for stream 'tt': DeploymentStatus{state=failed}
16:38:10,550  INFO Deployer server.StreamDeploymentListener - Stream Stream{name='tt'} deployment attempt complete
{code}

h6. 4. Shut down and restart admin. The following is logged:
{code}
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/tt/modules
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
	at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
	at org.springframework.xd.dirt.server.StreamDeploymentListener.recalculateStreamStates(StreamDeploymentListener.java:207)
	at org.springframework.xd.dirt.server.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:352)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)

{code}
",1.0,False,1.0,0,0.0,0,0,False
"XD-1958","Bug","Sprint 31","2014-07-11T12:59:17.000+0000","thomas.risberg","","filejdbc job broken in distributed mode","The filejdbc job is broken in distributed mode (redis and rabbit)

To reproduce:

export XD_TRANSPORT=rabbit

start xd-admin
start xd-container

start shell and create this job:

{code}
>job create mydata --definition ""filejdbc --names=col1,col2,col3 --resources=file:///home/trisberg/Test/input/*.csv --initializeDatabase=true"" --deploy
>job launch mydata
{code}

results in JOB starting but never completing:

{code}
>job execution list
  Id  Job Name  Start Time                            Step Execution Count  Execution Status  Deployment Status  Definition Status
  --  --------  ------------------------------------  --------------------  ----------------  -----------------  -----------------
    0  mydata      2014-07-11 15:44:33 America/New_York  0                     STARTED           Deployed           Exists
{code}

Steps:

{code}
Step Id	Step Name	Reads	Writes	Commits	Rollbacks	Duration	Status	Details
0	step1-master	0	0	0	0	-1405349644032 ms	EXECUTING	
1	step1-master:partition0	292	292	3	0	302 ms	COMPLETED	
2	step1-master:partition1	292	292	3	0	203 ms	COMPLETED	
3	step1-master:partition2	292	292	3	0	193 ms	COMPLETED	
{code}

When using Redis, I also get this stacktrace in container:

{code}
15:40:51,220  INFO DeploymentsPathChildrenCache-0 boot.SpringApplication - Started application in 1.965 seconds (JVM running for 66.949)
15:40:51,220  INFO DeploymentsPathChildrenCache-0 core.SimpleModule - initialized module: SimpleModule [name=filejdbc, type=job, group=job1, index=0 @64a28a58]
15:40:51,233  INFO DeploymentsPathChildrenCache-0 redis.RedisMessageBus - binding requestor: job1.0
15:40:51,236  INFO DeploymentsPathChildrenCache-0 redis.RedisMessageBus - binding replier: job1.0
15:40:51,243  INFO DeploymentsPathChildrenCache-0 module.ModuleDeployer - deployed SimpleModule [name=filejdbc, type=job, group=job1, index=0 @64a28a58]
15:40:57,110 ERROR inbound.job1.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'
org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.integration.aggregator.AggregatingMessageHandler#0]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy84.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy78.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:251)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:247)
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:247)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IllegalStateException: Null correlation not allowed.  Maybe the CorrelationStrategy is failing?
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.integration.aggregator.AbstractCorrelatingMessageHandler.handleMessageInternal(AbstractCorrelatingMessageHandler.java:383)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 60 more
15:41:00,129 ERROR inbound.job1.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'
org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.integration.aggregator.AggregatingMessageHandler#0]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy84.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy78.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:251)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:247)
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:247)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IllegalStateException: Null correlation not allowed.  Maybe the CorrelationStrategy is failing?
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.integration.aggregator.AbstractCorrelatingMessageHandler.handleMessageInternal(AbstractCorrelatingMessageHandler.java:383)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 60 more
{code}
",5.0,0.884268000027663,0.8645995421758405,0.8262515819830287,True,True,True,1405083557000.0,1405339282000.0,1405342882000.0,1405371851000.0,1405372751000.0,1405083557000.0,-1.0,1405322504000.0,1405326104000.0,1405333594000.0,1405337194000.0,1405339282000.0,1405342882000.0,"thomas.risberg",1405326104000.0,1405083557000.0,44,1405083557000.0,-1.0,"filejdbc job broken in distributed mode","The filejdbc job is broken in distributed mode (redis and rabbit)

To reproduce:

export XD_TRANSPORT=rabbit

start xd-admin
start xd-container

start shell and create this job:

{code}
>job create mydata --definition ""filejdbc --names=col1,col2,col3 --resources=file:///home/trisberg/Test/input/*.csv --initializeDatabase=true"" --deploy
>job launch mydata
{code}

results in JOB starting but never completing:

{code}
>job execution list
  Id  Job Name  Start Time                            Step Execution Count  Execution Status  Deployment Status  Definition Status
  --  --------  ------------------------------------  --------------------  ----------------  -----------------  -----------------
    0  mydata      2014-07-11 15:44:33 America/New_York  0                     STARTED           Deployed           Exists
{code}

Steps:

{code}
Step Id	Step Name	Reads	Writes	Commits	Rollbacks	Duration	Status	Details
0	step1-master	0	0	0	0	-1405349644032 ms	EXECUTING	
1	step1-master:partition0	292	292	3	0	302 ms	COMPLETED	
2	step1-master:partition1	292	292	3	0	203 ms	COMPLETED	
3	step1-master:partition2	292	292	3	0	193 ms	COMPLETED	
{code}

When using Redis, I also get this stacktrace in container:

{code}
15:40:51,220  INFO DeploymentsPathChildrenCache-0 boot.SpringApplication - Started application in 1.965 seconds (JVM running for 66.949)
15:40:51,220  INFO DeploymentsPathChildrenCache-0 core.SimpleModule - initialized module: SimpleModule [name=filejdbc, type=job, group=job1, index=0 @64a28a58]
15:40:51,233  INFO DeploymentsPathChildrenCache-0 redis.RedisMessageBus - binding requestor: job1.0
15:40:51,236  INFO DeploymentsPathChildrenCache-0 redis.RedisMessageBus - binding replier: job1.0
15:40:51,243  INFO DeploymentsPathChildrenCache-0 module.ModuleDeployer - deployed SimpleModule [name=filejdbc, type=job, group=job1, index=0 @64a28a58]
15:40:57,110 ERROR inbound.job1.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'
org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.integration.aggregator.AggregatingMessageHandler#0]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy84.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy78.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:251)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:247)
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:247)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IllegalStateException: Null correlation not allowed.  Maybe the CorrelationStrategy is failing?
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.integration.aggregator.AbstractCorrelatingMessageHandler.handleMessageInternal(AbstractCorrelatingMessageHandler.java:383)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 60 more
15:41:00,129 ERROR inbound.job1.0-redis:queue-inbound-channel-adapter1 redis.RedisMessageBus$1 - Failed to deliver message; retries exhausted; message sent to queue 'ERRORS:name'
org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.integration.aggregator.AggregatingMessageHandler#0]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy84.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor89.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy78.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:251)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1$1.doWithRetry(RedisMessageBus.java:247)
	at org.springframework.retry.support.RetryTemplate.doExecute(RetryTemplate.java:263)
	at org.springframework.retry.support.RetryTemplate.execute(RetryTemplate.java:168)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$1.doSend(RedisMessageBus.java:247)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.IllegalStateException: Null correlation not allowed.  Maybe the CorrelationStrategy is failing?
	at org.springframework.util.Assert.state(Assert.java:385)
	at org.springframework.integration.aggregator.AbstractCorrelatingMessageHandler.handleMessageInternal(AbstractCorrelatingMessageHandler.java:383)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 60 more
{code}
",5.0,False,5.0,0,0.0,0,3,False
"XD-1957","Improvement","","2014-07-11T11:40:42.000+0000","iperumal","iperumal","Remove footer from admin UI","Please see the discussion here:

https://github.com/spring-projects/spring-xd/pull/1052#issuecomment-48761686",1.0,-1.0,0.0,-1.0,False,False,True,1405078842000.0,-1.0,-1.0,1405437193000.0,1405438093000.0,1405078842000.0,-1.0,-1.0,-1.0,1405078842000.0,1405082442000.0,-1.0,-1.0,"iperumal",-1.0,-1.0,0,1405078842000.0,-1.0,"Remove footer from admin UI","Please see the discussion here:

https://github.com/spring-projects/spring-xd/pull/1052#issuecomment-48761686",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1956","Bug","","2014-07-11T11:16:59.000+0000","thomas.risberg","","filepollhdfs --deleteFiles=true has no effect, files are not deleted","Setting --deleteFiles=true has no effect any longer. This also causes the Script Integration Tests to fail.

Suspect this is related to the change here https://github.com/spring-projects/spring-xd/commit/6dbac167758ce23b9a4dbf07169b2d26d1eddef1
",3.0,-1.0,0.9993265993265992,-1.0,False,False,True,1405077419000.0,-1.0,-1.0,1405086914000.0,1405087814000.0,1405077419000.0,-1.0,-1.0,-1.0,1405087807000.0,1405087814000.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1405077419000.0,-1.0,"filepollhdfs --deleteFiles=true has no effect, files are not deleted","Setting --deleteFiles=true has no effect any longer. This also causes the Script Integration Tests to fail.

Suspect this is related to the change here https://github.com/spring-projects/spring-xd/commit/6dbac167758ce23b9a4dbf07169b2d26d1eddef1
",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1954","Bug","Sprint 32","2014-07-11T10:33:13.000+0000","iperumal","iperumal","Investigate deployed module context close upon container shutdown","Upon the container shutdown, the deployed modules' contexts get closed before the corresponding `Stream/JobModuleWatcher` does the undeployment of the stream/job modules.",3.0,0.0003876239338912,0.0,0.9793358709932664,True,True,True,1405074793000.0,1405075132000.0,1405078732000.0,1405948452000.0,1405949352000.0,1405074793000.0,-1.0,1405931280000.0,1405934880000.0,1405074793000.0,1405078393000.0,1405075132000.0,1405078732000.0,"iperumal",1405934880000.0,1405074793000.0,44,1405074793000.0,-1.0,"Investigate deployed module context close upon container shutdown","Upon the container shutdown, the deployed modules' contexts get closed before the corresponding `Stream/JobModuleWatcher` does the undeployment of the stream/job modules.",3.0,False,3.0,0,0.0,0,0,False
"XD-1953","Bug","","2014-07-11T10:09:24.000+0000","iperumal","iperumal","Stacktrace on container with deployed modules is shutdown","When the container that has deployed module is shutdown, following stacktrace is thrown:

10:10:27,560  INFO main-EventThread server.ContainerRegistrar:254 - Undeploying module [ModuleDescriptor@3a615460 moduleName = 'job', moduleLabel = 'job', group = 'j4', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map[[empty]], children = list[[empty]]]
10:10:27,560  INFO main-EventThread module.ModuleDeployer:158 - removed SimpleModule [name=job, type=job, group=j4, index=0 @7df1aff2]
10:10:27,561 ERROR main-EventThread imps.CuratorFrameworkImpl:555 - Watcher exception
java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@422fd7b7 has been closed already
	at org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:956)
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:978)
	at org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:164)
	at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.unbindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:219)
	at org.springframework.xd.dirt.plugins.job.JobPlugin.removeModule(JobPlugin.java:70)
	at org.springframework.xd.dirt.module.ModuleDeployer.removeModule(ModuleDeployer.java:204)
	at org.springframework.xd.dirt.module.ModuleDeployer.destroyModule(ModuleDeployer.java:162)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleUndeploy(ModuleDeployer.java:140)
	at org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:112)
	at org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:256)
	at org.springframework.xd.dirt.server.ContainerRegistrar$JobModuleWatcher.process(ContainerRegistrar.java:753)
	at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
10:10:27,561  INFO main-EventThread zookeeper.ClientCnxn:512 - EventThread shut down
10:10:27,564  INFO Thread-2 jmx.EndpointMBeanExporter:433 - Unregistering JMX-exposed beans on shutdown",2.0,0.0005036831832777,0.0,-1.0,True,False,True,1405073364000.0,1405073372000.0,1405076972000.0,1405088347000.0,1405089247000.0,1405073364000.0,-1.0,-1.0,-1.0,1405073364000.0,1405076964000.0,1405073372000.0,1405076972000.0,"iperumal",-1.0,-1.0,0,1405073364000.0,-1.0,"Stacktrace on container with deployed modules is shutdown","When the container that has deployed module is shutdown, following stacktrace is thrown:

10:10:27,560  INFO main-EventThread server.ContainerRegistrar:254 - Undeploying module [ModuleDescriptor@3a615460 moduleName = 'job', moduleLabel = 'job', group = 'j4', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map[[empty]], children = list[[empty]]]
10:10:27,560  INFO main-EventThread module.ModuleDeployer:158 - removed SimpleModule [name=job, type=job, group=j4, index=0 @7df1aff2]
10:10:27,561 ERROR main-EventThread imps.CuratorFrameworkImpl:555 - Watcher exception
java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@422fd7b7 has been closed already
	at org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:956)
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:978)
	at org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:164)
	at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.unbindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:219)
	at org.springframework.xd.dirt.plugins.job.JobPlugin.removeModule(JobPlugin.java:70)
	at org.springframework.xd.dirt.module.ModuleDeployer.removeModule(ModuleDeployer.java:204)
	at org.springframework.xd.dirt.module.ModuleDeployer.destroyModule(ModuleDeployer.java:162)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleUndeploy(ModuleDeployer.java:140)
	at org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:112)
	at org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:256)
	at org.springframework.xd.dirt.server.ContainerRegistrar$JobModuleWatcher.process(ContainerRegistrar.java:753)
	at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:67)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
10:10:27,561  INFO main-EventThread zookeeper.ClientCnxn:512 - EventThread shut down
10:10:27,564  INFO Thread-2 jmx.EndpointMBeanExporter:433 - Unregistering JMX-exposed beans on shutdown",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1950","Bug","Sprint 31","2014-07-10T12:50:17.000+0000","iperumal","iperumal","Single step partition support on filejdbc module uses module's datasource","The filejdbc module's single step partition support configures to use jdbc module's datasource rather than XD's batch datasource.

```
org.springframework.messaging.MessageHandlingException: org.springframework.jdbc.UncategorizedSQLException: PreparedStatementCallback; uncategorized SQLException for SQL [SELECT JOB_EXECUTION_ID, START_TIME, END_TIME, STATUS, EXIT_CODE, EXIT_MESSAGE, CREATE_TIME, LAST_UPDATED, VERSION, JOB_CONFIGURATION_LOCATION from BATCH_JOB_EXECUTION where JOB_EXECUTION_ID = ?]; SQL state [null]; error code [0]; [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION); nested exception is java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:78)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy117.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy115.send(Unknown Source)
	at org.springframework.xd.dirt.integration.bus.LocalMessageBus$3.handleMessage(LocalMessageBus.java:188)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.access$000(UnicastingDispatcher.java:48)
	at org.springframework.integration.dispatcher.UnicastingDispatcher$1.run(UnicastingDispatcher.java:92)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.springframework.jdbc.UncategorizedSQLException: PreparedStatementCallback; uncategorized SQLException for SQL [SELECT JOB_EXECUTION_ID, START_TIME, END_TIME, STATUS, EXIT_CODE, EXIT_MESSAGE, CREATE_TIME, LAST_UPDATED, VERSION, JOB_CONFIGURATION_LOCATION from BATCH_JOB_EXECUTION where JOB_EXECUTION_ID = ?]; SQL state [null]; error code [0]; [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION); nested exception is java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION)
	at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:84)
	at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
	at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
	at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:660)
	at org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:695)
	at org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:727)
	at org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:737)
	at org.springframework.jdbc.core.JdbcTemplate.queryForObject(JdbcTemplate.java:811)
	at org.springframework.batch.core.repository.dao.JdbcJobExecutionDao.getJobExecution(JdbcJobExecutionDao.java:267)
	at org.springframework.batch.core.explore.support.SimpleJobExplorer.getStepExecution(SimpleJobExplorer.java:142)
	at org.springframework.batch.integration.partition.StepExecutionRequestHandler.handle(StepExecutionRequestHandler.java:52)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)
	at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	... 41 more
Caused by: java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION)
	at org.sqlite.DB.newSQLException(DB.java:383)
	at org.sqlite.DB.newSQLException(DB.java:387)
	at org.sqlite.DB.throwex(DB.java:374)
	at org.sqlite.NestedDB.prepare(NestedDB.java:134)
	at org.sqlite.DB.prepare(DB.java:123)
	at org.sqlite.PrepStmt.<init>(PrepStmt.java:42)
	at org.sqlite.Conn.prepareStatement(Conn.java:404)
	at org.sqlite.Conn.prepareStatement(Conn.java:399)
	at org.sqlite.Conn.prepareStatement(Conn.java:383)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.tomcat.jdbc.pool.ProxyConnection.invoke(ProxyConnection.java:126)
	at org.apache.tomcat.jdbc.pool.JdbcInterceptor.invoke(JdbcInterceptor.java:109)
	at org.apache.tomcat.jdbc.pool.DisposableConnectionFacade.invoke(DisposableConnectionFacade.java:80)
	at com.sun.proxy.$Proxy109.prepareStatement(Unknown Source)
	at org.springframework.jdbc.core.JdbcTemplate$SimplePreparedStatementCreator.createPreparedStatement(JdbcTemplate.java:1557)
	at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:638)
	... 63 more
12:23:37,941  INFO main-EventThread server.ContainerRegistrar:254 - Undeploying module [ModuleDescriptor@d192973 moduleName = 'filejdbc', moduleLabel = 'filejdbc', group = 'csvjdbcjob0', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map['resources' -> 'file:///tmp/xdtest/jdbc/delete_after_use.csv', 'initializeDatabase' -> 'true', 'names' -> 'col1,col2,col3', 'deleteFiles' -> 'true', 'driverClassName' -> 'org.sqlite.JDBC', 'url' -> 'jdbc:sqlite:/tmp/xdtest/jdbc/jdbc.db'], children = list[[empty]]]
12:23:37,941  INFO main-EventThread module.ModuleDeployer:158 - removed SimpleModule [name=filejdbc, type=job, group=csvjdbcjob0, index=0 @73cc35b5]
12:23:37,944 ERROR task-scheduler-1 step.AbstractStep:225 - Encountered an error executing step step1-master in job csvjdbcjob0
org.springframework.integration.MessageTimeoutException: Timeout occurred before all partitions returned
	at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:141)
	at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:162)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:141)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy44.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)
	at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy117.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy115.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy117.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy115.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.endpoint.PollingConsumer.handleMessage(PollingConsumer.java:74)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:205)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:284)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:278)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
```",1.0,0.2775817093754795,0.2775817093754795,0.3363510817860979,True,True,True,1404996617000.0,1404998426000.0,1405002026000.0,1405002234000.0,1405003134000.0,1404996617000.0,-1.0,1404998809000.0,1405002409000.0,1404998426000.0,1405002026000.0,1404998426000.0,1405002026000.0,"iperumal",1405002409000.0,1404997055000.0,44,1404997055000.0,-1.0,"Single step partition support on filejdbc module uses module's datasource","The filejdbc module's single step partition support configures to use jdbc module's datasource rather than XD's batch datasource.

```
org.springframework.messaging.MessageHandlingException: org.springframework.jdbc.UncategorizedSQLException: PreparedStatementCallback; uncategorized SQLException for SQL [SELECT JOB_EXECUTION_ID, START_TIME, END_TIME, STATUS, EXIT_CODE, EXIT_MESSAGE, CREATE_TIME, LAST_UPDATED, VERSION, JOB_CONFIGURATION_LOCATION from BATCH_JOB_EXECUTION where JOB_EXECUTION_ID = ?]; SQL state [null]; error code [0]; [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION); nested exception is java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:78)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy117.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy115.send(Unknown Source)
	at org.springframework.xd.dirt.integration.bus.LocalMessageBus$3.handleMessage(LocalMessageBus.java:188)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.access$000(UnicastingDispatcher.java:48)
	at org.springframework.integration.dispatcher.UnicastingDispatcher$1.run(UnicastingDispatcher.java:92)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.springframework.jdbc.UncategorizedSQLException: PreparedStatementCallback; uncategorized SQLException for SQL [SELECT JOB_EXECUTION_ID, START_TIME, END_TIME, STATUS, EXIT_CODE, EXIT_MESSAGE, CREATE_TIME, LAST_UPDATED, VERSION, JOB_CONFIGURATION_LOCATION from BATCH_JOB_EXECUTION where JOB_EXECUTION_ID = ?]; SQL state [null]; error code [0]; [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION); nested exception is java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION)
	at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:84)
	at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
	at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:81)
	at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:660)
	at org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:695)
	at org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:727)
	at org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:737)
	at org.springframework.jdbc.core.JdbcTemplate.queryForObject(JdbcTemplate.java:811)
	at org.springframework.batch.core.repository.dao.JdbcJobExecutionDao.getJobExecution(JdbcJobExecutionDao.java:267)
	at org.springframework.batch.core.explore.support.SimpleJobExplorer.getStepExecution(SimpleJobExplorer.java:142)
	at org.springframework.batch.integration.partition.StepExecutionRequestHandler.handle(StepExecutionRequestHandler.java:52)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)
	at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	... 41 more
Caused by: java.sql.SQLException: [SQLITE_ERROR] SQL error or missing database (no such table: BATCH_JOB_EXECUTION)
	at org.sqlite.DB.newSQLException(DB.java:383)
	at org.sqlite.DB.newSQLException(DB.java:387)
	at org.sqlite.DB.throwex(DB.java:374)
	at org.sqlite.NestedDB.prepare(NestedDB.java:134)
	at org.sqlite.DB.prepare(DB.java:123)
	at org.sqlite.PrepStmt.<init>(PrepStmt.java:42)
	at org.sqlite.Conn.prepareStatement(Conn.java:404)
	at org.sqlite.Conn.prepareStatement(Conn.java:399)
	at org.sqlite.Conn.prepareStatement(Conn.java:383)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.tomcat.jdbc.pool.ProxyConnection.invoke(ProxyConnection.java:126)
	at org.apache.tomcat.jdbc.pool.JdbcInterceptor.invoke(JdbcInterceptor.java:109)
	at org.apache.tomcat.jdbc.pool.DisposableConnectionFacade.invoke(DisposableConnectionFacade.java:80)
	at com.sun.proxy.$Proxy109.prepareStatement(Unknown Source)
	at org.springframework.jdbc.core.JdbcTemplate$SimplePreparedStatementCreator.createPreparedStatement(JdbcTemplate.java:1557)
	at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:638)
	... 63 more
12:23:37,941  INFO main-EventThread server.ContainerRegistrar:254 - Undeploying module [ModuleDescriptor@d192973 moduleName = 'filejdbc', moduleLabel = 'filejdbc', group = 'csvjdbcjob0', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = job, parameters = map['resources' -> 'file:///tmp/xdtest/jdbc/delete_after_use.csv', 'initializeDatabase' -> 'true', 'names' -> 'col1,col2,col3', 'deleteFiles' -> 'true', 'driverClassName' -> 'org.sqlite.JDBC', 'url' -> 'jdbc:sqlite:/tmp/xdtest/jdbc/jdbc.db'], children = list[[empty]]]
12:23:37,941  INFO main-EventThread module.ModuleDeployer:158 - removed SimpleModule [name=filejdbc, type=job, group=csvjdbcjob0, index=0 @73cc35b5]
12:23:37,944 ERROR task-scheduler-1 step.AbstractStep:225 - Encountered an error executing step step1-master in job csvjdbcjob0
org.springframework.integration.MessageTimeoutException: Timeout occurred before all partitions returned
	at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:141)
	at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:106)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:148)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:162)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:141)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:127)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy44.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)
	at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy117.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy115.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy117.handleMessage(Unknown Source)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.GeneratedMethodAccessor107.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy115.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:109)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:94)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.endpoint.PollingConsumer.handleMessage(PollingConsumer.java:74)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:205)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:284)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:278)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
```",1.0,False,1.0,0,0.0,0,0,False
"XD-1949","Story","Sprint 32","2014-07-10T10:39:58.000+0000","mark.pollack","","Ensure DSM matrix is diagonal","",2.0,0.8916222128410791,0.8916222128410791,0.7381635081019398,True,True,True,1404988798000.0,1406127215000.0,1406130815000.0,1406264691000.0,1406265591000.0,1404988798000.0,-1.0,1405931280000.0,1405934880000.0,1406127215000.0,1406130815000.0,1406127215000.0,1406130815000.0,"mark.pollack",1405934880000.0,1404988798000.0,44,1404988798000.0,-1.0,"Ensure DSM matrix is diagonal","",2.0,False,2.0,0,0.0,0,0,False
"XD-1948","Story","Sprint 31","2014-07-10T10:32:10.000+0000","thomas.risberg","thomas.risberg","Build should use Spring Boot plugin version 1.1.4 ","The platform uses Boot version 1.1.4 so the plugin version used in build.gradle should match that.",1.0,0.0786145021326665,0.0786145021326665,0.0760405942050301,True,True,True,1404988330000.0,1404989399000.0,1404992999000.0,1405001028000.0,1405001928000.0,1404988330000.0,-1.0,1404989364000.0,1404992964000.0,1404989399000.0,1404992999000.0,1404989399000.0,1404992999000.0,"thomas.risberg",1404992964000.0,1404988330000.0,44,1404988330000.0,-1.0,"Build should use Spring Boot plugin version 1.1.4 ","The platform uses Boot version 1.1.4 so the plugin version used in build.gradle should match that.",1.0,False,1.0,0,0.0,0,0,False
"XD-1947","Story","Sprint 31","2014-07-10T08:58:11.000+0000","eric.bottard","eric.bottard","Fix support for @CliAvailabilityIndicator","See PR https://github.com/spring-projects/spring-xd/pull/1043/",3.0,-1.0,0.0,0.0,False,True,True,1404982691000.0,-1.0,-1.0,1404988083000.0,1404988983000.0,1404982691000.0,-1.0,1404982691000.0,1404986291000.0,1404982691000.0,1404986291000.0,-1.0,-1.0,"eric.bottard",1404986291000.0,1404982691000.0,44,1404982691000.0,-1.0,"Fix support for @CliAvailabilityIndicator","See PR https://github.com/spring-projects/spring-xd/pull/1043/",3.0,False,3.0,0,0.0,-1,0,False
"XD-1946","Story","","2014-07-10T08:54:00.000+0000","mark.pollack","","Add stream state tests","Test to verify stream state is correct after starting/stopping containers.",3.0,-1.0,-1.0,-1.0,False,False,False,1404982440000.0,-1.0,-1.0,1405922863000.0,1405923763000.0,1404982440000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1404982440000.0,-1.0,"Add stream state tests","Test to verify stream state is correct after starting/stopping containers.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1945","Bug","","2014-07-10T06:51:45.000+0000","thomas.risberg","","XD_CONFIG_LOCATION doesn't seem to be set for log4j config files ","Starting xd-singlenode and then ctrl-c to shut down produces WARN message that should be suppressed according to log4j config

Setting XD_CONFIG_LOCATION explicitly works for suppressing the message.

The message I see:

[2014-07-10 09:39:59.786] boot - 58034  WARN [Thread-2] --- MBeanRegistry: Failed to unregister MBean InMemoryDataTree
[2014-07-10 09:39:59.786] boot - 58034  WARN [Thread-2] --- MBeanRegistry: Error during unregister
javax.management.InstanceNotFoundException: org.apache.ZooKeeperService:name0=StandaloneServer_port-1,name1=InMemoryDataTree
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.zookeeper.jmx.MBeanRegistry.unregister(MBeanRegistry.java:115)
	at org.apache.zookeeper.jmx.MBeanRegistry.unregister(MBeanRegistry.java:132)
	at org.apache.zookeeper.server.ZooKeeperServer.unregisterJMX(ZooKeeperServer.java:465)
	at org.apache.zookeeper.server.ZooKeeperServer.shutdown(ZooKeeperServer.java:458)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.shutdown(NIOServerCnxnFactory.java:271)
	at org.apache.zookeeper.server.ZooKeeperServerMain.shutdown(ZooKeeperServerMain.java:132)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.xd.dirt.zookeeper.EmbeddedZooKeeper.stop(EmbeddedZooKeeper.java:176)
	at org.springframework.xd.dirt.zookeeper.EmbeddedZooKeeper.stop(EmbeddedZooKeeper.java:204)
	at org.springframework.context.support.DefaultLifecycleProcessor.doStop(DefaultLifecycleProcessor.java:229)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$300(DefaultLifecycleProcessor.java:51)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.stop(DefaultLifecycleProcessor.java:363)
	at org.springframework.context.support.DefaultLifecycleProcessor.stopBeans(DefaultLifecycleProcessor.java:202)
	at org.springframework.context.support.DefaultLifecycleProcessor.onClose(DefaultLifecycleProcessor.java:118)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:888)
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:841)
	at org.springframework.boot.builder.ParentContextCloserApplicationListener$ContextCloserListener.onApplicationEvent(ParentContextCloserApplicationListener.java:100)
	at org.springframework.boot.builder.ParentContextCloserApplicationListener$ContextCloserListener.onApplicationEvent(ParentContextCloserApplicationListener.java:84)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:98)
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:333)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:880)
	at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:809)
",3.0,-1.0,0.2032034749558843,-1.0,False,False,True,1404975105000.0,-1.0,-1.0,1404981572000.0,1404982472000.0,1404975105000.0,-1.0,-1.0,-1.0,1404976602000.0,1404980202000.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1404975105000.0,-1.0,"XD_CONFIG_LOCATION doesn't seem to be set for log4j config files ","Starting xd-singlenode and then ctrl-c to shut down produces WARN message that should be suppressed according to log4j config

Setting XD_CONFIG_LOCATION explicitly works for suppressing the message.

The message I see:

[2014-07-10 09:39:59.786] boot - 58034  WARN [Thread-2] --- MBeanRegistry: Failed to unregister MBean InMemoryDataTree
[2014-07-10 09:39:59.786] boot - 58034  WARN [Thread-2] --- MBeanRegistry: Error during unregister
javax.management.InstanceNotFoundException: org.apache.ZooKeeperService:name0=StandaloneServer_port-1,name1=InMemoryDataTree
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.exclusiveUnregisterMBean(DefaultMBeanServerInterceptor.java:427)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.unregisterMBean(DefaultMBeanServerInterceptor.java:415)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.unregisterMBean(JmxMBeanServer.java:546)
	at org.apache.zookeeper.jmx.MBeanRegistry.unregister(MBeanRegistry.java:115)
	at org.apache.zookeeper.jmx.MBeanRegistry.unregister(MBeanRegistry.java:132)
	at org.apache.zookeeper.server.ZooKeeperServer.unregisterJMX(ZooKeeperServer.java:465)
	at org.apache.zookeeper.server.ZooKeeperServer.shutdown(ZooKeeperServer.java:458)
	at org.apache.zookeeper.server.NIOServerCnxnFactory.shutdown(NIOServerCnxnFactory.java:271)
	at org.apache.zookeeper.server.ZooKeeperServerMain.shutdown(ZooKeeperServerMain.java:132)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.xd.dirt.zookeeper.EmbeddedZooKeeper.stop(EmbeddedZooKeeper.java:176)
	at org.springframework.xd.dirt.zookeeper.EmbeddedZooKeeper.stop(EmbeddedZooKeeper.java:204)
	at org.springframework.context.support.DefaultLifecycleProcessor.doStop(DefaultLifecycleProcessor.java:229)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$300(DefaultLifecycleProcessor.java:51)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.stop(DefaultLifecycleProcessor.java:363)
	at org.springframework.context.support.DefaultLifecycleProcessor.stopBeans(DefaultLifecycleProcessor.java:202)
	at org.springframework.context.support.DefaultLifecycleProcessor.onClose(DefaultLifecycleProcessor.java:118)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:888)
	at org.springframework.context.support.AbstractApplicationContext.close(AbstractApplicationContext.java:841)
	at org.springframework.boot.builder.ParentContextCloserApplicationListener$ContextCloserListener.onApplicationEvent(ParentContextCloserApplicationListener.java:100)
	at org.springframework.boot.builder.ParentContextCloserApplicationListener$ContextCloserListener.onApplicationEvent(ParentContextCloserApplicationListener.java:84)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:98)
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:333)
	at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:880)
	at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:809)
",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1944","Bug","Sprint 31","2014-07-10T06:29:48.000+0000","thomas.risberg","pperalta","Error deploying stream when admin running and container arrives after stream deployment request","Steps to reproduce:

1. start xd-admin

2. start shell and create and deploy stream (""time | hdfs"")

3. start container

I got:

[2014-07-10 09:10:29.019] boot - 19923INFO [DeploymentSupervisorCacheListener-0] --- InitialDeploymentListener: Path cache event: /deployments/streams/test, type: CHILD_ADDED
[2014-07-10 09:10:29.137] boot - 19923INFO [Deployer] --- StreamDeploymentListener: Deploying stream Stream{name='test'}
[2014-07-10 09:10:29.146] boot - 19923WARN [Deployer] --- StreamDeploymentListener: No containers available for deployment of stream test
[2014-07-10 09:10:29.146] boot - 19923INFO [Deployer] --- StreamDeploymentListener: Stream Stream{name='test'} deployment attempt complete
[2014-07-10 09:11:08.003] boot - 19923INFO [DeploymentSupervisorCacheListener-0] --- ContainerListener: Path cache event: /containers/007c2bcc-13f4-466e-95d3-bd926bb456ea, type: CHILD_ADDED
[2014-07-10 09:11:08.006] boot - 19923INFO [DeploymentSupervisorCacheListener-0] --- ArrivingContainerModuleRedeployer: Container arrived: 007c2bcc-13f4-466e-95d3-bd926bb456ea
[2014-07-10 09:11:08.176] boot - 19923 ERROR [DeploymentSupervisorCacheListener-0] --- PathChildrenCache: 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/test/modules
at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
at org.springframework.xd.dirt.server.ArrivingContainerModuleRedeployer.deployUnallocatedStreamModules(ArrivingContainerModuleRedeployer.java:133)
at org.springframework.xd.dirt.server.ArrivingContainerModuleRedeployer.deployModules(ArrivingContainerModuleRedeployer.java:106)
at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:99)
at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:744)
",3.0,0.154439637871828,0.1543058466752887,0.3994113187352272,True,True,True,1404973788000.0,1404977251000.0,1404980851000.0,1404995311000.0,1404996211000.0,1404973788000.0,-1.0,1404982744000.0,1404986344000.0,1404977248000.0,1404980848000.0,1404977251000.0,1404980851000.0,"thomas.risberg",1404986344000.0,1404973788000.0,44,1404973788000.0,-1.0,"Error deploying stream when admin running and container arrives after stream deployment request","Steps to reproduce:

1. start xd-admin

2. start shell and create and deploy stream (""time | hdfs"")

3. start container

I got:

[2014-07-10 09:10:29.019] boot - 19923INFO [DeploymentSupervisorCacheListener-0] --- InitialDeploymentListener: Path cache event: /deployments/streams/test, type: CHILD_ADDED
[2014-07-10 09:10:29.137] boot - 19923INFO [Deployer] --- StreamDeploymentListener: Deploying stream Stream{name='test'}
[2014-07-10 09:10:29.146] boot - 19923WARN [Deployer] --- StreamDeploymentListener: No containers available for deployment of stream test
[2014-07-10 09:10:29.146] boot - 19923INFO [Deployer] --- StreamDeploymentListener: Stream Stream{name='test'} deployment attempt complete
[2014-07-10 09:11:08.003] boot - 19923INFO [DeploymentSupervisorCacheListener-0] --- ContainerListener: Path cache event: /containers/007c2bcc-13f4-466e-95d3-bd926bb456ea, type: CHILD_ADDED
[2014-07-10 09:11:08.006] boot - 19923INFO [DeploymentSupervisorCacheListener-0] --- ArrivingContainerModuleRedeployer: Container arrived: 007c2bcc-13f4-466e-95d3-bd926bb456ea
[2014-07-10 09:11:08.176] boot - 19923 ERROR [DeploymentSupervisorCacheListener-0] --- PathChildrenCache: 
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/test/modules
at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1590)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:214)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl$3.call(GetChildrenBuilderImpl.java:203)
at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl.pathInForeground(GetChildrenBuilderImpl.java:199)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:191)
at org.apache.curator.framework.imps.GetChildrenBuilderImpl.forPath(GetChildrenBuilderImpl.java:38)
at org.springframework.xd.dirt.server.ArrivingContainerModuleRedeployer.deployUnallocatedStreamModules(ArrivingContainerModuleRedeployer.java:133)
at org.springframework.xd.dirt.server.ArrivingContainerModuleRedeployer.deployModules(ArrivingContainerModuleRedeployer.java:106)
at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:99)
at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:744)
",3.0,False,3.0,0,0.0,0,0,False
"XD-1943","Improvement","Sprint 32","2014-07-09T16:24:04.000+0000","mark.pollack","","Update to Spring Batch Admin 1.3.0.GA","",1.0,0.8358550502643491,0.9985844510915488,0.6232346590659726,True,True,True,1404923044000.0,1406275246000.0,1406278846000.0,1406539891000.0,1406540791000.0,1404923044000.0,-1.0,1405931280000.0,1405934880000.0,1406538501000.0,1406540791000.0,1406275246000.0,1406278846000.0,"mark.pollack",1405934880000.0,1404923044000.0,44,1404923044000.0,-1.0,"Update to Spring Batch Admin 1.3.0.GA","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1942","Story","Sprint 31","2014-07-09T15:29:18.000+0000","mark.pollack","mark.pollack","Use guava 15.0 for spring-xd-integration-test","jclouds is not compatible with versions of guava higher than 15.",3.0,0.9749430523917996,0.0,0.0,True,True,True,1404919758000.0,1404921898000.0,1404921953000.0,1404921053000.0,1404921953000.0,1404919758000.0,-1.0,1404919758000.0,1404921953000.0,1404919758000.0,1404921953000.0,1404921898000.0,1404921953000.0,"mark.pollack",1404921953000.0,1404919758000.0,44,1404919758000.0,-1.0,"Use guava 15.0 for spring-xd-integration-test","jclouds is not compatible with versions of guava higher than 15.",3.0,False,3.0,0,0.0,-1,0,False
"XD-1941","Bug","Sprint 31","2014-07-09T13:39:02.000+0000","thomas.risberg","thomas.risberg","No main manifest attribute in xd-yarn-client jar","Error deploying to YARN - 

$ ./spring-xd-1.0.0.BUILD-SNAPSHOT-yarn/bin/xd-yarn push -p spring-xd-1.0.0.BUILD-SNAPSHOT-yarn
no main manifest attribute, in spring-xd-1.0.0.BUILD-SNAPSHOT-yarn/lib/spring-xd-yarn-client-1.0.0.BUILD-SNAPSHOT.jar

probably related to boot changes",3.0,0.9995068060761492,0.9995068060761492,0.9449595580982444,True,True,True,1404913142000.0,1404923275000.0,1404923280000.0,1404922380000.0,1404923280000.0,1404913142000.0,-1.0,1404922722000.0,1404923280000.0,1404923275000.0,1404923280000.0,1404923275000.0,1404923280000.0,"thomas.risberg",1404923280000.0,1404913142000.0,44,1404913142000.0,-1.0,"No main manifest attribute in xd-yarn-client jar","Error deploying to YARN - 

$ ./spring-xd-1.0.0.BUILD-SNAPSHOT-yarn/bin/xd-yarn push -p spring-xd-1.0.0.BUILD-SNAPSHOT-yarn
no main manifest attribute, in spring-xd-1.0.0.BUILD-SNAPSHOT-yarn/lib/spring-xd-yarn-client-1.0.0.BUILD-SNAPSHOT.jar

probably related to boot changes",3.0,False,3.0,0,0.0,0,0,False
"XD-1940","Story","Sprint 31","2014-07-09T10:35:16.000+0000","thomas.risberg","thomas.risberg","Clean up duplicated dependencies from XD on YARN installation","Remove unnecessary/duplicated jars from the lib directory in spring-xd-yarn zip distribution",3.0,0.9723454038997216,0.9723454038997216,0.9337715877437326,True,True,True,1404902116000.0,1404989384000.0,1404991866000.0,1404990966000.0,1404991866000.0,1404902116000.0,-1.0,1404985922000.0,1404989522000.0,1404989384000.0,1404991866000.0,1404989384000.0,1404991866000.0,"thomas.risberg",1404989522000.0,1404902116000.0,44,1404902116000.0,-1.0,"Clean up duplicated dependencies from XD on YARN installation","Remove unnecessary/duplicated jars from the lib directory in spring-xd-yarn zip distribution",3.0,False,3.0,0,0.0,-1,1,False
"XD-1939","Story","Sprint 31","2014-07-09T10:12:00.000+0000","mark.pollack","","Update to Spring Batch Admin 1.3.0.RC1","",1.0,0.9659549654615592,0.9659549654615592,0.0,True,True,True,1404900720000.0,1404922255000.0,1404923014000.0,1404922114000.0,1404923014000.0,1404900720000.0,-1.0,1404900720000.0,1404904320000.0,1404922255000.0,1404923014000.0,1404922255000.0,1404923014000.0,"mark.pollack",1404904320000.0,1404900720000.0,44,1404900720000.0,-1.0,"Update to Spring Batch Admin 1.3.0.RC1","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1938","Story","Sprint 31","2014-07-09T09:48:17.000+0000","eric.bottard","eric.bottard","Shell completion crashes","The XD shell completion crashes on:
job launch --name <TAB> gives
{noformat}
xd:>job launch --name Exception in thread ""Spring Shell"" java.lang.IllegalStateException: Could not determine kind: tab-completion-count-1 existing-job disable-string-converter
	at org.springframework.xd.shell.converter.CompletionConverter.determineKind(CompletionConverter.java:109)
	at org.springframework.xd.shell.converter.CompletionConverter.getAllPossibleValues(CompletionConverter.java:69)
	at org.springframework.shell.core.SimpleParser.completeAdvanced(SimpleParser.java:857)
	at org.springframework.shell.core.ParserCompleter.complete(ParserCompleter.java:47)
	at jline.console.ConsoleReader.complete(ConsoleReader.java:3077)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2501)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2162)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2150)
	at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:522)
	at org.springframework.shell.core.JLineShell.run(JLineShell.java:179)
	at java.lang.Thread.run(Thread.java:744)
{noformat}

Moreover, seems completion generally crashes when the server is not up (which was taken care of previously if I'm not mistaken):

xd:>job destroy --name <TAB>
{noformat}
Exception in thread ""Spring Shell"" org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://localhost:9393/jobs/definitions?size=10000&deployments=true"":Connection refused; nested exception is java.net.ConnectException: Connection refused
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:561)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:506)
	at org.springframework.web.client.RestTemplate.getForObject(RestTemplate.java:243)
	at org.springframework.xd.rest.client.impl.JobTemplate.list(JobTemplate.java:121)
	at org.springframework.xd.rest.client.impl.JobTemplate.list(JobTemplate.java:40)
	at org.springframework.xd.shell.converter.ExistingXDEntityConverter.getAllPossibleValues(ExistingXDEntityConverter.java:72)
	at org.springframework.shell.core.SimpleParser.completeAdvanced(SimpleParser.java:857)
	at org.springframework.shell.core.ParserCompleter.complete(ParserCompleter.java:47)
	at jline.console.ConsoleReader.complete(ConsoleReader.java:3077)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2501)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2162)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2150)
	at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:522)
	at org.springframework.shell.core.JLineShell.run(JLineShell.java:179)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:180)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)
	at sun.net.www.http.HttpClient.New(HttpClient.java:308)
	at sun.net.www.http.HttpClient.New(HttpClient.java:326)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
	at org.springframework.http.client.SimpleBufferingClientHttpRequest.executeInternal(SimpleBufferingClientHttpRequest.java:78)
	at org.springframework.http.client.AbstractBufferingClientHttpRequest.executeInternal(AbstractBufferingClientHttpRequest.java:48)
	at org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:52)
{noformat}",2.0,0.9999662678780248,0.9998819375730864,0.9998819375730864,True,True,True,1404899297000.0,1405077163000.0,1405077169000.0,1405076269000.0,1405077169000.0,1404899297000.0,-1.0,1405077148000.0,1405077169000.0,1405077148000.0,1405077169000.0,1405077163000.0,1405077169000.0,"eric.bottard",1405077169000.0,1404899297000.0,44,1404899297000.0,-1.0,"Shell completion crashes","The XD shell completion crashes on:
job launch --name <TAB> gives
{noformat}
xd:>job launch --name Exception in thread ""Spring Shell"" java.lang.IllegalStateException: Could not determine kind: tab-completion-count-1 existing-job disable-string-converter
	at org.springframework.xd.shell.converter.CompletionConverter.determineKind(CompletionConverter.java:109)
	at org.springframework.xd.shell.converter.CompletionConverter.getAllPossibleValues(CompletionConverter.java:69)
	at org.springframework.shell.core.SimpleParser.completeAdvanced(SimpleParser.java:857)
	at org.springframework.shell.core.ParserCompleter.complete(ParserCompleter.java:47)
	at jline.console.ConsoleReader.complete(ConsoleReader.java:3077)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2501)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2162)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2150)
	at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:522)
	at org.springframework.shell.core.JLineShell.run(JLineShell.java:179)
	at java.lang.Thread.run(Thread.java:744)
{noformat}

Moreover, seems completion generally crashes when the server is not up (which was taken care of previously if I'm not mistaken):

xd:>job destroy --name <TAB>
{noformat}
Exception in thread ""Spring Shell"" org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://localhost:9393/jobs/definitions?size=10000&deployments=true"":Connection refused; nested exception is java.net.ConnectException: Connection refused
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:561)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:506)
	at org.springframework.web.client.RestTemplate.getForObject(RestTemplate.java:243)
	at org.springframework.xd.rest.client.impl.JobTemplate.list(JobTemplate.java:121)
	at org.springframework.xd.rest.client.impl.JobTemplate.list(JobTemplate.java:40)
	at org.springframework.xd.shell.converter.ExistingXDEntityConverter.getAllPossibleValues(ExistingXDEntityConverter.java:72)
	at org.springframework.shell.core.SimpleParser.completeAdvanced(SimpleParser.java:857)
	at org.springframework.shell.core.ParserCompleter.complete(ParserCompleter.java:47)
	at jline.console.ConsoleReader.complete(ConsoleReader.java:3077)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2501)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2162)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2150)
	at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:522)
	at org.springframework.shell.core.JLineShell.run(JLineShell.java:179)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:180)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
	at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)
	at sun.net.www.http.HttpClient.New(HttpClient.java:308)
	at sun.net.www.http.HttpClient.New(HttpClient.java:326)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
	at org.springframework.http.client.SimpleBufferingClientHttpRequest.executeInternal(SimpleBufferingClientHttpRequest.java:78)
	at org.springframework.http.client.AbstractBufferingClientHttpRequest.executeInternal(AbstractBufferingClientHttpRequest.java:48)
	at org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:52)
{noformat}",2.0,False,2.0,0,0.0,-1,0,False
"XD-1937","Story","Sprint 32","2014-07-09T09:48:01.000+0000","mark.pollack","mark.pollack","Add licence files in distribution for 3rd party dependencies.","postgresql is BSD - http://jdbc.postgresql.org/about/license.html",4.0,0.9603253628908172,0.9603253628908172,0.9594644472521824,True,True,True,1404899281000.0,1405932206000.0,1405935806000.0,1405973980000.0,1405974880000.0,1404899281000.0,-1.0,1405931280000.0,1405934880000.0,1405932206000.0,1405935806000.0,1405932206000.0,1405935806000.0,"mark.pollack",1405934880000.0,1404899281000.0,44,1404899281000.0,-1.0,"Add licence files in distribution for 3rd party dependencies.","postgresql is BSD - http://jdbc.postgresql.org/about/license.html",4.0,False,4.0,0,0.0,0,0,False
"XD-1936","Story","Sprint 32","2014-07-09T09:45:06.000+0000","mark.pollack","","Remove jars from .zip packaging whose license prevents distribution","The work here is doing the research....

mysql client jar should be removed as it is GPL - http://dev.mysql.com/downloads/connector/j/5.0.html  (GPL)

postgresql is BSD so that is ok (another issue will handle license file inclusion.",3.0,0.9999480498276369,0.9999480498276369,0.957528878729744,True,True,True,1404899106000.0,1405977006000.0,1405977062000.0,1405976162000.0,1405977062000.0,1404899106000.0,-1.0,1405931280000.0,1405934880000.0,1405977006000.0,1405977062000.0,1405977006000.0,1405977062000.0,"mark.pollack",1405934880000.0,1404899106000.0,44,1404899106000.0,-1.0,"Remove jars from .zip packaging whose license prevents distribution","The work here is doing the research....

mysql client jar should be removed as it is GPL - http://dev.mysql.com/downloads/connector/j/5.0.html  (GPL)

postgresql is BSD so that is ok (another issue will handle license file inclusion.",3.0,False,3.0,0,0.0,0,0,False
"XD-1934","Improvement","Sprint 31","2014-07-09T09:32:17.000+0000","mark.pollack","hillert","Update Spring Integration Splunk Extension to 1.1 GA","",1.0,0.0106745364246291,1.3819163955935292e-05,2.61028652501e-05,True,True,True,1404898337000.0,1404912241000.0,1404915841000.0,1406199976000.0,1406200876000.0,1404898337000.0,-1.0,1404898371000.0,1404901971000.0,1404898355000.0,1404901955000.0,1404912241000.0,1404915841000.0,"mark.pollack",1404901971000.0,1404898337000.0,44,1404898337000.0,-1.0,"Update Spring Integration Splunk Extension to 1.1 GA","",1.0,False,1.0,0,0.0,0,0,False
"XD-1931","Story","Sprint 32","2014-07-08T17:46:10.000+0000","iperumal","","Verify platform compatibility versions with the XD dependencies","We need to make sure there is no conflicting/missing dependency with build.gradle using spring IO platform dependencies.

https://jira.spring.io/browse/XD-1929 is one such scenario where jolokia dependency went missing.",1.0,0.9927395556722992,0.9927395556722992,0.6389226187788875,True,True,True,1404841570000.0,1406534730000.0,1406538330000.0,1406546213000.0,1406547113000.0,1404841570000.0,-1.0,1405931280000.0,1405934880000.0,1406534730000.0,1406538330000.0,1406534730000.0,1406538330000.0,"iperumal",1405934880000.0,1405928799000.0,44,1405928799000.0,-1.0,"Verify platform compatibility versions with the XD dependencies","We need to make sure there is no conflicting/missing dependency with build.gradle using spring IO platform dependencies.

https://jira.spring.io/browse/XD-1929 is one such scenario where jolokia dependency went missing.",1.0,False,1.0,0,0.0,0,0,False
"XD-1930","Improvement","Sprint 32","2014-07-08T13:07:41.000+0000","pperalta","","JDK 1.8 compile warning for ContainerConfiguration","The following warning appears when compiling with JDK 8:


{panel}
/Users/pperalta/src/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/SingleNodeApplication.java:67: warning: auxiliary class ContainerConfiguration in /Users/pperalta/src/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/ContainerServerApplication.java should not be accessed from outside its own source file
						.child(ContainerConfiguration.class)}}
{panel}

Can this be turned into a static inner class?",1.0,0.9829471619570788,0.982941289478784,0.9282030803665432,True,True,True,1404824861000.0,1405996535000.0,1406000135000.0,1406015962000.0,1406016862000.0,1404824861000.0,-1.0,1405931280000.0,1405934880000.0,1405996528000.0,1406000128000.0,1405996535000.0,1406000135000.0,"pperalta",1405934880000.0,1404824861000.0,44,1404824861000.0,-1.0,"JDK 1.8 compile warning for ContainerConfiguration","The following warning appears when compiling with JDK 8:


{panel}
/Users/pperalta/src/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/SingleNodeApplication.java:67: warning: auxiliary class ContainerConfiguration in /Users/pperalta/src/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/ContainerServerApplication.java should not be accessed from outside its own source file
						.child(ContainerConfiguration.class)}}
{panel}

Can this be turned into a static inner class?",1.0,False,1.0,0,0.0,0,1,False
"XD-1929","Story","Sprint 31","2014-07-08T12:31:56.000+0000","mark.pollack","iperumal","Jolokia endpoints returning 404","http://localhost:9393/management/jolokia/search/xd.*:type=*,*

for singlenode in M7 returned a value..... on master it returns 404 error...

",4.0,0.2735757965883489,0.0339555841647891,0.0,True,True,True,1404822716000.0,1404827816000.0,1404831416000.0,1404840458000.0,1404841358000.0,1404822716000.0,-1.0,1404822716000.0,1404826316000.0,1404823349000.0,1404826949000.0,1404827816000.0,1404831416000.0,"mark.pollack",1404826316000.0,1404822716000.0,44,1404822716000.0,-1.0,"Jolokia endpoints returning 404","http://localhost:9393/management/jolokia/search/xd.*:type=*,*

for singlenode in M7 returned a value..... on master it returns 404 error...

",4.0,False,4.0,0,0.0,0,0,False
"XD-1927","Story","Sprint 31","2014-07-08T07:56:42.000+0000","mark.pollack","nebhale","Find logging configuration relative to environment","Previously, the scripts all looked for the logging configuration in $XD_HOME/config (or %XD_HOME%/config). This caused issues because it meant that if you moved all of the configuration and overrode $XD_CONFIG_LOCATION (or %XD_CONFIG_LOCATION%), the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in $XD_CONFIG_LOCATION (or %XD_CONFIG_LOCATION%).",2.0,0.0069844512810766,0.0069844512810766,0.0,True,True,True,1404806202000.0,1404806790000.0,1404810390000.0,1404889489000.0,1404890389000.0,1404806202000.0,-1.0,1404806202000.0,1404809802000.0,1404806790000.0,1404810390000.0,1404806790000.0,1404810390000.0,"mark.pollack",1404809802000.0,1404806202000.0,44,1404806202000.0,-1.0,"Find logging configuration relative to environment","Previously, the scripts all looked for the logging configuration in $XD_HOME/config (or %XD_HOME%/config). This caused issues because it meant that if you moved all of the configuration and overrode $XD_CONFIG_LOCATION (or %XD_CONFIG_LOCATION%), the logging configuration changes would not be found in the new location. This change updates the scripts to look for logging configuration in $XD_CONFIG_LOCATION (or %XD_CONFIG_LOCATION%).",2.0,False,2.0,0,0.0,0,0,False
"XD-1920","Story","Sprint 32","2014-07-07T08:58:26.000+0000","dturanski","dturanski","Update https://github.com/spring-guides/gs-spring-xd/ for new Release","",1.0,0.9918338385577172,4.723358160703339e-06,0.8576143682455978,True,True,True,1404723506000.0,1406613370000.0,1406616970000.0,1406628030000.0,1406628930000.0,1404723506000.0,-1.0,1406357625000.0,1406361225000.0,1404723515000.0,1404727115000.0,1406613370000.0,1406616970000.0,"dturanski",1406361225000.0,1404723506000.0,44,1404723506000.0,-1.0,"Update https://github.com/spring-guides/gs-spring-xd/ for new Release","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1918","Improvement","Sprint 32","2014-07-07T08:15:38.000+0000","dturanski","dturanski","Update TypeConversion Page","Need to update the examples in the TypeConversion doc, re spring social Tweet which is no longer used.",1.0,-1.0,7.602784744726283e-05,0.9894591068281182,False,True,True,1404720938000.0,-1.0,-1.0,1405943274000.0,1405944174000.0,1404720938000.0,-1.0,1405931280000.0,1405934880000.0,1404721031000.0,1404724631000.0,-1.0,-1.0,"dturanski",1405934880000.0,1404720938000.0,44,1404720938000.0,-1.0,"Update TypeConversion Page","Need to update the examples in the TypeConversion doc, re spring social Tweet which is no longer used.",1.0,False,1.0,0,0.0,-1,0,False
"XD-1915","Story","Sprint 31","2014-07-04T04:51:04.000+0000","thomas.risberg","thomas.risberg","Add Hadoop 2.4.x as an option","Hadoop 2.4.1 is now a stable release and we should add support for running against it",3.0,0.0143333827673142,0.0,0.999154678926294,True,True,True,1404449464000.0,1404453330000.0,1404456930000.0,1404718284000.0,1404719184000.0,1404449464000.0,-1.0,1404718956000.0,1404719184000.0,1404449464000.0,1404453064000.0,1404453330000.0,1404456930000.0,"thomas.risberg",1404719184000.0,1404449464000.0,44,1404449464000.0,-1.0,"Add Hadoop 2.4.x as an option","Hadoop 2.4.1 is now a stable release and we should add support for running against it",3.0,False,3.0,0,0.0,0,0,False
"XD-1914","Bug","","2014-07-02T14:35:09.000+0000","iperumal","","Stream/Job deployment state is always ""incomplete"" in case of module count zero","In case of the deployment property module count for a specific module in a stream/job is zero, the stream/job deployment status is calculated as ""zero"" even though the modules are deployed to all matching containers.

Currently, the DefaultStateCalculator's 'calculate' method doesn't check if the expected count is zero. 
Also, we would need all the matching containers to determine the same module is deployed to all the matching containers (in case of module count = 0)",4.0,0.018293436611594,0.018293436611594,-1.0,True,False,True,1404311709000.0,1404312153000.0,1404315753000.0,1404335080000.0,1404335980000.0,1404311709000.0,-1.0,-1.0,-1.0,1404312153000.0,1404315753000.0,1404312153000.0,1404315753000.0,"iperumal",-1.0,-1.0,0,1404311709000.0,-1.0,"Stream/Job deployment state is always ""incomplete"" in case of module count zero","In case of the deployment property module count for a specific module in a stream/job is zero, the stream/job deployment status is calculated as ""zero"" even though the modules are deployed to all matching containers.

Currently, the DefaultStateCalculator's 'calculate' method doesn't check if the expected count is zero. 
Also, we would need all the matching containers to determine the same module is deployed to all the matching containers (in case of module count = 0)",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1910","Story","Sprint 32","2014-07-01T12:55:40.000+0000","mark.pollack","","Improve getting started docs for installation","https://github.com/spring-guides/gs-spring-xd/issues/1",1.0,-1.0,-1.0,0.834602506625377,False,True,False,1404219340000.0,-1.0,-1.0,1406269644000.0,1406270544000.0,1404219340000.0,-1.0,1405931280000.0,1405934880000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1405934880000.0,1404219340000.0,44,1404219340000.0,-1.0,"Improve getting started docs for installation","https://github.com/spring-guides/gs-spring-xd/issues/1",1.0,False,1.0,0,0.0,0,0,False
"XD-1909","Story","Sprint 31","2014-07-01T12:53:25.000+0000","mark.pollack","","Update to Spring Shell 1.0 RC5","",2.0,0.9254645566729538,0.9254544070506044,0.0,True,True,True,1404219205000.0,1404766298000.0,1404769898000.0,1404809460000.0,1404810360000.0,1404219205000.0,-1.0,1404219205000.0,1404222805000.0,1404766292000.0,1404769892000.0,1404766298000.0,1404769898000.0,"mark.pollack",1404222805000.0,1404219205000.0,44,1404219205000.0,-1.0,"Update to Spring Shell 1.0 RC4","",2.0,False,2.0,0,0.0,-1,0,True
"XD-1908","Story","Sprint 31","2014-07-01T11:55:09.000+0000","grussell","","Remove Retry from TCP Sink","Now that the bus supports retry it is no longer necessary to have the retry advice in the TCP Sink.",1.0,0.422267826550194,0.3466211430543196,0.0228526112513545,True,True,True,1404215709000.0,1404285463000.0,1404289063000.0,1404379998000.0,1404380898000.0,1404215709000.0,-1.0,1404219484000.0,1404223084000.0,1404272967000.0,1404276567000.0,1404285463000.0,1404289063000.0,"grussell",1404223084000.0,1404215709000.0,44,1404215709000.0,-1.0,"Remove Retry from TCP Sink","Now that the bus supports retry it is no longer necessary to have the retry advice in the TCP Sink.",1.0,False,1.0,0,0.0,0,0,False
"XD-1906","Story","Sprint 31","2014-07-01T09:14:09.000+0000","hillert","","Handle Status Changes in Client (Dynamically update UI)","As a minimum we need some common polling strategy on the client side to detect status changes of job + streams etc. (E.g. during deployment of streams/jobs)

Ideally, I would like to have this addressed on the server-side as well. It would be nice if we could propagate events between, containers and admin-server that would inform about any changes in the system. We could then use those to notify connected UI clients.",3.0,0.7289244152736973,0.68882660567699,0.0146416474546129,True,True,True,1404206049000.0,1404842193000.0,1404845793000.0,1405077865000.0,1405078765000.0,1404206049000.0,-1.0,1404218827000.0,1404222427000.0,1404807199000.0,1404810799000.0,1404842193000.0,1404845793000.0,"hillert",1404222427000.0,1404208178000.0,44,1404208178000.0,-1.0,"Handle Status Changes in Client (Dynamically update UI)","As a minimum we need some common polling strategy on the client side to detect status changes of job + streams etc. (E.g. during deployment of streams/jobs)

Ideally, I would like to have this addressed on the server-side as well. It would be nice if we could propagate events between, containers and admin-server that would inform about any changes in the system. We could then use those to notify connected UI clients.",3.0,False,3.0,0,0.0,0,0,False
"XD-1905","Story","Sprint 31","2014-07-01T09:01:50.000+0000","hillert","","DefaultContainerMatcher - Improve Logging and mention affected Module","When deploying a definition with a container match criteria specified, and no container could be selected - the logging is ambiguous and should mention the affected module:

{code}
11:58:24,089  WARN DeploymentSupervisorCacheListener-0 cluster.DefaultContainerMatcher - No currently available containers match criteria 'somecriteria'
{code}",1.0,0.9990139472764444,0.9976061373483578,0.0263930191372331,True,True,True,1404205310000.0,1404716948000.0,1404717453000.0,1404716553000.0,1404717453000.0,1404205310000.0,1404717073000.0,1404218827000.0,1404222427000.0,1404716227000.0,1404717453000.0,1404716948000.0,1404717453000.0,"hillert",1404222427000.0,1404205310000.0,44,1404717073000.0,1404717073000.0,"DefaultContainerMatcher - Improve Logging and mention affected Module","When deploying a definition with a container match criteria specified, and no container could be selected - the logging is ambiguous and should mention the affected module:

{code}
11:58:24,089  WARN DeploymentSupervisorCacheListener-0 cluster.DefaultContainerMatcher - No currently available containers match criteria 'somecriteria'
{code}",2.0,True,2.0,1,-50.0,0,0,False
"XD-1901","Bug","Sprint 30","2014-06-30T10:28:45.000+0000","iperumal","iperumal","Job undeploy operation throws exception","Job `undeploy` operation throws the following stacktrace:

```
http-nio-9393-exec-5 zookeeper.ZooKeeperJobRepository - Exception while transitioning job 'j' state to undeploying
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/jobs/j/status
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1266)
	at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:260)
	at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:256)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:252)
	at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:239)
	at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:39)
	at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:177)
	at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:199)
	at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:1)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:68)
	at org.springframework.xd.dirt.rest.XDController.undeploy(XDController.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
```",2.0,0.000184264751595,0.0,0.0086219880724625,True,True,True,1404124125000.0,1404124240000.0,1404127840000.0,1404747327000.0,1404748227000.0,1404124125000.0,-1.0,1404129506000.0,1404133106000.0,1404124125000.0,1404127725000.0,1404124240000.0,1404127840000.0,"iperumal",1404133106000.0,1404124125000.0,44,1404124125000.0,-1.0,"Job undeploy operation throws exception","Job `undeploy` operation throws the following stacktrace:

```
http-nio-9393-exec-5 zookeeper.ZooKeeperJobRepository - Exception while transitioning job 'j' state to undeploying
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/jobs/j/status
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1266)
	at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:260)
	at org.apache.curator.framework.imps.SetDataBuilderImpl$4.call(SetDataBuilderImpl.java:256)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.SetDataBuilderImpl.pathInForeground(SetDataBuilderImpl.java:252)
	at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:239)
	at org.apache.curator.framework.imps.SetDataBuilderImpl.forPath(SetDataBuilderImpl.java:39)
	at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:177)
	at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:199)
	at org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository.delete(ZooKeeperJobRepository.java:1)
	at org.springframework.xd.dirt.stream.AbstractInstancePersistingDeployer.undeploy(AbstractInstancePersistingDeployer.java:68)
	at org.springframework.xd.dirt.rest.XDController.undeploy(XDController.java:125)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
```",2.0,False,2.0,0,0.0,0,0,False
"XD-1900","Bug","Sprint 30","2014-06-30T10:26:11.000+0000","hillert","iperumal","Batch job is marked as undeployed once computer comes back from hibernation","I have a deployed Batch Job (Single Node Server, running inside STS). My machine goes to sleep. Once I bring it back up I see the following log: 

{code}
12:57:35,854 ERROR LeaderSelector-5 leader.LeaderSelector - The leader threw an exception
java.lang.IllegalArgumentException: Label is required
	at org.springframework.util.Assert.hasText(Assert.java:162)
	at org.springframework.xd.module.ModuleDescriptor$Key.<init>(ModuleDescriptor.java:616)
	at org.springframework.xd.dirt.server.JobDeploymentListener.recalculateJobStates(JobDeploymentListener.java:218)
	at org.springframework.xd.dirt.server.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:354)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
{code}

In the UI the job is marked as *undeployed* - however, when I click *deploy* I get an error: *The job named 'bbb' is already deployed*.

",4.0,0.0294233953683794,0.0293739650527668,0.0685969204913373,True,True,True,1404123971000.0,1404126352000.0,1404129952000.0,1404203993000.0,1404204893000.0,1404123971000.0,-1.0,1404129522000.0,1404133122000.0,1404126348000.0,1404129948000.0,1404126352000.0,1404129952000.0,"hillert",1404133122000.0,1404123971000.0,44,1404123971000.0,-1.0,"Batch job is marked as undeployed once computer comes back from hibernation","I have a deployed Batch Job (Single Node Server, running inside STS). My machine goes to sleep. Once I bring it back up I see the following log: 

{code}
12:57:35,854 ERROR LeaderSelector-5 leader.LeaderSelector - The leader threw an exception
java.lang.IllegalArgumentException: Label is required
	at org.springframework.util.Assert.hasText(Assert.java:162)
	at org.springframework.xd.module.ModuleDescriptor$Key.<init>(ModuleDescriptor.java:616)
	at org.springframework.xd.dirt.server.JobDeploymentListener.recalculateJobStates(JobDeploymentListener.java:218)
	at org.springframework.xd.dirt.server.DeploymentSupervisor$LeaderListener.takeLeadership(DeploymentSupervisor.java:354)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$WrappedListener.takeLeadership(LeaderSelector.java:536)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWork(LeaderSelector.java:398)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.doWorkLoop(LeaderSelector.java:443)
	at org.apache.curator.framework.recipes.leader.LeaderSelector.access$100(LeaderSelector.java:63)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:244)
	at org.apache.curator.framework.recipes.leader.LeaderSelector$2.call(LeaderSelector.java:238)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
{code}

In the UI the job is marked as *undeployed* - however, when I click *deploy* I get an error: *The job named 'bbb' is already deployed*.

",4.0,False,4.0,0,0.0,0,0,False
"XD-1899","Improvement","Sprint 30","2014-06-30T10:23:28.000+0000","pperalta","pperalta","IllegalStateException on single node shutdown","Upon shutdown via ^C, an IllegalStateException stack trace appears in the server logs. While harmless, the traces are annoying and should be prevented.",2.0,0.001299537267571,0.0,0.9991133998081058,True,True,True,1404123808000.0,1404123915000.0,1404127515000.0,1404205245000.0,1404206145000.0,1404123808000.0,-1.0,1404206072000.0,1404206145000.0,1404123808000.0,1404127408000.0,1404123915000.0,1404127515000.0,"pperalta",1404206145000.0,1404123808000.0,44,1404123808000.0,-1.0,"IllegalStateException on single node shutdown","Upon shutdown via ^C, an IllegalStateException stack trace appears in the server logs. While harmless, the traces are annoying and should be prevented.",2.0,False,2.0,0,0.0,-1,0,False
"XD-1897","Improvement","Sprint 31","2014-06-29T20:11:51.000+0000","vagees","grussell","Spring XD - Handling sink failures","If a sink fails for whatever reason, will it be possible to handle it? Say by sending the payload to an error queue for later processing when a JDBC or Mongo sink fails due to a database connectivity loss? Or the modules are designed by certain principles / contracts not to be meant to handle such failures? ",3.0,0.984133742322943,0.8817974685266172,0.9841160587367118,True,True,True,1404072711000.0,1404796192000.0,1404799792000.0,1404806956000.0,1404807856000.0,1404072711000.0,-1.0,1404796179000.0,1404799779000.0,1404720960000.0,1404724560000.0,1404796192000.0,1404799792000.0,"vagees",1404799779000.0,1404072711000.0,44,1404072711000.0,-1.0,"Spring XD - Handling sink failures","If a sink fails for whatever reason, will it be possible to handle it? Say by sending the payload to an error queue for later processing when a JDBC or Mongo sink fails due to a database connectivity loss? Or the modules are designed by certain principles / contracts not to be meant to handle such failures? ",3.0,False,3.0,0,0.0,0,0,False
"XD-1896","Story","Sprint 31","2014-06-27T13:29:26.000+0000","mark.pollack","","Investigate why netty 3.7 is in xd/lib and not 3.6.6","",3.0,0.3731295398424686,0.9999062742200032,0.3091698058794903,True,True,True,1403875766000.0,1404289798000.0,1404293398000.0,1404984486000.0,1404985386000.0,1403875766000.0,-1.0,1404218827000.0,1404222427000.0,1404985282000.0,1404985386000.0,1404289798000.0,1404293398000.0,"mark.pollack",1404222427000.0,1403875766000.0,44,1403875766000.0,-1.0,"Investigate why netty 3.7 is in xd/lib and not 3.6.6","",3.0,False,3.0,0,0.0,0,0,False
"XD-1895","Story","Sprint 30","2014-06-27T13:27:22.000+0000","mark.pollack","david_syer","Use Boot plugin and IO Platform for versions where possible","",4.0,0.000215740924304,0.0,0.0,True,True,True,1403875642000.0,1403875716000.0,1403879316000.0,1404217746000.0,1404218646000.0,1403875642000.0,-1.0,1403875642000.0,1403879242000.0,1403875642000.0,1403879242000.0,1403875716000.0,1403879316000.0,"mark.pollack",1403879242000.0,1403875642000.0,44,1403875642000.0,-1.0,"Use Boot plugin and IO Platform for versions where possible","",4.0,False,4.0,0,0.0,0,1,False
"XD-1894","Story","Sprint 32","2014-06-27T13:25:12.000+0000","mark.pollack","","Create documentation section on performance tuning","This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency.

It should also discuss the 'bypass' functionality - or reference another section that covers it.

We should probably include how to scale out http sources, e.g. the need to use a load balancer.",4.0,0.8860843507133479,0.8860788955926261,0.7476308410705129,True,True,True,1403875512000.0,1406311987000.0,1406315587000.0,1406624322000.0,1406625222000.0,1403875512000.0,-1.0,1405931280000.0,1405934880000.0,1406311972000.0,1406315572000.0,1406311987000.0,1406315587000.0,"mark.pollack",1405934880000.0,1403875512000.0,44,1403875512000.0,-1.0,"Create documentation section on best practices","This should include some guidance on setting rabbit message bus paramters relating to prefetch and concurrency.

It should also discuss the 'bypass' functionality - or reference another section that covers it.

We should probably include how to scale out http sources, e.g. the need to use a load balancer.",4.0,False,4.0,0,0.0,0,0,True
"XD-1893","Story","Sprint 31","2014-06-27T12:49:28.000+0000","mark.pollack","mark.pollack","Update to Spring Batch 3.0.1 snapshots","",1.0,-1.0,0.9998958173562495,0.999745331315277,False,True,True,1403873368000.0,-1.0,-1.0,1404218015000.0,1404218915000.0,1403873368000.0,-1.0,1404218827000.0,1404218915000.0,1404218879000.0,1404218915000.0,-1.0,-1.0,"mark.pollack",1404218915000.0,1403873368000.0,44,1403873368000.0,-1.0,"Update to Spring Batch 3.0.1 snapshots","",1.0,False,1.0,0,0.0,0,0,False
"XD-1892","Story","Sprint 31","2014-06-27T12:38:09.000+0000","mark.pollack","","Update to Spring Batch 3.0.1","",2.0,0.92210395003891,0.92210395003891,0.3383968340248476,True,True,True,1403872689000.0,1404815887000.0,1404819487000.0,1404894665000.0,1404895565000.0,1403872689000.0,-1.0,1404218827000.0,1404222427000.0,1404815887000.0,1404819487000.0,1404815887000.0,1404819487000.0,"mark.pollack",1404222427000.0,1403872689000.0,44,1403872689000.0,-1.0,"Update to Spring Platform 1.0.1","",2.0,False,2.0,0,0.0,0,0,True
"XD-1891","Story","Sprint 30","2014-06-27T10:15:15.000+0000","mark.pollack","grussell","Add configuration files for hornetmq jms provider.","See https://jira.spring.io/browse/XD-1684

Requires update to gradle 2.1",1.0,0.0348951429653474,0.0348803493656504,1.0000325459193335,True,True,True,1403864115000.0,1403875909000.0,1403879509000.0,1404201199000.0,1404202099000.0,1403864115000.0,-1.0,1404202110000.0,1404202099000.0,1403875904000.0,1403879504000.0,1403875909000.0,1403879509000.0,"mark.pollack",1404202099000.0,1403864115000.0,44,1403864115000.0,-1.0,"Add configuration files for hornetmq jms provider.","See https://jira.spring.io/browse/XD-1684

Requires update to gradle 2.1",1.0,False,1.0,0,0.0,0,0,False
"XD-1890","Story","Sprint 30","2014-06-27T09:50:20.000+0000","mark.pollack","pperalta","Create multi-container, single host, testing framework","Use external JVM launch support provided by the Oracle Tools framework (https://java.net/projects/oracletools).",8.0,6.136941465267834e-05,0.0,0.0,True,True,True,1403862620000.0,1403862641000.0,1403866241000.0,1404203910000.0,1404204810000.0,1403862620000.0,-1.0,1403862620000.0,1403866220000.0,1403862620000.0,1403866220000.0,1403862641000.0,1403866241000.0,"mark.pollack",1403866220000.0,1403862620000.0,44,1403862620000.0,-1.0,"Create multi-container, single host, testing framework","Use external JVM launch support provided by the Oracle Tools framework (https://java.net/projects/oracletools).",8.0,False,8.0,0,0.0,-1,0,False
"XD-1889","Improvement","Sprint 30","2014-06-27T08:50:05.000+0000","hillert","hillert","UI Needs to handle the finer-grained deployment statuses","Addressed for the REST API by XD-1848

The status of a stream, as returned by the REST API (and thus the shell also), may now contain any of the following states:

* deploying (deployment has been initiated)
* deployed (fully deployed based on each of the stream's modules' count properties)
* incomplete (at least 1 of each module, but 1 or more of them not at requested capacity)
* failed (1 or more of the modules does not have even a single instance deployed)
* undeployed (intentionally undeployed, or created but not yet deployed)
",4.0,-1.0,0.0,0.0043538091493593,False,True,True,1403859005000.0,-1.0,-1.0,1404204009000.0,1404204909000.0,1403859005000.0,-1.0,1403860511000.0,1403864111000.0,1403859005000.0,1403862605000.0,-1.0,-1.0,"hillert",1403864111000.0,1403859005000.0,44,1403859005000.0,-1.0,"UI Needs to handle the finer-grained deployment statuses","Addressed for the REST API by XD-1848

The status of a stream, as returned by the REST API (and thus the shell also), may now contain any of the following states:

* deploying (deployment has been initiated)
* deployed (fully deployed based on each of the stream's modules' count properties)
* incomplete (at least 1 of each module, but 1 or more of them not at requested capacity)
* failed (1 or more of the modules does not have even a single instance deployed)
* undeployed (intentionally undeployed, or created but not yet deployed)
",4.0,False,4.0,0,0.0,0,0,False
"XD-1888","Improvement","Sprint 31","2014-06-27T07:32:14.000+0000","eric.bottard","","Build up on SHL-155 for module otpions tab completion","",1.0,0.7694739281064501,0.769462894719421,0.6702653897360078,True,True,True,1403854334000.0,1404272777000.0,1404276377000.0,1404397238000.0,1404398138000.0,1403854334000.0,-1.0,1404218827000.0,1404222427000.0,1404272771000.0,1404276371000.0,1404272777000.0,1404276377000.0,"eric.bottard",1404222427000.0,1403854334000.0,44,1403854334000.0,-1.0,"Use 2 tabs for hidden options in shell","",1.0,False,1.0,0,0.0,-1,0,True
"XD-1886","Story","","2014-06-26T07:13:05.000+0000","grussell","","Update Netty to 4","Spring IO Compatibility",1.0,-1.0,-1.0,-1.0,False,False,False,1403766785000.0,-1.0,-1.0,1426354416000.0,1426355316000.0,1403766785000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grussell",-1.0,-1.0,0,1403766785000.0,-1.0,"Update Netty to 4","Spring IO Compatibility",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1885","Story","Sprint 30","2014-06-26T06:36:26.000+0000","eric.bottard","eric.bottard","Provide ability to disable tab completion for specific module options","Not all module options are born equal. Some are more important/useful than others, and having the more ""expert"" ones show up e.g. in TAB completion is very noisy (esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit)",3.0,0.9997797564735864,0.0002202435264134,0.006953402762483,True,True,True,1403764586000.0,1403796362000.0,1403796369000.0,1403795469000.0,1403796369000.0,1403764586000.0,-1.0,1403764807000.0,1403768407000.0,1403764593000.0,1403768193000.0,1403796362000.0,1403796369000.0,"eric.bottard",1403768407000.0,1403764586000.0,44,1403764586000.0,-1.0,"Provide ability to disable tab completion for specific module options","Not all module options are born equal. Some are more important/useful than others, and having the more ""expert"" ones show up e.g. in TAB completion is very noisy (esp. given how JLine2 currently presents the whole stream definition typed so far when doing completion, as opposed to just the last bit)",3.0,False,3.0,0,0.0,0,1,False
"XD-1884","Story","Sprint 30","2014-06-25T08:21:04.000+0000","eric.bottard","dturanski","Module option validation not happening anymore","It seems that no option validation (being Spring or jsr 303) is happening anymore at stream creation time.

eg
{noformat}
stream create foo --definition ""http --port=bar | log""
{noformat}",2.0,0.111831387837185,0.110938219730589,0.1116853351908234,True,True,True,1403684464000.0,1403704372000.0,1403707972000.0,1403861582000.0,1403862482000.0,1403684464000.0,-1.0,1403704346000.0,1403707946000.0,1403704213000.0,1403707813000.0,1403704372000.0,1403707972000.0,"eric.bottard",1403707946000.0,1403684464000.0,44,1403684464000.0,-1.0,"Module option validation not happening anymore","It seems that no option validation (being Spring or jsr 303) is happening anymore at stream creation time.

eg
{noformat}
stream create foo --definition ""http --port=bar | log""
{noformat}",2.0,False,2.0,0,0.0,0,1,False
"XD-1881","Story","Sprint 31","2014-06-24T08:33:10.000+0000","eric.bottard","","HadoopDistroOptionHandler fails when XD_HOME ends with a ""/""","Also, the approach may not work as expected on windows.",3.0,0.6833434788550172,0.681330051271858,0.5501981482500307,True,True,True,1403598790000.0,1404368873000.0,1404372473000.0,1404724824000.0,1404725724000.0,1403598790000.0,-1.0,1404218827000.0,1404222427000.0,1404366604000.0,1404370204000.0,1404368873000.0,1404372473000.0,"eric.bottard",1404222427000.0,1403598790000.0,44,1403598790000.0,-1.0,"HadoopDistroOptionHandler fails when XD_HOME ends with a ""/""","Also, the approach may not work as expected on windows.",3.0,False,3.0,0,0.0,0,0,False
"XD-1880","Story","Sprint 37","2014-06-24T07:56:40.000+0000","mark.pollack","","Integration test for field-value-counter and aggregate-counter","https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/tweet_tests  use field-value-counter and aggregate-counter.

Should do a simplified version of this so that we can assert values of the field-value-counter and aggregate-counter.",3.0,0.9432318042717076,0.9432318042717076,0.9056800159802464,True,True,True,1403596600000.0,1414131357000.0,1414134957000.0,1414764489000.0,1414765389000.0,1403596600000.0,-1.0,1413711949000.0,1413715549000.0,1414131357000.0,1414134957000.0,1414131357000.0,1414134957000.0,"mark.pollack",1413715549000.0,1403596600000.0,44,1403596600000.0,-1.0,"Integration test for field-value-counter and aggregate-counter","https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/tweet_tests  use field-value-counter and aggregate-counter.

Should do a simplified version of this so that we can assert values of the field-value-counter and aggregate-counter.",3.0,False,3.0,0,0.0,0,0,False
"XD-1879","Story","Sprint 36","2014-06-24T07:55:18.000+0000","mark.pollack","","Create test with jdbc sink and initializeDb=false","See

https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/jdbc_tests#L96

and

https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/jdbc_tests#L64

The second assert has initializeDb=false and so there are double the number of rows running the job a second time.",2.0,0.912439717100172,0.912439717100172,0.9060555073748664,True,True,True,1403596518000.0,1412661446000.0,1412665046000.0,1413530442000.0,1413531342000.0,1403596518000.0,-1.0,1412598020000.0,1412601620000.0,1412661446000.0,1412665046000.0,1412661446000.0,1412665046000.0,"mark.pollack",1412601620000.0,1403596518000.0,44,1403596518000.0,-1.0,"Create test with jdbc sink and initializeDb=false","See

https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/jdbc_tests#L96

and

https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/jdbc_tests#L64

The second assert has initializeDb=false and so there are double the number of rows running the job a second time.",2.0,False,2.0,0,0.0,-1,0,False
"XD-1877","Story","Sprint 36","2014-06-24T07:18:57.000+0000","mark.pollack","","Create gemfire test","Port https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/gemfire_stream_tests

Need to consider how to start the server, maybe use the jvm fork utilities?  Look into spring-data-gemfire as well.",5.0,0.9740471186437268,0.9644891791311312,0.9057928948779578,True,True,True,1403594337000.0,1413275966000.0,1413279566000.0,1413533027000.0,1413533927000.0,1403594337000.0,-1.0,1412597547000.0,1412601147000.0,1413180964000.0,1413184564000.0,1413275966000.0,1413279566000.0,"mark.pollack",1412601147000.0,1403594337000.0,44,1403594337000.0,-1.0,"Create gemfire test","Port https://github.com/spring-projects/spring-xd/blob/master/src/test/scripts/gemfire_stream_tests

Need to consider how to start the server, maybe use the jvm fork utilities?  Look into spring-data-gemfire as well.",5.0,False,5.0,0,0.0,0,0,False
"XD-1876","Story","Sprint 36","2014-06-24T07:15:48.000+0000","mark.pollack","","Create test that uses #jsonPath with the filter module","The script tests does the following.

{code}
# Filter for good and bad
create_stream 'httpfilter' ""http | good: filter --expression=#jsonPath(payload,'\$.entities.hashtags[*].text').contains('good') \
| aftergood: filter --expression=true \
| bad: filter --expression=#jsonPath(payload,'\$.entities.hashtags[*].text').contains('bad') \
| goodandbad: splitter --expression=#jsonPath(payload,'$.id') \
| file --dir=$TEST_DIR"" 'true'
{code}",2.0,0.9121548721525906,0.9121544697576568,0.9057781196346294,True,True,True,1403594148000.0,1412661408000.0,1412665008000.0,1413533731000.0,1413534631000.0,1403594148000.0,-1.0,1412598020000.0,1412601620000.0,1412661404000.0,1412665004000.0,1412661408000.0,1412665008000.0,"mark.pollack",1412601620000.0,1403594148000.0,44,1403594148000.0,-1.0,"Create test that uses #jsonPath with the filter module","The script tests does the following.

{code}
# Filter for good and bad
create_stream 'httpfilter' ""http | good: filter --expression=#jsonPath(payload,'\$.entities.hashtags[*].text').contains('good') \
| aftergood: filter --expression=true \
| bad: filter --expression=#jsonPath(payload,'\$.entities.hashtags[*].text').contains('bad') \
| goodandbad: splitter --expression=#jsonPath(payload,'$.id') \
| file --dir=$TEST_DIR"" 'true'
{code}",2.0,False,2.0,0,0.0,-1,0,False
"XD-1874","Story","Sprint 31","2014-06-24T05:16:59.000+0000","eric.bottard","eric.bottard","Document json-to-tuple, object-to-json and http-client","Note that the documentation for the module options (in particular for http-client) should be autogenerated using the following syntax (see others):
{noformat}
//^processor.http-client
//$processor.http-client
{noformat}

",4.0,-1.0,0.9529277218664228,0.9999814001186372,False,True,True,1403587019000.0,-1.0,-1.0,1405575379000.0,1405576279000.0,1403587019000.0,-1.0,1405576242000.0,1405576279000.0,1405482640000.0,1405486240000.0,-1.0,-1.0,"eric.bottard",1405576279000.0,1403587019000.0,44,1403587019000.0,-1.0,"Document json-to-tuple, object-to-json and http-client","Note that the documentation for the module options (in particular for http-client) should be autogenerated using the following syntax (see others):
{noformat}
//^processor.http-client
//$processor.http-client
{noformat}

",4.0,False,4.0,0,0.0,-1,0,False
"XD-1873","Story","Sprint 31","2014-06-23T14:32:10.000+0000","iperumal","iperumal","Investigate JobExecutions page list performance","Investigate ""Job Executions"" list page load timing based on number of job executions to load. 

The investigation can be of the following steps:

1) Return all the size restrictions to retrieve the number of job executions.
2) Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings.

Based on this, we can address the paging support mentioned in XD-1864.",4.0,0.9389844920228276,0.0,0.5011733610812923,True,True,True,1403533930000.0,1404817134000.0,1404820734000.0,1404899617000.0,1404900517000.0,1403533930000.0,-1.0,1404218827000.0,1404222427000.0,1403533930000.0,1403537530000.0,1404817134000.0,1404820734000.0,"iperumal",1404222427000.0,1403533930000.0,44,1403533930000.0,-1.0,"Investigate JobExecutions page list performance","Investigate ""Job Executions"" list page load timing based on number of job executions to load. 

The investigation can be of the following steps:

1) Return all the size restrictions to retrieve the number of job executions.
2) Setup 5, 10, 100, 500, 1000 number of job executions and measure the page load timings.

Based on this, we can address the paging support mentioned in XD-1864.",4.0,False,4.0,0,0.0,0,0,False
"XD-1872","Bug","Sprint 30","2014-06-23T10:48:12.000+0000","iperumal","iperumal","Tap lifecycle connection listener should close the tap path children cache upon ZK disconnect","Currently, TapLifecycleConnectionListener (which implements ZooKeeperConnectionListener) clears(but not closes) the taps (PathChildrenCache) upon ZK `onDisconnect` child event.
Since, `onConnect` child event re-creates the tap PathChildrenCache, the previously created PathChildrenCache is still hanging in there.
I would be better to close the cache upon disconnect.",1.0,0.0033090668431502,0.0,0.9298477829252152,True,True,True,1403520492000.0,1403520497000.0,1403522003000.0,1403521103000.0,1403522003000.0,1403520492000.0,-1.0,1403521897000.0,1403522003000.0,1403520492000.0,1403522003000.0,1403520497000.0,1403522003000.0,"iperumal",1403522003000.0,1403520492000.0,44,1403520492000.0,-1.0,"Tap lifecycle connection listener should close the tap path children cache upon ZK disconnect","Currently, TapLifecycleConnectionListener (which implements ZooKeeperConnectionListener) clears(but not closes) the taps (PathChildrenCache) upon ZK `onDisconnect` child event.
Since, `onConnect` child event re-creates the tap PathChildrenCache, the previously created PathChildrenCache is still hanging in there.
I would be better to close the cache upon disconnect.",1.0,False,1.0,0,0.0,0,0,False
"XD-1871","Story","Sprint 30","2014-06-20T14:20:37.000+0000","iperumal","mark.pollack","Create documentation for Batch DB migration","Update documentation related to database migration with the changes from XD-1822",1.0,-1.0,0.9996837730393928,0.9999096494398264,False,True,True,1403274037000.0,-1.0,-1.0,1403339545000.0,1403340445000.0,1403274037000.0,-1.0,1403340439000.0,1403340445000.0,1403340424000.0,1403340445000.0,-1.0,-1.0,"iperumal",1403340445000.0,1403274037000.0,44,1403274037000.0,-1.0,"Create documentation for Batch DB migration","Update documentation related to database migration with the changes from XD-1822",1.0,False,1.0,0,0.0,-1,0,False
"XD-1870","Bug","Sprint 30","2014-06-20T14:18:32.000+0000","grenfro","grenfro","Rabbit Sink & Source Options and fixtures need to be updated","Acceptance Tests failed on the Rabbit Source and Sink Tests.  The test started failing when XD-1824 was introduced (Support RabbitMQ Cluster in source/sink).  This story replaced the host and port setting with a addresses.  
* RabbitConnectionMixin needs to remove the host and port attributes.  
* RabbitSource and RabbitSink need to be updated to use addresses instead of host and port.
* Rabbit Source also fails to create the queue on a remote Rabbit instance, since we started using addresses.  Where when we used host and port this feature would work.  ",5.0,0.6990496079099898,0.00104415274463,0.0010806828698066,True,True,True,1403273912000.0,1403503547000.0,1403507147000.0,1403601508000.0,1403602408000.0,1403273912000.0,-1.0,1403274267000.0,1403277867000.0,1403274255000.0,1403277855000.0,1403503547000.0,1403507147000.0,"grenfro",1403277867000.0,1403273912000.0,44,1403273912000.0,-1.0,"Rabbit Sink & Source --host and --port are not updating module host/port.","Acceptance Tests failed on the Rabbit Source and Sink Tests.  The test started failing when XD-1824 was introduced (Support RabbitMQ Cluster in source/sink).  This story added addresses to support rabbit cluster failover.  
Currently if a user set --host --port to a remote Rabbit instance, XD will use the default host=localhost and port=5672.  However using --addresses does work.  ",5.0,False,5.0,0,0.0,0,1,True
"XD-1869","Improvement","Sprint 30","2014-06-20T08:11:43.000+0000","abilan","eric.bottard","Provide option for sources/sinks to configure mapped headers to/from Messages","See the discussion: https://gopivotal-com.socialcast.com/messages/20771872",1.0,0.90679459688553,0.4456100743998288,0.9067337510018588,True,True,True,1403251903000.0,1404116285000.0,1404119885000.0,1404204231000.0,1404205131000.0,1403251903000.0,-1.0,1404116227000.0,1404119827000.0,1403676671000.0,1403680271000.0,1404116285000.0,1404119885000.0,"abilan",1404119827000.0,1403251903000.0,44,1403251903000.0,-1.0,"Provide option for sources/sinks to configure mapped headers to/from Messages","See the discussion: https://gopivotal-com.socialcast.com/messages/20771872",1.0,False,1.0,0,0.0,0,0,False
"XD-1864","Improvement","Sprint 36","2014-06-19T11:02:55.000+0000","iperumal","liujiong","Add paging support for UI list views","As a user, I'd like to have _paging_ support so that I can scroll through the list of streams, jobs and containers. 

Currently the following error is thrown when we cross >20 rows:

http://localhost:9393/jobs/definitions.json

JSON Response:
{code:xml}
[
	{
		links: [ ],
		logref: ""IllegalStateException"",
		message: ""Not all instances were looked at""
	}
]
{code}

Stack trace:
{code}
15:51:21,931 ERROR http-nio-9393-exec-9 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.IllegalStateException: Not all instances were looked at
	at org.springframework.util.Assert.state(Assert.java:385)
{code}",5.0,0.932080815435877,0.9223698359971424,0.8910809837425199,True,True,True,1403175775000.0,1412678969000.0,1412682569000.0,1413370551000.0,1413371451000.0,1403175775000.0,-1.0,1412260948000.0,1412264548000.0,1412579959000.0,1412583559000.0,1412678969000.0,1412682569000.0,"iperumal",1412264548000.0,1403176486000.0,44,1403176486000.0,-1.0,"Add paging support for UI list views","As a user, I'd like to have _paging_ support so that I can scroll through the list of streams, jobs and containers. 

Currently the following error is thrown when we cross >20 rows:

http://localhost:9393/jobs/definitions.json

JSON Response:
{code:xml}
[
	{
		links: [ ],
		logref: ""IllegalStateException"",
		message: ""Not all instances were looked at""
	}
]
{code}

Stack trace:
{code}
15:51:21,931 ERROR http-nio-9393-exec-9 rest.RestControllerAdvice - Caught exception while handling a request
java.lang.IllegalStateException: Not all instances were looked at
	at org.springframework.util.Assert.state(Assert.java:385)
{code}",5.0,False,5.0,0,0.0,0,2,False
"XD-1863","Story","Sprint 30","2014-06-19T09:28:54.000+0000","thomas.risberg","thomas.risberg","Create way to deploy custom modules for XD on YARN","Need a way for end-user to package and add custom modules/scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It's not convenient to un-zip/re-zip this archive to add custom modules/scripts.

See - https://github.com/spring-projects/spring-xd/issues/931",5.0,0.7307338480809787,0.1063829479476149,0.6234309502388109,True,True,True,1403170134000.0,1403675321000.0,1403678921000.0,1403860576000.0,1403861476000.0,1403170134000.0,-1.0,1403601138000.0,1403604738000.0,1403243681000.0,1403247281000.0,1403675321000.0,1403678921000.0,"thomas.risberg",1403604738000.0,1403170134000.0,44,1403170134000.0,-1.0,"Create way to deploy custom modules for XD on YARN","Need a way for end-user to package and add custom modules/scripts when deploying XD on YARN. Currently we have a zip file containing all code including modules. It's not convenient to un-zip/re-zip this archive to add custom modules/scripts.

See - https://github.com/spring-projects/spring-xd/issues/931",5.0,False,5.0,0,0.0,0,0,False
"XD-1861","Bug","Sprint 30","2014-06-17T17:17:52.000+0000","iperumal","iperumal","Fix XD config initializer for ZK connection string","Spring Boot 1.1.1 has the following change:

https://github.com/spring-projects/spring-boot/commit/b75578d99c8d435e1f8bf18d0dbb3a2ddf56fdc4

where, an external property source precedence would get re-ordered after the application configuration properties. This change affects Spring XD config initializer which expects an external ""zk-properties"" property source always preceding over the application configuration properties.
",3.0,0.0001331358484914,0.0,0.99953402453028,True,True,True,1403025472000.0,1403025480000.0,1403029080000.0,1403084661000.0,1403085561000.0,1403025472000.0,-1.0,1403085533000.0,1403085561000.0,1403025472000.0,1403029072000.0,1403025480000.0,1403029080000.0,"iperumal",1403085561000.0,1403025472000.0,44,1403025472000.0,-1.0,"Fix XD config initializer for ZK connection string","Spring Boot 1.1.1 has the following change:

https://github.com/spring-projects/spring-boot/commit/b75578d99c8d435e1f8bf18d0dbb3a2ddf56fdc4

where, an external property source precedence would get re-ordered after the application configuration properties. This change affects Spring XD config initializer which expects an external ""zk-properties"" property source always preceding over the application configuration properties.
",3.0,False,3.0,0,0.0,0,0,False
"XD-1858","Story","Sprint 31","2014-06-17T08:50:26.000+0000","eric.bottard","","Provide error location when tapping inexistent stream/module","See impacted code at https://github.com/spring-projects/spring-xd/pull/951",3.0,0.9659513218809628,0.9659460301237062,0.9251511174679397,True,True,True,1402995026000.0,1404272798000.0,1404276398000.0,1404316938000.0,1404317838000.0,1402995026000.0,-1.0,1404218827000.0,1404222427000.0,1404272791000.0,1404276391000.0,1404272798000.0,1404276398000.0,"eric.bottard",1404222427000.0,1402995026000.0,44,1402995026000.0,-1.0,"Provide error location when tapping inexistent stream/module","See impacted code at https://github.com/spring-projects/spring-xd/pull/951",3.0,False,3.0,0,0.0,-1,0,False
"XD-1857","Bug","Sprint 30","2014-06-17T08:08:43.000+0000","thomas.risberg","","Can't use webhdfs with hdfs sink","When using spring.hadoop.fsUri set to webhdfs://localhost/ I'm getting an error:

java.lang.NoClassDefFoundError: javax/ws/rs/core/MediaType

including the following in xd/lib seems to fix this:
- jersey-core-1.9.jar
- jersey-server-1.9.jar
",3.0,0.1727983982415477,0.1727983982415477,0.0387055354247597,True,True,True,1402992523000.0,1403008403000.0,1403012003000.0,1403083522000.0,1403084422000.0,1402992523000.0,-1.0,1402996080000.0,1402999680000.0,1403008403000.0,1403012003000.0,1403008403000.0,1403012003000.0,"thomas.risberg",1402999680000.0,1402992523000.0,44,1402992523000.0,-1.0,"Can't use webhdfs with hdfs sink","When using spring.hadoop.fsUri set to webhdfs://localhost/ I'm getting an error:

java.lang.NoClassDefFoundError: javax/ws/rs/core/MediaType

including the following in xd/lib seems to fix this:
- jersey-core-1.9.jar
- jersey-server-1.9.jar
",3.0,False,3.0,0,0.0,0,0,False
"XD-1856","Story","Sprint 30","2014-06-17T07:39:10.000+0000","thomas.risberg","","Add option to specify fsUri to hdfs sinks","We should have an --fsUri parameter for hdfs and hdfs-dataset sinks so we can write to different file systems (hdfs, webhdfs)",5.0,0.1851743119266055,0.1851743119266055,0.0558846657929226,True,True,True,1402990750000.0,1403008411000.0,1403012011000.0,1403085225000.0,1403086125000.0,1402990750000.0,-1.0,1402996080000.0,1402999680000.0,1403008411000.0,1403012011000.0,1403008411000.0,1403012011000.0,"thomas.risberg",1402999680000.0,1402990750000.0,44,1402990750000.0,-1.0,"Add option to specify fsUri to hdfs sinks","We should have an --fsUri parameter for hdfs and hdfs-dataset sinks so we can write to different file systems (hdfs, webhdfs)",5.0,False,5.0,0,0.0,0,0,False
"XD-1855","Story","Sprint 30","2014-06-17T07:06:43.000+0000","thomas.risberg","thomas.risberg","Update to use SHDP 2.0.0.RELEASE","",1.0,0.4161229931355307,0.4161229931355307,0.4128325863731775,True,True,True,1402988803000.0,1402996138000.0,1402999738000.0,1403005530000.0,1403006430000.0,1402988803000.0,-1.0,1402996080000.0,1402999680000.0,1402996138000.0,1402999738000.0,1402996138000.0,1402999738000.0,"thomas.risberg",1402999680000.0,1402988803000.0,44,1402988803000.0,-1.0,"Update to use SHDP 2.0.0.RELEASE","",1.0,False,1.0,0,0.0,0,0,False
"XD-1854","Story","Sprint 30","2014-06-17T06:33:48.000+0000","thomas.risberg","","Remove Hadoop v1 support","Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP 2.1 will also drop any v1 support.

Remove support for:
- hadoop12 - Apache Hadoop 1.2.1
- cdh4 - Cloudera CDH 4.6.0
- hdp13 - Hortonworks Data Platform 1.3

Keep:
- hadoop22 - Apache Hadoop 2.2.0 (default)
- phd1 - Pivotal HD 1.1
- phd20 - Pivotal HD 2.0
- cdh5 - Cloudera CDH 5.0.0
- hdp21 - Hortonworks Data Platform 2.1

This should make configuration and documentation easier too. Not to mention testing.

This affects startup scripts and the shell plus the build script.",5.0,0.6684672044896528,0.6684672044896528,0.3245177130831287,True,True,True,1402986828000.0,1403005886000.0,1403009486000.0,1403014438000.0,1403015338000.0,1402986828000.0,-1.0,1402996080000.0,1402999680000.0,1403005886000.0,1403009486000.0,1403005886000.0,1403009486000.0,"thomas.risberg",1402999680000.0,1402986828000.0,44,1402986828000.0,-1.0,"Remove Hadoop v1 support","Going forward it seems that providing Hadoop v1 will be of lesser importance and we might as well drop it now. SHDP 2.1 will also drop any v1 support.

Remove support for:
- hadoop12 - Apache Hadoop 1.2.1
- cdh4 - Cloudera CDH 4.6.0
- hdp13 - Hortonworks Data Platform 1.3

Keep:
- hadoop22 - Apache Hadoop 2.2.0 (default)
- phd1 - Pivotal HD 1.1
- phd20 - Pivotal HD 2.0
- cdh5 - Cloudera CDH 5.0.0
- hdp21 - Hortonworks Data Platform 2.1

This should make configuration and documentation easier too. Not to mention testing.

This affects startup scripts and the shell plus the build script.",5.0,False,5.0,0,0.0,0,2,False
"XD-1853","Story","Sprint 30","2014-06-17T05:59:43.000+0000","grenfro","grenfro","Label Fixture does not need to inherit from AbstractModuleFixture","The Tap fixture does not need to inherit from AbstractModuleFixture
Replace moduleName method with moduleToTap.  
The current tap syntax is: tap:stream:<streamname>.<modulelabel>
and not  tap:stream:<streamname>.<modulelabel>.<modulename> as currently implemented by the label fixture.
",3.0,0.9961901908812056,0.0,0.0374240480361573,True,True,True,1402984783000.0,1403290715000.0,1403291885000.0,1403290985000.0,1403291885000.0,1402984783000.0,-1.0,1402996276000.0,1402999876000.0,1402984783000.0,1402988383000.0,1403290715000.0,1403291885000.0,"grenfro",1402999876000.0,1402984783000.0,44,1402984783000.0,-1.0,"Tap Fixture refactoring","The Tap fixture does not need to inherit from AbstractModuleFixture
Replace moduleName method with moduleToTap.  
The current tap syntax is: tap:stream:<streamname>.<modulelabel>
and not  tap:stream:<streamname>.<modulelabel>.<modulename> as currently implemented by the label fixture.
",3.0,False,3.0,0,0.0,0,1,True
"XD-1851","Improvement","Sprint 30","2014-06-13T13:36:10.000+0000","iperumal","pperalta","Introduce cache to ZooKeeperContainerRepository","Add Cache implementation for ZooKeeperContainerRepository",5.0,0.7835723598435462,0.0,0.7774880486744894,True,True,True,1402666570000.0,1402668373000.0,1402668871000.0,1402667971000.0,1402668871000.0,1402666570000.0,-1.0,1402668359000.0,1402668871000.0,1402666570000.0,1402668871000.0,1402668373000.0,1402668871000.0,"iperumal",1402668871000.0,1402666570000.0,44,1402666570000.0,-1.0,"Introduce cache to ZooKeeperContainerRepository","Add Cache implementation for ZooKeeperContainerRepository",5.0,False,5.0,0,0.0,0,0,False
"XD-1850","Bug","Sprint 30","2014-06-13T12:52:16.000+0000","iperumal","pperalta","IllegalStateException when deploying orphaned stream modules upon a matching container arrival","Upon a matching container arrival, if there are orphaned stream modules to be deployed, then following exception is thrown:

java.lang.IllegalStateException: Container missing
    at org.springframework.util.Assert.state(Assert.java:385)
    at org.springframework.xd.dirt.core.StreamDeploymentsPath.hasDeploymentInfo(StreamDeploymentsPath.java:275)
    at org.springframework.xd.dirt.core.StreamDeploymentsPath.build(StreamDeploymentsPath.java:233)
    at org.springframework.xd.dirt.server.ContainerListener.getContainersForStreamModule(ContainerListener.java:337)
    at org.springframework.xd.dirt.server.ContainerListener.redeployStreams(ContainerListener.java:278)
    at org.springframework.xd.dirt.server.ContainerListener.onChildAdded(ContainerListener.java:186)
    at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:155)",3.0,0.9808118081180812,0.0199261992619926,0.9586715867158672,True,True,True,1402663936000.0,1402665265000.0,1402665291000.0,1402664391000.0,1402665291000.0,1402663936000.0,-1.0,1402665235000.0,1402665291000.0,1402663963000.0,1402665291000.0,1402665265000.0,1402665291000.0,"iperumal",1402665291000.0,1402663936000.0,44,1402663936000.0,-1.0,"IllegalStateException when deploying orphaned stream modules upon a matching container arrival","Upon a matching container arrival, if there are orphaned stream modules to be deployed, then following exception is thrown:

java.lang.IllegalStateException: Container missing
    at org.springframework.util.Assert.state(Assert.java:385)
    at org.springframework.xd.dirt.core.StreamDeploymentsPath.hasDeploymentInfo(StreamDeploymentsPath.java:275)
    at org.springframework.xd.dirt.core.StreamDeploymentsPath.build(StreamDeploymentsPath.java:233)
    at org.springframework.xd.dirt.server.ContainerListener.getContainersForStreamModule(ContainerListener.java:337)
    at org.springframework.xd.dirt.server.ContainerListener.redeployStreams(ContainerListener.java:278)
    at org.springframework.xd.dirt.server.ContainerListener.onChildAdded(ContainerListener.java:186)
    at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:155)",3.0,False,3.0,0,0.0,0,0,False
"XD-1849","Story","Sprint 31","2014-06-13T09:39:51.000+0000","dturanski","","Consolidate REST endpoints for batch resources under /jobs","currently we have /batch and /jobs. Everything should move to /jobs. See https://github.com/spring-projects/spring-xd/wiki/REST-API for details.",4.0,0.7389534320470712,0.7358138653621433,0.6714814690977303,True,True,True,1402652391000.0,1404376226000.0,1404379826000.0,1404984297000.0,1404985197000.0,1402652391000.0,-1.0,1404218827000.0,1404222427000.0,1404368902000.0,1404372502000.0,1404376226000.0,1404379826000.0,"dturanski",1404222427000.0,1402652391000.0,44,1402652391000.0,-1.0,"Consolidate REST endpoints for batch resources under /jobs","currently we have /batch and /jobs. Everything should move to /jobs. See https://github.com/spring-projects/spring-xd/wiki/REST-API for details.",4.0,False,4.0,0,0.0,0,0,False
"XD-1848","Story","Sprint 30","2014-06-12T16:21:26.000+0000","pperalta","","Modify REST controller to obtain stream/job state","See the [design document|https://docs.google.com/a/gopivotal.com/document/d/1kWtoH_xEF1wMklzQ8AZaiuhBZWIlpCDi8G9_hAP8Fgc/edit#heading=h.2rk74f16ow4i] for more details.

The REST controller needs to be modified to obtain stream/job state once it is available in ZooKeeper. This depends on XD-1847.",5.0,0.9296407403196174,0.4962490734890926,0.0001812413493589,True,True,True,1402590086000.0,1403677497000.0,1403681097000.0,1403758897000.0,1403759797000.0,1402590086000.0,1403677976000.0,1402590298000.0,1402593898000.0,1403170554000.0,1403174154000.0,1403677497000.0,1403681097000.0,"pperalta",1402593898000.0,1402590086000.0,44,1403677976000.0,1403677976000.0,"Modify REST controller to obtain stream/job state","See the [design document|https://docs.google.com/a/gopivotal.com/document/d/1kWtoH_xEF1wMklzQ8AZaiuhBZWIlpCDi8G9_hAP8Fgc/edit#heading=h.2rk74f16ow4i] for more details.

The REST controller needs to be modified to obtain stream/job state once it is available in ZooKeeper. This depends on XD-1847.",3.0,True,3.0,1,66.66666666666666,0,0,False
"XD-1846","Story","Sprint 30","2014-06-12T16:13:24.000+0000","pperalta","","Improve DeploymentVerifier when stream state is complete","As part of XD-1591, {{DeploymentVerifier}} was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties (such as count) into account:

https://github.com/spring-projects/spring-xd/pull/939/files#r13730134

This means the implementation is incorrect. For now this won't affect us since all tests at the moment are single node. However this can be drastically improved (and simplified) once XD-1270 is completed. At that point we'll be able to simply read a single ZK node to determine if/when a deployment succeeded. ",3.0,0.9400677337819752,0.9400677337819752,0.0006662013200729,True,True,True,1402589604000.0,1403535032000.0,1403538632000.0,1403594406000.0,1403595306000.0,1402589604000.0,-1.0,1402590274000.0,1402593874000.0,1403535032000.0,1403538632000.0,1403535032000.0,1403538632000.0,"pperalta",1402593874000.0,1402589604000.0,44,1402589604000.0,-1.0,"Improve DeploymentVerifier when stream state is complete","As part of XD-1591, {{DeploymentVerifier}} was modified to take the node structure into account. As indicated in the review below, the implementation does not take module properties (such as count) into account:

https://github.com/spring-projects/spring-xd/pull/939/files#r13730134

This means the implementation is incorrect. For now this won't affect us since all tests at the moment are single node. However this can be drastically improved (and simplified) once XD-1270 is completed. At that point we'll be able to simply read a single ZK node to determine if/when a deployment succeeded. ",3.0,False,3.0,0,0.0,0,0,False
"XD-1840","Improvement","Sprint 29","2014-06-12T08:43:40.000+0000","dturanski","dturanski","Document and review REST API","REST API needs to be finalized and documented for the GA release. The API to be reviewed by REST experts ",8.0,0.0001812974399334,0.0,0.0,True,True,True,1402562620000.0,1402562793000.0,1402566393000.0,1403515953000.0,1403516853000.0,1402562620000.0,-1.0,1402562620000.0,1402566220000.0,1402562620000.0,1402566220000.0,1402562793000.0,1402566393000.0,"dturanski",1402566220000.0,1402562620000.0,44,1402562620000.0,-1.0,"Document and review REST API","REST API needs to be finalized and documented for the GA release. The API to be reviewed by REST experts ",8.0,False,8.0,0,0.0,0,0,False
"XD-1839","Story","Sprint 30","2014-06-12T06:11:17.000+0000","eric.bottard","eric.bottard","Do not allow the use of named channels in composed modules","This needs closer inspection, but here are some things that currently do not work, either at the parser level, or at actual deployment time:

{noformat}
xd:>module compose foo --definition ""queue:bar > filter""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'filter' and type 'sink'

xd:>module compose foolog --definition ""queue:foo > log""
Successfully created module 'foolog' with type sink
==> should fail (not a module, but a full stream)

xd:>module compose foo --definition ""queue:bar > filter | transform""
Successfully created module 'foo' with type processor
==> should be source
{noformat}",8.0,-1.0,0.7183390981316934,0.9999878411283432,False,True,True,1402553477000.0,-1.0,-1.0,1404115222000.0,1404116122000.0,1402553477000.0,-1.0,1404116103000.0,1404116122000.0,1403675986000.0,1403679586000.0,-1.0,-1.0,"eric.bottard",1404116122000.0,1402553477000.0,44,1402553477000.0,-1.0,"Do not allow the use of named channels in composed modules","This needs closer inspection, but here are some things that currently do not work, either at the parser level, or at actual deployment time:

{noformat}
xd:>module compose foo --definition ""queue:bar > filter""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Could not find module with name 'filter' and type 'sink'

xd:>module compose foolog --definition ""queue:foo > log""
Successfully created module 'foolog' with type sink
==> should fail (not a module, but a full stream)

xd:>module compose foo --definition ""queue:bar > filter | transform""
Successfully created module 'foo' with type processor
==> should be source
{noformat}",8.0,False,8.0,0,0.0,0,1,False
"XD-1838","Bug","Sprint 30","2014-06-11T18:01:45.000+0000","grenfro","grenfro","FileSourceTest needs to apply label to source and sink","* Currently Acceptance FileSource Acceptance Tests are failing
** This is because the sink that tests the result for the file source test is a filesink.  Both use the ""file"" token.  Thus causing a failure
* SimpleFileSource and SimpleFileSink needs to support a label method.
* Update testFileSource to use the labels.",3.0,7.22291897854793e-05,0.0,0.3532532683708378,True,True,True,1402509705000.0,1402509716000.0,1402513316000.0,1402661098000.0,1402661998000.0,1402509705000.0,-1.0,1402563503000.0,1402567103000.0,1402509705000.0,1402513305000.0,1402509716000.0,1402513316000.0,"grenfro",1402567103000.0,1402509705000.0,44,1402509705000.0,-1.0,"FileSourceTest needs to apply label to source and sink","* Currently Acceptance FileSource Acceptance Tests are failing
** This is because the sink that tests the result for the file source test is a filesink.  Both use the ""file"" token.  Thus causing a failure
* SimpleFileSource and SimpleFileSink needs to support a label method.
* Update testFileSource to use the labels.",3.0,False,3.0,0,0.0,-1,0,False
"XD-1837","Improvement","Sprint 32","2014-06-11T10:45:57.000+0000","abilan","","Resolve compile warnings","For example:
{noformat}
:spring-xd-dirt:compileTestJava                                                                        
warning: [options] bootstrap class path not set in conjunction with -source 1.7
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\bus\LocalMessageBusTests.java:95: warning: [overrides] Class Foo overrides equals, but neither it nor any superclass overrides hashCode method
        static class Foo {                      
               ^                                
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\bus\serializer\kryo\CompositeCodecTests.java:77: warning: [overrides] Class SomeClassWithNoDefaultConstructors overrides equals, but neither it nor any superclass override
s hashCode method
        static class SomeClassWithNoDefaultConstructors {
               ^                                
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\bus\serializer\kryo\KryoCodecTests.java:77: warning: [overrides] Class SomeClassWithNoDefaultConstructors overrides equals, but neither it nor any superclass overrides has
hCode method
        static class SomeClassWithNoDefaultConstructors {
               ^                                
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\redis\KryoMessageSerializerTests.java:64: warning: [overrides] Class Foo overrides equals, but neither it nor any superclass overrides hashCode method
        public static class Foo {               
                      ^                         
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\stream\TypeConvertingStreamTests.java:181: warning: [rawtypes] found raw type: Map
                                Map map = (Map) message.getPayload();
                                ^               
  missing type arguments for generic class Map<K,V>
  where K,V are type-variables:                 
    K extends Object declared in interface Map  
    V extends Object declared in interface Map  
6 warnings 
{noformat}",1.0,-1.0,0.9994243405287948,0.971946326661096,False,True,True,1402483557000.0,-1.0,-1.0,1406029893000.0,1406030793000.0,1402483557000.0,-1.0,1405931280000.0,1405934880000.0,1406028751000.0,1406030793000.0,-1.0,-1.0,"abilan",1405934880000.0,1402483557000.0,44,1402483557000.0,-1.0,"Resolve compile warnings","For example:
{noformat}
:spring-xd-dirt:compileTestJava                                                                        
warning: [options] bootstrap class path not set in conjunction with -source 1.7
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\bus\LocalMessageBusTests.java:95: warning: [overrides] Class Foo overrides equals, but neither it nor any superclass overrides hashCode method
        static class Foo {                      
               ^                                
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\bus\serializer\kryo\CompositeCodecTests.java:77: warning: [overrides] Class SomeClassWithNoDefaultConstructors overrides equals, but neither it nor any superclass override
s hashCode method
        static class SomeClassWithNoDefaultConstructors {
               ^                                
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\bus\serializer\kryo\KryoCodecTests.java:77: warning: [overrides] Class SomeClassWithNoDefaultConstructors overrides equals, but neither it nor any superclass overrides has
hCode method
        static class SomeClassWithNoDefaultConstructors {
               ^                                
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\integration\redis\KryoMessageSerializerTests.java:64: warning: [overrides] Class Foo overrides equals, but neither it nor any superclass overrides hashCode method
        public static class Foo {               
                      ^                         
D:\SpringIO\spring-xd\spring-xd-dirt\src\test\java\org\springframework\xd\dirt\stream\TypeConvertingStreamTests.java:181: warning: [rawtypes] found raw type: Map
                                Map map = (Map) message.getPayload();
                                ^               
  missing type arguments for generic class Map<K,V>
  where K,V are type-variables:                 
    K extends Object declared in interface Map  
    V extends Object declared in interface Map  
6 warnings 
{noformat}",1.0,False,1.0,0,0.0,0,0,False
"XD-1835","Story","Sprint 31","2014-06-10T11:26:53.000+0000","iperumal","Ilayaperumal Gopinathan","Job execution display to show job deployment/definition status","With XD-1311, the job execution list shows the definition/deployment status of the associated job. We need to show the same information for a given job execution.",2.0,0.9608192359116436,0.9283849435138743,0.7017711561621683,True,True,True,1402399613000.0,1404890362000.0,1404893962000.0,1404991031000.0,1404991931000.0,1402399613000.0,-1.0,1404218827000.0,1404222427000.0,1404806282000.0,1404809882000.0,1404890362000.0,1404893962000.0,"iperumal",1404222427000.0,1402399613000.0,44,1402399613000.0,-1.0,"Job execution display to show job deployment/definition status","With XD-1311, the job execution list shows the definition/deployment status of the associated job. We need to show the same information for a given job execution.",2.0,False,2.0,0,0.0,0,0,False
"XD-1834","Story","Sprint 29","2014-06-10T03:45:37.000+0000","mark.pollack","pperalta","Add single threaded executor service to DeploymentSupervisor","This will
eliminate any race conditions between deployments and containers
joining/leaving the cluster.",2.0,0.9716457320534018,0.0,0.9713249265849024,True,True,True,1402371937000.0,1402411311000.0,1402412460000.0,1402411560000.0,1402412460000.0,1402371937000.0,-1.0,1402411298000.0,1402412460000.0,1402371937000.0,1402375537000.0,1402411311000.0,1402412460000.0,"mark.pollack",1402412460000.0,1402371937000.0,44,1402371937000.0,-1.0,"Add single threaded executor service to DeploymentSupervisor","This will
eliminate any race conditions between deployments and containers
joining/leaving the cluster.",2.0,False,2.0,0,0.0,0,0,False
"XD-1833","Story","Sprint 30","2014-06-09T14:26:10.000+0000","grussell","","Fix JMS Property Names","The JMS Source/Sink has a pluggable provider (default {{activemq}}) but the URL property {{amqUrl}} implies activeMQ - the property name should be generic (found while testing XD-1149).",1.0,0.5428454125870126,0.5428454125870126,0.2036172980140027,True,True,True,1402323970000.0,1402969208000.0,1402972808000.0,1403511692000.0,1403512592000.0,1402323970000.0,-1.0,1402565994000.0,1402569594000.0,1402969208000.0,1402972808000.0,1402969208000.0,1402972808000.0,"grussell",1402569594000.0,1402323970000.0,44,1402323970000.0,-1.0,"Fix JMS Property Names","The JMS Source/Sink has a pluggable provider (default {{activemq}}) but the URL property {{amqUrl}} implies activeMQ - the property name should be generic (found while testing XD-1149).",1.0,False,1.0,0,0.0,0,1,False
"XD-1831","Improvement","Sprint 30","2014-06-09T10:11:43.000+0000","mminella","hillert","Mask Database Password in Admin UI","When deploying a batch job, the UI displays the database password found in the server.yml in plain text to the user.  At the very least, this should be displayed in a password field so it's masked out and have it masked out in the resulting definition at the bottom of the page.  Ideally, we wouldn't provide the password on that page at all and only accept overriding options (if the user wants a password other than the configured one, enter itotherwise, we'll use what we have).

I'm finding that this occurs in other places as well.  A full pass though of the UI should be done to mask out passwords (or eliminate their display all together).",2.0,0.27942869510189,0.0063257961969514,0.2125992545978671,True,True,True,1402308703000.0,1402648260000.0,1402651860000.0,1403522986000.0,1403523886000.0,1402308703000.0,-1.0,1402567050000.0,1402570650000.0,1402316390000.0,1402319990000.0,1402648260000.0,1402651860000.0,"mminella",1402570650000.0,1402308703000.0,44,1402308703000.0,-1.0,"Mask Database Passwords in REST Controllers and Admin UI","When deploying a batch job, the UI displays the database password found in the server.yml in plain text to the user.  At the very least, this should be displayed in a password field so it's masked out and have it masked out in the resulting definition at the bottom of the page.  Ideally, we wouldn't provide the password on that page at all and only accept overriding options (if the user wants a password other than the configured one, enter itotherwise, we'll use what we have).

I'm finding that this occurs in other places as well.  A full pass though of the UI should be done to mask out passwords (or eliminate their display all together).",2.0,False,2.0,0,0.0,0,1,True
"XD-1828","Improvement","","2014-06-09T06:44:16.000+0000","grenfro","grenfro","Need to add generous timeout to TwitterSearchTest","TwitterSearchTest is the only test that is dependent on an external system for its success.  As such there are times that the service is running slower than the test expects, thus the test fails un-necessarily.   Once XD-1814 is merged we can utilize the ""waitForFile"" feature to wait for the result file from the stream to be written.  But the wait time for twitter will be extended to 1 min.  ",4.0,-1.0,0.0,-1.0,False,False,True,1402296256000.0,-1.0,-1.0,1404992334000.0,1404993234000.0,1402296256000.0,-1.0,-1.0,-1.0,1402296256000.0,1402299856000.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1402296256000.0,-1.0,"Update TwitterSearchTest to use #katyperry","TwitterSearchTest is the only test that is dependent on an external system for its success.  As such there are times that the service is running slower than the test expects, thus the test fails un-necessarily.   Once XD-1814 is merged we can utilize the ""waitForFile"" feature to wait for the result file from the stream to be written.  But the wait time for twitter will be extended to 1 min.  
AAAAND make the the search string configurable.  Some tests fail because the #springio is not consistently present.",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1827","Improvement","Sprint 32","2014-06-06T16:32:33.000+0000","grenfro","","Modules utilizing Jdbc Data Source need to offer connection pool configurations externally.","Currently JdbcSink, HdfsJdbc&  FileJdbc offer only driverClassName, url, user name & password.  They need to offer a full range of configurations offered by the Tomcat Jdbc Connection pool.",2.0,-1.0,1.000562135670188,0.9991892921311104,False,True,True,1402072353000.0,-1.0,-1.0,1405933510000.0,1405934410000.0,1402072353000.0,-1.0,1405931279000.0,1405934410000.0,1405936581000.0,1405934410000.0,-1.0,-1.0,"grenfro",1405934410000.0,1405930123000.0,44,1405930123000.0,-1.0,"Modules utilizing Jdbc Data Source need to offer connection pool configurations externally.","Currently JdbcSink, HdfsJdbc&  FileJdbc offer only driverClassName, url, user name & password.  They need to offer a full range of configurations offered by the Tomcat Jdbc Connection pool.",2.0,False,2.0,0,0.0,0,0,False
"XD-1825","Improvement","Sprint 32","2014-06-06T12:32:19.000+0000","mark.pollack","","Remove all javadoc warnings","",5.0,0.984577196789337,0.9825953396043066,0.9584728044603046,True,True,True,1402057939000.0,1406036772000.0,1406040372000.0,1406098198000.0,1406099098000.0,1402057939000.0,-1.0,1405931280000.0,1405934880000.0,1406028763000.0,1406032363000.0,1406036772000.0,1406040372000.0,"mark.pollack",1405934880000.0,1402057939000.0,44,1402057939000.0,-1.0,"Remove all javadoc warnings","",5.0,False,5.0,0,0.0,0,0,False
"XD-1824","Story","Sprint 30","2014-06-06T12:11:28.000+0000","grussell","grussell","Add Support for addresses Property on RabbitMQ Source","Support receiving messages from an HA cluster.",1.0,0.8834681703768258,0.8712242398462163,0.8712629921657229,True,True,True,1402056688000.0,1403036994000.0,1403040594000.0,1403165399000.0,1403166299000.0,1402056688000.0,-1.0,1403023451000.0,1403027051000.0,1403023408000.0,1403027008000.0,1403036994000.0,1403040594000.0,"grussell",1403027051000.0,1402056688000.0,44,1402056688000.0,-1.0,"Add Support for addresses Property on RabbitMQ Source","Support receiving messages from an HA cluster.",1.0,False,1.0,0,0.0,0,1,False
"XD-1823","Story","Sprint 31","2014-06-06T12:03:06.000+0000","hillert","hillert","Investigate need for UI Pagination","This issue could be more involved. Proper pagination may not be implemented correctly by the REST controller (making the respective service call).

This would also necessitate some form of improved state management for the UI. E.g.

* User is on page 5 of the listing of Job Executions
* User views details
* User presses the back-button (on the screen)
* The the listing of Job Executions *should* be still on page 5

",8.0,0.7920611798980335,0.0,0.7616638796575571,True,True,True,1402056186000.0,1404305136000.0,1404308736000.0,1404894650000.0,1404895550000.0,1402056186000.0,-1.0,1404218827000.0,1404222427000.0,1402056186000.0,1402059786000.0,1404305136000.0,1404308736000.0,"hillert",1404222427000.0,1402056186000.0,44,1402056186000.0,-1.0,"Investigate need for UI Pagination","This issue could be more involved. Proper pagination may not be implemented correctly by the REST controller (making the respective service call).

This would also necessitate some form of improved state management for the UI. E.g.

* User is on page 5 of the listing of Job Executions
* User views details
* User presses the back-button (on the screen)
* The the listing of Job Executions *should* be still on page 5

",8.0,False,8.0,0,0.0,0,1,False
"XD-1822","Story","Sprint 30","2014-06-06T11:29:14.000+0000","iperumal","iperumal","Combine Distributed job locator related schema changes into one table","Currently, there are JOB_REGISTRY_NAMES, JOB_REGISTRY_RESTARTABLES and JOB_REGISTRY_INCREMENTABLES tables and we can possibly combine them into one table and have a better schema for this.",5.0,0.7739335442673408,1.2875603374492344e-05,0.8263650341982802,True,True,True,1402054154000.0,1403196216000.0,1403199816000.0,1403528913000.0,1403529813000.0,1402054154000.0,-1.0,1403273587000.0,1403277187000.0,1402054173000.0,1402057773000.0,1403196216000.0,1403199816000.0,"iperumal",1403277187000.0,1402054154000.0,44,1402054154000.0,-1.0,"Combine Distributed job locator related schema changes into one table","Currently, there are JOB_REGISTRY_NAMES, JOB_REGISTRY_RESTARTABLES and JOB_REGISTRY_INCREMENTABLES tables and we can possibly combine them into one table and have a better schema for this.",5.0,False,5.0,0,0.0,0,0,False
"XD-1819","Story","Sprint 30","2014-06-06T10:41:48.000+0000","mark.pollack","dturanski","Investigate increased size of XD distribution.","",2.0,0.6969809313716454,0.6410381406704778,0.696952938469553,True,True,True,1402051308000.0,1403072146000.0,1403075746000.0,1403515065000.0,1403515965000.0,1402051308000.0,-1.0,1403072105000.0,1403075705000.0,1402990209000.0,1402993809000.0,1403072146000.0,1403075746000.0,"mark.pollack",1403075705000.0,1402051308000.0,44,1402051308000.0,-1.0,"Investigate increased size of XD distribution.","",2.0,False,2.0,0,0.0,0,0,False
"XD-1817","Bug","Sprint 30","2014-06-06T08:58:51.000+0000","mark.pollack","","ContainerListener to redeploy modules based on stream order.","When redeploying in the case of a container failure the modules are now redeployed in a random order.  The list of modules in the failed container needs to be sorted based on its position in a given stream and then redeployed.",5.0,0.9679773080073488,0.9679636911358356,0.5466849202575055,True,True,True,1402045131000.0,1402969257000.0,1402972857000.0,1402998929000.0,1402999829000.0,1402045131000.0,-1.0,1402567050000.0,1402570650000.0,1402969244000.0,1402972844000.0,1402969257000.0,1402972857000.0,"mark.pollack",1402570650000.0,1402045131000.0,44,1402045131000.0,-1.0,"ContainerListener to redeploy modules based on stream order.","When redeploying in the case of a container failure the modules are now redeployed in a random order.  The list of modules in the failed container needs to be sorted based on its position in a given stream and then redeployed.",5.0,False,5.0,0,0.0,-1,0,False
"XD-1816","Story","Sprint 30","2014-06-06T04:32:41.000+0000","eric.bottard","eric.bottard","Generate asciidoc doc from module options","Generate asciidoc fragments for each module's options, this way it is always up to date.",3.0,-1.0,0.99997910570824,0.9999951782403632,False,True,True,1402029161000.0,-1.0,-1.0,1403272620000.0,1403273520000.0,1402029161000.0,-1.0,1403273514000.0,1403273520000.0,1403273494000.0,1403273520000.0,-1.0,-1.0,"eric.bottard",1403273520000.0,1402029161000.0,44,1402029161000.0,-1.0,"Generate asciidoc doc from module options","Generate asciidoc fragments for each module's options, this way it is always up to date.",3.0,False,3.0,0,0.0,0,0,False
"XD-1814","Story","Sprint 30","2014-06-05T13:44:58.000+0000","grenfro","grenfro","Acceptance Tests for Labels and taps","",8.0,3.1599025600956014e-05,0.0,0.8439683205405157,True,True,True,1401975898000.0,1401975920000.0,1401979520000.0,1402671222000.0,1402672122000.0,1401975898000.0,-1.0,1402563489000.0,1402567089000.0,1401975898000.0,1401979498000.0,1401975920000.0,1401979520000.0,"grenfro",1402567089000.0,1401975898000.0,44,1401975898000.0,-1.0,"Acceptance Tests for Labels and taps","",8.0,False,8.0,0,0.0,-1,1,False
"XD-1813","Story","Sprint 29","2014-06-05T12:43:38.000+0000","mark.pollack","dturanski","Upgrade to Spring Boot 1.1 SNAPSHOT","",1.0,0.8244796046973166,0.7929211467399203,0.7849803087230094,True,True,True,1401972218000.0,1402318276000.0,1402321876000.0,1402391047000.0,1402391947000.0,1401972218000.0,-1.0,1402301697000.0,1402305297000.0,1402305030000.0,1402308630000.0,1402318276000.0,1402321876000.0,"mark.pollack",1402305297000.0,1402050509000.0,44,1402050509000.0,-1.0,"Upgrade to Spring Boot 1.1 SNAPSHOT","",1.0,False,1.0,0,0.0,0,0,False
"XD-1812","Story","Sprint 29","2014-06-05T12:36:24.000+0000","grussell","grussell","Support Bus Producer Properties for Dynamic Producers","Pass module properties from stream plugin to {{MessageBusAwareChannelResolver}}.

Disallow partitioning properties.",2.0,0.9436089737977816,0.0008008845396859,0.943586276665078,True,True,True,1401971784000.0,1402553819000.0,1402557419000.0,1402587702000.0,1402588602000.0,1401971784000.0,-1.0,1402553805000.0,1402557405000.0,1401972278000.0,1401975878000.0,1402553819000.0,1402557419000.0,"grussell",1402557405000.0,1401971784000.0,44,1401971784000.0,-1.0,"Support Bus Producer Properties for Dynamic Producers","Pass module properties from stream plugin to {{MessageBusAwareChannelResolver}}.

Disallow partitioning properties.",2.0,False,2.0,0,0.0,0,0,False
"XD-1811","Story","Sprint 34","2014-06-05T08:28:44.000+0000","iperumal","iperumal","Add ResourceModuleRegistry with custom modules location","If someone wants to have a dedicated location (module registry) for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support.

Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location `xd.module.home` and `classpath:/modules/`. 

Maybe we can add an additional ResourceModuleRegistry with `xd.custom.module.home` and use it for custom modules.",1.0,0.7324084466493372,0.7048122619112193,0.7403079872657707,True,True,True,1401956924000.0,1408978348000.0,1408981948000.0,1411542784000.0,1411543684000.0,1401956924000.0,1409063252000.0,1409054079000.0,1409057679000.0,1408713790000.0,1408717390000.0,1408978348000.0,1408981948000.0,"iperumal",1409057679000.0,1401956924000.0,44,1409063252000.0,1409063252000.0,"Add ResourceModuleRegistry with custom modules location","If someone wants to have a dedicated location (module registry) for the custom modules and can be accessed from any Spring supported resource URL location, then we need to support.

Currently, we use Delegating ModuleRegistry which uses ResourceModuleRegistry implementations that look for location `xd.module.home` and `classpath:/modules/`. 

Maybe we can add an additional ResourceModuleRegistry with `xd.custom.module.home` and use it for custom modules.",2.0,True,2.0,1,-50.0,0,0,False
"XD-1810","Bug","Sprint 30","2014-06-05T07:09:52.000+0000","grenfro","grenfro","Assess XD Fails to connect to remote Redis Instance","Deployment: Admin/Container Redis as data transport
SHA: 45e1beb

[Description]
In the case that the Redis is not running locally XD cannot connect to the Redis instance even though the environment variable spring_redis_host has been set.  

[Steps to reproduce]
* Shutdown local instance of Redis.
* For both the admin and container execute the command prior to running the instances:
** export spring_redis_host=YourRedisHost
* Start admin and container instances
* deploy a simple stream 
** You will see the following error:  13:56:59,647 ERROR task-scheduler-9 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.integration.redis.outbound.RedisQueueOutboundChannelAdapter@6a1f1d12]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$SendingHandler.handleMessageInternal(RedisMessageBus.java:235)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy57.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:110)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:205)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:284)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:278)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Cannot get Jedis connection; nested exception is redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.fetchJedisConnector(JedisConnectionFactory.java:97)
	at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.getConnection(JedisConnectionFactory.java:143)
	at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.getConnection(JedisConnectionFactory.java:41)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:85)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:55)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:169)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:149)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:84)
	at org.springframework.data.redis.core.DefaultListOperations.leftPush(DefaultListOperations.java:68)
	at org.springframework.data.redis.core.DefaultBoundListOperations.leftPush(DefaultBoundListOperations.java:60)
	at org.springframework.integration.redis.outbound.RedisQueueOutboundChannelAdapter.handleMessageInternal(RedisQueueOutboundChannelAdapter.java:109)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 43 more
Caused by: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.util.Pool.getResource(Pool.java:42)
	at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.fetchJedisConnector(JedisConnectionFactory.java:90)
	... 54 more
Caused by: redis.clients.jedis.exceptions.JedisConnectionException: java.net.ConnectException: Connection refused
	at redis.clients.jedis.Connection.connect(Connection.java:142)
	at redis.clients.jedis.BinaryClient.connect(BinaryClient.java:75)
	at redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:1724)
	at redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:65)
	at org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:819)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:429)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:360)
	at redis.clients.util.Pool.getResource(Pool.java:40)
	... 55 more
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at redis.clients.jedis.Connection.connect(Connection.java:137)
	... 62 more",5.0,0.9874819335153364,0.9873229484502972,0.9874064557571864,True,True,True,1401952192000.0,1402567097000.0,1402570697000.0,1402573992000.0,1402574892000.0,1401952192000.0,-1.0,1402567050000.0,1402570650000.0,1402566998000.0,1402570598000.0,1402567097000.0,1402570697000.0,"grenfro",1402570650000.0,1402565584000.0,44,1402565584000.0,-1.0,"Assess XD Fails to connect to remote Redis Instance","Deployment: Admin/Container Redis as data transport
SHA: 45e1beb

[Description]
In the case that the Redis is not running locally XD cannot connect to the Redis instance even though the environment variable spring_redis_host has been set.  

[Steps to reproduce]
* Shutdown local instance of Redis.
* For both the admin and container execute the command prior to running the instances:
** export spring_redis_host=YourRedisHost
* Start admin and container instances
* deploy a simple stream 
** You will see the following error:  13:56:59,647 ERROR task-scheduler-9 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [org.springframework.integration.redis.outbound.RedisQueueOutboundChannelAdapter@6a1f1d12]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)
	at org.springframework.xd.dirt.integration.redis.RedisMessageBus$SendingHandler.handleMessageInternal(RedisMessageBus.java:235)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:116)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy57.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:93)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:110)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:205)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:55)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:149)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:146)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:284)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:278)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.springframework.data.redis.RedisConnectionFailureException: Cannot get Jedis connection; nested exception is redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.fetchJedisConnector(JedisConnectionFactory.java:97)
	at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.getConnection(JedisConnectionFactory.java:143)
	at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.getConnection(JedisConnectionFactory.java:41)
	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:85)
	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:55)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:169)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:149)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:84)
	at org.springframework.data.redis.core.DefaultListOperations.leftPush(DefaultListOperations.java:68)
	at org.springframework.data.redis.core.DefaultBoundListOperations.leftPush(DefaultBoundListOperations.java:60)
	at org.springframework.integration.redis.outbound.RedisQueueOutboundChannelAdapter.handleMessageInternal(RedisQueueOutboundChannelAdapter.java:109)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 43 more
Caused by: redis.clients.jedis.exceptions.JedisConnectionException: Could not get a resource from the pool
	at redis.clients.util.Pool.getResource(Pool.java:42)
	at org.springframework.data.redis.connection.jedis.JedisConnectionFactory.fetchJedisConnector(JedisConnectionFactory.java:90)
	... 54 more
Caused by: redis.clients.jedis.exceptions.JedisConnectionException: java.net.ConnectException: Connection refused
	at redis.clients.jedis.Connection.connect(Connection.java:142)
	at redis.clients.jedis.BinaryClient.connect(BinaryClient.java:75)
	at redis.clients.jedis.BinaryJedis.connect(BinaryJedis.java:1724)
	at redis.clients.jedis.JedisFactory.makeObject(JedisFactory.java:65)
	at org.apache.commons.pool2.impl.GenericObjectPool.create(GenericObjectPool.java:819)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:429)
	at org.apache.commons.pool2.impl.GenericObjectPool.borrowObject(GenericObjectPool.java:360)
	at redis.clients.util.Pool.getResource(Pool.java:40)
	... 55 more
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at redis.clients.jedis.Connection.connect(Connection.java:137)
	... 62 more",5.0,False,5.0,0,0.0,0,1,False
"XD-1809","Story","Sprint 29","2014-06-04T21:37:45.000+0000","hillert","hillert","Improve E2E Test Coverage","",3.0,6.826446896449007e-05,0.0,0.999869676922886,True,True,True,1401917865000.0,1401917909000.0,1401921509000.0,1402561517000.0,1402562417000.0,1401917865000.0,-1.0,1402562333000.0,1402562417000.0,1401917865000.0,1401921465000.0,1401917909000.0,1401921509000.0,"hillert",1402562417000.0,1401917865000.0,44,1401917865000.0,-1.0,"Improve E2E Test Coverage","",3.0,False,3.0,0,0.0,0,2,False
"XD-1808","Story","","2014-06-04T20:52:04.000+0000","hillert","hillert","Apply global Http Interceptor in order to centralize error logging","Theoretically I would have liked to centralize logging of Http/Resource calls global more substantially - but see this limitation:

https://github.com/angular/angular.js/issues/4013",4.0,0.0002440107865281,0.0,-1.0,True,False,True,1401915124000.0,1401915163000.0,1401918763000.0,1402074053000.0,1402074953000.0,1401915124000.0,-1.0,-1.0,-1.0,1401915124000.0,1401918724000.0,1401915163000.0,1401918763000.0,"hillert",-1.0,-1.0,0,1401915124000.0,-1.0,"Add global Http Interceptor in order to centralize error logging","Theoretically I would have liked to centralize logging of Http/Resource calls global more substantially - but see this limitation:

https://github.com/angular/angular.js/issues/4013",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1807","Story","Sprint 29","2014-06-04T14:10:49.000+0000","iperumal","iperumal","Paging support for ModuleMetadata/Container repositories","Add paging support for the appropriate accessor methods in ModuleMetadata/Container repositories",3.0,0.0985262675171374,0.0985262675171374,0.2550697707659597,True,True,True,1401891049000.0,1401957222000.0,1401960822000.0,1402561777000.0,1402562677000.0,1401891049000.0,-1.0,1402062361000.0,1402065961000.0,1401957222000.0,1401960822000.0,1401957222000.0,1401960822000.0,"iperumal",1402065961000.0,1401891049000.0,44,1401891049000.0,-1.0,"Paging support for ModuleMetadata/Container repositories","Add paging support for the appropriate accessor methods in ModuleMetadata/Container repositories",3.0,False,3.0,0,0.0,0,0,False
"XD-1806","Story","","2014-06-04T08:04:42.000+0000","grenfro","grenfro","Add Eclipse target to EC2 build.gradle.","So that the code format matches that of the XD Project.",5.0,0.0723201323768487,0.0,-1.0,True,False,True,1401869082000.0,1401906494000.0,1401910094000.0,1402385493000.0,1402386393000.0,1401869082000.0,-1.0,-1.0,-1.0,1401869082000.0,1401872682000.0,1401906494000.0,1401910094000.0,"grenfro",-1.0,-1.0,0,1401869082000.0,-1.0,"Add Eclipse target to EC2 build.gradle.","So that the code format matches that of the XD Project.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1805","Story","Sprint 38","2014-06-04T05:41:53.000+0000","dturanski","","Support the ability to create module definitions in Groovy","XML is currently required for module definitions. XD should also support Java @Config and Groovy bean definitions and potentially, SI DSLs. ",8.0,0.9338909837786088,0.9338909837786088,0.9217378332117828,True,True,True,1401860513000.0,1415025577000.0,1415029177000.0,1415956616000.0,1415957516000.0,1401860513000.0,-1.0,1414854254000.0,1414857854000.0,1415025577000.0,1415029177000.0,1415025577000.0,1415029177000.0,"dturanski",1414857854000.0,1401860513000.0,44,1401860513000.0,-1.0,"Support the ability to create module definitions in Groovy","XML is currently required for module definitions. XD should also support Java @Config and Groovy bean definitions and potentially, SI DSLs. ",8.0,False,8.0,0,0.0,-1,1,False
"XD-1804","Story","Sprint 29","2014-06-04T01:07:07.000+0000","eric.bottard","","Remove unused parser code related to ""substreams"" & co","The XD parser had initial support for substreams, which have been subsumed by composed modules.
That legacy code is unused and not needed",5.0,0.2831627679133975,0.2831627679133975,0.1780680424697421,True,True,True,1401844027000.0,1402019753000.0,1402023353000.0,1402463710000.0,1402464610000.0,1401844027000.0,-1.0,1401954533000.0,1401958133000.0,1402019753000.0,1402023353000.0,1402019753000.0,1402023353000.0,"eric.bottard",1401958133000.0,1401844027000.0,44,1401844027000.0,-1.0,"Remove unused parser code related to ""substreams"" & co","The XD parser had initial support for substreams, which have been subsumed by composed modules.
That legacy code is unused and not needed",5.0,False,5.0,0,0.0,0,0,False
"XD-1802","Story","","2014-06-03T08:44:03.000+0000","eric.bottard","","Add @XmlRootElement to all REST resources","Some REST resources lack an @XmlRootElement annotation.

This causes a JAXB marshalling error when trying to access the API with an Accept header of xml (which is the default in most browsers)

This is a preliminary to XD-1800 (which is much more involved), only to fix ugly exception",1.0,-1.0,0.999972863326142,-1.0,False,False,True,1401785043000.0,-1.0,-1.0,1402078947000.0,1402079847000.0,1401785043000.0,-1.0,-1.0,-1.0,1402079839000.0,1402079847000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1401785043000.0,-1.0,"Add @XmlRootElement to all REST resources","Some REST resources lack an @XmlRootElement annotation.

This causes a JAXB marshalling error when trying to access the API with an Accept header of xml (which is the default in most browsers)

This is a preliminary to XD-1800 (which is much more involved), only to fix ugly exception",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1799","Bug","Sprint 29","2014-06-02T20:53:46.000+0000","iperumal","dturanski","Twittersearch: ArrayIndexOutOfBoundsException","{noformat}
0:44:30,715  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException: 255
	at groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)
	at groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:260)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:272)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.toJson(JsonOutput.java:204)
	at org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
20:44:30,718  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:31,136  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException: 255
	at groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)
	at groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:260)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:272)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.toJson(JsonOutput.java:204)
	at org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
20:44:31,137  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:31,525  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException: 255
	at groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)
	at groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:260)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:272)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.toJson(JsonOutput.java:204)
	at org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
20:44:31,526  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:31,948  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException: 255
	at groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)
	at groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:260)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:272)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.toJson(JsonOutput.java:204)
	at org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
20:44:31,949  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:32,345  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:32,346  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:32,727  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:32,727  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:33,103  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:33,104  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:33,548  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:33,548  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:33,935  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:33,935  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:34,318  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:34,318  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:34,696  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:34,696  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:35,060  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:35,061  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:35,445  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:35,445  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:35,825  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:35,825  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:36,221  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:36,221  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:36,602  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:36,602  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:37,006  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:37,006  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:37,396  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:37,396  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:37,790  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:37,790  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:38,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:38,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:38,559  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:38,559  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:38,967  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:38,967  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:39,365  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:39,365  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:39,747  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:39,747  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:40,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:40,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:40,596  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:40,597  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:40,978  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:40,979  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:41,342  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:41,342  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:41,732  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:41,732  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:42,125  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:42,126  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:42,511  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:42,512  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:42,918  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:42,918  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:43,309  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:43,309  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:43,689  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:43,689  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:44,071  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:44,071  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:44,470  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:44,470  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:44,847  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:44,847  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:45,307  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:45,308  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:45,710  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:45,710  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:46,136  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:46,137  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:46,530  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:46,530  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:46,913  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:46,913  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:47,306  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:47,306  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:47,700  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:47,700  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:48,124  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:48,124  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:48,520  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:48,520  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:48,912  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:48,912  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:49,292  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:49,293  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:49,683  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:49,683  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:50,135  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:50,135  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:45:06,533  INFO main-EventThread server.ContainerRegistrar:260 - Undeploying module [ModuleDescriptor@707a9713 moduleName = 'twittersearch', moduleLabel = [null], group = 's2', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map['query' -> 'clinton'], children = list[[empty]]]
20:45:06,533  INFO main-EventThread module.ModuleDeployer:158 - removed SimpleModule [name=twittersearch, type=source, group=s2, index=0 @21839092]
20:45:06,539  INFO DeploymentsPathChildrenCache-0 server.ContainerRegistrar:83 - Path cache event: /deployments/modules/db6bb769-0764-41a6-8080-7ca4870cb364/s2.source.twittersearch-0, type: CHILD_REMOVED
igopinathan:spring-xd-1.0.0.M7 igopinatha$ 20:46:08,739  INFO DeploymentsPathChildrenCache-0 server.ContainerRegistrar:83 - Path cache event: /deployments/modules/db6bb769-0764-41a6-8080-7ca4870cb364/s2.source.twittersearch-0, type: CHILD_ADDED
20:46:08,739  INFO DeploymentsPathChildrenCache-0 server.ContainerRegistrar:545 - Deploying module 'twittersearch-0' for stream 's2'
{noformat}",2.0,0.996192933181835,0.007840281913878,0.0084124716602214,True,True,True,1401742426000.0,1406636429000.0,1406640029000.0,1406654232000.0,1406655132000.0,1401742426000.0,-1.0,1401783754000.0,1401787354000.0,1401780943000.0,1401784543000.0,1406636429000.0,1406640029000.0,"iperumal",1401787354000.0,1401742426000.0,44,1401742426000.0,-1.0,"Twittersearch: ArrayIndexOutOfBoundsException","{noformat}
0:44:30,715  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException: 255
	at groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)
	at groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:260)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:272)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.toJson(JsonOutput.java:204)
	at org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
20:44:30,718  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:31,136  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException: 255
	at groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)
	at groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:260)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:272)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.toJson(JsonOutput.java:204)
	at org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
20:44:31,137  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:31,525  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException: 255
	at groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)
	at groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:260)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:272)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.toJson(JsonOutput.java:204)
	at org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
20:44:31,526  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:31,948  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException: 255
	at groovy.json.internal.CharBuf.doAddJsonEscapedString(CharBuf.java:525)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:394)
	at groovy.json.internal.CharBuf.addJsonEscapedString(CharBuf.java:357)
	at groovy.json.JsonOutput.writeCharSequence(JsonOutput.java:309)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:260)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeIterator(JsonOutput.java:442)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:272)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.writeObject(JsonOutput.java:270)
	at groovy.json.JsonOutput.writeMap(JsonOutput.java:425)
	at groovy.json.JsonOutput.toJson(JsonOutput.java:204)
	at org.springframework.integration.x.twitter.TwitterSearchChannelAdapter.doSendLine(TwitterSearchChannelAdapter.java:137)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:200)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask$2.extractData(AbstractTwitterInboundChannelAdapter.java:186)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:549)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:517)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.readStream(AbstractTwitterInboundChannelAdapter.java:180)
	at org.springframework.integration.x.twitter.AbstractTwitterInboundChannelAdapter$StreamReadingTask.run(AbstractTwitterInboundChannelAdapter.java:158)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
20:44:31,949  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:32,345  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:32,346  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:32,727  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:32,727  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:33,103  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:33,104  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:33,548  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:33,548  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:33,935  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:33,935  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:34,318  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:34,318  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:34,696  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:34,696  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:35,060  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:35,061  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:35,445  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:35,445  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:35,825  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:35,825  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:36,221  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:36,221  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:36,602  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:36,602  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:37,006  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:37,006  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:37,396  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:37,396  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:37,790  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:37,790  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:38,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:38,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:38,559  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:38,559  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:38,967  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:38,967  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:39,365  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:39,365  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:39,747  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:39,747  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:40,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:40,179  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:40,596  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:40,597  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:40,978  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:40,979  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:41,342  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:41,342  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:41,732  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:41,732  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:42,125  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:42,126  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:42,511  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:42,512  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:42,918  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:42,918  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:43,309  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:43,309  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:43,689  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:43,689  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:44,071  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:44,071  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:44,470  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:44,470  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:44,847  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:44,847  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:45,307  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:45,308  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:45,710  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:45,710  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:46,136  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:46,137  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:46,530  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:46,530  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:46,913  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:46,913  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:47,306  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:47,306  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:47,700  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:47,700  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:48,124  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:48,124  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:48,520  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:48,520  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:48,912  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:48,912  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:49,292  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:49,293  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:49,683  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:49,683  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:44:50,135  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:173 - Exception while reading stream.
java.lang.ArrayIndexOutOfBoundsException
20:44:50,135  WARN task-scheduler-6 twitter.TwitterSearchChannelAdapter:231 - Exception while reading stream, waiting for 250 ms before restarting
20:45:06,533  INFO main-EventThread server.ContainerRegistrar:260 - Undeploying module [ModuleDescriptor@707a9713 moduleName = 'twittersearch', moduleLabel = [null], group = 's2', sourceChannelName = [null], sinkChannelName = [null], sinkChannelName = [null], index = 0, type = source, parameters = map['query' -> 'clinton'], children = list[[empty]]]
20:45:06,533  INFO main-EventThread module.ModuleDeployer:158 - removed SimpleModule [name=twittersearch, type=source, group=s2, index=0 @21839092]
20:45:06,539  INFO DeploymentsPathChildrenCache-0 server.ContainerRegistrar:83 - Path cache event: /deployments/modules/db6bb769-0764-41a6-8080-7ca4870cb364/s2.source.twittersearch-0, type: CHILD_REMOVED
igopinathan:spring-xd-1.0.0.M7 igopinatha$ 20:46:08,739  INFO DeploymentsPathChildrenCache-0 server.ContainerRegistrar:83 - Path cache event: /deployments/modules/db6bb769-0764-41a6-8080-7ca4870cb364/s2.source.twittersearch-0, type: CHILD_ADDED
20:46:08,739  INFO DeploymentsPathChildrenCache-0 server.ContainerRegistrar:545 - Deploying module 'twittersearch-0' for stream 's2'
{noformat}",2.0,False,2.0,0,0.0,0,1,False
"XD-1795","Story","Sprint 29","2014-06-02T16:08:02.000+0000","grenfro","grenfro","XD-EC2 Needs to support XD_CONTAINER_GROUPS for created containers","XD-EC2 applies the environment variables to all container instances that are created.  This behavior has to be altered such that a environment variable can be applied to to a specific container instance.  

For example if we create a 3 node cluster Admin, Container1, Container2 & Container3.
For Example:
* XD1_XD_CONTAINER_GROUPS=GROUP1
* XD2_XD_CONTAINER_GROUPS=GROUP2

* In this example XD1_XD_CONTAINER_GROUPS=GROUP1 would apply XD_CONTAINER_GROUPS=GROUP1 to container1's environment.  
* XD2_XD_CONTAINER_GROUPS=GROUP2 would apply XD_CONTAINER_GROUPS=GROUP2 to container2's environment.  
* While container3 would not receive a specific environment setting for XD_CONTAINER_GROUPS.
",8.0,0.2257345255365141,0.0,0.088298871682855,True,True,True,1401725282000.0,1401874509000.0,1401878109000.0,1402385455000.0,1402386355000.0,1401725282000.0,-1.0,1401783654000.0,1401787254000.0,1401725282000.0,1401728882000.0,1401874509000.0,1401878109000.0,"grenfro",1401787254000.0,1401725282000.0,44,1401725282000.0,-1.0,"XD-EC2 Needs to support XD_CONTAINER_GROUPS for created containers","XD-EC2 applies the environment variables to all container instances that are created.  This behavior has to be altered such that a environment variable can be applied to to a specific container instance.  

For example if we create a 3 node cluster Admin, Container1, Container2 & Container3.
For Example:
* XD1_XD_CONTAINER_GROUPS=GROUP1
* XD2_XD_CONTAINER_GROUPS=GROUP2

* In this example XD1_XD_CONTAINER_GROUPS=GROUP1 would apply XD_CONTAINER_GROUPS=GROUP1 to container1's environment.  
* XD2_XD_CONTAINER_GROUPS=GROUP2 would apply XD_CONTAINER_GROUPS=GROUP2 to container2's environment.  
* While container3 would not receive a specific environment setting for XD_CONTAINER_GROUPS.
",8.0,False,8.0,0,0.0,0,0,False
"XD-1794","Bug","Sprint 30","2014-06-02T14:33:45.000+0000","iperumal","iperumal","Module count value at module deployments path","In case of module count > 1, the module deployments path for each deployed module always has: {""count"":""1""}

For a scenario:
The stream test1: ""http | log""
with the deployment manifest:
module.log.count=3,module.log.criteria=groups.contains('test')

get /xd/deployments/streams/test1module.log.count=3,module.log.criteria=groups.contains('test')
get /xd/deployments/modules/9ecaf59a-a1f5-4ed9-984d-f5dff8cc9b57/test1.sink.log-1{""count"":""1""}
get /xd/deployments/modules/1bbdb2dd-97ed-48a2-a3cd-3633c3e82f52/test1.sink.log-1{""count"":""1""}",5.0,0.2734347365838134,0.2734347365838134,0.2732441742249416,True,True,True,1401719625000.0,1402567641000.0,1402571241000.0,1404820072000.0,1404820972000.0,1401719625000.0,-1.0,1402567050000.0,1402570650000.0,1402567641000.0,1402571241000.0,1402567641000.0,1402571241000.0,"iperumal",1402570650000.0,1401743444000.0,44,1401743444000.0,-1.0,"Module count value at module deployments path","In case of module count > 1, the module deployments path for each deployed module always has: {""count"":""1""}

For a scenario:
The stream test1: ""http | log""
with the deployment manifest:
module.log.count=3,module.log.criteria=groups.contains('test')

get /xd/deployments/streams/test1module.log.count=3,module.log.criteria=groups.contains('test')
get /xd/deployments/modules/9ecaf59a-a1f5-4ed9-984d-f5dff8cc9b57/test1.sink.log-1{""count"":""1""}
get /xd/deployments/modules/1bbdb2dd-97ed-48a2-a3cd-3633c3e82f52/test1.sink.log-1{""count"":""1""}",5.0,False,5.0,0,0.0,0,0,False
"XD-1792","Improvement","Sprint 29","2014-06-02T05:43:41.000+0000","dturanski","dturanski","Array class names cannot be parsed to MimeType","MessageBusSupport creates an 'original content type' message header to support serialization for remote transports. The form of the header application/x-java-object;type=<classname>.  For java array types, the ""["" prefix causes an error when converting this value to a MimeType.    This can be avoided by quoting the classname. However, a further complication is if the array element is an object. In this case the classname is '[L<classname>;'. The trailing colon causes a parse exception even in a quoted string.  A simple fix is to check for the trailing colon, remove it and add it back if MimeType.getParameter(""type"").contains(""[L"").   See http://docs.oracle.com/javase/7/docs/api/java/lang/Class.html#getName for more info.  Preliminary testing indicates primitive array and multi-dimensional arrays will work fine with quoting, but tests should be added for these cases.  ",1.0,0.0108849016657756,0.0,0.1352302973495349,True,True,True,1401687821000.0,1401695497000.0,1401699097000.0,1402392118000.0,1402393018000.0,1401687821000.0,-1.0,1401783185000.0,1401786785000.0,1401687821000.0,1401691421000.0,1401695497000.0,1401699097000.0,"dturanski",1401786785000.0,1401687821000.0,44,1401687821000.0,-1.0,"Array class names cannot be parsed to MimeType","MessageBusSupport creates an 'original content type' message header to support serialization for remote transports. The form of the header application/x-java-object;type=<classname>.  For java array types, the ""["" prefix causes an error when converting this value to a MimeType.    This can be avoided by quoting the classname. However, a further complication is if the array element is an object. In this case the classname is '[L<classname>;'. The trailing colon causes a parse exception even in a quoted string.  A simple fix is to check for the trailing colon, remove it and add it back if MimeType.getParameter(""type"").contains(""[L"").   See http://docs.oracle.com/javase/7/docs/api/java/lang/Class.html#getName for more info.  Preliminary testing indicates primitive array and multi-dimensional arrays will work fine with quoting, but tests should be added for these cases.  ",1.0,False,1.0,0,0.0,0,0,False
"XD-1791","Story","Sprint 37","2014-06-01T05:59:50.000+0000","thomas.risberg","","New job that executes a Spark job","Create OOTB batch job that executes a job on Spark as a tasklet

could be something along this:

job create yarnJob --definition ""sparkjob --master=spark://localhost:7077 --class=SimpleApp""
",5.0,0.9527630701458076,0.9374630215040124,0.9054008240103224,True,True,True,1401602390000.0,1413987461000.0,1413991061000.0,1414600599000.0,1414601499000.0,1401602390000.0,-1.0,1413371794000.0,1413375394000.0,1413788574000.0,1413792174000.0,1413987461000.0,1413991061000.0,"thomas.risberg",1413375394000.0,1401602390000.0,44,1401602390000.0,-1.0,"New job that executes a Spark job","Create OOTB batch job that executes a job on Spark as a tasklet

could be something along this:

job create yarnJob --definition ""sparkjob --master=spark://localhost:7077 --class=SimpleApp""
",5.0,False,5.0,0,0.0,0,0,False
"XD-1788","Story","Sprint 31","2014-05-30T16:18:49.000+0000","grussell","eric.bottard","Detect Module Properties for Non-existent Modules","stream create foo --definition=""bar | baz""

stream deploy foo --properties=module.qux.fiz",4.0,-1.0,0.6051100293028989,0.9807181009811464,False,True,True,1401466729000.0,-1.0,-1.0,1404272036000.0,1404272936000.0,1401466729000.0,-1.0,1404218827000.0,1404222427000.0,1403164793000.0,1403168393000.0,-1.0,-1.0,"grussell",1404222427000.0,1401466729000.0,44,1401466729000.0,-1.0,"Detect Module Properties for Non-existent Modules","stream create foo --definition=""bar | baz""

stream deploy foo --properties=module.qux.fiz",4.0,False,4.0,0,0.0,0,0,False
"XD-1787","Story","Sprint 29","2014-05-30T16:16:54.000+0000","grussell","grussell","Detect Invalid Deployment Properties in the Bus","Detect properties the bus doesn't support.",3.0,-1.0,0.7847821341457627,0.7847512884029498,False,True,True,1401466614000.0,-1.0,-1.0,1401887166000.0,1401888066000.0,1401466614000.0,-1.0,1401797349000.0,1401800949000.0,1401797362000.0,1401800962000.0,-1.0,-1.0,"grussell",1401800949000.0,1401466614000.0,44,1401466614000.0,-1.0,"Detect Invalid Deployment Properties in the Bus","Detect properties the bus doesn't support.",3.0,False,3.0,0,0.0,0,0,False
"XD-1786","Story","Sprint 29","2014-05-30T16:14:47.000+0000","grussell","grussell","Support Partitioning/Bus Properties in the RedisMessageBus","",5.0,0.4859632689260695,0.4859523074010224,0.4859303843509282,True,True,True,1401466487000.0,1401954156000.0,1401957756000.0,1402469097000.0,1402469997000.0,1401466487000.0,-1.0,1401954123000.0,1401957723000.0,1401954145000.0,1401957745000.0,1401954156000.0,1401957756000.0,"grussell",1401957723000.0,1401466487000.0,44,1401466487000.0,-1.0,"Support Partitioning/Bus Properties in the RedisMessageBus","PR: https://github.com/spring-projects/spring-xd/pull/926",5.0,False,5.0,0,0.0,0,0,True
"XD-1784","Story","Sprint 29","2014-05-30T15:31:50.000+0000","iperumal","iperumal","REST endpoint/command interface for runtime module deployment properties","We need a way to access the deployment properties for the deployed modules.

For example: 'runtime module foo.sink.bar-2'",5.0,0.4464196590151443,7.619773311743975e-05,0.6082903133631774,True,True,True,1401463910000.0,1401698258000.0,1401701858000.0,1401987960000.0,1401988860000.0,1401463910000.0,-1.0,1401783232000.0,1401786832000.0,1401463950000.0,1401467550000.0,1401698258000.0,1401701858000.0,"iperumal",1401786832000.0,1401743498000.0,44,1401743498000.0,-1.0,"REST endpoint/command interface for runtime module deployment properties","We need a way to access the deployment properties for the deployed modules.

For example: 'runtime module foo.sink.bar-2'",5.0,False,5.0,0,0.0,0,0,False
"XD-1783","Bug","","2014-05-30T14:32:28.000+0000","mark.fisher","","Exceptions thrown on module redeployment","During a redeployment attempt, the following traces were logged on the admin:

{code}
17:22:59,702  WARN ContainersPathChildrenCache-0 server.ModuleDeploymentWriter:361 - Error while cleaning up failed deployment /deployments/modules/40494bdb-0d8c-4b5d-a895-bf94432d9d3b/s.sink.log-1
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/40494bdb-0d8c-4b5d-a895-bf94432d9d3b/s.sink.log-1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)
	at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:239)
	at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:234)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)
	at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:215)
	at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:42)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.processResults(ModuleDeploymentWriter.java:355)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:325)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:247)
	at org.springframework.xd.dirt.server.ContainerListener.redeployStreamModule(ContainerListener.java:432)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
17:22:59,718 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:557 -

java.lang.IllegalStateException: Container 40494bdb-0d8c-4b5d-a895-bf94432d9d3b experienced the following error deploying module log-1 of type sink: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.validateResults(ModuleDeploymentWriter.java:523)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.validateResult(ModuleDeploymentWriter.java:474)
	at org.springframework.xd.dirt.server.ContainerListener.redeployStreamModule(ContainerListener.java:436)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}
",2.0,-1.0,0.6166245259165614,-1.0,False,False,True,1401460348000.0,-1.0,-1.0,1401465776000.0,1401466676000.0,1401460348000.0,-1.0,-1.0,-1.0,1401464250000.0,1401466676000.0,-1.0,-1.0,"mark.fisher",-1.0,-1.0,0,1401460348000.0,-1.0,"Make failed deployment cleanup more robust","When a deployment fails, the supervisor will clean up failed deployment attempts. If the deployment path is removed while the supervisor is waiting (for instance if the target container departs the cluster) then a NoNodeException will be thrown:

{code}
17:22:59,702  WARN ContainersPathChildrenCache-0 server.ModuleDeploymentWriter:361 - Error while cleaning up failed deployment /deployments/modules/40494bdb-0d8c-4b5d-a895-bf94432d9d3b/s.sink.log-1
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/40494bdb-0d8c-4b5d-a895-bf94432d9d3b/s.sink.log-1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873)
	at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:239)
	at org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:234)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230)
	at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:215)
	at org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:42)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.processResults(ModuleDeploymentWriter.java:355)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:325)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:247)
	at org.springframework.xd.dirt.server.ContainerListener.redeployStreamModule(ContainerListener.java:432)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
17:22:59,718 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:557 -

java.lang.IllegalStateException: Container 40494bdb-0d8c-4b5d-a895-bf94432d9d3b experienced the following error deploying module log-1 of type sink: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.validateResults(ModuleDeploymentWriter.java:523)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.validateResult(ModuleDeploymentWriter.java:474)
	at org.springframework.xd.dirt.server.ContainerListener.redeployStreamModule(ContainerListener.java:436)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}

Additionally, if the supervisor tries to remove a failed deployment after the container has written to it, the following appears in the log:

{code}
WARN ContainersPathChildrenCache-0 server.ModuleDeploymentWriter:361 - Error while cleaning up failed deployment /deployments/modules/18c7b4d7-991c-487b-a6e3-006b2dbf87fa/s.source.http-0
org.apache.zookeeper.KeeperException$NotEmptyException: KeeperErrorCode = Directory not empty for /xd/deployments/modules/18c7b4d7-991c-487b-a6e3-006b2dbf87fa/s.source.http-0
{code}

The supervisor has to force the removal of the node, including children.",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1782","Bug","","2014-05-30T14:20:30.000+0000","pperalta","pperalta","Exception thrown when all containers are shut down","If all containers are shut down and there's a stream deployed this exception is thrown on the admin:

{noformat}
java.util.NoSuchElementException
	at java.util.ArrayList$Itr.next(ArrayList.java:839)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:260)
	at org.springframework.xd.dirt.server.ContainerListener.redeployStreamModule(ContainerListener.java:432)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
{noformat}

This is a regression caused by the earlier code refactoring for stream deployment.",2.0,0.0009885609377206,0.0,-1.0,True,False,True,1401459630000.0,1401459637000.0,1401463237000.0,1401465811000.0,1401466711000.0,1401459630000.0,-1.0,-1.0,-1.0,1401459630000.0,1401463230000.0,1401459637000.0,1401463237000.0,"pperalta",-1.0,-1.0,0,1401459630000.0,-1.0,"Exception thrown when all containers are shut down","If all containers are shut down and there's a stream deployed this exception is thrown on the admin:

{noformat}
java.util.NoSuchElementException
	at java.util.ArrayList$Itr.next(ArrayList.java:839)
	at org.springframework.xd.dirt.server.ModuleDeploymentWriter.writeDeployment(ModuleDeploymentWriter.java:260)
	at org.springframework.xd.dirt.server.ContainerListener.redeployStreamModule(ContainerListener.java:432)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:509)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:503)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:500)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$10.run(PathChildrenCache.java:762)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
{noformat}

This is a regression caused by the earlier code refactoring for stream deployment.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1781","Story","Sprint 30","2014-05-30T07:07:38.000+0000","grenfro","grenfro","Add HdfsMongoDb Acceptance Test.","Create Acceptance Test
Add Mongo to Ec2 Acceptance Test Environment.",10.0,8.084871749679434e-06,0.0,0.9134328527146574,True,True,True,1401433658000.0,1401433668000.0,1401437268000.0,1402669636000.0,1402670536000.0,1401433658000.0,-1.0,1402563463000.0,1402567063000.0,1401433658000.0,1401437258000.0,1401433668000.0,1401437268000.0,"grenfro",1402567063000.0,1401433658000.0,44,1401433658000.0,-1.0,"Add HdfsMongoDb Acceptance Test.","Create Acceptance Test
Add Mongo to Ec2 Acceptance Test Environment.",10.0,False,10.0,0,0.0,-1,0,False
"XD-1780","Improvement","Sprint 28","2014-05-29T19:27:38.000+0000","grenfro","grenfro","Acceptance Tests failed initially due to Zookeeper IOException.","* Environment: 
** Admin/Container on separate EC2 instances with rabbit transport.
*** Redis, Rabbit & Zookeeper deployed on admin instance
** Admin/Container on separate EC2 instances with redis transport.
*** Redis, Rabbit & Zookeeper deployed on admin instance
* XD Deployment Type: XD-SingleNode 
* Commit: https://github.com/spring-projects/spring-xd/commit/8fba31d21e96a371dacf26b40eeb542c3564b2e3

Both Redis and Rabbit clusters failed acceptance tests.  I've attached the portion of the admin log that was available.  ",10.0,0.9555039313795568,0.784466583273767,0.7764474624731952,True,True,True,1401391658000.0,1401434434000.0,1401436426000.0,1401435526000.0,1401436426000.0,1401391658000.0,-1.0,1401426418000.0,1401430018000.0,1401426777000.0,1401430377000.0,1401434434000.0,1401436426000.0,"grenfro",1401430018000.0,1401391658000.0,44,1401391658000.0,-1.0,"Upgrade ZK installation on EC2 to 3.4.6","* Environment: 
** Admin/Container on separate EC2 instances with rabbit transport.
*** Redis, Rabbit & Zookeeper deployed on admin instance
** Admin/Container on separate EC2 instances with redis transport.
*** Redis, Rabbit & Zookeeper deployed on admin instance
* XD Deployment Type: XD-SingleNode 
* Commit: https://github.com/spring-projects/spring-xd/commit/8fba31d21e96a371dacf26b40eeb542c3564b2e3

Both Redis and Rabbit clusters failed acceptance tests.  I've attached the portion of the admin log that was available.  ",10.0,False,10.0,0,0.0,0,0,True
"XD-1779","Bug","","2014-05-29T19:11:36.000+0000","grenfro","","Exception thrown when running undeploy and redeploy a stream tests","Environment: Running on local Mac Instance:
XD Deployment Type: XD-SingleNode 
SHA: 66c28e3

While running a test that deployed and undeployed a stream over 500 times the following exception is thrown:
org.apache.zookeeper.KeeperException$NotEmptyException: KeeperErrorCode = Directory not empty for /xd/deployments/modules/54ef010f-3d5d-4f86-a50f-44453e48633d/streamfoo.source.http-0

[Steps to reproduce]
* Created the following Job and Stream using xd-shell:
** stream create streamfoo --definition ""http --port=9000 | hdfs --directory=/xd/streamfoo --fileName=streamfoo "" 
** job create foo --definition ""hdfsjdbc --resources=/xd/streamfoo/*.txt --names=data --tableName=streamfoo --initializeDatabase=true "" --deploy
* Repeated the following 500 times using xd-shell --cmdfile
** Ran a script that would deploy streamfoo
** Ran a script that would execute the following 25 times
*** http post --data ""hello world0 ""
** Ran a script to undeploy streamfoo
** Ran a script that would launch the job",10.0,-1.0,0.1260305753421406,-1.0,False,False,True,1401390696000.0,-1.0,-1.0,1401726865000.0,1401727765000.0,1401390696000.0,-1.0,-1.0,-1.0,1401433177000.0,1401436777000.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1401390696000.0,-1.0,"Exception thrown when running undeploy and redeploy a stream tests","Environment: Running on local Mac Instance:
XD Deployment Type: XD-SingleNode 
SHA: 66c28e3

While running a test that deployed and undeployed a stream over 500 times the following exception is thrown:
org.apache.zookeeper.KeeperException$NotEmptyException: KeeperErrorCode = Directory not empty for /xd/deployments/modules/54ef010f-3d5d-4f86-a50f-44453e48633d/streamfoo.source.http-0

[Steps to reproduce]
* Created the following Job and Stream using xd-shell:
** stream create streamfoo --definition ""http --port=9000 | hdfs --directory=/xd/streamfoo --fileName=streamfoo "" 
** job create foo --definition ""hdfsjdbc --resources=/xd/streamfoo/*.txt --names=data --tableName=streamfoo --initializeDatabase=true "" --deploy
* Repeated the following 500 times using xd-shell --cmdfile
** Ran a script that would deploy streamfoo
** Ran a script that would execute the following 25 times
*** http post --data ""hello world0 ""
** Ran a script to undeploy streamfoo
** Ran a script that would launch the job

[Artifacts]
The logfile of the singlenode is attached.",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1778","Story","Sprint 28","2014-05-29T10:38:29.000+0000","iperumal","iperumal","Check job ""restartable"" flag for JobExecution restart action","job create bogus --definition ""jdbchdfs --sql='select * from bogus' --restartable=false""
job deploy bogus
job launch bogus

http://localhost:9393/admin-ui/#/jobs/executions

click ""Restart Job Execution"" on the failed job execution

get message ""Job was relaunched""

container log has:

12:36:27,231 ERROR task-scheduler-10 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: org.springframework.batch.core.repository.JobRestartException: JobInstance already exists and is not restartable",2.0,0.0006895532679899,0.0,3.4747081712062258,True,True,True,1401359909000.0,1401359923000.0,1401363523000.0,1401379312000.0,1401380212000.0,1401359909000.0,-1.0,1401430456000.0,1401380212000.0,1401359909000.0,1401363509000.0,1401359923000.0,1401363523000.0,"iperumal",1401380212000.0,1401359909000.0,44,1401359909000.0,-1.0,"Check job ""restartable"" flag for JobExecution restart action","job create bogus --definition ""jdbchdfs --sql='select * from bogus' --restartable=false""
job deploy bogus
job launch bogus

http://localhost:9393/admin-ui/#/jobs/executions

click ""Restart Job Execution"" on the failed job execution

get message ""Job was relaunched""

container log has:

12:36:27,231 ERROR task-scheduler-10 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: org.springframework.batch.core.repository.JobRestartException: JobInstance already exists and is not restartable",2.0,False,2.0,0,0.0,0,0,False
"XD-1777","Story","Sprint 30","2014-05-29T08:48:22.000+0000","pperalta","","Handle redeployment of ""orphaned"" modules for a partitioned stream","As part of XD-1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container.

However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for.

This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn't have enough containers to support the stream.",20.0,0.479209317012892,0.3523190877932654,0.350021844859681,True,True,True,1401353302000.0,1403015025000.0,1403018625000.0,1404820037000.0,1404820937000.0,1401353302000.0,-1.0,1402567050000.0,1402570650000.0,1402575016000.0,1402578616000.0,1403015025000.0,1403018625000.0,"pperalta",1402570650000.0,1401353302000.0,44,1401353302000.0,-1.0,"Restore deployment properties for orphaned modules","As part of XD-1338 we modified how module deployment works. Now module deployment requests include deployment properties as the data for the ZooKeeper node. This allows us to reuse those properties when a container exit the cluster and the module is redeployed to another container.

However if there are no other containers to handle the deployment, the module deployment node is erased, along with the properties. This mean no module will ever handle the partition that module was responsible for.

This condition needs to be handled so that partitioned streams continue to function in cases where the cluster temporarily doesn't have enough containers to support the stream.",20.0,False,20.0,0,0.0,0,0,True
"XD-1775","Story","Sprint 30","2014-05-29T06:02:27.000+0000","grenfro","grenfro","HdfsJdbc Acceptance Test","",8.0,0.0684916850200763,3.315292616014521e-05,0.9197548491805576,True,True,True,1401343347000.0,1401434248000.0,1401437848000.0,1402669630000.0,1402670530000.0,1401343347000.0,-1.0,1402564030000.0,1402567630000.0,1401343391000.0,1401346991000.0,1401434248000.0,1401437848000.0,"grenfro",1402567630000.0,1401343347000.0,44,1401343347000.0,-1.0,"HdfsJdbc Acceptance Test","",8.0,False,8.0,0,0.0,-1,0,False
"XD-1774","Story","","2014-05-28T21:42:59.000+0000","hillert","hillert","UI Automatically close notification messages","* Automatically close notification messages
* Polish UI",3.0,0.0005392870625033,0.0,-1.0,True,False,True,1401313379000.0,1401313399000.0,1401316999000.0,1401349565000.0,1401350465000.0,1401313379000.0,-1.0,-1.0,-1.0,1401313379000.0,1401316979000.0,1401313399000.0,1401316999000.0,"hillert",-1.0,-1.0,0,1401313379000.0,-1.0,"UI Automatically close notification messages","* Automatically close notification messages
* Polish UI",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1773","Story","Sprint 28","2014-05-28T18:48:14.000+0000","mark.fisher","mark.fisher","Upgrade Curator to 2.5.0","",1.0,0.1004366812227074,0.0655021834061135,0.0,False,False,False,1401302894000.0,1401302963000.0,1401303581000.0,1401302894000.0,1401303581000.0,1401302894000.0,-1.0,1401302894000.0,1401303581000.0,1401302939000.0,1401303581000.0,1401302963000.0,1401303581000.0,"mark.fisher",1401303581000.0,1401302894000.0,44,1401302894000.0,-1.0,"Upgrade Curator to 2.5.0","",1.0,False,1.0,0,0.0,0,0,False
"XD-1772","Bug","Sprint 29","2014-05-28T14:53:14.000+0000","mark.pollack","eric.bottard","Do not allow a stream definition to contain ambiguous module references","A stream definition such as http | transform | transform | file 

will limit functionality such as taps since you can't reference which specific module to apply the tap. 

Should be proactive in parsing the DSL and force the use of a label to disambiguate.  ",4.0,0.9999940471381616,0.4099217198668259,0.9999880942763232,True,True,True,1401288794000.0,1402464692000.0,1402464699000.0,1402463799000.0,1402464699000.0,1401288794000.0,-1.0,1402464685000.0,1402464699000.0,1401770823000.0,1401774423000.0,1402464692000.0,1402464699000.0,"mark.pollack",1402464699000.0,1401288794000.0,44,1401288794000.0,-1.0,"Do not allow a stream definition to contain ambiguous module references","A stream definition such as http | transform | transform | file 

will limit functionality such as taps since you can't reference which specific module to apply the tap. 

Should be proactive in parsing the DSL and force the use of a label to disambiguate.  ",4.0,False,4.0,0,0.0,0,0,False
"XD-1771","Story","Sprint 28","2014-05-28T09:31:07.000+0000","grenfro","grenfro","Update twitterSearchTest to handle the latest release of twitterSearch","The changes to twitterSearch means that it will send multiple messages during the duration of the test.
To support these changes:
1) Remove assertReceived.  Since the number of messages is indeterminate
2) Change file sink that captures the results to append mode.  Because each message will overwrite the previous messages result.",5.0,0.0010169491525423,0.0,0.8356779661016949,True,True,True,1401269467000.0,1401269479000.0,1401273079000.0,1401280367000.0,1401281267000.0,1401269467000.0,-1.0,1401279328000.0,1401281267000.0,1401269467000.0,1401273067000.0,1401269479000.0,1401273079000.0,"grenfro",1401281267000.0,1401269467000.0,44,1401269467000.0,-1.0,"Update twitterSearchTest to handle the latest release of twitterSearch","The changes to twitterSearch means that it will send multiple messages during the duration of the test.
To support these changes:
1) Remove assertReceived.  Since the number of messages is indeterminate
2) Change file sink that captures the results to append mode.  Because each message will overwrite the previous messages result.",5.0,False,5.0,0,0.0,0,0,False
"XD-1770","Bug","Sprint 28","2014-05-28T08:13:29.000+0000","iperumal","iperumal","Handle NPE while deploying stream module at the Container","When trying to deploy a stream module, the ContainerRegistrar throws NPE if the deployment loader couldn't load a non-null stream based on the stream name.

07:10:29,902 ERROR DeploymentsPathChildrenCache-0 server.ContainerRegistrar:450 - Exception deploying module
java.lang.NullPointerException
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:549)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:436)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:96)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:803)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)",1.0,0.0108833665880645,0.0,0.3725739161980773,True,True,True,1401264809000.0,1401264869000.0,1401268469000.0,1401269422000.0,1401270322000.0,1401264809000.0,-1.0,1401266863000.0,1401270322000.0,1401264809000.0,1401268409000.0,1401264869000.0,1401268469000.0,"iperumal",1401270322000.0,1401264809000.0,44,1401264809000.0,-1.0,"Handle NPE while deploying stream module at the Container","When trying to deploy a stream module, the ContainerRegistrar throws NPE if the deployment loader couldn't load a non-null stream based on the stream name.

07:10:29,902 ERROR DeploymentsPathChildrenCache-0 server.ContainerRegistrar:450 - Exception deploying module
java.lang.NullPointerException
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:549)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:436)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:96)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:803)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)",1.0,False,1.0,0,0.0,0,0,False
"XD-1768","Story","Sprint 30","2014-05-27T23:25:51.000+0000","iperumal","hillert","User should be able to specify deploy properties for Jobs","When clicking deploy from the job definitions page, user should be able to specify the deployment manifest (module count, module criteria etc.,)",3.0,0.7681790226466487,0.6567417113911512,0.6567343096459544,True,True,True,1401233151000.0,1403516388000.0,1403519988000.0,1404204523000.0,1404205423000.0,1401233151000.0,-1.0,1403185144000.0,1403188744000.0,1403185166000.0,1403188766000.0,1403516388000.0,1403519988000.0,"iperumal",1403188744000.0,1401233151000.0,44,1401233151000.0,-1.0,"User should be able to specify deploy properties for Jobs","When clicking deploy from the job definitions page, user should be able to specify the deployment manifest (module count, module criteria etc.,)",3.0,False,3.0,0,0.0,0,0,False
"XD-1767","Story","Sprint 30","2014-05-27T22:43:56.000+0000","iperumal","iperumal","JobExecution restart action should depend on job deployment status","At the JobExecution page, if the job execution is failed and restartable, then we should enable the ""restart"" action only if the job is deployed.

Please see https://github.com/spring-projects/spring-xd/pull/884 for the discussion related to this.",3.0,0.4554060405868097,0.4437888640695478,0.7184284685508284,True,True,True,1401230636000.0,1402075498000.0,1402079098000.0,1403084920000.0,1403085820000.0,1401230636000.0,-1.0,1402563453000.0,1402567053000.0,1402053946000.0,1402057546000.0,1402075498000.0,1402079098000.0,"iperumal",1402567053000.0,1401230636000.0,44,1401230636000.0,-1.0,"JobExecution restart action should depend on job deployment status","At the JobExecution page, if the job execution is failed and restartable, then we should enable the ""restart"" action only if the job is deployed.

Please see https://github.com/spring-projects/spring-xd/pull/884 for the discussion related to this.",3.0,False,3.0,0,0.0,0,0,False
"XD-1766","Story","Sprint 28","2014-05-27T13:21:06.000+0000","mark.pollack","","Failing tcp to file in script tests","build	22-May-2014 08:45:04	Creating stream tcptofile with definition 'tcp+--port%3D21234+--socketTimeout%3D2000+%7C+file+--dir%3D%2Ftmp%2Fxdtest%2Fbasic' ...
build	22-May-2014 08:45:04	{""name"":""tcptofile"",""deployed"":null,""definition"":""tcp --port=21234 --socketTimeout=2000 | file --dir=/tmp/xdtest/basic"",""links"":[{""rel"":""self"",""href"":""http://127.0.0.1:9393/streams/tcptofile""}]}
build	22-May-2014 08:45:04	
build	22-May-2014 08:45:11	Destroying stream tcptofile ...
build	22-May-2014 08:45:11	
build	22-May-2014 08:45:11	
build	22-May-2014 08:45:11	Expected blahblah does not match actual value (98,108,97,104,98,108,97,104)
simple	22-May-2014 08:45:11	Failing task since return code of [/bin/sh /tmp/XD-SCRIPTS-RS-513-ScriptBuildTask-7280766559152712153.sh] was 1 while expected 0
simple	22-May-2014 08:45:11	Finished task 'Run basic_stream_tests'

See https://build.spring.io/download/XD-SCRIPTS-RS/build_logs/XD-SCRIPTS-RS-513.log",2.0,0.9287619288377678,0.9287619288377678,0.0,True,True,True,1401196866000.0,1401270345000.0,1401273945000.0,1401275081000.0,1401275981000.0,1401196866000.0,-1.0,1401196866000.0,1401200466000.0,1401270345000.0,1401273945000.0,1401270345000.0,1401273945000.0,"mark.pollack",1401200466000.0,1401196866000.0,44,1401196866000.0,-1.0,"Failing tcp to file in script tests","build	22-May-2014 08:45:04	Creating stream tcptofile with definition 'tcp+--port%3D21234+--socketTimeout%3D2000+%7C+file+--dir%3D%2Ftmp%2Fxdtest%2Fbasic' ...
build	22-May-2014 08:45:04	{""name"":""tcptofile"",""deployed"":null,""definition"":""tcp --port=21234 --socketTimeout=2000 | file --dir=/tmp/xdtest/basic"",""links"":[{""rel"":""self"",""href"":""http://127.0.0.1:9393/streams/tcptofile""}]}
build	22-May-2014 08:45:04	
build	22-May-2014 08:45:11	Destroying stream tcptofile ...
build	22-May-2014 08:45:11	
build	22-May-2014 08:45:11	
build	22-May-2014 08:45:11	Expected blahblah does not match actual value (98,108,97,104,98,108,97,104)
simple	22-May-2014 08:45:11	Failing task since return code of [/bin/sh /tmp/XD-SCRIPTS-RS-513-ScriptBuildTask-7280766559152712153.sh] was 1 while expected 0
simple	22-May-2014 08:45:11	Finished task 'Run basic_stream_tests'

See https://build.spring.io/download/XD-SCRIPTS-RS/build_logs/XD-SCRIPTS-RS-513.log",2.0,False,2.0,0,0.0,0,0,False
"XD-1765","Story","Sprint 28","2014-05-27T12:40:44.000+0000","mark.pollack","thomas.risberg","Update documentation to list supported Hadoop distributions","After spring hadoop 2.0 RC4 update.",1.0,0.0058952485886615,0.0,0.0024194372820518,True,True,True,1401194444000.0,1401194963000.0,1401198563000.0,1401281581000.0,1401282481000.0,1401194444000.0,-1.0,1401194657000.0,1401198257000.0,1401194444000.0,1401198044000.0,1401194963000.0,1401198563000.0,"mark.pollack",1401198257000.0,1401194444000.0,44,1401194444000.0,-1.0,"Update documentation to list supported Hadoop distributions","After spring hadoop 2.0 RC4 update.",1.0,False,1.0,0,0.0,-1,0,False
"XD-1764","Story","Sprint 28","2014-05-27T12:38:04.000+0000","mark.pollack","jvalkeal","Documentation for enhanced HDFS sink with paths based off date/time/message content","",2.0,0.9612777176215808,0.0,0.0,True,True,True,1401194284000.0,1401275958000.0,1401279248000.0,1401278348000.0,1401279248000.0,1401194284000.0,-1.0,1401194284000.0,1401197884000.0,1401194284000.0,1401197884000.0,1401275958000.0,1401279248000.0,"mark.pollack",1401197884000.0,1401194284000.0,44,1401194284000.0,-1.0,"Documentation for enhanced HDFS sink with paths based off date/time/message content","",2.0,False,2.0,0,0.0,0,0,False
"XD-1763","Story","Sprint 28","2014-05-27T12:37:22.000+0000","mark.pollack","mark.pollack","Update to Spring Batch 3.0 RELEASE","",2.0,0.0627776179976352,0.0096507206084427,0.0,True,True,True,1401194242000.0,1401198171000.0,1401201771000.0,1401255928000.0,1401256828000.0,1401194242000.0,-1.0,1401194242000.0,1401197842000.0,1401194846000.0,1401198446000.0,1401198171000.0,1401201771000.0,"mark.pollack",1401197842000.0,1401194242000.0,44,1401194242000.0,-1.0,"Update to Spring Batch 3.0 RELEASE","",2.0,False,2.0,0,0.0,-1,0,False
"XD-1761","Story","Sprint 28","2014-05-27T12:35:26.000+0000","mark.pollack","grussell","Documentation for data partitioning, and all Rabbit Bus properties","",4.0,0.008953846537555,0.0087854942697876,0.0,True,True,True,1401194126000.0,1401195562000.0,1401199162000.0,1401353604000.0,1401354504000.0,1401194126000.0,1401195772000.0,1401194126000.0,1401197726000.0,1401195535000.0,1401199135000.0,1401195562000.0,1401199162000.0,"mark.pollack",1401197726000.0,1401195772000.0,44,1401195772000.0,-1.0,"Documentation for data partitioning, and all Rabbit Bus properties","",4.0,False,4.0,0,0.0,0,1,False
"XD-1760","Improvement","Sprint 30","2014-05-27T11:46:33.000+0000","glingappa","","Support in-memory transport for co-located modules","We are looking to speed up the message passing from source to sink  and wondering if we could use a in-memory transport whenever we know that source and sink modules are co-located on the same container. Currently we do not see a straight forward way of doing it

Option 1 : Create a composite module and let users deploy a composite module by itself or in other words deploy a stream with one module

Option 2 : Let users define a transport as in-memory when defining a stream. This could be used along with the deployment manifest feature enforcing co-location of a source and sink module, with in-memory transport

cc @adenissov
",8.0,0.5344298218607029,0.5344298218607029,0.5286524559577835,True,True,True,1401191193000.0,1402579864000.0,1402583464000.0,1403788709000.0,1403789609000.0,1401191193000.0,-1.0,1402564852000.0,1402568452000.0,1402579864000.0,1402583464000.0,1402579864000.0,1402583464000.0,"glingappa",1402568452000.0,1401200297000.0,44,1401200297000.0,-1.0,"Support in-memory transport for co-located modules","We are looking to speed up the message passing from source to sink  and wondering if we could use a in-memory transport whenever we know that source and sink modules are co-located on the same container. Currently we do not see a straight forward way of doing it

Option 1 : Create a composite module and let users deploy a composite module by itself or in other words deploy a stream with one module

Option 2 : Let users define a transport as in-memory when defining a stream. This could be used along with the deployment manifest feature enforcing co-location of a source and sink module, with in-memory transport

cc @adenissov
",8.0,False,8.0,0,0.0,0,0,False
"XD-1759","Bug","Sprint 29","2014-05-26T07:32:36.000+0000","grenfro","grenfro","Status on Shell command prompt is inconsistent","Deployment: xd-shell local, xd-singlenode (ec2)
SHA: 942c7868e3e0d0cf7730b536170438a0291f5cab

[Description]
<Case 1>
Once successfully connected to a server, if you connect to a server that is not present.  The prompt still shows XD when it should show server-unknown.  Can be reproduced consistently.
Conversely:
<Case 2>
Attempted to connect to a xd-singlenode on ec2 using a local xd-shell.  
The xd-singlenode was not running.  After bringing up the xd-singlenode, I was able to connect however the status did not change from ""server-unknown""  *This behavior, can not be consistently reproduced, but have seen it happen on multiple accounts.* 

[Steps to reproduce]
<Case 1>
1. Bring up shell while xd-singlenode is not running.
2. Bring up xd-singlenode
3. Connect to xd-singlenode
* xd:>admin config server http://localhost:9393

4. Connect to a fake address
* xd:>admin config server http://foo.bar:9393

<Case 2>
1.  Attempt to connect to remote server that is not available
* xd:>admin config server http://ec2-54-237-186-186.compute-1.amazonaws.com:9393
* Unable to contact XD Admin Server at 'http://ec2-54-237-186-186.compute-1.amazonaws.com:9393'.

2. Bring up xd-singlenode on remote
* server-unknown:>admin config server http://ec2-54-237-186-186.compute-1.amazonaws.com:9393
* Successfully targeted http://ec2-54-237-186-186.compute-1.amazonaws.com:9393

3. Still see the incorrect prompt.
server-unknown:>
server-unknown:>",5.0,0.9549309724837782,0.902980868918624,0.902965438849534,True,True,True,1401089556000.0,1402389197000.0,1402392797000.0,1402449635000.0,1402450535000.0,1401089556000.0,1402318487000.0,1402318473000.0,1402322073000.0,1402318494000.0,1402322094000.0,1402389197000.0,1402392797000.0,"grenfro",1402322073000.0,1402318487000.0,44,1402318487000.0,-1.0,"Status on Shell command prompt is inconsistent","Deployment: xd-shell local, xd-singlenode (ec2)
SHA: 942c7868e3e0d0cf7730b536170438a0291f5cab

[Description]
<Case 1>
Once successfully connected to a server, if you connect to a server that is not present.  The prompt still shows XD when it should show server-unknown.  Can be reproduced consistently.
Conversely:
<Case 2>
Attempted to connect to a xd-singlenode on ec2 using a local xd-shell.  
The xd-singlenode was not running.  After bringing up the xd-singlenode, I was able to connect however the status did not change from ""server-unknown""  *This behavior, can not be consistently reproduced, but have seen it happen on multiple accounts.* 

[Steps to reproduce]
<Case 1>
1. Bring up shell while xd-singlenode is not running.
2. Bring up xd-singlenode
3. Connect to xd-singlenode
* xd:>admin config server http://localhost:9393

4. Connect to a fake address
* xd:>admin config server http://foo.bar:9393

<Case 2>
1.  Attempt to connect to remote server that is not available
* xd:>admin config server http://ec2-54-237-186-186.compute-1.amazonaws.com:9393
* Unable to contact XD Admin Server at 'http://ec2-54-237-186-186.compute-1.amazonaws.com:9393'.

2. Bring up xd-singlenode on remote
* server-unknown:>admin config server http://ec2-54-237-186-186.compute-1.amazonaws.com:9393
* Successfully targeted http://ec2-54-237-186-186.compute-1.amazonaws.com:9393

3. Still see the incorrect prompt.
server-unknown:>
server-unknown:>",5.0,False,5.0,0,0.0,0,5,False
"XD-1758","Bug","Sprint 30","2014-05-26T07:08:37.000+0000","grenfro","","JMS Source (ActiveMQ) failing to use jmsUrl environment variable","Deployed on: SingleNode Ec2, SingleNode Mac
SHA: 942c7868e3e0d0cf7730b536170438a0291f5cab

[Description]
JMS Source (Activemq) tried to access a broker on localhost.  
The current deployment uses the following to set the JMS Broker:
* export amq_url=tcp://ec2-54-221-32-82.compute-1.amazonaws.com:61616

[Analysis]
After reviewing the configuration of the jms-activemq-infrastructure-context.xml, it was noted that the brokerUrl environment variable has been changed from amq.url to amqUrl.  While the jms-activemq.properties has not been changed (still amq.url).  
After setting the following, the test still failed:
* export amqUrl=tcp://ec2-54-221-32-82.compute-1.amazonaws.com:61616

After going into the jms-activemq-infrastructure-context.xml and replacing the amqUrl with amq.url, the jms source (activemq) returned to normal operation.


[Incident]
Acceptance tests reported a failure on Saturday Morning's build that the JMS Source failed.",2.0,0.6805536485479586,0.6805518178206656,0.6751662758063897,True,True,True,1401088117000.0,1402575075000.0,1402578675000.0,1403272141000.0,1403273041000.0,1401088117000.0,-1.0,1402563304000.0,1402566904000.0,1402575071000.0,1402578671000.0,1402575075000.0,1402578675000.0,"grenfro",1402566904000.0,1401088117000.0,44,1401088117000.0,-1.0,"JMS Source (ActiveMQ) failing to use jmsUrl environment variable","Deployed on: SingleNode Ec2, SingleNode Mac
SHA: 942c7868e3e0d0cf7730b536170438a0291f5cab

[Description]
JMS Source (Activemq) tried to access a broker on localhost.  
The current deployment uses the following to set the JMS Broker:
* export amq_url=tcp://ec2-54-221-32-82.compute-1.amazonaws.com:61616

[Analysis]
After reviewing the configuration of the jms-activemq-infrastructure-context.xml, it was noted that the brokerUrl environment variable has been changed from amq.url to amqUrl.  While the jms-activemq.properties has not been changed (still amq.url).  
After setting the following, the test still failed:
* export amqUrl=tcp://ec2-54-221-32-82.compute-1.amazonaws.com:61616

After going into the jms-activemq-infrastructure-context.xml and replacing the amqUrl with amq.url, the jms source (activemq) returned to normal operation.


[Incident]
Acceptance tests reported a failure on Saturday Morning's build that the JMS Source failed.",2.0,False,2.0,0,0.0,0,0,False
"XD-1757","Bug","Sprint 28","2014-05-23T11:33:54.000+0000","iperumal","iperumal","Resolve runtime module option properties using module metadata","Since the module metadata properties are resolved at runtime (when the module gets deployed), we can resolve the module options values that are already resolved in there.

For example, currently the ""runtime modules"" command for ""log"" module would show this:

runtime modules

[7m[27;32m  Module            Container Id                          Options
  ----------------  ------------------------------------  --------------------------------------------------------
  s1.source.http-0  633f0fb1-5396-4bc0-8f1e-c9d5104e0ea7  {port=9000}
  s1.sink.log-1     633f0fb1-5396-4bc0-8f1e-c9d5104e0ea7  {name=${xd.stream.name}, expression=payload, level=INFO}

In this case, we can resolve the module option ""name"" from the module metadata.


",2.0,0.000386001029336,0.0,0.2605506948018528,True,True,True,1400844834000.0,1400844840000.0,1400848440000.0,1400859478000.0,1400860378000.0,1400844834000.0,-1.0,1400848884000.0,1400852484000.0,1400844834000.0,1400848434000.0,1400844840000.0,1400848440000.0,"iperumal",1400852484000.0,1400844834000.0,44,1400844834000.0,-1.0,"Resolve runtime module option properties using module metadata","Since the module metadata properties are resolved at runtime (when the module gets deployed), we can resolve the module options values that are already resolved in there.

For example, currently the ""runtime modules"" command for ""log"" module would show this:

runtime modules

[7m[27;32m  Module            Container Id                          Options
  ----------------  ------------------------------------  --------------------------------------------------------
  s1.source.http-0  633f0fb1-5396-4bc0-8f1e-c9d5104e0ea7  {port=9000}
  s1.sink.log-1     633f0fb1-5396-4bc0-8f1e-c9d5104e0ea7  {name=${xd.stream.name}, expression=payload, level=INFO}

In this case, we can resolve the module option ""name"" from the module metadata.


",2.0,False,2.0,0,0.0,0,0,False
"XD-1756","Story","Sprint 28","2014-05-23T11:20:25.000+0000","thomas.risberg","thomas.risberg","Update spring-data-hadoop version to 2.0.0.RC4","Update spring-data-hadoop version to 2.0.0.RC4 and make necessary changes to the YARN configuration.",3.0,0.7705806543783466,0.0003035867886172,0.000285728742228,True,True,True,1400844025000.0,1401102927000.0,1401106527000.0,1401179108000.0,1401180008000.0,1400844025000.0,-1.0,1400844121000.0,1400847721000.0,1400844127000.0,1400847727000.0,1401102927000.0,1401106527000.0,"thomas.risberg",1400847721000.0,1400844025000.0,44,1400844025000.0,-1.0,"Update spring-data-hadoop version to 2.0.0.RC4","Update spring-data-hadoop version to 2.0.0.RC4 and make necessary changes to the YARN configuration.",3.0,False,3.0,0,0.0,0,0,False
"XD-1755","Story","Sprint 28","2014-05-23T10:49:57.000+0000","grenfro","dturanski","gemfire source does not offer --host nor --port options","",2.0,0.0269588968237699,0.0269450461780528,0.0270613916020764,True,True,True,1400842197000.0,1400851929000.0,1400855529000.0,1401202291000.0,1401203191000.0,1400842197000.0,1400852081000.0,1400851966000.0,1400855566000.0,1400851924000.0,1400855524000.0,1400851929000.0,1400855529000.0,"grenfro",1400855566000.0,1400852081000.0,44,1400852081000.0,-1.0,"gemfire source does not offer --host nor --port options","",2.0,False,2.0,0,0.0,0,0,False
"XD-1754","Bug","Sprint 30","2014-05-23T10:27:47.000+0000","grenfro","dturanski","Assess if GemfireJsonServer & gemfireServer sinks should close the client cache","* OS - Mac
* XD Deployment Type - Singlenode
* SHA - bb4dd58
* Required Software - XD Gemfire Sample Server

[Description]
After creating and destroying 3 streams with gemfireJsonServer sink the 4th will fail with this error:  
* 44707 refused connection: The number of clients, 4, exceeded the limit of 3 allowed by the default evaluation license.

[Steps to reproduce]
* From your shell execute the following 4 times:
** stream create --name stocks --definition ""http --port=9090 | gemfire-json-server --regionName=Stocks --host=ec2-54-221-32-82.compute-1.amazonaws.com --port=40404 --keyExpression=payload.getField('symbol')"" --deploy
** stream destroy stocks",4.0,-1.0,0.8052341263544982,0.8055509925986766,False,True,True,1400840867000.0,-1.0,-1.0,1402982827000.0,1402983727000.0,1400840867000.0,-1.0,1402567050000.0,1402570650000.0,1402566371000.0,1402569971000.0,-1.0,-1.0,"grenfro",1402570650000.0,1402566263000.0,44,1402566263000.0,-1.0,"Assess if GemfireJsonServer & gemfireServer sinks should close the client cache","* OS - Mac
* XD Deployment Type - Singlenode
* SHA - bb4dd58
* Required Software - XD Gemfire Sample Server

[Description]
After creating and destroying 3 streams with gemfireJsonServer sink the 4th will fail with this error:  
* 44707 refused connection: The number of clients, 4, exceeded the limit of 3 allowed by the default evaluation license.

[Steps to reproduce]
* From your shell execute the following 4 times:
** stream create --name stocks --definition ""http --port=9090 | gemfire-json-server --regionName=Stocks --host=ec2-54-221-32-82.compute-1.amazonaws.com --port=40404 --keyExpression=payload.getField('symbol')"" --deploy
** stream destroy stocks",4.0,False,4.0,0,0.0,0,3,False
"XD-1753","Story","Sprint 31","2014-05-23T06:18:07.000+0000","thomas.risberg","hillert","Change default date formats to be 'yyyy-MM-dd'","We have some places where we us a default data format specified as 'yyyy/MM/dd'. In Spring for Apache Hadoop we use 'yyyy-MM-dd' for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.",3.0,0.9441493628044468,0.3529227430702526,0.9736613211437466,True,True,True,1400825887000.0,1404115986000.0,1404119586000.0,1404309710000.0,1404310610000.0,1400825887000.0,-1.0,1404218827000.0,1404222427000.0,1402055725000.0,1402059325000.0,1404115986000.0,1404119586000.0,"thomas.risberg",1404222427000.0,1400825887000.0,44,1400825887000.0,-1.0,"Change default date formats to be 'yyyy-MM-dd'","We have some places where we us a default data format specified as 'yyyy/MM/dd'. In Spring for Apache Hadoop we use 'yyyy-MM-dd' for partitioning path expressions. This seems more in line with ISO standard date format. For consistency we should have both SHDP and XD use the same default format.",3.0,False,3.0,0,0.0,0,0,False
"XD-1751","Story","Sprint 30","2014-05-22T16:33:43.000+0000","grenfro","","Modules that use tomcat connection pool need to expose configurations","filejdbc, hdfsjdbc, jdbchdfs & jdbc modules each support a tomcat connection  pool.  At this time none of the configurations allowed by the tomcat connection pool are available unless the user adds them to the appropriate module xml file.
We need to allow the user to configure them via yml, property file and environment variables.
",8.0,0.6751336555297752,0.5251542448389659,0.5222064156987358,True,True,True,1400776423000.0,1403091432000.0,1403095032000.0,1404204487000.0,1404205387000.0,1400776423000.0,-1.0,1402567050000.0,1402570650000.0,1402577158000.0,1402580758000.0,1403091432000.0,1403095032000.0,"grenfro",1402570650000.0,1400776423000.0,44,1400776423000.0,-1.0,"Modules that use tomcat connection pool need to expose configurations","filejdbc, hdfsjdbc, jdbchdfs & jdbc modules each support a tomcat connection  pool.  At this time none of the configurations allowed by the tomcat connection pool are available unless the user adds them to the appropriate module xml file.
We need to allow the user to configure them via yml, property file and environment variables.
",8.0,False,8.0,0,0.0,0,0,False
"XD-1750","Bug","Sprint 28","2014-05-22T16:28:32.000+0000","iperumal","iperumal","Exception handling at Module info command","When not prefixing with appropriate module type, module info command throws StringIndexOutOfBoundsException:


xd:>module info file
java.lang.StringIndexOutOfBoundsException: Failed to convert 'file' to type QualifiedModuleName for option 'name,'
String index out of range: -1",2.0,0.0097174639911813,0.0,0.1664730339802494,True,True,True,1400776112000.0,1400780361000.0,1400783961000.0,1401212466000.0,1401213366000.0,1400776112000.0,-1.0,1400848903000.0,1400852503000.0,1400776112000.0,1400779712000.0,1400780361000.0,1400783961000.0,"iperumal",1400852503000.0,1400780369000.0,44,1400780369000.0,-1.0,"Exception handling at Module info command","When not prefixing with appropriate module type, module info command throws StringIndexOutOfBoundsException:


xd:>module info file
java.lang.StringIndexOutOfBoundsException: Failed to convert 'file' to type QualifiedModuleName for option 'name,'
String index out of range: -1",2.0,False,2.0,0,0.0,0,1,False
"XD-1749","Story","Sprint 32","2014-05-22T15:18:02.000+0000","mark.pollack","","Investigate skipped tests in build, enable or remove.","We have 13 skipped tests now...",8.0,0.9822489835682956,0.9822456151119432,0.9655114982257592,True,True,True,1400771882000.0,1406020720000.0,1406024320000.0,1406114676000.0,1406115576000.0,1400771882000.0,-1.0,1405931280000.0,1405934880000.0,1406020702000.0,1406024302000.0,1406020720000.0,1406024320000.0,"mark.pollack",1405934880000.0,1400771882000.0,44,1400771882000.0,-1.0,"Investigate skipped tests in build, enable or remove.","We have 13 skipped tests now...",8.0,False,8.0,0,0.0,0,0,False
"XD-1748","Story","Sprint 28","2014-05-22T09:03:53.000+0000","grussell","grussell","Update to Spring Integration 4.0.1","Add messages store optimization to the `hdfs-dataset`",1.0,0.0446833780167113,0.0446833780167113,0.0446530446530446,True,True,True,1400749433000.0,1400768583000.0,1400772183000.0,1401177104000.0,1401178004000.0,1400749433000.0,-1.0,1400768570000.0,1400772170000.0,1400768583000.0,1400772183000.0,1400768583000.0,1400772183000.0,"grussell",1400772170000.0,1400749433000.0,44,1400749433000.0,-1.0,"Update to Spring Integration 4.0.1","Add messages store optimization to the `hdfs-dataset`",1.0,False,1.0,0,0.0,0,0,False
"XD-1747","Story","Sprint 28","2014-05-22T07:20:33.000+0000","mark.pollack","pperalta","Module deployment order is not guaranteed","We are getting test failures such as

https://build.spring.io/browse/XD-MASTER-1381

quite often in the CI environment recently.  I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests

			// Deploys in reverse order
			assertModuleRequest(streamName, ""log"", false);
			assertModuleRequest(streamName, ""filter"", false);
			assertModuleRequest(streamName, ""transform"", false);
			assertModuleRequest(streamName, ""http"", false);
			// Undeploys in stream order
			assertModuleRequest(streamName, ""http"", true);
			assertModuleRequest(streamName, ""transform"", true);
			assertModuleRequest(streamName, ""filter"", true);
			assertModuleRequest(streamName, ""log"", true);

isn't happening

perhaps  changing nextUndeployEvent poll time from 5 seconds to higher is appropriate (no idea why CI environment seems so slow)
",4.0,0.0831985956483577,0.0828290294267104,0.0,True,True,True,1400743233000.0,1400745034000.0,1400748634000.0,1400763980000.0,1400764880000.0,1400743233000.0,-1.0,1400743233000.0,1400746833000.0,1400745026000.0,1400748626000.0,1400745034000.0,1400748634000.0,"mark.pollack",1400746833000.0,1400743233000.0,44,1400743233000.0,-1.0,"Module deployment order is not guaranteed","We are getting test failures such as

https://build.spring.io/browse/XD-MASTER-1381

quite often in the CI environment recently.  I suspect the ordering of checks in AbstractSingleNodeStreamDeploymentIntegrationTests

			// Deploys in reverse order
			assertModuleRequest(streamName, ""log"", false);
			assertModuleRequest(streamName, ""filter"", false);
			assertModuleRequest(streamName, ""transform"", false);
			assertModuleRequest(streamName, ""http"", false);
			// Undeploys in stream order
			assertModuleRequest(streamName, ""http"", true);
			assertModuleRequest(streamName, ""transform"", true);
			assertModuleRequest(streamName, ""filter"", true);
			assertModuleRequest(streamName, ""log"", true);

isn't happening

perhaps  changing nextUndeployEvent poll time from 5 seconds to higher is appropriate (no idea why CI environment seems so slow)
",4.0,False,4.0,0,0.0,0,2,False
"XD-1745","Improvement","Sprint 30","2014-05-21T14:09:38.000+0000","glingappa","jvalkeal","Support for hadoop name node HA configuration","Hadoop supports namenode HA with two name nodes running, one being active and other in standby. If the active name node fails the standby name node has all the data readily available and can start serving requests. In this configuration name node url is no longer a host:port url but a logical name that translates to any active name node at runtime. 

This is to ensure spring xd stream can handle a name node failure, for instance when writing a hdfs sink, seamlessly",5.0,0.7061062790550549,0.3332584346465327,0.457069115005358,True,True,True,1400681378000.0,1403594471000.0,1403598071000.0,1404806051000.0,1404806951000.0,1400681378000.0,-1.0,1402567050000.0,1402570650000.0,1402056260000.0,1402059860000.0,1403594471000.0,1403598071000.0,"glingappa",1402570650000.0,1401199958000.0,44,1401199958000.0,-1.0,"Support for hadoop name node HA configuration","Hadoop supports namenode HA with two name nodes running, one being active and other in standby. If the active name node fails the standby name node has all the data readily available and can start serving requests. In this configuration name node url is no longer a host:port url but a logical name that translates to any active name node at runtime. 

This is to ensure spring xd stream can handle a name node failure, for instance when writing a hdfs sink, seamlessly",5.0,False,5.0,0,0.0,0,0,False
"XD-1744","Story","Sprint 28","2014-05-21T13:20:31.000+0000","mark.pollack","Kashyap Parikh","Update CI server to run test that depend on rabbit and redis servers","",2.0,-1.0,0.2613165958461168,0.0,False,True,True,1400678431000.0,-1.0,-1.0,1405929773000.0,1405930673000.0,1400678431000.0,-1.0,1400678431000.0,1400682031000.0,1402050929000.0,1402054529000.0,-1.0,-1.0,"mark.pollack",1400682031000.0,1400678431000.0,44,1400678431000.0,-1.0,"Update CI server to run tests that depend on rabbit/redis and hadoop","",2.0,False,2.0,0,0.0,0,0,True
"XD-1742","Improvement","Sprint 27","2014-05-21T09:37:23.000+0000","grussell","grussell","Remove toStringTransformer from tcp Source; Add Binary Support to the http Source","The TCP source unconditionally converts to String. This prevents binary transfers.

Remove the transformer; if the user wants a String; (s)he can use

{{tcp --outputType=text/plain;charset=UTF-8}} (assuming the byte stream has valid UTF-8 encoding).

Another option would be to add a {{--binary}} option, but since conversion can already handle it, it's probably better to use that.

On the other hand, a {{--binary}} option would enable backwards compatibility.

The http source also unconditionally converts to String.",1.0,0.0695996818300259,0.0003017129064552,0.0706556770026194,True,True,True,1400665043000.0,1400670118000.0,1400673718000.0,1400737060000.0,1400737960000.0,1400665043000.0,-1.0,1400670195000.0,1400673795000.0,1400665065000.0,1400668665000.0,1400670118000.0,1400673718000.0,"grussell",1400673795000.0,1400665043000.0,44,1400665043000.0,-1.0,"Remove toStringTransformer from tcp Source; Add Binary Support to the http Source","The TCP source unconditionally converts to String. This prevents binary transfers.

Remove the transformer; if the user wants a String; (s)he can use

{{tcp --outputType=text/plain;charset=UTF-8}} (assuming the byte stream has valid UTF-8 encoding).

Another option would be to add a {{--binary}} option, but since conversion can already handle it, it's probably better to use that.

On the other hand, a {{--binary}} option would enable backwards compatibility.

The http source also unconditionally converts to String.",1.0,False,1.0,0,0.0,0,4,False
"XD-1741","Improvement","Sprint 27","2014-05-21T07:47:04.000+0000","dturanski","dturanski","Register StringToByteArrayMessageConverter","The converter was not configured, therefore String to byte[] for --outPutType application/octet-stream fails for a String payload.",1.0,0.0597570913041791,0.0597570913041791,0.0028326413410422,True,True,True,1400658424000.0,1400659964000.0,1400663564000.0,1400683295000.0,1400684195000.0,1400658424000.0,-1.0,1400658497000.0,1400662097000.0,1400659964000.0,1400663564000.0,1400659964000.0,1400663564000.0,"dturanski",1400662097000.0,1400658424000.0,44,1400658424000.0,-1.0,"Register StringToByteArrayMessageConverter","The converter was not configured, therefore String to byte[] for --outPutType application/octet-stream fails for a String payload.",1.0,False,1.0,0,0.0,0,0,False
"XD-1740","Story","","2014-05-20T10:17:28.000+0000","iperumal","","ZooKeeper Admin server node data to have admin server host address","It would be useful to store admin server ip address in ZooKeeper leadership group node (/xd/admins) to identify admin server and it's admin port.

",1.0,-1.0,0.9996546365049216,-1.0,False,False,True,1400581048000.0,-1.0,-1.0,1400591730000.0,1400592630000.0,1400581048000.0,-1.0,-1.0,-1.0,1400592626000.0,1400592630000.0,-1.0,-1.0,"iperumal",-1.0,-1.0,0,1400581048000.0,-1.0,"ZooKeeper Admin server node data to have admin server host address","It would be useful to store admin server ip address in ZooKeeper leadership group node (/xd/admins) to identify admin server and it's admin port.

",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1739","Bug","Sprint 27","2014-05-20T09:27:56.000+0000","pperalta","pperalta","Container reconnection to ZK fails intermittently","As reported by Matt Stine:

After closing and reopening a laptop, the following stack trace appears in the container log:

{noformat}
00:47:28,226  INFO main-EventThread state.ConnectionStateManager:194 - State change: RECONNECTED
00:47:28,226  INFO ConnectionStateManager-0 zookeeper.ZooKeeperConnection:255 - >>> Curator connected event: RECONNECTED
00:47:28,322 ERROR ConnectionStateManager-0 listen.ListenerContainer:96 - Listener (org.springframework.xd.dirt.zookeeper.ZooKeeperConnection$DelegatingConnectionStateListener@6abf4158) threw an exception
java.lang.RuntimeException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a
        at org.springframework.xd.dirt.server.ContainerRegistrar.registerWithZooKeeper(ContainerRegistrar.java:301)
        at org.springframework.xd.dirt.server.ContainerRegistrar.access$100(ContainerRegistrar.java:93)
        at org.springframework.xd.dirt.server.ContainerRegistrar$ContainerAttributesRegisteringZooKeeperConnectionListener.onConnect(ContainerRegistrar.java:316)
        at org.springframework.xd.dirt.zookeeper.ZooKeeperConnection$DelegatingConnectionStateListener.stateChanged(ZooKeeperConnection.java:257)
        at org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:222)
        at org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:218)
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
        at org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:215)
        at org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:42)
        at org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a
        at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:75)
        at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:42)
        at org.springframework.xd.dirt.server.ContainerRegistrar.registerWithZooKeeper(ContainerRegistrar.java:295)
        ... 15 more
Caused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
        at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
        at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)
        at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:69)
        ... 17 more
{noformat}

This can occur if ZK does not remove the ephemeral node before the container creates a new one. This can be fixed in the following ways:

* Remove the existing ephemeral node if it already exists
* Register containers with a new UUID upon every new connection

For now I'll implement the first solution.",2.0,0.1282437878743947,0.0,0.0023679047425749,True,True,True,1400578076000.0,1400591345000.0,1400594945000.0,1400680643000.0,1400681543000.0,1400578076000.0,-1.0,1400578321000.0,1400581921000.0,1400578076000.0,1400581676000.0,1400591345000.0,1400594945000.0,"pperalta",1400581921000.0,1400578076000.0,44,1400578076000.0,-1.0,"Container reconnection to ZK fails intermittently","As reported by Matt Stine:

After closing and reopening a laptop, the following stack trace appears in the container log:

{noformat}
00:47:28,226  INFO main-EventThread state.ConnectionStateManager:194 - State change: RECONNECTED
00:47:28,226  INFO ConnectionStateManager-0 zookeeper.ZooKeeperConnection:255 - >>> Curator connected event: RECONNECTED
00:47:28,322 ERROR ConnectionStateManager-0 listen.ListenerContainer:96 - Listener (org.springframework.xd.dirt.zookeeper.ZooKeeperConnection$DelegatingConnectionStateListener@6abf4158) threw an exception
java.lang.RuntimeException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a
        at org.springframework.xd.dirt.server.ContainerRegistrar.registerWithZooKeeper(ContainerRegistrar.java:301)
        at org.springframework.xd.dirt.server.ContainerRegistrar.access$100(ContainerRegistrar.java:93)
        at org.springframework.xd.dirt.server.ContainerRegistrar$ContainerAttributesRegisteringZooKeeperConnectionListener.onConnect(ContainerRegistrar.java:316)
        at org.springframework.xd.dirt.zookeeper.ZooKeeperConnection$DelegatingConnectionStateListener.stateChanged(ZooKeeperConnection.java:257)
        at org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:222)
        at org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:218)
        at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
        at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
        at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
        at org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:215)
        at org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:42)
        at org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:110)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a
        at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:75)
        at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:42)
        at org.springframework.xd.dirt.server.ContainerRegistrar.registerWithZooKeeper(ContainerRegistrar.java:295)
        ... 15 more
Caused by: org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/containers/5a8deb7b-fd93-42a7-a393-2f15023e007a
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
        at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
        at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
        at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
        at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
        at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)
        at org.springframework.xd.dirt.container.store.ZooKeeperContainerAttributesRepository.save(ZooKeeperContainerAttributesRepository.java:69)
        ... 17 more
{noformat}

This can occur if ZK does not remove the ephemeral node before the container creates a new one. This can be fixed in the following ways:

* Remove the existing ephemeral node if it already exists
* Register containers with a new UUID upon every new connection

For now I'll implement the first solution.",2.0,False,2.0,0,0.0,0,0,False
"XD-1737","Improvement","Sprint 27","2014-05-20T07:20:33.000+0000","dturanski","dturanski","Add --verbose option to display all property values","",1.0,0.0021967898522481,0.0,0.0,True,True,True,1400570433000.0,1400570681000.0,1400574281000.0,1400682425000.0,1400683325000.0,1400570433000.0,-1.0,1400570433000.0,1400574033000.0,1400570433000.0,1400574033000.0,1400570681000.0,1400574281000.0,"dturanski",1400574033000.0,1400570433000.0,44,1400570433000.0,-1.0,"Add --verbose option to display all property values","",1.0,False,1.0,0,0.0,0,0,False
"XD-1735","Bug","","2014-05-19T19:01:14.000+0000","grenfro","grenfro","FileJdbcTest & JdbcHdfsTest failing","JdbcHdfsTest is failing for the same reason as hdfs and I'm working on this.
Not sure why FileJdbc test is failing investigating.",5.0,0.6840647421992324,0.0,-1.0,True,False,True,1400526074000.0,1400567070000.0,1400570670000.0,1400585104000.0,1400586004000.0,1400526074000.0,-1.0,-1.0,-1.0,1400526074000.0,1400529674000.0,1400567070000.0,1400570670000.0,"grenfro",-1.0,-1.0,0,1400526074000.0,-1.0,"FileJdbcTest & JdbcHdfsTest failing","JdbcHdfsTest, FileJdbcTest works for singlenode but not for admin & Container on the same machine.",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-1734","Story","Sprint 27","2014-05-19T12:58:52.000+0000","mark.pollack","grenfro","Improve JMX checks for processors taking into account error channels","",2.0,-1.0,0.0,0.0,False,True,True,1400504332000.0,-1.0,-1.0,1400508084000.0,1400508984000.0,1400504332000.0,-1.0,1400504332000.0,1400507932000.0,1400504332000.0,1400507932000.0,-1.0,-1.0,"mark.pollack",1400507932000.0,1400504332000.0,44,1400504332000.0,-1.0,"Improve JMX checks for processors taking into account error channels","",2.0,False,2.0,0,0.0,-1,0,False
"XD-1733","Story","Sprint 27","2014-05-19T10:50:07.000+0000","mark.pollack","eric.bottard","reactor-ip module uses @Configuration, resulting in no prefix being added to @Value key names","We don't support using @Configuration for modules ATM.  The current code was committed during the same time as improvements to handling module configuration.  We should switch the reactor-ip.xml to include all bean definitions and remove referencing @Configuration classes or see how to add support for @Configuration.  

Another short term hack is to put the prefix 'sink.reactor-ip' in all @Value used in NetServerInboundChannelAdapterConfiguration.",3.0,0.0500001605450826,0.0,0.0,True,True,True,1400496607000.0,1400574467000.0,1400578067000.0,1402052902000.0,1402053802000.0,1400496607000.0,-1.0,1400496607000.0,1400500207000.0,1400496607000.0,1400500207000.0,1400574467000.0,1400578067000.0,"mark.pollack",1400500207000.0,1400496607000.0,44,1400496607000.0,-1.0,"Investigate fall through of server.yml values when running in YARN","We don't support using @Configuration for modules ATM.  The current code was committed during the same time as improvements to handling module configuration.  We should switch the reactor-ip.xml to include all bean definitions and remove referencing @Configuration classes or see how to add support for @Configuration.  

Another short term hack is to put the prefix 'sink.reactor-ip' in all @Value used in NetServerInboundChannelAdapterConfiguration.",3.0,False,3.0,0,0.0,0,0,True
"XD-1732","Story","Sprint 27","2014-05-19T10:32:17.000+0000","mark.pollack","david_syer","Remove dependency on Sprint Boot in xd-dirt tests","",1.0,0.0342105263157894,0.0,0.0,False,False,False,1400495537000.0,1400495563000.0,1400496297000.0,1400495537000.0,1400496297000.0,1400495537000.0,-1.0,1400495537000.0,1400496297000.0,1400495537000.0,1400496297000.0,1400495563000.0,1400496297000.0,"mark.pollack",1400496297000.0,1400495537000.0,44,1400495537000.0,-1.0,"Remove dependency on Sprint Boot in xd-dirt tests","",1.0,False,1.0,0,0.0,-1,1,False
"XD-1731","Story","Sprint 27","2014-05-19T09:30:18.000+0000","grenfro","grenfro","HdfsTest will need to support a UUID appended to filenames on hdfs.","",3.0,0.0019174930148468,0.0,0.148468744863858,True,True,True,1400491818000.0,1400491853000.0,1400495453000.0,1400509171000.0,1400510071000.0,1400491818000.0,-1.0,1400494528000.0,1400498128000.0,1400491818000.0,1400495418000.0,1400491853000.0,1400495453000.0,"grenfro",1400498128000.0,1400491818000.0,44,1400491818000.0,-1.0,"Support for UUID suffix for hdfs file names in acceptance tests","",3.0,False,3.0,0,0.0,0,0,True
"XD-1730","Bug","Sprint 27","2014-05-19T01:00:59.000+0000","askogman","pperalta","Zookeeper NoNode exception when deploying stream","Same problem on M6 and using BUILD-SNAPSHOT.

When deploying a stream that has a slow-starting component (that connects to Gemfire), the deployment fails with a ZK NoNode exception.

No log from the component seen, but in all honesty, the component could be waiting on a timeout.

org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/2af8624b-777c-4084-aa1a-9d675b53afe3/test1.sink.reactor-batching-client-1/metadata
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:370)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:93)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:706)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)",2.0,0.6333390081961232,0.6253131308218012,0.719969842157745,True,True,True,1400461259000.0,1400487300000.0,1400490900000.0,1400501476000.0,1400502376000.0,1400461259000.0,1400502589000.0,1400490862000.0,1400494462000.0,1400486970000.0,1400490570000.0,1400487300000.0,1400490900000.0,"askogman",1400494462000.0,1400461259000.0,44,1400461259000.0,-1.0,"Zookeeper NoNode exception when deploying stream","Same problem on M6 and using BUILD-SNAPSHOT.

When deploying a stream that has a slow-starting component (that connects to Gemfire), the deployment fails with a ZK NoNode exception.

No log from the component seen, but in all honesty, the component could be waiting on a timeout.

org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/2af8624b-777c-4084-aa1a-9d675b53afe3/test1.sink.reactor-batching-client-1/metadata
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:44)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:370)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$800(ContainerRegistrar.java:93)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:706)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)",0.0,False,0.0,1,-1.0,0,0,False
"XD-1729","Story","Sprint 28","2014-05-18T15:06:08.000+0000","grenfro","","Spring XD Samples need to be updated to use the latest dependencies and Hadoop 2.2 vs. Hadoop 1.2","",4.0,0.8572353297455474,0.8572353297455474,0.7663970705585784,True,True,True,1400425568000.0,1401290341000.0,1401293941000.0,1401433461000.0,1401434361000.0,1400425568000.0,-1.0,1401198704000.0,1401202304000.0,1401290341000.0,1401293941000.0,1401290341000.0,1401293941000.0,"grenfro",1401202304000.0,1400425568000.0,44,1400425568000.0,-1.0,"Update dependencies in Spring XD Sample Repository","",4.0,False,4.0,0,0.0,0,0,True
"XD-1728","Story","Sprint 29","2014-05-17T12:24:31.000+0000","hillert","hillert","Add Support for Bold/Strong Fonts ","Hitting this issue in Chrome:

http://stackoverflow.com/questions/22891611/google-font-varela-round-doesnt-support-font-weight-in-chrome

Looks like Chrome has some issues with making text bold if the font does not explicitly support it.
",2.0,0.9229682637146874,0.7726991181951687,1.0000344857392511,True,True,True,1400329471000.0,1402390281000.0,1402393881000.0,1402561378000.0,1402562278000.0,1400329471000.0,-1.0,1402562355000.0,1402562278000.0,1402054759000.0,1402058359000.0,1402390281000.0,1402393881000.0,"hillert",1402562278000.0,1400329471000.0,44,1400329471000.0,-1.0,"Add Support for Bold/Strong Fonts ","Hitting this issue in Chrome:

http://stackoverflow.com/questions/22891611/google-font-varela-round-doesnt-support-font-weight-in-chrome

Looks like Chrome has some issues with making text bold if the font does not explicitly support it.
",2.0,False,2.0,0,0.0,0,0,False
"XD-1727","Story","Sprint 27","2014-05-16T12:03:38.000+0000","iperumal","iperumal","Add Stream/Job destroy option at the UI","Add an option to destroy the stream/job definitions.",3.0,0.000135060608448,0.000135060608448,0.0425328366104289,True,True,True,1400241818000.0,1400241830000.0,1400245430000.0,1400329767000.0,1400330667000.0,1400241818000.0,1400270302000.0,1400245597000.0,1400249197000.0,1400241830000.0,1400245430000.0,1400241830000.0,1400245430000.0,"iperumal",1400249197000.0,1400241818000.0,44,1400270302000.0,1400270302000.0,"Add Stream/Job destroy option at the UI","Add an option to destroy the stream/job definitions.
Also add confirm action that asks for user to confirm to proceed with destroy.",2.0,True,2.0,1,50.0,0,0,True
"XD-1726","Improvement","","2014-05-16T11:04:18.000+0000","kparikh","","Change jacoco to 0.7 for jdk 8 builds","https://build.spring.io/browse/XD-JDK8-5

apply plugin: 'jacoco'

jacoco {
    toolVersion = ""0.7.0.201403182114""
}",1.0,0.1997029702970297,0.233069306930693,-1.0,True,False,True,1400238258000.0,1400240275000.0,1400243875000.0,1400247458000.0,1400248358000.0,1400238258000.0,-1.0,-1.0,-1.0,1400240612000.0,1400244212000.0,1400240275000.0,1400243875000.0,"kparikh",-1.0,-1.0,0,1400238258000.0,-1.0,"Change jacoco to 0.7 for jdk 8 builds","https://build.spring.io/browse/XD-JDK8-5

apply plugin: 'jacoco'

jacoco {
    toolVersion = ""0.7.0.201403182114""
}",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1723","Improvement","Sprint 29","2014-05-16T09:03:29.000+0000","dbeauregard","eric.bottard","'--type=' not supported by module delete as shown in documentation examples","In the Module Composition example here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#composing-modules on of the examples is ""module delete --name foo --type sink"" which fails as the '--type' argument is not supported by the CLI.  

There are 3 other references to the '--type' argument in the documentation which may not be supported by the CLI anymore. ",1.0,-1.0,0.9323370152667984,0.9971838629574058,False,True,True,1400231009000.0,-1.0,-1.0,1401788982000.0,1401789882000.0,1400231009000.0,-1.0,1401785492000.0,1401789092000.0,1401684404000.0,1401688004000.0,-1.0,-1.0,"dbeauregard",1401789092000.0,1400231009000.0,44,1400231009000.0,-1.0,"'--type=' not supported by module delete as shown in documentation examples","In the Module Composition example here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#composing-modules on of the examples is ""module delete --name foo --type sink"" which fails as the '--type' argument is not supported by the CLI.  

There are 3 other references to the '--type' argument in the documentation which may not be supported by the CLI anymore. ",1.0,False,1.0,0,0.0,0,0,False
"XD-1721","Story","","2014-05-16T01:40:41.000+0000","eric.bottard","","Add a test for ${xd.stream.name} inside the DSL definition of a stream","",2.0,-1.0,0.9359256226149658,-1.0,False,False,True,1400204441000.0,-1.0,-1.0,1400497824000.0,1400498724000.0,1400204441000.0,-1.0,-1.0,-1.0,1400479868000.0,1400483468000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1400204441000.0,-1.0,"Add a test for ${xd.stream.name} inside the DSL definition of a stream","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1720","Story","Sprint 27","2014-05-16T01:39:54.000+0000","eric.bottard","","Document OOTB available ${xd.???} keys","As a consequence of not fixing XD-1289, we should document keys of the form ${xd.stream.name} that are available to users

${xd.[stream|job].name} and ${xd.container.???} come to mind, there may be others",4.0,-1.0,0.9998158724864856,0.9748926501855172,False,True,True,1400204394000.0,-1.0,-1.0,1400491338000.0,1400492238000.0,1400204394000.0,-1.0,1400485011000.0,1400488611000.0,1400492185000.0,1400492238000.0,-1.0,-1.0,"eric.bottard",1400488611000.0,1400204394000.0,44,1400204394000.0,-1.0,"Document OOTB available ${xd.???} keys","As a consequence of not fixing XD-1289, we should document keys of the form ${xd.stream.name} that are available to users

${xd.[stream|job].name} and ${xd.container.???} come to mind, there may be others",4.0,False,4.0,0,0.0,0,0,False
"XD-1719","Bug","Sprint 27","2014-05-15T17:47:25.000+0000","iperumal","iperumal","ZooKeeper Job deployments path state is not updated after successful deployment","After successful job deployment, the Job deployments path in ZK doesn't get updated with the data {""state"": ""deployed""}

Though this data is not used for deployed instance repository (org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository) to check for the deployment status, it may be better to have this state updated like stream deployment path.",1.0,0.0076626601465502,0.0,0.9995886842466066,True,True,True,1400176045000.0,1400176548000.0,1400180148000.0,1400240788000.0,1400241688000.0,1400176045000.0,-1.0,1400241661000.0,1400241688000.0,1400176045000.0,1400179645000.0,1400176548000.0,1400180148000.0,"iperumal",1400241688000.0,1400176045000.0,44,1400176045000.0,-1.0,"ZooKeeper Job deployments path state is not updated after successful deployment","After successful job deployment, the Job deployments path in ZK doesn't get updated with the data {""state"": ""deployed""}

Though this data is not used for deployed instance repository (org.springframework.xd.dirt.stream.zookeeper.ZooKeeperJobRepository) to check for the deployment status, it may be better to have this state updated like stream deployment path.",1.0,False,1.0,0,0.0,0,1,False
"XD-1718","Story","","2014-05-15T17:38:47.000+0000","grenfro","grenfro","Twitter Search test uses case sensitive search when it should be case insensitive.","The TwitterSearch does a case insensitive search.  Tests need to do a insensitive check for the keywords in the search result.",3.0,1.948861864671032e-05,0.0,-1.0,True,False,True,1400175527000.0,1400175535000.0,1400179135000.0,1400585123000.0,1400586023000.0,1400175527000.0,-1.0,-1.0,-1.0,1400175527000.0,1400179127000.0,1400175535000.0,1400179135000.0,"grenfro",-1.0,-1.0,0,1400175527000.0,-1.0,"Twitter Search test uses case sensitive search when it should be case insensitive.","The TwitterSearch does a case insensitive search.  Tests need to do a insensitive check for the keywords in the search result.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1716","Story","Sprint 27","2014-05-15T10:40:28.000+0000","mark.pollack","","Document that modules can reference property values in servers.yml","Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml (or individual modules).   The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq/mqtt where using the server config values as defaults is useful and that they can still be overridden.
",1.0,-1.0,0.9984870531936269,0.0,False,True,True,1400150428000.0,-1.0,-1.0,1400478026000.0,1400478926000.0,1400150428000.0,-1.0,1400150428000.0,1400154028000.0,1400478429000.0,1400478926000.0,-1.0,-1.0,"mark.pollack",1400154028000.0,1400150428000.0,44,1400150428000.0,-1.0,"Document that modules can reference property values in servers.yml","Modules can use property values in servers.yml which is very handy to keep batch and hdfs functionality working without duplication of config values in servers.yml and modules.yml (or individual modules).   The configuration section should highlight the common cases where this occurs, batch, hdfs, rabbitmq/mqtt where using the server config values as defaults is useful and that they can still be overridden.
",1.0,False,1.0,0,0.0,0,0,False
"XD-1715","Story","Sprint 27","2014-05-15T10:35:53.000+0000","mark.pollack","","Create documentation section for the shell","Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes.

Include some discussion of basic commands to manipulate streams, jobs and list modules.  How to pass in a file that can be executed when the shell starts up.

Also point to spring-shell ref docs for extensibility in terms of adding custom commands.",3.0,0.0977390889439092,0.0977390889439092,0.0,True,True,True,1400150153000.0,1400250935000.0,1400254535000.0,1401180386000.0,1401181286000.0,1400150153000.0,-1.0,1400150153000.0,1400153753000.0,1400250935000.0,1400254535000.0,1400250935000.0,1400254535000.0,"mark.pollack",1400153753000.0,1400150153000.0,44,1400150153000.0,-1.0,"Create documentation section for the shell","Create a new section in the docs regaring shell usage, in particular how to represent single and double quotes.

Include some discussion of basic commands to manipulate streams, jobs and list modules.  How to pass in a file that can be executed when the shell starts up.

Also point to spring-shell ref docs for extensibility in terms of adding custom commands.",3.0,False,3.0,0,0.0,0,0,False
"XD-1713","Story","","2014-05-15T06:42:48.000+0000","mark.pollack","iperumal","Show visual representation of stream in admin UI","",5.0,-1.0,0.0,-1.0,False,False,True,1400136168000.0,-1.0,-1.0,1417866773000.0,1417867673000.0,1400136168000.0,-1.0,-1.0,-1.0,1400136168000.0,1400139768000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1400136168000.0,-1.0,"Show visual representation of stream in admin UI","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1712","Story","","2014-05-15T05:12:42.000+0000","grenfro","","StreamUtil Cleanup","Update StreamUtils based on Mark's comments.",3.0,0.0030398174443879,0.0029565347746787,-1.0,True,False,True,1400130762000.0,1400131127000.0,1400134727000.0,1400249935000.0,1400250835000.0,1400130762000.0,-1.0,-1.0,-1.0,1400131117000.0,1400134717000.0,1400131127000.0,1400134727000.0,"grenfro",-1.0,-1.0,0,1400130762000.0,-1.0,"StreamUtil Cleanup","Update StreamUtils based on Code Review comments.",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-1710","Story","Sprint 27","2014-05-14T13:33:45.000+0000","grenfro","grenfro","ProcessorTest.testfailedSink needs to use http as its test source","Also check the JMX output to see that the filter rejected the entry.",5.0,0.0085955533258002,0.0,1.0000253148826197,True,True,True,1400074425000.0,1400078160000.0,1400081760000.0,1400508052000.0,1400508952000.0,1400074425000.0,-1.0,1400508963000.0,1400508952000.0,1400074425000.0,1400078025000.0,1400078160000.0,1400081760000.0,"grenfro",1400508952000.0,1400074425000.0,44,1400074425000.0,-1.0,"ProcessorTest.testfailedSink needs to use http as its test source","Also check the JMX output to see that the filter rejected the entry.",5.0,False,5.0,0,0.0,-1,1,False
"XD-1709","Bug","","2014-05-14T12:22:08.000+0000","iperumal","iperumal","Handling JobExecution stop action if the JobExecution is COMPLETED","Currently, the flag ""stoppable"" on JobExecutionInfoResource is used to find if the jobExecution can be stopped.

Since this flag is set to true even if the JobExecution status is COMPLETED, the jobExecution can still say it can be stopped.",1.0,0.0011013854269317,0.0,-1.0,True,False,True,1400070128000.0,1400070166000.0,1400073766000.0,1400103730000.0,1400104630000.0,1400070128000.0,-1.0,-1.0,-1.0,1400070128000.0,1400073728000.0,1400070166000.0,1400073766000.0,"iperumal",-1.0,-1.0,0,1400070128000.0,-1.0,"Handling JobExecution stop action if the JobExecution is COMPLETED","Currently, the flag ""stoppable"" on JobExecutionInfoResource is used to find if the jobExecution can be stopped.

Since this flag is set to true even if the JobExecution status is COMPLETED, the jobExecution can still say it can be stopped.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1708","Bug","Sprint 27","2014-05-14T10:50:59.000+0000","iperumal","iperumal","Re-deployment of stream/job modules upon container departure doesn't choose appropriate container candidates","Upon container departure, the ContainerListener's onChildLeft() event triggers redeployment of stream/job modules that were deployed into the leaving container. During the redeployment, it happens that the container candidates from the DefaultContainerMatcher *sometimes* (based on the subset from distributeForRequestedCount(List<Container> candidates, int count))  includes the container which already have the module of the *same* stream/job definition deployed. This causes the re-deployment silently swallowing the NodeExistsException and the module being re-deployed doesn't actually get deployed.
",4.0,2.309122187200536e-05,0.0,0.0039393624513641,True,True,True,1400064659000.0,1400064669000.0,1400068269000.0,1400496824000.0,1400497724000.0,1400064659000.0,-1.0,1400066365000.0,1400069965000.0,1400064659000.0,1400068259000.0,1400064669000.0,1400068269000.0,"iperumal",1400069965000.0,1400064659000.0,44,1400064659000.0,-1.0,"Re-deployment of stream/job modules upon container departure doesn't choose appropriate container candidates","Upon container departure, the ContainerListener's onChildLeft() event triggers redeployment of stream/job modules that were deployed into the leaving container. During the redeployment, it happens that the container candidates from the DefaultContainerMatcher *sometimes* (based on the subset from distributeForRequestedCount(List<Container> candidates, int count))  includes the container which already have the module of the *same* stream/job definition deployed. This causes the re-deployment silently swallowing the NodeExistsException and the module being re-deployed doesn't actually get deployed.
",4.0,False,4.0,0,0.0,0,1,False
"XD-1707","Bug","Sprint 27","2014-05-14T07:55:46.000+0000","dbeauregard","","The Dynamic Router example in the docs throws an exception with Rabbit Transport","The example in the M6 documentation for the Dynamic Router (here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#dynamic-router) for the SpEL-Based Routing throws an exception when processing the message (from the HTTP post) saying ""No bean named 'queue:foo' is defined"", when using RabbitMQ as the transport.  I do not know a workaround.

Steps to reproduce:
1) Run RabbitMQ locally
2) Run xd-singlenode --transport rabbit
3) xd:>stream create f --definition ""queue:foo > transform --expression=payload+'-foo' | log"" --deploy

xd:>stream create b --definition ""queue:bar > transform --expression=payload+'-bar' | log"" --deploy

xd:>stream create r --definition ""http | router --expression=payload.contains('a')?'queue:foo':'queue:bar'"" --deploy

4) xd:>http post --data ""a""

5) This should give a stacktrace:
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'queue:foo' is defined
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)
	... 83 more
",1.0,0.1609726987915858,0.0848644120313522,0.0537072952409368,True,True,True,1400054146000.0,1400082200000.0,1400085800000.0,1400227524000.0,1400228424000.0,1400054146000.0,-1.0,1400063506000.0,1400067106000.0,1400068936000.0,1400072536000.0,1400082200000.0,1400085800000.0,"dbeauregard",1400067106000.0,1400054146000.0,44,1400054146000.0,-1.0,"The Dynamic Router example in the docs throws an exception with Rabbit Transport","The example in the M6 documentation for the Dynamic Router (here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#dynamic-router) for the SpEL-Based Routing throws an exception when processing the message (from the HTTP post) saying ""No bean named 'queue:foo' is defined"", when using RabbitMQ as the transport.  I do not know a workaround.

Steps to reproduce:
1) Run RabbitMQ locally
2) Run xd-singlenode --transport rabbit
3) xd:>stream create f --definition ""queue:foo > transform --expression=payload+'-foo' | log"" --deploy

xd:>stream create b --definition ""queue:bar > transform --expression=payload+'-bar' | log"" --deploy

xd:>stream create r --definition ""http | router --expression=payload.contains('a')?'queue:foo':'queue:bar'"" --deploy

4) xd:>http post --data ""a""

5) This should give a stacktrace:
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'queue:foo' is defined
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)
	... 83 more
",1.0,False,1.0,0,0.0,0,0,False
"XD-1705","Story","Sprint 27","2014-05-14T07:24:19.000+0000","thomas.risberg","thomas.risberg","Add defaultYarnClasspath entry for phd20, cdh5 and hdp21","Each Hadoop distro uses different settings for ""yarn.application.classpath"" and we should provide some starting points for the distros we support running XD on YARN for.

We should add a commented out stub ""defaultYarnClasspath"" entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros.
",3.0,0.1841816258559471,0.1841726424819265,0.2584381955096605,True,True,True,1400052259000.0,1400134269000.0,1400137869000.0,1400496626000.0,1400497526000.0,1400052259000.0,-1.0,1400167333000.0,1400170933000.0,1400134265000.0,1400137865000.0,1400134269000.0,1400137869000.0,"thomas.risberg",1400170933000.0,1400052259000.0,44,1400052259000.0,-1.0,"Add defaultYarnClasspath entry for phd20, cdh5 and hdp21","Each Hadoop distro uses different settings for ""yarn.application.classpath"" and we should provide some starting points for the distros we support running XD on YARN for.

We should add a commented out stub ""defaultYarnClasspath"" entry for phd20, cdh5 and hdp21 to replace the one for hadoop22 when someone deploys on these distros.
",3.0,False,3.0,0,0.0,0,1,False
"XD-1704","Story","Sprint 27","2014-05-14T06:36:18.000+0000","eric.bottard","eric.bottard","Create doc section about quotes handling","Document the different ""onion layers"" that come in play with regard to quoting and escaping (shell, xd-parser, SpEL expressions in some cases) and provide practical examples to common scenarios

",5.0,-1.0,0.9998826336245046,0.9969946511717396,False,True,True,1400049378000.0,-1.0,-1.0,1400568218000.0,1400569118000.0,1400049378000.0,-1.0,1400567556000.0,1400569118000.0,1400569057000.0,1400569118000.0,-1.0,-1.0,"eric.bottard",1400569118000.0,1400049378000.0,44,1400049378000.0,-1.0,"Create doc section about quotes handling","Document the different ""onion layers"" that come in play with regard to quoting and escaping (shell, xd-parser, SpEL expressions in some cases) and provide practical examples to common scenarios

",5.0,False,5.0,0,0.0,0,0,False
"XD-1703","Story","","2014-05-14T05:10:07.000+0000","thomas.risberg","","Add admin and container memory settings to servers.yml","We should add an easy way to configure the memory. Currently we only have the number of YARN containers configurable without diving into Spring YARN/Boot specific config options.

xd:
    adminServers: 1
    containers: 1

Proposing we do:

xd:
    adminServers: 1
    adminMemory: 512M
    containers: 1
    containerMemory: 512M

",3.0,0.9961150905039582,0.7363002816138419,-1.0,True,False,True,1400044207000.0,1404185168000.0,1404188768000.0,1404200418000.0,1404201318000.0,1400044207000.0,-1.0,-1.0,-1.0,1403105089000.0,1403108689000.0,1404185168000.0,1404188768000.0,"thomas.risberg",-1.0,-1.0,0,1400044207000.0,-1.0,"Add admin and container memory settings to servers.yml","We should add an easy way to configure the memory. Currently we only have the number of YARN containers configurable without diving into Spring YARN/Boot specific config options.
{code}
xd:
    adminServers: 1
    containers: 1
{code}

Proposing we do:
{code}
xd:
    adminServers: 1
    adminMemory: 512M
    containers: 1
    containerMemory: 512M
{code}

",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1702","Story","Sprint 31","2014-05-14T04:52:38.000+0000","thomas.risberg","thomas.risberg","Remove Hadoop from admin classpath","Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn't be needed except for module info for hdfs sink (see XD-1701)",1.0,0.6812556741516785,0.9957028133648592,0.9999971822958766,True,True,True,1400043158000.0,1403669811000.0,1403673411000.0,1405365741000.0,1405366641000.0,1400043158000.0,-1.0,1405366626000.0,1405366641000.0,1405343765000.0,1405347365000.0,1403669811000.0,1403673411000.0,"thomas.risberg",1405366641000.0,1402051385000.0,44,1402051385000.0,-1.0,"Remove Hadoop from admin classpath","Not sure why the Hadoop classes are on the admin servers classpath. There is no way to select the distro, and the Hadoop classes shouldn't be needed except for module info for hdfs sink (see XD-1701)",1.0,False,1.0,0,0.0,0,1,False
"XD-1701","Bug","Sprint 30","2014-05-14T04:49:33.000+0000","thomas.risberg","liujiong","hdfs sink loads Codecs class during 'module info --name sink:hdfs' command","The hdfs sink metadata causes loading of  org.springframework.data.hadoop.store.codec.Codecs class during 'module info --name sink:hdfs' command since the type is a specific Spring Hadoop class

options.codec.description = compression codec alias name
options.codec.type = org.springframework.data.hadoop.store.codec.Codecs
options.codec.default =

Don't think we want to tie the sink module to specific Spring Hadoop classes during runtime of the admin, we can't be sure that admin has hadoop classes on classpath in all environments and there is no way of specifying the hadoop distro for admin.

Wouldn't it be better to have this option as a String to be passed in to the module's context that could then load the class",3.0,-1.0,0.9999986949694728,0.9999966069206292,False,True,True,1400042973000.0,-1.0,-1.0,1403873401000.0,1403874301000.0,1400042973000.0,-1.0,1403874288000.0,1403874301000.0,1403874296000.0,1403874301000.0,-1.0,-1.0,"thomas.risberg",1403874301000.0,1400042973000.0,44,1400042973000.0,-1.0,"hdfs sink loads Codecs class during 'module info --name sink:hdfs' command","The hdfs sink metadata causes loading of  org.springframework.data.hadoop.store.codec.Codecs class during 'module info --name sink:hdfs' command since the type is a specific Spring Hadoop class

options.codec.description = compression codec alias name
options.codec.type = org.springframework.data.hadoop.store.codec.Codecs
options.codec.default =

Don't think we want to tie the sink module to specific Spring Hadoop classes during runtime of the admin, we can't be sure that admin has hadoop classes on classpath in all environments and there is no way of specifying the hadoop distro for admin.

Wouldn't it be better to have this option as a String to be passed in to the module's context that could then load the class",3.0,False,3.0,0,0.0,0,0,False
"XD-1700","Story","Sprint 27","2014-05-13T10:50:28.000+0000","mark.fisher","mark.fisher","Disable auto-formatting of JavaDoc","",1.0,0.0016827934371055,0.0,0.0,True,True,True,1399978228000.0,1399978240000.0,1399981840000.0,1399984459000.0,1399985359000.0,1399978228000.0,-1.0,1399978228000.0,1399981828000.0,1399978228000.0,1399981828000.0,1399978240000.0,1399981840000.0,"mark.fisher",1399981828000.0,1399978228000.0,44,1399978228000.0,-1.0,"Disable auto-formatting of JavaDoc","",1.0,False,1.0,0,0.0,0,0,False
"XD-1699","Story","Sprint 27","2014-05-12T16:45:09.000+0000","grenfro","grenfro","Rabbit Sink Acceptance Tests","",5.0,0.9999785030606269,4.836811358982765e-05,0.9999462576515668,True,True,True,1399913109000.0,1400099178000.0,1400099182000.0,1400098282000.0,1400099182000.0,1399913109000.0,-1.0,1400099172000.0,1400099182000.0,1399913118000.0,1399916718000.0,1400099178000.0,1400099182000.0,"grenfro",1400099182000.0,1399913109000.0,44,1399913109000.0,-1.0,"Rabbit Sink Acceptance Tests","",5.0,False,5.0,0,0.0,-1,0,False
"XD-1697","Story","","2014-05-12T14:40:24.000+0000","grenfro","","hdfsjdbc Acceptance Test","",5.0,-1.0,-1.0,-1.0,False,False,False,1399905624000.0,-1.0,-1.0,1403182624000.0,1403183524000.0,1399905624000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1399905624000.0,-1.0,"hdfsjdbc Acceptance Test","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1696","Story","","2014-05-12T12:37:25.000+0000","iperumal","iperumal","Enable Job deployment properties for job deploy","Support the ability to provide deployment properties for ""job deploy"".",8.0,0.0001479202414058,0.0,-1.0,True,False,True,1399898245000.0,1399898270000.0,1399901870000.0,1400066355000.0,1400067255000.0,1399898245000.0,1399937330000.0,-1.0,-1.0,1399898245000.0,1399901845000.0,1399898270000.0,1399901870000.0,"iperumal",-1.0,-1.0,0,1399937330000.0,-1.0,"Enable Job deployment properties for job deploy","Support the ability to provide deployment properties for ""job deploy"".",-1.0,False,-1.0,1,-1.0,0,0,False
"XD-1695","Story","Sprint 34","2014-05-12T09:35:57.000+0000","sabby","","Research how to secure Admin's REST endpoints","As a user, I'd like to have the option to provide security configurations so that I can access REST endpoints in a secured manner. 

Ideally, all the listed [REST|https://github.com/spring-projects/spring-xd/wiki/REST-API#xd-resources] endpoints needs to be wrapped within a security layer. 

Scope: Research the approach, design considerations and patterns that fits XD model.
",8.0,0.9846333818277168,0.9846031614332756,0.8368647915123036,True,True,True,1399887357000.0,1410769662000.0,1410773262000.0,1410938596000.0,1410939496000.0,1399887357000.0,-1.0,1409136503000.0,1409140103000.0,1410769328000.0,1410772928000.0,1410769662000.0,1410773262000.0,"sabby",1409140103000.0,1399887357000.0,44,1399887357000.0,-1.0,"Research how to secure Admin's REST endpoints","As a user, I'd like to have the option to provide security configurations so that I can access REST endpoints in a secured manner. 

Ideally, all the listed [REST|https://github.com/spring-projects/spring-xd/wiki/REST-API#xd-resources] endpoints needs to be wrapped within a security layer. 

*Scope of this spike:*

* Research Spring Security and Spring Boot and the OOTB features 
* Design considerations and approach for XD
* Developer experience
** How users will be configuring security credentials?
** How DSL shell will be handled?
** How Admin UI will be handled?",8.0,False,8.0,0,0.0,0,4,True
"XD-1687","Story","Sprint 27","2014-05-12T07:39:36.000+0000","mark.pollack","hillert","Add UI screen shots to docs for new features in Alpha ","* StepExecutionContext
* StepExecutionProgress
* JobScheduler
* Stream page
* Job definition (XD1615)

",4.0,0.4196783806101434,0.0,0.0,True,True,True,1399880376000.0,1400137544000.0,1400141144000.0,1400492250000.0,1400493150000.0,1399880376000.0,-1.0,1399880376000.0,1399883976000.0,1399880376000.0,1399883976000.0,1400137544000.0,1400141144000.0,"mark.pollack",1399883976000.0,1399880376000.0,44,1399880376000.0,-1.0,"Add UI screen shots to docs for new features in Alpha ","* StepExecutionContext
* StepExecutionProgress
* JobScheduler
* Stream page
* Job definition (XD1615)

",4.0,False,4.0,0,0.0,0,0,False
"XD-1686","Story","","2014-05-12T06:50:02.000+0000","iperumal","iperumal","Pluralization of admin nodes leadership selector group path (/xd/admin)","Currently, the admin nodes that participate in the leadership election are grouped under /xd/admin. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to /xd/admins.",1.0,0.0001525633269326,0.0,-1.0,True,False,True,1399877402000.0,1399877431000.0,1399881031000.0,1400066587000.0,1400067487000.0,1399877402000.0,-1.0,-1.0,-1.0,1399877402000.0,1399881002000.0,1399877431000.0,1399881031000.0,"iperumal",-1.0,-1.0,0,1399877402000.0,-1.0,"Pluralization of admin nodes leadership selector group path (/xd/admin)","Currently, the admin nodes that participate in the leadership election are grouped under /xd/admin. Since, there are multiple lock nodes that correspond to all the admin servers that participate in leadership election, we can pluralize this node name to /xd/admins.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1685","Story","Sprint 27","2014-05-12T06:11:47.000+0000","grenfro","grenfro","Rabbit Source AcceptanceTest","",5.0,0.0004284891739532,0.0,0.9999687559977326,True,True,True,1399875107000.0,1399875203000.0,1399878803000.0,1400098250000.0,1400099150000.0,1399875107000.0,-1.0,1400099143000.0,1400099150000.0,1399875107000.0,1399878707000.0,1399875203000.0,1399878803000.0,"grenfro",1400099150000.0,1399875107000.0,44,1399875107000.0,-1.0,"Rabbit Source AcceptanceTest","",5.0,False,5.0,0,0.0,-1,0,False
"XD-1684","Story","Sprint 29","2014-05-11T12:31:59.000+0000","sivangopal","","Test integration with jboss queue message","I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e ""stream create --name TEST-LOG  --definition ""jms | file"" --deploy"". I am trying to configure the ""spring-xd-1.0.0.M6\xd\modules\common\jms-jbossmq-infrastructure-context.xml"" to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",3.0,0.828804984827092,0.8288030107945037,0.4865965654794012,True,True,True,1399811519000.0,1403170349000.0,1403173949000.0,1403863237000.0,1403864137000.0,1399811519000.0,-1.0,1401783509000.0,1401787109000.0,1403170341000.0,1403173941000.0,1403170349000.0,1403173949000.0,"sivangopal",1401787109000.0,1401199033000.0,44,1401199033000.0,-1.0,"Test integration with jboss queue message","I am new to Spring XD, I want to read the jboss queue message and then want to write it into text file by using stream create comment i.e ""stream create --name TEST-LOG  --definition ""jms | file"" --deploy"". I am trying to configure the ""spring-xd-1.0.0.M6\xd\modules\common\jms-jbossmq-infrastructure-context.xml"" to invoke the jboss queue and read the queue message. Can you help me to do the configuration and resolve my objective?",3.0,False,3.0,0,0.0,0,1,False
"XD-1683","Bug","Sprint 27","2014-05-09T14:51:28.000+0000","grenfro","","syslog-tcp throws exception when receiving syslog data","XD Deployment 
Description	XD Cluster (1 Container)
Environment	EC2
Type Of Test	Manual test via shell
Test Failed On	syslog-tcp (only test that was run)
Build Used	Built May 7, 10:29 UTC

[Setting up the Environment]
* Used the wiki instructions to setup the syslog on the ec2 instance. 
* Deploy the stream below:
stream create mystream --definition ""syslog-tcp | file --binary=true --mode=REPLACE"" --deploy 
* On the EC2 Instance execute the line below:
logger -p local3.info -t TESTING ""Test Syslog Message""

[What occurred]
Stream fails to process inbound syslog information and throws the exception below: 

Exception in thread ""inbound.mystream.0-redis:queue-inbound-channel-adapter17"" org.springframework.messaging.core.DestinationResolutionException: failed to look up MessageChannel with name 'errorChannel' in the BeanFactory (and there is no HeaderChannelRegistry present).
	at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:108)
	at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:44)
	at org.springframework.integration.channel.MessagePublishingErrorHandler.resolveErrorChannel(MessagePublishingErrorHandler.java:111)
	at org.springframework.integration.channel.MessagePublishingErrorHandler.handleError(MessagePublishingErrorHandler.java:78)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)
	at java.lang.Thread.run(Thread.java:724)
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'errorChannel' is defined
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)
	... 5 more",5.0,-1.0,0.7709834457717109,0.9752625632372712,False,True,True,1399647088000.0,-1.0,-1.0,1399981631000.0,1399982531000.0,1399647088000.0,-1.0,1399974233000.0,1399977833000.0,1399905709000.0,1399909309000.0,-1.0,-1.0,"grenfro",1399977833000.0,1399647088000.0,44,1399647088000.0,-1.0,"syslog-tcp throws exception when receiving syslog data","XD Deployment 
Description	XD Cluster (1 Container)
Environment	EC2
Type Of Test	Manual test via shell
Test Failed On	syslog-tcp (only test that was run)
Build Used	Built May 7, 10:29 UTC

[Setting up the Environment]
* Used the wiki instructions to setup the syslog on the ec2 instance. 
* Deploy the stream below:
stream create mystream --definition ""syslog-tcp | file --binary=true --mode=REPLACE"" --deploy 
* On the EC2 Instance execute the line below:
logger -p local3.info -t TESTING ""Test Syslog Message""

[What occurred]
Stream fails to process inbound syslog information and throws the exception below: 

Exception in thread ""inbound.mystream.0-redis:queue-inbound-channel-adapter17"" org.springframework.messaging.core.DestinationResolutionException: failed to look up MessageChannel with name 'errorChannel' in the BeanFactory (and there is no HeaderChannelRegistry present).
	at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:108)
	at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:44)
	at org.springframework.integration.channel.MessagePublishingErrorHandler.resolveErrorChannel(MessagePublishingErrorHandler.java:111)
	at org.springframework.integration.channel.MessagePublishingErrorHandler.handleError(MessagePublishingErrorHandler.java:78)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:55)
	at java.lang.Thread.run(Thread.java:724)
Caused by: org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'errorChannel' is defined
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanDefinition(DefaultListableBeanFactory.java:641)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getMergedLocalBeanDefinition(AbstractBeanFactory.java:1159)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:282)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:273)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.integration.support.channel.BeanFactoryChannelResolver.resolveDestination(BeanFactoryChannelResolver.java:99)
	... 5 more",5.0,False,5.0,0,0.0,0,0,False
"XD-1682","Story","","2014-05-09T13:14:17.000+0000","grenfro","","Syslog Acceptance Tests","",8.0,3.3770093205457245e-05,3.3770093205457245e-05,-1.0,True,False,True,1399641257000.0,1399641274000.0,1399644874000.0,1400143761000.0,1400144661000.0,1399641257000.0,1399645713000.0,-1.0,-1.0,1399641274000.0,1399644874000.0,1399641274000.0,1399644874000.0,"grenfro",-1.0,-1.0,0,1399645713000.0,-1.0,"Syslog Acceptance Tests","",-1.0,False,-1.0,1,-1.0,-1,0,False
"XD-1680","Story","Sprint 27","2014-05-08T14:25:00.000+0000","grenfro","grenfro","jdbchdfs Acceptance Test","",5.0,6.666358038979676e-05,5.925651590204157e-05,0.9999907411693904,True,True,True,1399559100000.0,1399559136000.0,1399562736000.0,1400098225000.0,1400099125000.0,1399559100000.0,-1.0,1400099120000.0,1400099125000.0,1399559132000.0,1399562732000.0,1399559136000.0,1399562736000.0,"grenfro",1400099125000.0,1399559100000.0,44,1399559100000.0,-1.0,"jdbchdfs Acceptance Test","",5.0,False,5.0,0,0.0,-1,0,False
"XD-1679","Story","Sprint 28","2014-05-08T13:21:17.000+0000","grussell","dturanski","Remove %L from Log4j PatternLayout","The xd-dirt log4j.properties includes the calling line number {{%L}} which is not recommended for production.

https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html

{{WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue.}}",1.0,0.9999479539530566,0.9999187444369148,0.9999373323108232,True,True,True,1399555277000.0,1401438127000.0,1401438225000.0,1401437325000.0,1401438225000.0,1399555277000.0,-1.0,1401438107000.0,1401438225000.0,1401438072000.0,1401438225000.0,1401438127000.0,1401438225000.0,"grussell",1401438225000.0,1399555277000.0,44,1399555277000.0,-1.0,"Remove %L from Log4j PatternLayout","The xd-dirt log4j.properties includes the calling line number {{%L}} which is not recommended for production.

https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html

{{WARNING Generating caller location information is extremely slow and should be avoided unless execution speed is not an issue.}}",1.0,False,1.0,0,0.0,-1,0,False
"XD-1678","Story","Sprint 27","2014-05-08T08:59:24.000+0000","grenfro","grenfro","XD does not reconnect to jobstore if connection is  lost, leaving Jobs in inconsistent state.","XD Deployment Description	XD Cluster (1 Container)
Environment	EC2
Type Of Test	Shell Command Line
Test Failed On	hdfs (only test that was run)
Build Used	Built May 7, 10:29 PST

[Overall issue]
XD Admin and container lost connectivity to the the jobstore (MySql on RDS) and did not reconnect.  
Exception Displayed in log.
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: The last packet successfully received from the server was 54,321,927 milliseconds ago.  The last packet sent successfully to the server was 54,321,928 milliseconds ago. is longer than the server configured value of 'wait_timeout'. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property 'autoReconnect=true' to avoid this problem.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1117)
	at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3871)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2484)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2664)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2788)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2738)
	at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1617)
	at org.springframework.jdbc.core.JdbcTemplate$1QueryStatementCallback.doInStatement(JdbcTemplate.java:452)
	at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:402)
	... 57 more

[Side Effects]
[Unable to create new Job with same name]
User executed a job list and then removed the jobs in the list.  When the user tried to create new jobs using the same names the application reported:
""Command failed org.springframework.xd.rest.client.impl.SpringXDException: StatementCallback; SQL [SELECT JOB_NAME FROM JOB_REGISTRY_NAMES]; No operations allowed after connection closed.; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: No operations allowed after connection closed.""
The only way to resolve it completely was to: 
1) shutdown the admin and the containers.  
2) Clear the jobs from the Batch job tables by hand
3) restart XD admin and containers.
",5.0,0.8763222296311194,0.033355374124715,0.1515146973271851,True,True,True,1399539564000.0,1402053665000.0,1402057265000.0,1402407587000.0,1402408487000.0,1399539564000.0,-1.0,1399974248000.0,1399977848000.0,1399635258000.0,1399638858000.0,1402053665000.0,1402057265000.0,"grenfro",1399977848000.0,1399539564000.0,44,1399539564000.0,-1.0,"XD does not reconnect to jobstore if connection is  lost, leaving Jobs in inconsistent state.","XD Deployment Description	XD Cluster (1 Container)
Environment	EC2
Type Of Test	Shell Command Line
Test Failed On	hdfs (only test that was run)
Build Used	Built May 7, 10:29 PST

[Overall issue]
XD Admin and container lost connectivity to the the jobstore (MySql on RDS) and did not reconnect.  
Exception Displayed in log.
Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: The last packet successfully received from the server was 54,321,927 milliseconds ago.  The last packet sent successfully to the server was 54,321,928 milliseconds ago. is longer than the server configured value of 'wait_timeout'. You should consider either expiring and/or testing connection validity before use in your application, increasing the server configured values for client timeouts, or using the Connector/J connection property 'autoReconnect=true' to avoid this problem.
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)
	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1117)
	at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3871)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2484)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2664)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2788)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2738)
	at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1617)
	at org.springframework.jdbc.core.JdbcTemplate$1QueryStatementCallback.doInStatement(JdbcTemplate.java:452)
	at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:402)
	... 57 more

[Side Effects]
[Unable to create new Job with same name]
User executed a job list and then removed the jobs in the list.  When the user tried to create new jobs using the same names the application reported:
""Command failed org.springframework.xd.rest.client.impl.SpringXDException: StatementCallback; SQL [SELECT JOB_NAME FROM JOB_REGISTRY_NAMES]; No operations allowed after connection closed.; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: No operations allowed after connection closed.""
The only way to resolve it completely was to: 
1) shutdown the admin and the containers.  
2) Clear the jobs from the Batch job tables by hand
3) restart XD admin and containers.
",5.0,False,5.0,0,0.0,0,0,False
"XD-1677","Story","Sprint 27","2014-05-08T08:07:23.000+0000","grussell","grussell","Add ""log-full-message"" Property to the Log Sink","Allows looking at message headers without turning on debugging.",1.0,0.7602555910543131,0.7602555910543131,0.5862619808306709,True,True,True,1399536443000.0,1399548341000.0,1399551941000.0,1399551193000.0,1399552093000.0,1399536443000.0,-1.0,1399545618000.0,1399549218000.0,1399548341000.0,1399551941000.0,1399548341000.0,1399551941000.0,"grussell",1399549218000.0,1399536443000.0,44,1399536443000.0,-1.0,"Add ""log-full-message"" Property to the Log Sink","Allows looking at message headers without turning on debugging.",1.0,False,1.0,0,0.0,0,0,False
"XD-1676","Story","Sprint 27","2014-05-08T07:12:22.000+0000","grenfro","grenfro","FileJdbc Acceptance Test","",5.0,0.0001802247508657,0.0,0.999991165453389,True,True,True,1399533142000.0,1399533244000.0,1399536844000.0,1400098202000.0,1400099102000.0,1399533142000.0,-1.0,1400099097000.0,1400099102000.0,1399533142000.0,1399536742000.0,1399533244000.0,1399536844000.0,"grenfro",1400099102000.0,1399533142000.0,44,1399533142000.0,-1.0,"FileJdbc Acceptance Test","",5.0,False,5.0,0,0.0,-1,0,False
"XD-1673","Improvement","Sprint 29","2014-05-07T06:14:58.000+0000","grenfro","","filepollhdfs documentation needs to be updated with all of the options available.","currently the documentation only shows the --name as an option.  Review the FilePollHdfsJobOptionsMetadata to find all the available options.",2.0,-1.0,0.8337686456457275,0.7505681298784194,False,True,True,1399443298000.0,-1.0,-1.0,1402560083000.0,1402560983000.0,1399443298000.0,-1.0,1401783333000.0,1401786933000.0,1402042726000.0,1402046326000.0,-1.0,-1.0,"grenfro",1401786933000.0,1399443298000.0,44,1399443298000.0,-1.0,"filepollhdfs documentation needs to be updated with all of the options available.","currently the documentation only shows the --name as an option.  Review the FilePollHdfsJobOptionsMetadata to find all the available options.",2.0,False,2.0,0,0.0,0,0,False
"XD-1672","Story","","2014-05-07T05:23:40.000+0000","grenfro","grenfro","Add filepollhdfs Acceptance Tests","",5.0,0.0003176284968678,0.0,-1.0,True,False,True,1399440220000.0,1399440771000.0,1399444371000.0,1401174051000.0,1401174951000.0,1399440220000.0,-1.0,-1.0,-1.0,1399440220000.0,1399443820000.0,1399440771000.0,1399444371000.0,"grenfro",-1.0,-1.0,0,1399440220000.0,-1.0,"Add filepollhdfs Acceptance Tests","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1671","Improvement","Sprint 27","2014-05-05T19:49:34.000+0000","mminella","mminella","Set fetch size when reading from database in pre-packaged jobs","When running a query against a large dataset, JDBC will attempt to load the entire result set into memory by default.  If this isn't desired (which would be the case in the prepackaged jobs), you can set the fetchSize on the JdbcCursorItemReader to set the number of rows to return with each fetch.  It is good practice to make this match the commit interval.  

If the fetch size is not set with large datasets, the stack blows with an OutOfMemoryException.",4.0,0.9999397584313096,0.0,0.9999364116774934,True,True,True,1399319374000.0,1400514490000.0,1400514562000.0,1400513662000.0,1400514562000.0,1399319374000.0,-1.0,1400514486000.0,1400514562000.0,1399319374000.0,1399322974000.0,1400514490000.0,1400514562000.0,"mminella",1400514562000.0,1399319374000.0,44,1399319374000.0,-1.0,"Set fetch size when reading from database in pre-packaged jobs","When running a query against a large dataset, JDBC will attempt to load the entire result set into memory by default.  If this isn't desired (which would be the case in the prepackaged jobs), you can set the fetchSize on the JdbcCursorItemReader to set the number of rows to return with each fetch.  It is good practice to make this match the commit interval.  

If the fetch size is not set with large datasets, the stack blows with an OutOfMemoryException.",4.0,False,4.0,0,0.0,0,0,False
"XD-1670","Bug","Sprint 27","2014-05-05T12:15:13.000+0000","pperalta","pperalta","NPE when a container departs","When a container departs the cluster the admin will try to redeploy any modules that container was running. If the stream was *destroyed* and the container exited before it had the chance to clean up its deployments under {{/xd/deployments/modules}} (for example, with {{kill -9}}) the following NPE occurs:

{noformat}
java.lang.NullPointerException
	at org.springframework.xd.dirt.server.ContainerListener.loadStream(ContainerListener.java:347)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:158)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}

If the stream was *undeployed* the following stack appears:
{noformat}
15:13:06,002 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:550 - 
java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/t0
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:468)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/t0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)
	at org.springframework.xd.dirt.server.ContainerListener.loadStream(ContainerListener.java:358)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:417)
	... 16 more
{noformat}

In short, this logic makes the assumption that the stream is still present and deployed. It needs to take into account the fact that neither assumption can be made.",2.0,0.0028064328218438,1.917163777402538e-05,0.9999660809485535,True,True,True,1399292113000.0,1399294016000.0,1399297616000.0,1399969298000.0,1399970198000.0,1399292113000.0,-1.0,1399970175000.0,1399970198000.0,1399292126000.0,1399295726000.0,1399294016000.0,1399297616000.0,"pperalta",1399970198000.0,1399292113000.0,44,1399292113000.0,-1.0,"NPE when a container departs","When a container departs the cluster the admin will try to redeploy any modules that container was running. If the stream was *destroyed* and the container exited before it had the chance to clean up its deployments under {{/xd/deployments/modules}} (for example, with {{kill -9}}) the following NPE occurs:

{noformat}
java.lang.NullPointerException
	at org.springframework.xd.dirt.server.ContainerListener.loadStream(ContainerListener.java:347)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:403)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:158)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}

If the stream was *undeployed* the following stack appears:
{noformat}
15:13:06,002 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:550 - 
java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/t0
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:468)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:159)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/streams/t0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)
	at org.springframework.xd.dirt.server.ContainerListener.loadStream(ContainerListener.java:358)
	at org.springframework.xd.dirt.server.ContainerListener.onChildLeft(ContainerListener.java:417)
	... 16 more
{noformat}

In short, this logic makes the assumption that the stream is still present and deployed. It needs to take into account the fact that neither assumption can be made.",2.0,False,2.0,0,0.0,0,0,False
"XD-1668","Story","Sprint 27","2014-05-02T23:33:45.000+0000","iperumal","iperumal","Modularize angular app modules based on the functionality","When adding streams page to the UI (from XD-1667), it is necessary to modularize the angular app modules based on the functionality/components (job, stream, auth etc.,). 

As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.",5.0,-1.0,0.0,0.9998794465390608,False,True,True,1399073625000.0,-1.0,-1.0,1400068134000.0,1400069034000.0,1399073625000.0,-1.0,1400068914000.0,1400069034000.0,1399073625000.0,1399077225000.0,-1.0,-1.0,"iperumal",1400069034000.0,1399073625000.0,44,1399073625000.0,-1.0,"Modularize angular app modules based on the functionality","When adding streams page to the UI (from XD-1667), it is necessary to modularize the angular app modules based on the functionality/components (job, stream, auth etc.,). 

As we expand into more components and use cases in the UI, this definitely makes it easier to concentrate on specific modules based on the functionality.",5.0,False,5.0,0,0.0,0,1,False
"XD-1667","Story","","2014-05-02T23:29:35.000+0000","iperumal","iperumal","Add Steams page to show job triggers","The streams page needs to be added to the UI at least to show the job triggers that are created while scheduling XD jobs.",2.0,-1.0,0.0,-1.0,False,False,True,1399073375000.0,-1.0,-1.0,1400068123000.0,1400069023000.0,1399073375000.0,-1.0,-1.0,-1.0,1399073375000.0,1399076975000.0,-1.0,-1.0,"iperumal",-1.0,-1.0,0,1399073375000.0,-1.0,"Add Steams page to show job triggers","The streams page needs to be added to the UI at least to show the job triggers that are created while scheduling XD jobs.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1666","Improvement","Sprint 27","2014-05-02T13:47:04.000+0000","mark.pollack","mark.pollack","Upgrade to Spring Shell 1.1 RC3","",1.0,0.9954573046014172,0.0,0.0,True,True,True,1399038424000.0,1400245412000.0,1400249012000.0,1400250020000.0,1400250920000.0,1399038424000.0,-1.0,1399038424000.0,1399042024000.0,1399038424000.0,1399042024000.0,1400245412000.0,1400249012000.0,"mark.pollack",1399042024000.0,1399038424000.0,44,1399038424000.0,-1.0,"Upgrade to Spring Shell 1.1 RC3","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1665","Improvement","Sprint 27","2014-05-02T13:46:43.000+0000","mark.pollack","mark.pollack","Update to snapshot builds of Spring Shell","",1.0,-1.0,0.0,0.0,False,True,True,1399038403000.0,-1.0,-1.0,1399039358000.0,1399040258000.0,1399038403000.0,-1.0,1399038403000.0,1399040258000.0,1399038403000.0,1399040258000.0,-1.0,-1.0,"mark.pollack",1399040258000.0,1399038403000.0,44,1399038403000.0,-1.0,"Update to snapshot builds of Spring Shell","",1.0,False,1.0,0,0.0,-1,1,False
"XD-1664","Improvement","Sprint 27","2014-05-02T11:19:12.000+0000","iperumal","iperumal","Remove ""singlenode"" prefix from embeddedHsql propery in singlenode profile","The prefix ""singlenode"" in embeddedHsql option property defined in singlenode profile seems to be an overhead as it only exists in singlenode profile.

Also, we don't need a system property ""XD_SINGLENODE_EMBEDHSQL"" as config/servers.yml can be used to override the default (from application.yml)",1.0,2.481810397958297e-05,0.0,0.999280274984592,True,True,True,1399029552000.0,1399029558000.0,1399033158000.0,1399270411000.0,1399271311000.0,1399029552000.0,-1.0,1399271137000.0,1399271311000.0,1399029552000.0,1399033152000.0,1399029558000.0,1399033158000.0,"iperumal",1399271311000.0,1399029552000.0,44,1399029552000.0,-1.0,"Remove ""singlenode"" prefix from embeddedHsql propery in singlenode profile","The prefix ""singlenode"" in embeddedHsql option property defined in singlenode profile seems to be an overhead as it only exists in singlenode profile.

Also, we don't need a system property ""XD_SINGLENODE_EMBEDHSQL"" as config/servers.yml can be used to override the default (from application.yml)",1.0,False,1.0,0,0.0,0,0,False
"XD-1663","Improvement","Sprint 27","2014-05-02T10:27:46.000+0000","iperumal","iperumal"," Tap naming consistency for stream taps","Currently, when creating the taps for streams, the name of the pub/sub channel inside the message bus would be 

""tap:<name-of-the-stream>.<module-name>.<module-index>

For instance, the following stream with name ""test"":

http | transform --expression=payload.toLowerCase() | file

will have the exchanges as  'topic.tap:test.http.0', 'topic.tap:test.transform.1' when using rabbit message bus.

Though, the stream config parser takes care of translating what user would provide in the DSL (for example: tap:stream:test.transform.1 to use the message bus exchange topic.tap:test.transform.1), it would be better we have the consistency inside the message bus channel name as well.

Also, this would be in sync with how we name taps for jobs. (tap:job:*)",2.0,1.768030472987441e-05,0.0,0.8885099628517152,True,True,True,1399026466000.0,1399026475000.0,1399030075000.0,1399534607000.0,1399535507000.0,1399026466000.0,-1.0,1399478754000.0,1399482354000.0,1399026466000.0,1399030066000.0,1399026475000.0,1399030075000.0,"iperumal",1399482354000.0,1399026466000.0,44,1399026466000.0,-1.0," Tap naming consistency for stream taps","Currently, when creating the taps for streams, the name of the pub/sub channel inside the message bus would be 

""tap:<name-of-the-stream>.<module-name>.<module-index>

For instance, the following stream with name ""test"":

http | transform --expression=payload.toLowerCase() | file

will have the exchanges as  'topic.tap:test.http.0', 'topic.tap:test.transform.1' when using rabbit message bus.

Though, the stream config parser takes care of translating what user would provide in the DSL (for example: tap:stream:test.transform.1 to use the message bus exchange topic.tap:test.transform.1), it would be better we have the consistency inside the message bus channel name as well.

Also, this would be in sync with how we name taps for jobs. (tap:job:*)",2.0,False,2.0,0,0.0,0,0,False
"XD-1662","Story","Sprint 27","2014-05-02T06:19:13.000+0000","grenfro","grenfro","Tests are failing due to change in JMX endpoint data","Need to update the Jackson parser.",4.0,9.195537589518558e-06,0.0,0.9999935631236874,True,True,True,1399011553000.0,1399011563000.0,1399015163000.0,1400098137000.0,1400099037000.0,1399011553000.0,-1.0,1400099030000.0,1400099037000.0,1399011553000.0,1399015153000.0,1399011563000.0,1399015163000.0,"grenfro",1400099037000.0,1399011553000.0,44,1399011553000.0,-1.0,"Tests are failing due to change in JMX endpoint data","Need to update the Jackson parser.",4.0,False,4.0,0,0.0,-1,2,False
"XD-1661","Story","Sprint 27","2014-05-01T07:04:38.000+0000","mark.pollack","mark.pollack","Fix package tangles","Sonar build is currently failing.",2.0,0.0953383075058988,0.0,0.0,True,True,True,1398927878000.0,1398930868000.0,1398934468000.0,1398958340000.0,1398959240000.0,1398927878000.0,-1.0,1398927878000.0,1398931478000.0,1398927878000.0,1398931478000.0,1398930868000.0,1398934468000.0,"mark.pollack",1398931478000.0,1398927878000.0,44,1398927878000.0,-1.0,"Fix package tangles","Sonar build is currently failing.",2.0,False,2.0,0,0.0,0,0,False
"XD-1660","Story","Sprint 32","2014-04-30T22:08:27.000+0000","mark.pollack","","Add Zookeeper distribution in the download zip","This will reduce one extra step for getting started using XD in distributed mode 'out of the box'",1.0,0.9256451137100122,0.9966906430288752,0.9187602617554844,True,True,True,1398895707000.0,1405984002000.0,1405987602000.0,1406552488000.0,1406553388000.0,1398895707000.0,-1.0,1405931280000.0,1405934880000.0,1406528046000.0,1406531646000.0,1405984002000.0,1405987602000.0,"mark.pollack",1405934880000.0,1405930996000.0,44,1405930996000.0,-1.0,"Add Zookeeper distribution in the download zip","This will reduce one extra step for getting started using XD in distributed mode 'out of the box'",1.0,False,1.0,0,0.0,0,0,False
"XD-1659","Improvement","Sprint 27","2014-04-30T14:48:16.000+0000","mark.pollack","grenfro","Add HDFS (apache Hadoop 2.2 distro) acceptance tests","",8.0,0.2219945557774909,0.0,0.0,True,True,True,1398869296000.0,1399041453000.0,1399045053000.0,1399643897000.0,1399644797000.0,1398869296000.0,-1.0,1398869296000.0,1398872896000.0,1398869296000.0,1398872896000.0,1399041453000.0,1399045053000.0,"mark.pollack",1398872896000.0,1398869296000.0,44,1398869296000.0,-1.0,"Add HDFS (apache Hadoop 2.2 distro) acceptance tests","",8.0,False,8.0,0,0.0,-1,0,False
"XD-1658","Improvement","Sprint 27","2014-04-30T14:46:09.000+0000","mark.pollack","grenfro","Add Twitter Stream tests acceptance tests","",4.0,0.0758710239750614,0.0,0.9999943084942472,True,True,True,1398869169000.0,1398962483000.0,1398966083000.0,1400098172000.0,1400099072000.0,1398869169000.0,-1.0,1400099065000.0,1400099072000.0,1398869169000.0,1398872769000.0,1398962483000.0,1398966083000.0,"mark.pollack",1400099072000.0,1398869371000.0,44,1398869371000.0,-1.0,"Add Twitter Stream tests acceptance tests","",4.0,False,4.0,0,0.0,-1,1,False
"XD-1657","Improvement","Sprint 27","2014-04-30T14:44:59.000+0000","mark.pollack","grenfro","Add Twitter search module tests acceptance tests","",4.0,0.3566739083725261,0.0,0.0018919951297538,True,True,True,1398869099000.0,1398928859000.0,1398932459000.0,1399035747000.0,1399036647000.0,1398869099000.0,-1.0,1398869416000.0,1398873016000.0,1398869099000.0,1398872699000.0,1398928859000.0,1398932459000.0,"mark.pollack",1398873016000.0,1398869402000.0,44,1398869402000.0,-1.0,"Add Twitter search module tests acceptance tests","",4.0,False,4.0,0,0.0,0,1,False
"XD-1655","Improvement","","2014-04-30T06:47:27.000+0000","mark.pollack","","twittersearch module to produce json data as-is from twitter","Do not convert the data into a spring social tweet object, pass along the json as-is from twitter search.",5.0,-1.0,0.7985943219050595,-1.0,False,False,True,1398840447000.0,-1.0,-1.0,1401226580000.0,1401227480000.0,1398840447000.0,-1.0,-1.0,-1.0,1400746718000.0,1400750318000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1398840447000.0,-1.0,"twittersearch module to produce json data as-is from twitter","Do not convert the data into a spring social tweet object, pass along the json as-is from twitter search.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1654","Improvement","Sprint 28","2014-04-30T06:44:45.000+0000","mark.pollack","dturanski","Change twittersearch default outputType to be application/json","The current output type is a Java object - this raises issues wrt to consumers in other JVM that to no have the spring social tweet object in the main container classpath.  See https://jira.spring.io/browse/XD-1370

Will also create another issue to update twittersearch to generate the raw twitterstream output vs. the structure of the spring social tweet object ",1.0,0.8003946086901882,0.0089268696262318,0.7701550996470724,True,True,True,1398840285000.0,1400750967000.0,1400754567000.0,1401226560000.0,1401227460000.0,1398840285000.0,-1.0,1400678780000.0,1400682380000.0,1398861595000.0,1398865195000.0,1400750967000.0,1400754567000.0,"mark.pollack",1400682380000.0,1398840285000.0,44,1398840285000.0,-1.0,"Change twittersearch default outputType to be application/json","The current output type is a Java object - this raises issues wrt to consumers in other JVM that to no have the spring social tweet object in the main container classpath.  See https://jira.spring.io/browse/XD-1370

Will also create another issue to update twittersearch to generate the raw twitterstream output vs. the structure of the spring social tweet object ",1.0,False,1.0,0,0.0,0,0,False
"XD-1653","Story","Sprint 27","2014-04-29T14:55:08.000+0000","grussell","grussell","Add More Sophisticated Retry Configuration to the Rabbit MessageBus","XD-1019 added simple (stateless) retry to the message bus.

Use stateful retry and an {{AmqpRejectAndDontRequeueRecoverer}} enabling failed messages to be requeued on the broker until successful (perhaps because another instance can handle the message); also provides a mechanism to route failed messages to a dead-letter exchange.

Requires setting the message id header in bus-generated messages.

Also add profiles and properties for common retry/backoff policies.",8.0,0.9999868786723316,0.6595067955355994,0.9999711330791294,True,True,True,1398783308000.0,1399545416000.0,1399545426000.0,1399544526000.0,1399545426000.0,1398783308000.0,-1.0,1399545404000.0,1399545426000.0,1399285930000.0,1399289530000.0,1399545416000.0,1399545426000.0,"grussell",1399545426000.0,1398784187000.0,44,1398784187000.0,-1.0,"Add More Sophisticated Retry Configuration to the Rabbit MessageBus","XD-1019 added simple (stateless) retry to the message bus.

Use stateful retry and an {{AmqpRejectAndDontRequeueRecoverer}} enabling failed messages to be requeued on the broker until successful (perhaps because another instance can handle the message); also provides a mechanism to route failed messages to a dead-letter exchange.

Requires setting the message id header in bus-generated messages.

Also add profiles and properties for common retry/backoff policies.",8.0,False,8.0,0,0.0,0,1,False
"XD-1652","Improvement","Sprint 27","2014-04-29T13:30:16.000+0000","iperumal","","Try using EventuallyMatcher in StreamDeploymentIntegrationTests","There are intermittent test failurs in the StreamDeploymentIntegrationTests (especially Rabbit tests). We can try using EventuallyMatcher and see if that fixes this.",1.0,0.9926779441682656,0.9926779441682656,0.0121767397874917,True,True,True,1398778216000.0,1406044151000.0,1406047751000.0,1406096845000.0,1406097745000.0,1398778216000.0,-1.0,1398867344000.0,1398870944000.0,1406044151000.0,1406047751000.0,1406044151000.0,1406047751000.0,"iperumal",1398870944000.0,1398778216000.0,44,1398778216000.0,-1.0,"Fix intermittent test failures at StreamDeploymentIntegrationTests","There are intermittent test failurs in the StreamDeploymentIntegrationTests (especially Rabbit tests). We can try using EventuallyMatcher and see if that fixes this.",1.0,False,1.0,0,0.0,0,0,True
"XD-1651","Story","Sprint 27","2014-04-29T12:00:45.000+0000","thomas.risberg","","Update HDFS sink to use container id (GUID) as part of file name","HDFS sink needs to have unique identifier for container id added as part of file name. Part of the file name in the directory will be the container id (GUID) - like base-path/logfile-GUID-1.txt
",5.0,0.9895641757675976,0.6960113223387749,0.0547899615708411,True,True,True,1398772845000.0,1400479578000.0,1400483178000.0,1400496677000.0,1400497577000.0,1398772845000.0,-1.0,1398867343000.0,1398870943000.0,1399973278000.0,1399976878000.0,1400479578000.0,1400483178000.0,"thomas.risberg",1398870943000.0,1398772845000.0,44,1398772845000.0,-1.0,"Update HDFS sink to use unique id (GUID) as part of file name","HDFS sink needs to have unique identifier for container id added as part of file name. Part of the file name in the directory will be the container id (GUID) - like base-path/logfile-GUID-1.txt
",5.0,False,5.0,0,0.0,0,0,True
"XD-1650","Story","Sprint 27","2014-04-29T11:58:34.000+0000","thomas.risberg","jvalkeal","Update HDFS sink to accept a partition strategy based on message header or a field in the message","Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data.

The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation.

Partition configuration could be made available to the sink using a  --format parameter:

that could then be used in XML config like:
{code}
expression=""new java.text.SimpleDateFormat('${format}').format(${timestamp})
{code}
Similar to the time source.",8.0,0.96129477618499,0.0272995508199812,0.0378027890387931,True,True,True,1398772714000.0,1401179054000.0,1401182654000.0,1401275042000.0,1401275942000.0,1398772714000.0,-1.0,1398867343000.0,1398870943000.0,1398841051000.0,1398844651000.0,1401179054000.0,1401182654000.0,"thomas.risberg",1398870943000.0,1398772714000.0,44,1398772714000.0,-1.0,"Update HDFS sink to accept a partition strategy","Add configuration for the partition strategy to HDFS sink to support writing files into subdirectories based on a partition key provided in the header or field in the message of the stream data.

The writing using HDFS Store DataWriter should pass in the partition key value to be used for the write operation.

Partition configuration could be made available to the sink using a  --format parameter:

that could then be used in XML config like:
{code}
expression=""new java.text.SimpleDateFormat('${format}').format(${timestamp})
{code}
Similar to the time source.",8.0,False,8.0,0,0.0,-1,3,True
"XD-1646","Story","","2014-04-29T07:29:25.000+0000","grenfro","","CLI needs to be setup to use the updated acceptance test structure","This change has to be facilitated because of the XD-1456 and XD-1455 stories.",3.0,0.0012532898859506,0.0012532898859506,-1.0,True,False,True,1398756565000.0,1398756595000.0,1398760195000.0,1398779602000.0,1398780502000.0,1398756565000.0,-1.0,-1.0,-1.0,1398756595000.0,1398760195000.0,1398756595000.0,1398760195000.0,"grenfro",-1.0,-1.0,0,1398756565000.0,-1.0,"CLI needs to be setup to use the updated acceptance test structure","This change has to be facilitated because of the XD-1456 and XD-1455 stories.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1645","Story","Sprint 27","2014-04-29T06:57:28.000+0000","grenfro","grenfro","Refactor Exception Handling and update JavaDocs for acceptance test","[Add JavaDocs to]
* StreamUtils
* HttpTest
* MqttTest
* JmsSource

[Exception Handling]
StreamUtils stream method should throw  IllegalStateException instead of a checked exception.
XDEc2Validation assertReceived, assertValid should throw IllegalStateException instead of a checked exception ",3.0,-1.0,0.0,0.3994732532460857,False,True,True,1398754648000.0,-1.0,-1.0,1399035857000.0,1399036757000.0,1398754648000.0,-1.0,1398867343000.0,1398870943000.0,1398754648000.0,1398758248000.0,-1.0,-1.0,"grenfro",1398870943000.0,1398754648000.0,44,1398754648000.0,-1.0,"Refactor Exception Handling and update JavaDocs for acceptance test","[Add JavaDocs to]
* StreamUtils
* HttpTest
* MqttTest
* JmsSource

[Exception Handling]
StreamUtils stream method should throw  IllegalStateException instead of a checked exception.
XDEc2Validation assertReceived, assertValid should throw IllegalStateException instead of a checked exception ",3.0,False,3.0,0,0.0,0,1,False
"XD-1644","Bug","Sprint 27","2014-04-28T19:26:57.000+0000","hillert","","Rest: Improve the determination whether a Job Execution is Restartable","In *BatchJobExecutionsController$restartJobExecution()* we need to do a better check whether a Batch Job Execution is restartable. 

This is also true when executing *BatchJobExecutionsController$list()*. The check performed under *new JobExecutionInfo(jobExecution, timeZone)* is not sufficient.

*Reason*:

Currently in the UI when I have failed Job Executions, I can restart those (good). However, if the next execution succeeds, the previously restartable jobs should NOT be marked as restartable anymore. 

Right now you can restart those jobs, resulting in a:

{code}
Caused by: org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException: A job instance already exists and is complete for parameters={random=0.5735953106895085, throwError=true}.  If you want to run this job again, change the parameters.
	at org.springframework.batch.core.repository.support.SimpleJobRepository.createJobExecution(SimpleJobRepository.java:126)
	at sun.reflect.GeneratedMethodAccessor211.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:98)
	at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:262)
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:95)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.batch.core.repository.support.AbstractJobRepositoryFactoryBean$1.invoke(AbstractJobRepositoryFactoryBean.java:172)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy40.createJobExecution(Unknown Source)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:125)
	at sun.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy42.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)
	at sun.reflect.GeneratedMethodAccessor208.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:95)
	at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	... 98 more
{code}



",4.0,0.5264253517505629,0.1110438411341165,0.1065210870643981,True,True,True,1398713217000.0,1399474905000.0,1399478505000.0,1400159223000.0,1400160123000.0,1398713217000.0,-1.0,1398867343000.0,1398870943000.0,1398873887000.0,1398877487000.0,1399474905000.0,1399478505000.0,"hillert",1398870943000.0,1398713217000.0,44,1398713217000.0,-1.0,"Rest: Improve the determination whether a Job Execution is Restartable","In *BatchJobExecutionsController$restartJobExecution()* we need to do a better check whether a Batch Job Execution is restartable. 

This is also true when executing *BatchJobExecutionsController$list()*. The check performed under *new JobExecutionInfo(jobExecution, timeZone)* is not sufficient.

*Reason*:

Currently in the UI when I have failed Job Executions, I can restart those (good). However, if the next execution succeeds, the previously restartable jobs should NOT be marked as restartable anymore. 

Right now you can restart those jobs, resulting in a:

{code}
Caused by: org.springframework.batch.core.repository.JobInstanceAlreadyCompleteException: A job instance already exists and is complete for parameters={random=0.5735953106895085, throwError=true}.  If you want to run this job again, change the parameters.
	at org.springframework.batch.core.repository.support.SimpleJobRepository.createJobExecution(SimpleJobRepository.java:126)
	at sun.reflect.GeneratedMethodAccessor211.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.transaction.interceptor.TransactionInterceptor$1.proceedWithInvocation(TransactionInterceptor.java:98)
	at org.springframework.transaction.interceptor.TransactionAspectSupport.invokeWithinTransaction(TransactionAspectSupport.java:262)
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:95)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.batch.core.repository.support.AbstractJobRepositoryFactoryBean$1.invoke(AbstractJobRepositoryFactoryBean.java:172)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy40.createJobExecution(Unknown Source)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:125)
	at sun.reflect.GeneratedMethodAccessor209.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy42.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)
	at sun.reflect.GeneratedMethodAccessor208.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:95)
	at org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:84)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:114)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:159)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	... 98 more
{code}



",4.0,False,4.0,0,0.0,0,0,False
"XD-1642","Story","Sprint 26","2014-04-28T13:34:35.000+0000","iperumal","","Fail fast admin server if admin's embedded tomcat couldn't start","During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn't running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.",1.0,0.3372668224768077,0.3372668224768077,0.0042728302481969,True,True,True,1398692075000.0,1398715597000.0,1398719197000.0,1398760918000.0,1398761818000.0,1398692075000.0,-1.0,1398692373000.0,1398695973000.0,1398715597000.0,1398719197000.0,1398715597000.0,1398719197000.0,"iperumal",1398695973000.0,1398692075000.0,44,1398692075000.0,-1.0,"Fail fast admin server if admin's embedded tomcat couldn't start","During admin server startup, if it fails due to embedded tomcat failure, then the admin server instance still up and running. Since its tomcat isn't running it can not handle any REST client requests. In this scenario, we need fail fast the admin server process itself with better error message.",1.0,False,1.0,0,0.0,0,0,False
"XD-1641","Bug","Sprint 26","2014-04-25T17:16:58.000+0000","iperumal","","Upon a container departure, redeployment of batch job fails on an existing container","When there are multiple containers (A, B and C) and a batch job is deployed into one of the containers A. When the container A goes down, the admin server tries re-deploy the job module that was deployed in container A into other matching container. But, when the re-deployment happens, it tries to update the distributed job locator as if a new job is being deployed and following exception is thrown:

17:13:38,811 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache:550 - 
java.lang.RuntimeException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:411)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:355)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$8(ContainerRegistrar.java:349)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:695)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:253)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:167)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:103)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1514)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:252)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:699)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:648)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:311)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:241)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:186)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:176)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:166)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployModule(ContainerRegistrar.java:230)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:399)
	... 20 more
Caused by: org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists
	at org.springframework.xd.dirt.plugins.job.DistributedJobLocator.addJob(DistributedJobLocator.java:114)
	at org.springframework.xd.dirt.plugins.job.BatchJobRegistryBeanPostProcessor.postProcessAfterInitialization(BatchJobRegistryBeanPostProcessor.java:106)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:421)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.postProcessObjectFromFactoryBean(AbstractAutowireCapableBeanFactory.java:1698)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:164)
	... 36 more",2.0,0.6659430588090764,0.2985259340125475,0.1718545394643161,True,True,True,1398446218000.0,1399400272000.0,1399403872000.0,1399877954000.0,1399878854000.0,1398446218000.0,-1.0,1398692423000.0,1398696023000.0,1398873897000.0,1398877497000.0,1399400272000.0,1399403872000.0,"iperumal",1398696023000.0,1398446218000.0,44,1398446218000.0,-1.0,"Upon a container departure, redeployment of batch job fails on an existing container","When there are multiple containers (A, B and C) and a batch job is deployed into one of the containers A. When the container A goes down, the admin server tries re-deploy the job module that was deployed in container A into other matching container. But, when the re-deployment happens, it tries to update the distributed job locator as if a new job is being deployed and following exception is thrown:

17:13:38,811 ERROR DeploymentsPathChildrenCache-0 cache.PathChildrenCache:550 - 
java.lang.RuntimeException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:411)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:355)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$8(ContainerRegistrar.java:349)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:695)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:253)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'job': Post-processing of the FactoryBean's object failed; nested exception is org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:167)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:103)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getObjectForBeanInstance(AbstractBeanFactory.java:1514)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:252)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:699)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:648)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:311)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:241)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:186)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:176)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:166)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployModule(ContainerRegistrar.java:230)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployJob(ContainerRegistrar.java:399)
	... 20 more
Caused by: org.springframework.xd.dirt.job.BatchJobAlreadyExistsException: Batch Job with the name myjob3 already exists
	at org.springframework.xd.dirt.plugins.job.DistributedJobLocator.addJob(DistributedJobLocator.java:114)
	at org.springframework.xd.dirt.plugins.job.BatchJobRegistryBeanPostProcessor.postProcessAfterInitialization(BatchJobRegistryBeanPostProcessor.java:106)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyBeanPostProcessorsAfterInitialization(AbstractAutowireCapableBeanFactory.java:421)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.postProcessObjectFromFactoryBean(AbstractAutowireCapableBeanFactory.java:1698)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:164)
	... 36 more",2.0,False,2.0,0,0.0,0,0,False
"XD-1637","Improvement","Sprint 26","2014-04-25T11:39:55.000+0000","iperumal","iperumal","Re-enable JSHint during grunt build","JSHint should be enabled in grunt build. There are few minor issues and needs to be fixed. 
",1.0,0.001772656769333,0.001772656769333,0.7133835586084645,True,True,True,1398425995000.0,1398426011000.0,1398429611000.0,1398434121000.0,1398435021000.0,1398425995000.0,-1.0,1398432434000.0,1398435021000.0,1398426011000.0,1398429611000.0,1398426011000.0,1398429611000.0,"iperumal",1398435021000.0,1398425995000.0,44,1398425995000.0,-1.0,"Re-enable JSHint during grunt build","JSHint should be enabled in grunt build. There are few minor issues and needs to be fixed. 
",1.0,False,1.0,0,0.0,0,0,False
"XD-1636","Improvement","Sprint 27","2014-04-25T11:30:49.000+0000","dbeauregard","","servers.yaml's 'xd: -> transport: rabbit' overrides xd-singlenode's default of local transport","When working w/ SXD xd-singlenode, out of the box, it defaults to using all embedded components (transport, analytics, hsqldb, & zookeeper), which is easy and a great way to get going.  This is also great for development.

When I then started trying out the M6 distributed mode I set my transport to rabbit in servers.yaml (now that the --transport option is gone).  Rabbit is my preferred transport here.

I then went back to running the singlenode, for simplicity, and then got an exception saying that the singlenode couldn't contact RabbitMQ/AMQP (I was no longer running rabbit).  I then had to add the '--transport local' flag back to xd-singlenode.  

Having the --transport option on xd-singlenode but not on xd-container is confusing.  Also I would expect xd-singlenode to default to local transport unless I specify another option in --transport.

-Derek",1.0,0.9656998612222492,0.8702737284826837,0.854111339185966,True,True,True,1398425449000.0,1398925077000.0,1398928677000.0,1398941923000.0,1398942823000.0,1398425449000.0,-1.0,1398867344000.0,1398870944000.0,1398875706000.0,1398879306000.0,1398925077000.0,1398928677000.0,"dbeauregard",1398870944000.0,1398425449000.0,44,1398425449000.0,-1.0,"servers.yaml's 'xd: -> transport: rabbit' overrides xd-singlenode's default of local transport","When working w/ SXD xd-singlenode, out of the box, it defaults to using all embedded components (transport, analytics, hsqldb, & zookeeper), which is easy and a great way to get going.  This is also great for development.

When I then started trying out the M6 distributed mode I set my transport to rabbit in servers.yaml (now that the --transport option is gone).  Rabbit is my preferred transport here.

I then went back to running the singlenode, for simplicity, and then got an exception saying that the singlenode couldn't contact RabbitMQ/AMQP (I was no longer running rabbit).  I then had to add the '--transport local' flag back to xd-singlenode.  

Having the --transport option on xd-singlenode but not on xd-container is confusing.  Also I would expect xd-singlenode to default to local transport unless I specify another option in --transport.

-Derek",1.0,False,1.0,0,0.0,0,0,False
"XD-1635","Bug","Sprint 27","2014-04-25T10:59:52.000+0000","dbeauregard","hillert","Documentation: Hovering over some of the examples corrupts the text","If you mouse over any of the examples in the documentation, the grey boxes, containing code, shell commands, etc., typically in the upper right hand corner a label for the type of code/example will appear.  E.g., 'Ruby', 'Javascript' ,etc.  

1) The labels that appear seem to be random and incorrect.  Shell scripts show as 'Ruby' and 'Javascript'.

2) More importantly, on some of the examples the label appears in front of and part of the example, corrupting the example.  To see this hover your mouse over the two examples, grey boxes, here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#_xd_shell_in_distributed_mode

There may be more but this is the ones I noticed.  

-Derek",2.0,0.9608972788949032,0.255371182247912,0.2717137328859389,True,True,True,1398423592000.0,1399992891000.0,1399996491000.0,1400055852000.0,1400056752000.0,1398423592000.0,-1.0,1398867344000.0,1398870944000.0,1398840654000.0,1398844254000.0,1399992891000.0,1399996491000.0,"dbeauregard",1398870944000.0,1398840648000.0,44,1398840648000.0,-1.0,"Documentation: Hovering over some of the examples corrupts the text","If you mouse over any of the examples in the documentation, the grey boxes, containing code, shell commands, etc., typically in the upper right hand corner a label for the type of code/example will appear.  E.g., 'Ruby', 'Javascript' ,etc.  

1) The labels that appear seem to be random and incorrect.  Shell scripts show as 'Ruby' and 'Javascript'.

2) More importantly, on some of the examples the label appears in front of and part of the example, corrupting the example.  To see this hover your mouse over the two examples, grey boxes, here: http://docs.spring.io/spring-xd/docs/1.0.0.M6/reference/html/#_xd_shell_in_distributed_mode

There may be more but this is the ones I noticed.  

-Derek",2.0,False,2.0,0,0.0,0,0,False
"XD-1634","Story","Sprint 26","2014-04-25T10:52:23.000+0000","grussell","grussell","Update Spring Integration Version to 4.0.0.RELEASE","",1.0,0.8088356075219825,0.0,0.997344347270382,True,True,True,1398423143000.0,1398764872000.0,1398768472000.0,1398844738000.0,1398845638000.0,1398423143000.0,-1.0,1398844516000.0,1398845638000.0,1398423143000.0,1398426743000.0,1398764872000.0,1398768472000.0,"grussell",1398845638000.0,1398423143000.0,44,1398423143000.0,-1.0,"Update Spring Integration Version to 4.0.0.RELEASE","",1.0,False,1.0,0,0.0,0,0,False
"XD-1632","Improvement","Sprint 26","2014-04-24T18:29:08.000+0000","iperumal","iperumal","Use unique queue names in shell tests","There seems to be some cross talk among the shell integration tests. 
It looks like the same singlenode application might get shared among the test classes when they run in parallel.

Using unique queue names across the tests seem to fix the issue for now.",1.0,0.7887071841184606,0.0,0.7850459685949068,True,True,True,1398364148000.0,1398373842000.0,1398376439000.0,1398375539000.0,1398376439000.0,1398364148000.0,-1.0,1398373797000.0,1398376439000.0,1398364148000.0,1398367748000.0,1398373842000.0,1398376439000.0,"iperumal",1398376439000.0,1398364148000.0,44,1398364148000.0,-1.0,"Use unique queue names in shell tests","There seems to be some cross talk among the shell integration tests. 
It looks like the same singlenode application might get shared among the test classes when they run in parallel.

Using unique queue names across the tests seem to fix the issue for now.",1.0,False,1.0,0,0.0,0,0,False
"XD-1631","Story","Sprint 29","2014-04-24T11:00:15.000+0000","grenfro","","Update hdfs sink docs","Options that are not covered:
--codec
--idleTimeout
--inUsePrefix
--inUseSuffix
--inputType
--overwrite

Options Renamed:
--filename is now --fileName",1.0,0.999137089348062,0.9991363652272351,0.8318301791836986,True,True,True,1398337215000.0,1402476595000.0,1402480170000.0,1402479270000.0,1402480170000.0,1398337215000.0,-1.0,1401783450000.0,1401787050000.0,1402476592000.0,1402480170000.0,1402476595000.0,1402480170000.0,"grenfro",1401787050000.0,1398337215000.0,44,1398337215000.0,-1.0,"Update hdfs sink docs","Options that are not covered:
--codec
--idleTimeout
--inUsePrefix
--inUseSuffix
--inputType
--overwrite

Options Renamed:
--filename is now --fileName",1.0,False,1.0,0,0.0,0,0,False
"XD-1630","Story","Sprint 27","2014-04-24T09:12:34.000+0000","mark.pollack","","Packaging of lib directory for shell contains many jars that are not used","Between M5 and M6 the size of the shell/lib directory went up ~50 MB.  Investigate and remove jars from being packaged that are not used.",2.0,0.9945789291954076,0.9403164585098084,0.3267684096872866,True,True,True,1398330754000.0,1399963963000.0,1399967563000.0,1399971965000.0,1399972865000.0,1398330754000.0,-1.0,1398867344000.0,1398870944000.0,1399874858000.0,1399878458000.0,1399963963000.0,1399967563000.0,"mark.pollack",1398870944000.0,1398330754000.0,44,1398330754000.0,-1.0,"Packaging of lib directory for shell contains many jars that are not used","Between M5 and M6 the size of the shell/lib directory went up ~50 MB.  Investigate and remove jars from being packaged that are not used.",2.0,False,2.0,0,0.0,0,0,False
"XD-1629","Story","Sprint 27","2014-04-23T14:30:54.000+0000","mark.pollack","grussell","RabbitMessageBus should prefix all created queues with a prefix in order to support HA","To configure Rabbit HA a naming convention should be used to identify the queue that need to be mirrored.  ",3.0,0.7977026129793082,0.0,0.4711570494890837,True,True,True,1398263454000.0,1399285883000.0,1399289483000.0,1399544271000.0,1399545171000.0,1398263454000.0,-1.0,1398867344000.0,1398870944000.0,1398263454000.0,1398267054000.0,1399285883000.0,1399289483000.0,"mark.pollack",1398870944000.0,1398263454000.0,44,1398263454000.0,-1.0,"RabbitMessageBus should prefix all created queues with a prefix in order to support HA","To configure Rabbit HA a naming convention should be used to identify the queue that need to be mirrored.  ",3.0,False,3.0,0,0.0,0,0,False
"XD-1628","Story","Sprint 27","2014-04-23T14:15:17.000+0000","hillert","hillert","UI: The user can view the job properties that were specified when the job definition was created as well as job parameters when it was launched/executed","",6.0,0.8289424658259514,0.2002048292042104,0.200139310331135,True,True,True,1398262517000.0,1400767606000.0,1400771206000.0,1401283647000.0,1401284547000.0,1398262517000.0,-1.0,1398867344000.0,1398870944000.0,1398867542000.0,1398871142000.0,1400767606000.0,1400771206000.0,"hillert",1398870944000.0,1398262517000.0,44,1398262517000.0,-1.0,"UI: The user can view the job properties that were specified when the job definition was created as well as job parameters when it was launched/executed","",6.0,False,6.0,0,0.0,0,0,False
"XD-1625","Story","Sprint 27","2014-04-23T14:00:53.000+0000","hillert","hillert","UI: The user should be able to view metrics about an executed job.","",4.0,-1.0,0.3744040277712554,0.3742612910321782,False,True,True,1398261653000.0,-1.0,-1.0,1399879117000.0,1399880017000.0,1398261653000.0,-1.0,1398867344000.0,1398870944000.0,1398867575000.0,1398871175000.0,-1.0,-1.0,"hillert",1398870944000.0,1398261653000.0,44,1398261653000.0,-1.0,"UI: The user should be able to view metrics about an executed job.","",4.0,False,4.0,0,0.0,0,0,False
"XD-1623","Story","Sprint 27","2014-04-23T13:52:18.000+0000","hillert","","UI: For Hadoop Steps - provide a link to the MapReduce Job details in Hadoop. ","",6.0,0.5217810326992363,0.5217810326992363,0.4761461541422095,True,True,True,1398261138000.0,1398925443000.0,1398929043000.0,1399533387000.0,1399534287000.0,1398261138000.0,-1.0,1398867343000.0,1398870943000.0,1398925443000.0,1398929043000.0,1398925443000.0,1398929043000.0,"hillert",1398870943000.0,1398261138000.0,44,1398261138000.0,-1.0,"UI: For Hadoop Steps - provide a link to the MapReduce Job details in Hadoop. ","",6.0,False,6.0,0,0.0,0,0,False
"XD-1622","Story","Sprint 27","2014-04-23T13:16:49.000+0000","hillert","","Add support for typed Batch Steps","This may require additional support (Jiras) for Spring Batch",8.0,0.957313216202829,0.3968360810350059,0.1410032558914669,True,True,True,1398259009000.0,1402389177000.0,1402392777000.0,1402572442000.0,1402573342000.0,1398259009000.0,-1.0,1398867344000.0,1398870944000.0,1399971092000.0,1399974692000.0,1402389177000.0,1402392777000.0,"hillert",1398870944000.0,1398259009000.0,44,1398259009000.0,-1.0,"Add support for typed Batch Steps","This may require additional support (Jiras) for Spring Batch",8.0,False,8.0,0,0.0,0,0,False
"XD-1620","Story","Sprint 26","2014-04-23T13:09:25.000+0000","iperumal","iperumal","Fix JobCommandTests' verification of shell result table rows using specific index","Some of the tests in JobCommandTests use the verification of shell command results table row on a specific row (mostly first row) like this:

String id = jobExecutions.getRows().get(0).getValue(1);
		displayJobExecution(id);

It is possible that the list of table rows may have the intended row in different order. This poses inconsistent test failures. ",1.0,4.98051373999228e-05,0.0,0.5810889893292494,True,True,True,1398258565000.0,1398258573000.0,1398262173000.0,1398418291000.0,1398419191000.0,1398258565000.0,-1.0,1398351903000.0,1398355503000.0,1398258565000.0,1398262165000.0,1398258573000.0,1398262173000.0,"iperumal",1398355503000.0,1398258565000.0,44,1398258565000.0,-1.0,"Fix JobCommandTests' verification of shell result table rows using specific index","Some of the tests in JobCommandTests use the verification of shell command results table row on a specific row (mostly first row) like this:

String id = jobExecutions.getRows().get(0).getValue(1);
		displayJobExecution(id);

It is possible that the list of table rows may have the intended row in different order. This poses inconsistent test failures. ",1.0,False,1.0,0,0.0,0,0,False
"XD-1618","Story","Sprint 27","2014-04-23T12:58:49.000+0000","hillert","","UI: The user can stop a specific job execution","",3.0,0.804449613598588,0.804449613598588,0.4351042684076149,True,True,True,1398257929000.0,1399384654000.0,1399388254000.0,1399657645000.0,1399658545000.0,1398257929000.0,-1.0,1398867343000.0,1398870943000.0,1399384654000.0,1399388254000.0,1399384654000.0,1399388254000.0,"hillert",1398870943000.0,1398257929000.0,44,1398257929000.0,-1.0,"UI: The user can stop a specific job execution","",3.0,False,3.0,0,0.0,0,2,False
"XD-1617","Story","Sprint 27","2014-04-23T12:57:12.000+0000","hillert","hillert","UI: The user can view progress information about a given step","",6.0,0.9411533943271082,0.4349376355651672,0.4347443404105988,True,True,True,1398257832000.0,1399577330000.0,1399580930000.0,1399658933000.0,1399659833000.0,1398257832000.0,-1.0,1398867344000.0,1398870944000.0,1398867615000.0,1398871215000.0,1399577330000.0,1399580930000.0,"hillert",1398870944000.0,1398257832000.0,44,1398257832000.0,-1.0,"UI: The user can view progress information about a given step","",6.0,False,6.0,0,0.0,0,2,False
"XD-1616","Story","Sprint 33","2014-04-23T12:52:58.000+0000","hillert","","UI: The user should provide username/password to gain access to the UI","Secure Admin UI to challenge users to enter username and password to gain access.",3.0,0.955853351269834,0.955853351269834,0.6452255286731804,True,True,True,1398257578000.0,1411547633000.0,1411551233000.0,1412160542000.0,1412161442000.0,1398257578000.0,1411372641000.0,1407228706000.0,1407232306000.0,1411547633000.0,1411551233000.0,1411547633000.0,1411551233000.0,"hillert",1407232306000.0,1398257578000.0,44,1411372641000.0,1411372641000.0,"UI: The user should provide username/password to gain access to the UI","Secure Admin UI to challenge users to enter username and password to gain access.",8.0,True,8.0,1,-62.5,0,1,False
"XD-1615","Story","Sprint 27","2014-04-23T12:48:56.000+0000","hillert","hillert","UI: The user can create a new job definition by selecting a job template and providing additional configuration properties","",6.0,0.3529945107636902,0.3223791657354145,0.3223099492606254,True,True,True,1398257336000.0,1398925418000.0,1398929018000.0,1400149049000.0,1400149949000.0,1398257336000.0,-1.0,1398867344000.0,1398870944000.0,1398867475000.0,1398871075000.0,1398925418000.0,1398929018000.0,"hillert",1398870944000.0,1398257336000.0,44,1398257336000.0,-1.0,"UI: The user can create a new job definition by selecting a job template and providing additional configuration properties","",6.0,False,6.0,0,0.0,0,2,False
"XD-1614","Bug","Sprint 26","2014-04-23T10:55:40.000+0000","iperumal","iperumal","Job display command handling null date value for execution endtime","If the job display command is executed for JobExecution and StepExecution list, it may be possible that the Job/Step execution's endTime still null (because the status is still unknown and not completed). In this case, the display command throws assertion failure here:

Caused by: java.lang.IllegalArgumentException: The provided date must not be null.
	at org.springframework.util.Assert.notNull(Assert.java:112)
	at org.springframework.xd.shell.util.CommonUtils.getUtcTime(CommonUtils.java:144)
	at org.springframework.xd.shell.command.JobCommands.display(JobCommands.java:249)",1.0,5.84744028301611e-05,0.0,0.9995146624565096,True,True,True,1398250540000.0,1398250550000.0,1398254150000.0,1398420655000.0,1398421555000.0,1398250540000.0,-1.0,1398421472000.0,1398421555000.0,1398250540000.0,1398254140000.0,1398250550000.0,1398254150000.0,"iperumal",1398421555000.0,1398250540000.0,44,1398250540000.0,-1.0,"Job display command handling null date value for execution endtime","If the job display command is executed for JobExecution and StepExecution list, it may be possible that the Job/Step execution's endTime still null (because the status is still unknown and not completed). In this case, the display command throws assertion failure here:

Caused by: java.lang.IllegalArgumentException: The provided date must not be null.
	at org.springframework.util.Assert.notNull(Assert.java:112)
	at org.springframework.xd.shell.util.CommonUtils.getUtcTime(CommonUtils.java:144)
	at org.springframework.xd.shell.command.JobCommands.display(JobCommands.java:249)",1.0,False,1.0,0,0.0,0,0,False
"XD-1613","Bug","Sprint 26","2014-04-23T09:47:57.000+0000","mark.fisher","","Parser fails on + after literal within an expression","This fails:
{code}
xd:>stream create s --definition ""http | transform --expression='hi'+payload | log""

Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD115E:(pos 34): unexpected data in stream definition '+'
http | transform --expression='hi'+payload | log
{code}

But this works:
{code}
xd:>stream create s --definition ""http | transform --expression=payload+'hi' | log""

Created new stream 's'
{code}
",2.0,-1.0,0.0087915698888731,0.0,False,True,True,1398246477000.0,-1.0,-1.0,1399535904000.0,1399536804000.0,1398246477000.0,-1.0,1398246477000.0,1398250077000.0,1398257821000.0,1398261421000.0,-1.0,-1.0,"mark.fisher",1398250077000.0,1398246477000.0,44,1398246477000.0,-1.0,"Parser fails on + after literal within an expression","This fails:
{code}
xd:>stream create s --definition ""http | transform --expression='hi'+payload | log""

Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD115E:(pos 34): unexpected data in stream definition '+'
http | transform --expression='hi'+payload | log
{code}

But this works:
{code}
xd:>stream create s --definition ""http | transform --expression=payload+'hi' | log""

Created new stream 's'
{code}
",2.0,False,2.0,0,0.0,0,0,False
"XD-1612","Improvement","","2014-04-22T15:10:04.000+0000","iperumal","iperumal","Simplify/Refactor UI controllers","The UI controllers in spring-xd/spring-xd-ui/app/scripts/controllers.js definitions look overly complicated to get the modularization work. 

We can possibly refactor and make it look clean; especially we will follow this as the example for subsequent controllers definitions.",3.0,0.0003655961423303,0.0,-1.0,True,False,True,1398179404000.0,1398179462000.0,1398183062000.0,1398337149000.0,1398338049000.0,1398179404000.0,-1.0,-1.0,-1.0,1398179404000.0,1398183004000.0,1398179462000.0,1398183062000.0,"iperumal",-1.0,-1.0,0,1398179404000.0,-1.0,"Simplify/Refactor UI controllers","The UI controllers in spring-xd/spring-xd-ui/app/scripts/controllers.js definitions look overly complicated to get the modularization work. 

We can possibly refactor and make it look clean; especially we will follow this as the example for subsequent controllers definitions.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1611","Improvement","","2014-04-22T11:40:59.000+0000","grenfro","grenfro","XD-EC2 will have to support the --hadoopDistro command line for xd-container","",3.0,0.9190787691116702,0.0,-1.0,True,False,True,1398166859000.0,1398404303000.0,1398407903000.0,1398424309000.0,1398425209000.0,1398166859000.0,-1.0,-1.0,-1.0,1398166859000.0,1398170459000.0,1398404303000.0,1398407903000.0,"grenfro",-1.0,-1.0,0,1398166859000.0,-1.0,"XD-EC2 will have to support the --hadoopDistro command line for xd-container","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1600","Improvement","Sprint 26","2014-04-21T16:22:49.000+0000","iperumal","iperumal","Validate existence of batch job at the admin side","Since the batch job repository is not intended to be deleted, it is possible to have a batch job that already exists in the batch job repo even if the batch job definition is destroyed in XD. When a new job definition is created, we need to add a validation for the same job definition name against the batch job repository. Currently, we will only see a failure when the job is actually deployed into the container (when the batch job repository is updated during the deployment).",1.0,3.5938616842433124e-05,0.0,0.2473175815706773,True,True,True,1398097369000.0,1398097378000.0,1398100978000.0,1398346896000.0,1398347796000.0,1398097369000.0,-1.0,1398159304000.0,1398162904000.0,1398097369000.0,1398100969000.0,1398097378000.0,1398100978000.0,"iperumal",1398162904000.0,1398097369000.0,44,1398097369000.0,-1.0,"Validate existence of batch job at the admin side","Since the batch job repository is not intended to be deleted, it is possible to have a batch job that already exists in the batch job repo even if the batch job definition is destroyed in XD. When a new job definition is created, we need to add a validation for the same job definition name against the batch job repository. Currently, we will only see a failure when the job is actually deployed into the container (when the batch job repository is updated during the deployment).",1.0,False,1.0,0,0.0,0,0,False
"XD-1599","Story","Sprint 26","2014-04-21T13:03:59.000+0000","thomas.risberg","mark.fisher","Change SpringSource references in pom.xml to Spring/spring.io","This is currently in the M6 pom:

  <organization>
    <name>SpringSource</name>
    <url>http://springsource.org</url>
  </organization>
",3.0,0.9995663616050852,0.9993742104600332,0.9993404541777944,True,True,True,1398085439000.0,1398855332000.0,1398855666000.0,1398854766000.0,1398855666000.0,1398085439000.0,-1.0,1398855158000.0,1398855666000.0,1398855184000.0,1398855666000.0,1398855332000.0,1398855666000.0,"thomas.risberg",1398855666000.0,1398085439000.0,44,1398085439000.0,-1.0,"Change SpringSource references in pom.xml to Spring/spring.io","This is currently in the M6 pom:

  <organization>
    <name>SpringSource</name>
    <url>http://springsource.org</url>
  </organization>
",3.0,False,3.0,0,0.0,0,0,False
"XD-1598","Story","Sprint 26","2014-04-21T12:52:58.000+0000","iperumal","iperumal","Use MessageBus Binding to start() underlying endpoint","The messagebus implementations, upon registration of consumer and producer from/to messagebus the corresponding endpoints start. Instead of directly calling the start() on adapter/consumer we can call the corresponding Binding's start() which calls the underlying endpoint to start.
 This is in-line with the way the corresponding endpoints are stopped (using Binding's stop()) during undeploy/destroy.",1.0,0.0001178766486359,0.0,0.9784940603266492,True,True,True,1398084778000.0,1398084787000.0,1398088387000.0,1398160229000.0,1398161129000.0,1398084778000.0,-1.0,1398159487000.0,1398161129000.0,1398084778000.0,1398088378000.0,1398084787000.0,1398088387000.0,"iperumal",1398161129000.0,1398084778000.0,44,1398084778000.0,-1.0,"Use MessageBus Binding to start() underlying endpoint","The messagebus implementations, upon registration of consumer and producer from/to messagebus the corresponding endpoints start. Instead of directly calling the start() on adapter/consumer we can call the corresponding Binding's start() which calls the underlying endpoint to start.
 This is in-line with the way the corresponding endpoints are stopped (using Binding's stop()) during undeploy/destroy.",1.0,False,1.0,0,0.0,0,0,False
"XD-1597","Story","Sprint 26","2014-04-19T21:19:15.000+0000","grussell","grussell","Rabbit Source - Add ErrorHandler and Reject (don't requeue) MessageConversionException","If the rabbit source receives a message it can't convert, a {{MessageConversionException}} is thrown and the message is rejected (and requeued), causing an endless loop.

Add an {{ErrorHandler}} to the inbound adapter to detect and convert {{MCE}} to {{AmqpRejectAndDontRequeueException}}.

Also consider adding a retry interceptor to do the same for exceptions in modules (when using local transport).",3.0,-1.0,0.0,0.2488910009313258,False,True,True,1397942355000.0,-1.0,-1.0,1398412826000.0,1398413726000.0,1397942355000.0,-1.0,1398059675000.0,1398063275000.0,1397942355000.0,1397945955000.0,-1.0,-1.0,"grussell",1398063275000.0,1397942355000.0,44,1397942355000.0,-1.0,"Update to Spring AMQP 1.3.2","If the rabbit source receives a message it can't convert, a {{MessageConversionException}} is thrown and the message is rejected (and requeued), causing an endless loop.

Add an {{ErrorHandler}} to the inbound adapter to detect and convert {{MCE}} to {{AmqpRejectAndDontRequeueException}}.

Also consider adding a retry interceptor to do the same for exceptions in modules (when using local transport).",3.0,False,3.0,0,0.0,0,1,True
"XD-1596","Story","Sprint 26","2014-04-19T12:54:07.000+0000","grussell","grussell","Rabbit Source Should Expose More Container Options","acknowlege-more, tx-size, prefetch-count, concurrency etc.",3.0,0.9881207045651086,0.0,0.142962144490271,True,True,True,1397912047000.0,1398932417000.0,1398936017000.0,1398943784000.0,1398944684000.0,1397912047000.0,-1.0,1398059675000.0,1398063275000.0,1397912047000.0,1397915647000.0,1398932417000.0,1398936017000.0,"grussell",1398063275000.0,1397912047000.0,44,1397912047000.0,-1.0,"Rabbit Source Should Expose More Container Options","acknowlege-more, tx-size, prefetch-count, concurrency etc.",3.0,False,3.0,0,0.0,0,1,False
"XD-1595","Improvement","Sprint 26","2014-04-18T14:11:44.000+0000","iperumal","iperumal","Remove aliasHint flag usage when binding producer/consumer to MessageBus ","The MessageBus interface uses the aliasHint flag when binding consumer/producer on a point-to-point channel. 

Actually, the aliasHint is only needed when computing Source/Sink channel names in case named channel names. Otherwise, indexed channel names will be used for the input/output channel name. 

The only place where aliasHint is used in the message bus is on the LocalMessageBus where it provides a way to choose the channel provider (direct/queue channel) based on the alias hint. Otherwise, it is not needed in message bus bindproducer/consumer.

We need to simplify this.",3.0,0.0002003343762862,0.0,0.0002549710243643,True,True,True,1397830304000.0,1397830359000.0,1397833959000.0,1398103945000.0,1398104845000.0,1397830304000.0,1397840359000.0,1397830374000.0,1397833974000.0,1397830304000.0,1397833904000.0,1397830359000.0,1397833959000.0,"iperumal",1397833974000.0,1397830304000.0,44,1397840359000.0,1397840359000.0,"Remove aliasHint flag usage when binding producer/consumer to MessageBus ","The MessageBus interface uses the aliasHint flag when binding consumer/producer on a point-to-point channel. 

Actually, the aliasHint is only needed when computing Source/Sink channel names in case named channel names. Otherwise, indexed channel names will be used for the input/output channel name. 

The only place where aliasHint is used in the message bus is on the LocalMessageBus where it provides a way to choose the channel provider (direct/queue channel) based on the alias hint. Otherwise, it is not needed in message bus bindproducer/consumer.

We need to simplify this.",2.0,True,2.0,1,50.0,0,0,False
"XD-1592","Improvement","Sprint 26","2014-04-17T14:48:46.000+0000","dturanski","dturanski","Make transport configuration extensible. ","Change message-bus.xml to read 

<import resource=""classpath*:/META-INF/spring-xd/transports/${XD_TRANSPORT}-bus.xml""/>

So new transports may be configured in external jars",1.0,0.2004949943142545,0.0008918816473054,0.2014760641262904,True,True,True,1397746126000.0,1397755118000.0,1397758718000.0,1397790075000.0,1397790975000.0,1397746126000.0,-1.0,1397755162000.0,1397758762000.0,1397746166000.0,1397749766000.0,1397755118000.0,1397758718000.0,"dturanski",1397758762000.0,1397746126000.0,44,1397746126000.0,-1.0,"Make transport configuration extensible. ","Change message-bus.xml to read 

<import resource=""classpath*:/META-INF/spring-xd/transports/${XD_TRANSPORT}-bus.xml""/>

So new transports may be configured in external jars",1.0,False,1.0,0,0.0,0,0,False
"XD-1591","Improvement","Sprint 26","2014-04-17T14:12:59.000+0000","pperalta","pperalta","Flatten out ephemeral nodes ","Flatten out ephemeral nodes written by containers when deploying modules. For instance, instead of {{.../streams/moduleType/moduleLabel/container}} use {{.../streams/moduleType.moduleLabel.container}}.

This change allows us to derive state for a stream/job without having to traverse multiple layers of znodes. This is a big deal because:
* each level of children requires a network call
* Curator can only cache one level of children
",5.0,0.9621323892846846,0.0,8.667812881732028e-06,True,True,True,1397743979000.0,1402406004000.0,1402409604000.0,1402588592000.0,1402589492000.0,1397743979000.0,1398853782000.0,1397744021000.0,1397747621000.0,1397743979000.0,1397747579000.0,1402406004000.0,1402409604000.0,"pperalta",1397747621000.0,1397743979000.0,44,1398853782000.0,1398853782000.0,"Flatten out ephemeral nodes ","Flatten out ephemeral nodes written by containers when deploying modules. For instance, instead of {{.../streams/moduleType/moduleLabel/container}} use {{.../streams/moduleType.moduleLabel.container}}.

This change allows us to derive state for a stream/job without having to traverse multiple layers of znodes. This is a big deal because:
* each level of children requires a network call
* Curator can only cache one level of children
",2.0,True,2.0,1,150.0,0,0,False
"XD-1590","Improvement","Sprint 26","2014-04-17T14:01:00.000+0000","pperalta","pperalta","Move ephemeral nodes from /xd/streams to /xd/deployments/streams","To have a clear separation of definition vs runtime information, move the ephemeral nodes written by containers from {{/xd/streams/stream-name}} to {{/xd/deployments/streams/stream-name}}. Same for jobs.",2.0,0.002140968676928,0.0,0.0008073722314877,True,True,True,1397743260000.0,1397745278000.0,1397748878000.0,1398684924000.0,1398685824000.0,1397743260000.0,-1.0,1397744021000.0,1397747621000.0,1397743260000.0,1397746860000.0,1397745278000.0,1397748878000.0,"pperalta",1397747621000.0,1397743260000.0,44,1397743260000.0,-1.0,"Move ephemeral nodes from /xd/streams to /xd/deployments/streams","To have a clear separation of definition vs runtime information, move the ephemeral nodes written by containers from {{/xd/streams/stream-name}} to {{/xd/deployments/streams/stream-name}}. Same for jobs.",2.0,False,2.0,0,0.0,0,0,False
"XD-1589","Story","Sprint 26","2014-04-17T12:59:55.000+0000","dturanski","dturanski","Support Groovy bean definitions as XD extensions","Modify the PluginContextExtensionsInitializer to consume .groovy bean definitions as well as XML. ",3.0,0.2518709836658057,0.2518572714442005,0.5994562124689149,True,True,True,1397739595000.0,1397996752000.0,1398000352000.0,1398759682000.0,1398760582000.0,1397739595000.0,-1.0,1398351632000.0,1398355232000.0,1397996738000.0,1398000338000.0,1397996752000.0,1398000352000.0,"dturanski",1398355232000.0,1397739595000.0,44,1397739595000.0,-1.0,"Support Groovy bean definitions as XD extensions","Modify the PluginContextExtensionsInitializer to consume .groovy bean definitions as well as XML. ",3.0,False,3.0,0,0.0,0,0,False
"XD-1587","Improvement","Sprint 26","2014-04-16T14:48:49.000+0000","dturanski","iperumal","Provide module configuration templates for twitter sources","Provide module templates including required property keys but not values for  $XD_MODULE_CONFIG/source/twitter*/twitter*.properties. Also look for any other packaged modules that have required properties that should be statically configured and we cannot provide defaults.  The Source modules document should be more clear regarding the configuration of these properties.",2.0,0.00015218080529,0.0001431224240228,0.0620798043389646,True,True,True,1397659729000.0,1397659897000.0,1397663497000.0,1398762779000.0,1398763679000.0,1397659729000.0,1398075449000.0,1397728262000.0,1397731862000.0,1397659887000.0,1397663487000.0,1397659897000.0,1397663497000.0,"dturanski",1397731862000.0,1397659729000.0,44,1398075449000.0,1398075449000.0,"Provide module configuration templates for twitter sources","Provide module templates including required property keys but not values for  $XD_MODULE_CONFIG/source/twitter*/twitter*.properties. Also look for any other packaged modules that have required properties that should be statically configured and we cannot provide defaults.  The Source modules document should be more clear regarding the configuration of these properties.",1.0,True,1.0,1,100.0,0,0,False
"XD-1585","Bug","Sprint 29","2014-04-16T11:51:09.000+0000","dturanski","eric.bottard","Tab completion does not work for stream definition following > ",">stream create ""tap:stream:foo > 

does not suggest modules",8.0,0.99999653126918,0.123647194980216,0.999994490839286,True,True,True,1397649069000.0,1402549980000.0,1402549997000.0,1402549097000.0,1402549997000.0,1397649069000.0,-1.0,1402549970000.0,1402549997000.0,1398255055000.0,1398258655000.0,1402549980000.0,1402549997000.0,"dturanski",1402549997000.0,1402319363000.0,44,1402319363000.0,-1.0,"Tab completion does not work for stream definition following > ",">stream create ""tap:stream:foo > 

does not suggest modules",8.0,False,8.0,0,0.0,0,0,False
"XD-1582","Bug","Sprint 25","2014-04-15T15:00:39.000+0000","thomas.risberg","mark.fisher","Temporary race condition between deployment and ""runtime modules"" command","The ""runtime modules"" command can show a failure between the deployment command and the actual deployment on the container node, especially if there is a network hop. This clears up once the module is fully deployed.

{code}
xd:>stream create --name trois3 --definition ""time | jdbc"" --deploy 
Created and deployed new stream 'trois3'
xd:>runtime modules 
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/bc95653e-9da5-4738-beb2-f215e4003318/trois3.source.time-0/metadata

xd:>runtime modules 
  Module                Container Id                          Options
  --------------------  ------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  trois3.source.time-0  bc95653e-9da5-4738-beb2-f215e4003318  {format=yyyy-MM-dd HH:mm:ss, fixedDelay=1}
  wintu.sink.jdbc-1     bc95653e-9da5-4738-beb2-f215e4003318  {tableName=${xd.stream.name}, url=jdbc:hsqldb:hsql://carbon:9102/xdjob, columns=payload, driverClassName=org.hsqldb.jdbc.JDBCDriver, initializeDatabase=false, initializerScript=init_db.sql, username=sa}
  trois3.sink.jdbc-1    d0ad8eda-be27-46ac-86be-e43b5d9921af  {tableName=${xd.stream.name}, url=jdbc:hsqldb:hsql://carbon:9102/xdjob, columns=payload, driverClassName=org.hsqldb.jdbc.JDBCDriver, initializeDatabase=false, initializerScript=init_db.sql, username=sa}
  wintu.source.time-0   befa5f27-aac3-4d94-9171-77c07036ec75  {format=yyyy-MM-dd HH:mm:ss, fixedDelay=1}
{code}

 ",3.0,0.831904332129964,0.0,0.8298736462093863,True,True,True,1397574039000.0,1397577726000.0,1397578471000.0,1397577571000.0,1397578471000.0,1397574039000.0,-1.0,1397577717000.0,1397578471000.0,1397574039000.0,1397577639000.0,1397577726000.0,1397578471000.0,"thomas.risberg",1397578471000.0,1397574039000.0,44,1397574039000.0,-1.0,"Temporary race condition between deployment and ""runtime modules"" command","The ""runtime modules"" command can show a failure between the deployment command and the actual deployment on the container node, especially if there is a network hop. This clears up once the module is fully deployed.

{code}
xd:>stream create --name trois3 --definition ""time | jdbc"" --deploy 
Created and deployed new stream 'trois3'
xd:>runtime modules 
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/bc95653e-9da5-4738-beb2-f215e4003318/trois3.source.time-0/metadata

xd:>runtime modules 
  Module                Container Id                          Options
  --------------------  ------------------------------------  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  trois3.source.time-0  bc95653e-9da5-4738-beb2-f215e4003318  {format=yyyy-MM-dd HH:mm:ss, fixedDelay=1}
  wintu.sink.jdbc-1     bc95653e-9da5-4738-beb2-f215e4003318  {tableName=${xd.stream.name}, url=jdbc:hsqldb:hsql://carbon:9102/xdjob, columns=payload, driverClassName=org.hsqldb.jdbc.JDBCDriver, initializeDatabase=false, initializerScript=init_db.sql, username=sa}
  trois3.sink.jdbc-1    d0ad8eda-be27-46ac-86be-e43b5d9921af  {tableName=${xd.stream.name}, url=jdbc:hsqldb:hsql://carbon:9102/xdjob, columns=payload, driverClassName=org.hsqldb.jdbc.JDBCDriver, initializeDatabase=false, initializerScript=init_db.sql, username=sa}
  wintu.source.time-0   befa5f27-aac3-4d94-9171-77c07036ec75  {format=yyyy-MM-dd HH:mm:ss, fixedDelay=1}
{code}

 ",3.0,False,3.0,0,0.0,0,0,False
"XD-1581","Bug","Sprint 25","2014-04-15T12:44:01.000+0000","iperumal","iperumal","XD config home should use XD_CONFIG_LOCATION if this is set","If XD_CONFIG_LOCATION is set, then XD runtime's xd.config.home should use that. otherwise, they point to two different paths.",2.0,0.1522988505747126,0.0,0.9850574712643678,True,True,True,1397565841000.0,1397566106000.0,1397567581000.0,1397566681000.0,1397567581000.0,1397565841000.0,-1.0,1397567555000.0,1397567581000.0,1397565841000.0,1397567581000.0,1397566106000.0,1397567581000.0,"iperumal",1397567581000.0,1397565841000.0,44,1397565841000.0,-1.0,"XD config home should use XD_CONFIG_LOCATION if this is set","If XD_CONFIG_LOCATION is set, then XD runtime's xd.config.home should use that. otherwise, they point to two different paths.",2.0,False,2.0,0,0.0,0,0,False
"XD-1580","Bug","Sprint 26","2014-04-15T09:48:21.000+0000","pperalta","pperalta","Undeploy modules when container disconnected from ZK","Consider a module running in a container when it is disconnected from ZK:

{noformat}
12:30:13,021  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:13
12:30:14,025  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:14
12:30:15,029  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:15
12:30:16,031  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:16
12:30:32,590  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:32
12:37:42,985  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:42
12:37:43,398  INFO main-SendThread(fe80:0:0:0:0:0:0:1%1:2181) zookeeper.ClientCnxn:1096 - Client session timed out, have not heard from server in 430809ms for sessionid 0x145662be03e0002, closing socket connection and attempting reconnect
12:37:43,985  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:975 - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
12:37:43,986  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:852 - Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session
12:37:43,988  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:43
12:37:43,989  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:1094 - Unable to reconnect to ZooKeeper service, session 0x145662be03e0002 has expired, closing socket connection
{noformat}

Currently the module for the disconnected container continues to execute:

{noformat}
12:37:45,994  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:45
12:37:46,997  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:46
12:37:48,000  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:47
12:37:48,094 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /xd
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)
	at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:609)
	at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
12:37:48,097  INFO main-EventThread state.ConnectionStateManager:194 - State change: SUSPENDED
12:37:48,097  INFO ConnectionStateManager-0 zookeeper.ZooKeeperConnection:262 - >>> Curator disconnected event: SUSPENDED
12:37:48,097  WARN ConnectionStateManager-0 server.ContainerRegistrar:325 - >>> disconnected container: 88ba115b-6190-497a-a67c-df1e295bf158
12:37:49,001  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:49
12:37:50,004  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:50
12:37:51,008  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:51
12:37:52,012  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:52
12:37:53,016  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:53
12:37:54,021  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:54
12:37:55,023  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:55
...
{noformat}

This container should not continue executing the module because the leader admin will likely select another container to execute this module. If and when this container reconnects to ZK, it can be (re)assigned modules for deployment.

This can be done via a simple undeployment; or we may even consider closing and reopening the application context.",4.0,0.9182951786149784,0.0,0.9169909552640788,True,True,True,1397555301000.0,1397728508000.0,1397732108000.0,1397743019000.0,1397743919000.0,1397555301000.0,-1.0,1397728262000.0,1397731862000.0,1397555301000.0,1397558901000.0,1397728508000.0,1397732108000.0,"pperalta",1397731862000.0,1397555301000.0,44,1397555301000.0,-1.0,"Undeploy modules when container disconnected from ZK","Consider a module running in a container when it is disconnected from ZK:

{noformat}
12:30:13,021  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:13
12:30:14,025  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:14
12:30:15,029  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:15
12:30:16,031  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:16
12:30:32,590  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:30:32
12:37:42,985  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:42
12:37:43,398  INFO main-SendThread(fe80:0:0:0:0:0:0:1%1:2181) zookeeper.ClientCnxn:1096 - Client session timed out, have not heard from server in 430809ms for sessionid 0x145662be03e0002, closing socket connection and attempting reconnect
12:37:43,985  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:975 - Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
12:37:43,986  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:852 - Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session
12:37:43,988  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:43
12:37:43,989  INFO main-SendThread(localhost:2181) zookeeper.ClientCnxn:1094 - Unable to reconnect to ZooKeeper service, session 0x145662be03e0002 has expired, closing socket connection
{noformat}

Currently the module for the disconnected container continues to execute:

{noformat}
12:37:45,994  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:45
12:37:46,997  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:46
12:37:48,000  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:47
12:37:48,094 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception
org.apache.zookeeper.KeeperException$SessionExpiredException: KeeperErrorCode = Session expired for /xd
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:127)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:302)
	at org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:291)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:287)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:279)
	at org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:41)
	at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:609)
	at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
12:37:48,097  INFO main-EventThread state.ConnectionStateManager:194 - State change: SUSPENDED
12:37:48,097  INFO ConnectionStateManager-0 zookeeper.ZooKeeperConnection:262 - >>> Curator disconnected event: SUSPENDED
12:37:48,097  WARN ConnectionStateManager-0 server.ContainerRegistrar:325 - >>> disconnected container: 88ba115b-6190-497a-a67c-df1e295bf158
12:37:49,001  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:49
12:37:50,004  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:50
12:37:51,008  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:51
12:37:52,012  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:52
12:37:53,016  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:53
12:37:54,021  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:54
12:37:55,023  WARN inbound.t.0-redis:queue-inbound-channel-adapter1 logger.t:150 - 2014-04-15 12:37:55
...
{noformat}

This container should not continue executing the module because the leader admin will likely select another container to execute this module. If and when this container reconnects to ZK, it can be (re)assigned modules for deployment.

This can be done via a simple undeployment; or we may even consider closing and reopening the application context.",4.0,False,4.0,0,0.0,0,0,False
"XD-1579","Story","Sprint 25","2014-04-15T08:57:40.000+0000","mark.fisher","mark.fisher","Release 1.0.0.M6","",4.0,0.1164448875211304,0.0,0.0,True,True,True,1397552260000.0,1397557633000.0,1397561233000.0,1397597502000.0,1397598402000.0,1397552260000.0,-1.0,1397552260000.0,1397555860000.0,1397552260000.0,1397555860000.0,1397557633000.0,1397561233000.0,"mark.fisher",1397555860000.0,1397552260000.0,44,1397552260000.0,-1.0,"Release 1.0.0.M6","",4.0,False,4.0,0,0.0,0,0,False
"XD-1577","Bug","Sprint 25","2014-04-15T00:44:34.000+0000","iperumal","iperumal","Changing externalized module config properties at runtime","Once the container starts up and the module is deployed, the externalized module configuration properties could not be changed for the subsequent modules of same type.

Here is the scenario for a stream ""http | transform | log"", with XD_MODULE_CONFIG_LOCATION and XD_MODULE_CONFIG_NAME using their default values. 
In, ${xd.config.home}/modules/modules.yml, I have:
processor:
  transform:
    expression: ""'Inside modules.yml'""

and, in, ${xd.config.home}/modules/processor/transform/transform.properties, I have:
expression: ""'First module: inside transform.properties'""

Now, I deploy this stream: ""http | transform | log"".

Lets say, I have another stream that uses the transform module, but this time I want to change the expression in ${xd.config.home}/modules/processor/transform/transform.properties to,
expression: ""'Second module'""

Now, when the stream containing this transform module gets deployed, it uses the same transform.properties that is used by the previously deployed transform module.

What I understand from this behavior is that, the EnvironmentAwareModuleOptionsMetadataResolver caches the module environments for a given module type and name. When the same module is deployed from a given stream again, it uses the stored module environment and doesn't refresh/load the property sources from the module config locations mentioned above. Though this is good in one way that same module environment is re-used, changing the externalized module config properties would have no affect after the first module of same type/name is deployed.

Though the EnvironmentAwareModuleOptionsMetadataResolver is used by both admin and container, this JIRA focuses more on the container side.
There is one valid point with the current behavior where the module environment is cached and won't change. But is this by design?",4.0,0.0508618961715774,0.0506614552014431,0.6598516736821006,True,True,True,1397522674000.0,1397523689000.0,1397527289000.0,1397541730000.0,1397542630000.0,1397522674000.0,-1.0,1397535842000.0,1397539442000.0,1397523685000.0,1397527285000.0,1397523689000.0,1397527289000.0,"iperumal",1397539442000.0,1397522674000.0,44,1397522674000.0,-1.0,"Changing externalized module config properties at runtime","Once the container starts up and the module is deployed, the externalized module configuration properties could not be changed for the subsequent modules of same type.

Here is the scenario for a stream ""http | transform | log"", with XD_MODULE_CONFIG_LOCATION and XD_MODULE_CONFIG_NAME using their default values. 
In, ${xd.config.home}/modules/modules.yml, I have:
processor:
  transform:
    expression: ""'Inside modules.yml'""

and, in, ${xd.config.home}/modules/processor/transform/transform.properties, I have:
expression: ""'First module: inside transform.properties'""

Now, I deploy this stream: ""http | transform | log"".

Lets say, I have another stream that uses the transform module, but this time I want to change the expression in ${xd.config.home}/modules/processor/transform/transform.properties to,
expression: ""'Second module'""

Now, when the stream containing this transform module gets deployed, it uses the same transform.properties that is used by the previously deployed transform module.

What I understand from this behavior is that, the EnvironmentAwareModuleOptionsMetadataResolver caches the module environments for a given module type and name. When the same module is deployed from a given stream again, it uses the stored module environment and doesn't refresh/load the property sources from the module config locations mentioned above. Though this is good in one way that same module environment is re-used, changing the externalized module config properties would have no affect after the first module of same type/name is deployed.

Though the EnvironmentAwareModuleOptionsMetadataResolver is used by both admin and container, this JIRA focuses more on the container side.
There is one valid point with the current behavior where the module environment is cached and won't change. But is this by design?",4.0,False,4.0,0,0.0,0,0,False
"XD-1576","Story","Sprint 25","2014-04-14T19:10:49.000+0000","thomas.risberg","eric.bottard","Remove unused .properties files in config and update docs","There are some properties files in the config directory that no longer are needed. We should clean that up and also remove/update any documentation references to these files",3.0,0.8280759455822992,0.0008471620072756,0.0014202421886679,True,True,True,1397502649000.0,1397535883000.0,1397539483000.0,1397541883000.0,1397542783000.0,1397502649000.0,-1.0,1397502706000.0,1397506306000.0,1397502683000.0,1397506283000.0,1397535883000.0,1397539483000.0,"thomas.risberg",1397506306000.0,1397502649000.0,44,1397502649000.0,-1.0,"Remove unused .properties files in config and update docs","There are some properties files in the config directory that no longer are needed. We should clean that up and also remove/update any documentation references to these files",3.0,False,3.0,0,0.0,0,0,False
"XD-1575","Story","Sprint 27","2014-04-14T14:42:10.000+0000","mark.fisher","jbrisbin","Add UDP support to reactor-syslog source module","Currently the reactor-syslog source module only supports TCP.

Once we add UDP support, we can probably remove the existing syslog-tcp and syslog-udp modules.",8.0,-1.0,1.516323840872878e-05,0.5658814021651465,False,True,True,1397486530000.0,-1.0,-1.0,1399925742000.0,1399926642000.0,1397486530000.0,-1.0,1398867344000.0,1398870944000.0,1397486567000.0,1397490167000.0,-1.0,-1.0,"mark.fisher",1398870944000.0,1397486530000.0,44,1397486530000.0,-1.0,"Add UDP support to reactor-syslog source module","Currently the reactor-syslog source module only supports TCP.

Once we add UDP support, we can probably remove the existing syslog-tcp and syslog-udp modules.",8.0,False,8.0,0,0.0,0,0,False
"XD-1574","Story","Sprint 25","2014-04-14T14:16:29.000+0000","grenfro","grenfro","JDBC Acceptance tests must jdbc props vs. configProps setting.","In this case we will use environment variables to set the JDBC sink settings.  Thus we will just remove code.",3.0,0.0012833832426816,0.0012833832426816,0.3468190429627819,True,True,True,1397484989000.0,1397485010000.0,1397488610000.0,1397500452000.0,1397501352000.0,1397484989000.0,-1.0,1397490664000.0,1397494264000.0,1397485010000.0,1397488610000.0,1397485010000.0,1397488610000.0,"grenfro",1397494264000.0,1397484989000.0,44,1397484989000.0,-1.0,"JDBC Acceptance tests must jdbc props vs. configProps setting.","In this case we will use environment variables to set the JDBC sink settings.  Thus we will just remove code.",3.0,False,3.0,0,0.0,0,3,False
"XD-1573","Story","Sprint 25","2014-04-14T11:59:02.000+0000","mark.fisher","mark.fisher","Update diagrams that show control transport","The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers. Rather we need to show ZooKeeper.

https://github.com/spring-projects/spring-xd/wiki/Architecture",4.0,0.676870217085741,0.0,0.0,True,True,True,1397476742000.0,1397485410000.0,1397489010000.0,1397488648000.0,1397489548000.0,1397476742000.0,-1.0,1397476742000.0,1397480342000.0,1397476742000.0,1397480342000.0,1397485410000.0,1397489010000.0,"mark.fisher",1397480342000.0,1397476742000.0,44,1397476742000.0,-1.0,"Update diagrams that show control transport","The first 2 images in the documentation section linked below should no longer show redis, rabbit, or local for the communication between Admins and Containers. Rather we need to show ZooKeeper.

https://github.com/spring-projects/spring-xd/wiki/Architecture",4.0,False,4.0,0,0.0,-1,0,False
"XD-1572","Story","","2014-04-14T11:40:03.000+0000","grenfro","","Unable to delete composed module","After creating a composed module, I am unable to delete it.

[Steps to reproduce]
xd:>module compose doo --definition ""filter --expression=payload.contains('doo') | file""
Successfully created module 'doo' with type sink
xd:>module 
module compose    module delete     module display    module info       module list       
xd:>module delete doo
java.lang.StringIndexOutOfBoundsException: Failed to convert 'doo' to type QualifiedModuleName for option 'name,'
String index out of range: -1
xd:>
",3.0,-1.0,-1.0,-1.0,False,False,False,1397475603000.0,-1.0,-1.0,1397477566000.0,1397478466000.0,1397475603000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1397475603000.0,-1.0,"Unable to delete composed module","After creating a composed module, I am unable to delete it.

[Steps to reproduce]
xd:>module compose doo --definition ""filter --expression=payload.contains('doo') | file""
Successfully created module 'doo' with type sink
xd:>module 
module compose    module delete     module display    module info       module list       
xd:>module delete --name doo --type sink
java.lang.StringIndexOutOfBoundsException: Failed to convert 'doo' to type QualifiedModuleName for option 'name,'
String index out of range: -1
xd:>
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1571","Bug","Sprint 25","2014-04-14T09:37:21.000+0000","eric.bottard","mark.fisher","JLine 1 is brought up (and shows in IDE) through ZK/curator","We don't want jline1 anymore.
This shows in IDE only (either run shell integration tests, or run the shell as Gunnar mentioned)",1.0,0.2348146554043148,0.0,0.0719050949588422,True,True,True,1397468241000.0,1397476970000.0,1397480570000.0,1397504515000.0,1397505415000.0,1397468241000.0,-1.0,1397470914000.0,1397474514000.0,1397468241000.0,1397471841000.0,1397476970000.0,1397480570000.0,"eric.bottard",1397474514000.0,1397468241000.0,44,1397468241000.0,-1.0,"JLine 1 is brought up (and shows in IDE) through ZK/curator","We don't want jline1 anymore.
This shows in IDE only (either run shell integration tests, or run the shell as Gunnar mentioned)",1.0,False,1.0,0,0.0,0,0,False
"XD-1569","Story","Sprint 25","2014-04-13T08:00:35.000+0000","mark.pollack","","Rename reactor-tcp module to reactor since it also supports udp","the --transport option allows 'udp' as well as 'tcp/",2.0,0.936696177789458,0.8727024851759991,0.0,True,True,True,1397376035000.0,1397490089000.0,1397493689000.0,1397496897000.0,1397497797000.0,1397376035000.0,-1.0,1397376035000.0,1397379635000.0,1397482297000.0,1397485897000.0,1397490089000.0,1397493689000.0,"mark.pollack",1397379635000.0,1397376035000.0,44,1397376035000.0,-1.0,"Rename reactor-tcp module to reactor-ip since it also supports udp","the --transport option allows 'udp' as well as 'tcp/",2.0,False,2.0,0,0.0,0,0,True
"XD-1568","Story","Sprint 25","2014-04-12T17:43:36.000+0000","mark.fisher","dturanski","Update documentation related to transport and controlTransport","e.g. Need to update this section (maybe others):
https://github.com/spring-projects/spring-xd/wiki/Running-Distributed-Mode

Remove all mentions of Control Bus, and replace any mentions of the --transport cmd line arg with the xd.transport property in yml.
",2.0,-1.0,0.0,0.0,False,True,True,1397324616000.0,-1.0,-1.0,1397475677000.0,1397476577000.0,1397324616000.0,-1.0,1397324616000.0,1397328216000.0,1397324616000.0,1397328216000.0,-1.0,-1.0,"mark.fisher",1397328216000.0,1397324616000.0,44,1397324616000.0,-1.0,"Update documentation related to transport and controlTransport","e.g. Need to update this section (maybe others):
https://github.com/spring-projects/spring-xd/wiki/Running-Distributed-Mode

Remove all mentions of Control Bus, and replace any mentions of the --transport cmd line arg with the xd.transport property in yml.
",2.0,False,2.0,0,0.0,0,0,False
"XD-1567","Story","Sprint 25","2014-04-12T11:47:20.000+0000","grenfro","grenfro","Update XD-Ec2 deployer to use XD_TRANSPORT ","XD-EC2 must use environment variable XD_TRANSPORT instead of --transport to declare data-transport for XD.",3.0,0.5677748849721336,0.0,0.2239031568573921,True,True,True,1397303240000.0,1397388509000.0,1397392109000.0,1397452521000.0,1397453421000.0,1397303240000.0,-1.0,1397336866000.0,1397340466000.0,1397303240000.0,1397306840000.0,1397388509000.0,1397392109000.0,"grenfro",1397340466000.0,1397303240000.0,44,1397303240000.0,-1.0,"Update XD-Ec2 deployer to use XD_TRANSPORT ","XD-EC2 must use environment variable XD_TRANSPORT instead of --transport to declare data-transport for XD.",3.0,False,3.0,0,0.0,0,0,False
"XD-1566","Improvement","Sprint 25","2014-04-12T09:12:08.000+0000","grenfro","Thomas Risberg","jdbchdfs writes empty file to hdfs","Similar to XD-1565  so I'd link these 2 together.

[Steps to reproduce on Hadoop12]
1) Created Table People with columns forename,surname and address (use the result from filejdbc)
2) job create myjob --definition ""jdbchdfs --sql='select col1,col2,col3 from some_table'""
3)job launch myjob
4) myjob is created on hdfs but with zero bytes
5) throws an exception, stack trace attached.",3.0,0.9999585293316112,0.9999585293316112,0.2087514959062525,True,True,True,1397293928000.0,1397462715000.0,1397462722000.0,1397461822000.0,1397462722000.0,1397293928000.0,-1.0,1397329164000.0,1397332764000.0,1397462715000.0,1397462722000.0,1397462715000.0,1397462722000.0,"grenfro",1397332764000.0,1397293928000.0,44,1397293928000.0,-1.0,"Document append configuration, else jdbchdfs writes empty file to hdfs","Similar to XD-1565  so I'd link these 2 together.

[Steps to reproduce on Hadoop12]
1) Created Table People with columns forename,surname and address (use the result from filejdbc)
2) job create myjob --definition ""jdbchdfs --sql='select col1,col2,col3 from some_table'""
3)job launch myjob
4) myjob is created on hdfs but with zero bytes
5) throws an exception, stack trace attached.",3.0,False,3.0,0,0.0,0,0,True
"XD-1565","Improvement","Sprint 25","2014-04-12T08:21:06.000+0000","grenfro","thomas.risberg","filepollhdfs writes empty file to hdfs","When testing in both singlenode and cluster (redis), XD throws exception (stacktrace attached).  The file is created on hdfs, but it is empty.

[Steps to recreate Using Hadoop12]
1) job create myjob --definition ""filepollhdfs --names=forename,surname,address"" --deploy 
2) stream create csvStream --definition ""file --ref=true --dir=/tmp/dug --pattern=*.csv > queue:job:myjob"" --deploy
3) use excel to create a 3 column spreadsheet and save as csv. 
4) Copy csv to /tmp/dug directory",3.0,-1.0,0.2246184861342272,0.2247063925549122,False,True,True,1397290866000.0,-1.0,-1.0,1397460602000.0,1397461502000.0,1397290866000.0,-1.0,1397329209000.0,1397332809000.0,1397329194000.0,1397332794000.0,-1.0,-1.0,"grenfro",1397332809000.0,1397290866000.0,44,1397290866000.0,-1.0,"Document append support, else filepollhdfs writes empty file to hdfs","When testing in both singlenode and cluster (redis), XD throws exception (stacktrace attached).  The file is created on hdfs, but it is empty.

[Steps to recreate Using Hadoop12]
1) job create myjob --definition ""filepollhdfs --names=forename,surname,address"" --deploy 
2) stream create csvStream --definition ""file --ref=true --dir=/tmp/dug --pattern=*.csv > queue:job:myjob"" --deploy
3) use excel to create a 3 column spreadsheet and save as csv. 
4) Copy csv to /tmp/dug directory",3.0,False,3.0,0,0.0,0,0,True
"XD-1562","Bug","Sprint 25","2014-04-12T06:52:36.000+0000","grenfro","eric.bottard","Gauge & Rich Gauge fail to write results to redis for singlenode","Steps to reproduce:
stream create --name test --definition ""http --port=9090 | log""  
stream create --name simplegauge --definition ""tap:stream:test > gauge"" 
http post --target http://localhost:9090 --data ""10""
redis-cli
get gauges.simplegauge

[the result]
redis 127.0.0.1:6379> get gauges.simplegauge
(nil)

Note:  It worked with admin/container but failed only on xd-singlenode.",3.0,-1.0,0.2445250956560882,0.2446038712581589,False,True,True,1397285556000.0,-1.0,-1.0,1397462376000.0,1397463276000.0,1397285556000.0,-1.0,1397329027000.0,1397332627000.0,1397329013000.0,1397332613000.0,-1.0,-1.0,"grenfro",1397332627000.0,1397285556000.0,44,1397285556000.0,-1.0,"Gauge & Rich Gauge fail to write results to redis for singlenode","Steps to reproduce:
stream create --name test --definition ""http --port=9090 | log""  
stream create --name simplegauge --definition ""tap:stream:test > gauge"" 
http post --target http://localhost:9090 --data ""10""
redis-cli
get gauges.simplegauge

[the result]
redis 127.0.0.1:6379> get gauges.simplegauge
(nil)

Note:  It worked with admin/container but failed only on xd-singlenode.",3.0,False,3.0,0,0.0,0,1,False
"XD-1561","Story","","2014-04-11T19:07:35.000+0000","grenfro","","When using transform --script=foo.groovy shell displays error","When trying to create the stream for the gemfire example:
stream create hashtags --definition ""tap:stream:tweets  > transform --script=tweetSummary.groovy | gemfire-server --keyExpression=payload['id']"" --deploy

The shell displays: 
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module transform of type processor:
    valid: the 'script' and 'expression' options are mutually exclusive
 I get the following error ",3.0,-1.0,-1.0,-1.0,False,False,False,1397243255000.0,-1.0,-1.0,1397245800000.0,1397246700000.0,1397243255000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1397243255000.0,-1.0,"When using transform --script=foo.groovy shell displays error","When trying to create the stream for the gemfire example:
stream create hashtags --definition ""tap:stream:tweets  > transform --script=tweetSummary.groovy | gemfire-server --keyExpression=payload['id']"" --deploy

The shell displays: 
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module transform of type processor:
    valid: the 'script' and 'expression' options are mutually exclusive
 I get the following error ",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1560","Bug","Sprint 25","2014-04-11T16:28:16.000+0000","grenfro","David Turanski","Payload Conversion Sample throws exception.","After updating the dependency to use the snapshot (even with M5) the conversion throws an exception.
Stacktrace attached. ",2.0,0.9386099603094128,0.9385437403515452,0.3948364987852776,True,True,True,1397233696000.0,1397460482000.0,1397464082000.0,1397474415000.0,1397475315000.0,1397233696000.0,-1.0,1397329096000.0,1397332696000.0,1397460466000.0,1397464066000.0,1397460482000.0,1397464082000.0,"grenfro",1397332696000.0,1397233696000.0,44,1397233696000.0,-1.0,"Payload Conversion Sample throws exception.","After updating the dependency to use the snapshot (even with M5) the conversion throws an exception.
Stacktrace attached. ",2.0,False,2.0,0,0.0,0,0,False
"XD-1558","Story","Sprint 25","2014-04-11T15:45:11.000+0000","mark.fisher","mark.pollack","shell cp command fails","(from the wordcount sample):

{code}
xd:>! cp /tmp/nietzsche-chapter-1.txt /tmp/xd/input/wordCountFiles
You cannot specify option '' more than once in a single command
{code}",4.0,0.3218189631107305,0.0,0.0,True,True,True,1397231111000.0,1397333713000.0,1397337313000.0,1397549030000.0,1397549930000.0,1397231111000.0,-1.0,1397231111000.0,1397234711000.0,1397231111000.0,1397234711000.0,1397333713000.0,1397337313000.0,"mark.fisher",1397234711000.0,1397231111000.0,44,1397231111000.0,-1.0,"shell cp command fails","(from the wordcount sample):

{code}
xd:>! cp /tmp/nietzsche-chapter-1.txt /tmp/xd/input/wordCountFiles
You cannot specify option '' more than once in a single command
{code}",4.0,False,4.0,0,0.0,0,0,False
"XD-1557","Bug","","2014-04-11T15:13:16.000+0000","grenfro","","Batch wordcount sample returns zero counts","Batch Wordcount Hadoop 1.2 returns zero as result, when it should return the list of words in the result.

Documents have to be updated to mention that hadoopDistro needs to be added both xd-singlenode and xd-shell",3.0,0.712925668253387,0.712925668253387,-1.0,True,False,True,1397229196000.0,1397231143000.0,1397231927000.0,1397231027000.0,1397231927000.0,1397229196000.0,-1.0,-1.0,-1.0,1397231143000.0,1397231927000.0,1397231143000.0,1397231927000.0,"grenfro",-1.0,-1.0,0,1397229196000.0,-1.0,"Batch wordcount sample returns zero counts","if user already has /count/in on their hdfs the input file will not copy the sample file to hdfs.  need to make sure that a file is already present

if the hdfs 
Documents have to be updated to mention that hadoopDistro needs to be added both xd-singlenode and xd-shell",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1556","Bug","Sprint 25","2014-04-11T13:14:10.000+0000","grenfro","","Batch hashtag count throws exception when launched","1) Update Instructions to mention --hadoopDistro for both singlenode and shell.  Else demo will not work.
2) Pom needs to be updated to use 1.2.1 at the least.
3) I can see where hdfs is writing the results
4) throws NPE   Stacktrace is attached.",3.0,0.9910827017078624,0.4232609685143773,0.0691892105836021,True,True,True,1397222050000.0,1397472452000.0,1397474705000.0,1397473805000.0,1397474705000.0,1397222050000.0,-1.0,1397239531000.0,1397243131000.0,1397328989000.0,1397332589000.0,1397472452000.0,1397474705000.0,"grenfro",1397243131000.0,1397222050000.0,44,1397222050000.0,-1.0,"Batch hashtag count throws exception when launched","1) Update Instructions to mention --hadoopDistro for both singlenode and shell.  Else demo will not work.
2) Pom needs to be updated to use 1.2.1 at the least.
3) I can see where hdfs is writing the results
4) throws NPE   Stacktrace is attached.",3.0,False,3.0,0,0.0,0,2,False
"XD-1555","Story","Sprint 25","2014-04-11T12:26:58.000+0000","iperumal","iperumal","transform processor with script option is broken","Creating the following stream throws exception:

stream create s1 --definition ""http | transform --script=transform.groovy | log""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module transform of type processor:
    valid: the 'script' and 'expression' options are mutually exclusive

The ExpressionOrScriptMixin's assertions to check if script and expression options are mutually exclusive `always` fails.",2.0,0.0329640392299309,0.0329640392299309,0.0553941155103523,True,True,True,1397219218000.0,1397219581000.0,1397223181000.0,1397229330000.0,1397230230000.0,1397219218000.0,-1.0,1397219828000.0,1397223428000.0,1397219581000.0,1397223181000.0,1397219581000.0,1397223181000.0,"iperumal",1397223428000.0,1397219218000.0,44,1397219218000.0,-1.0,"transform processor with script option is broken","Creating the following stream throws exception:

stream create s1 --definition ""http | transform --script=transform.groovy | log""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Error with option(s) for module transform of type processor:
    valid: the 'script' and 'expression' options are mutually exclusive

The ExpressionOrScriptMixin's assertions to check if script and expression options are mutually exclusive `always` fails.",2.0,False,2.0,0,0.0,0,0,False
"XD-1554","Bug","Sprint 25","2014-04-11T12:11:34.000+0000","grenfro","","JMS source can only connect to localhost","Can't connect to remote activemq instance.  
Setup a jms-activemq.properties file with amq.url=tcp://:ec2-54-198-157-91.compute-1.amazonaws.com:61616.  
Source always refers to defaults of tcp://localhost:61616.  Localhost works",3.0,-1.0,-1.0,0.027538726333907,False,True,False,1397218294000.0,-1.0,-1.0,1397220880000.0,1397221780000.0,1397218294000.0,-1.0,1397218390000.0,1397221780000.0,-1.0,-1.0,-1.0,-1.0,"grenfro",1397221780000.0,1397218294000.0,44,1397218294000.0,-1.0,"JMS source can only connect to localhost","Can't connect to remote activemq instance.  
Setup a jms-activemq.properties file with amq.url=tcp://:ec2-54-198-157-91.compute-1.amazonaws.com:61616.  
Source always refers to defaults of tcp://localhost:61616.  Localhost works",3.0,False,3.0,0,0.0,0,0,False
"XD-1552","Improvement","Sprint 25","2014-04-11T11:38:04.000+0000","dturanski","dturanski","Remove --transport option except for single node","Since transport is now shared by Admin and Container, a command line arg is not appropriate since it allows the user to set them to different values which would break XD. The recommend way to configure transport is in servers.yml.  The command line arg is still valid for single node",2.0,-1.0,9.105996739465684e-05,0.99989425294109,False,True,True,1397216284000.0,-1.0,-1.0,1397555819000.0,1397556719000.0,1397216284000.0,-1.0,1397556683000.0,1397556719000.0,1397216315000.0,1397219915000.0,-1.0,-1.0,"dturanski",1397556719000.0,1397216284000.0,44,1397216284000.0,-1.0,"Remove --transport option except for single node","Since transport is now shared by Admin and Container, a command line arg is not appropriate since it allows the user to set them to different values which would break XD. The recommend way to configure transport is in servers.yml.  The command line arg is still valid for single node",2.0,False,2.0,0,0.0,0,0,False
"XD-1551","Improvement","","2014-04-11T09:45:53.000+0000","dturanski","","Fix Startup Messages","Started container : AdminServerApplication
Documentation: https://github.com/SpringSource/spring-xd/wiki",1.0,-1.0,-1.0,-1.0,False,False,False,1397209553000.0,-1.0,-1.0,1397209553000.0,1397210009000.0,1397209553000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"dturanski",-1.0,-1.0,0,1397209553000.0,-1.0,"Fix Startup Messages","Started container : AdminServerApplication
Documentation: https://github.com/SpringSource/spring-xd/wiki",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1550","Improvement","Sprint 25","2014-04-11T09:24:46.000+0000","dturanski","grussell","Fix 'cannot find MessageBuilderFactory' warning","5:27:47,887 WARN DeploymentsPathChildrenCache-0 org.springframework.integration.context.IntegrationContextUtils:195 - No 'beanFactory' supplied; cannot find MessageBuilderFactory, using default.
a lot of those",1.0,0.545261053242607,0.1741752394466122,0.4449675900545003,True,True,True,1397208286000.0,1397225194000.0,1397228794000.0,1397238395000.0,1397239295000.0,1397208286000.0,-1.0,1397222084000.0,1397225684000.0,1397213687000.0,1397217287000.0,1397225194000.0,1397228794000.0,"dturanski",1397225684000.0,1397208286000.0,44,1397208286000.0,-1.0,"Fix 'cannot find MessageBuilderFactory' warning","5:27:47,887 WARN DeploymentsPathChildrenCache-0 org.springframework.integration.context.IntegrationContextUtils:195 - No 'beanFactory' supplied; cannot find MessageBuilderFactory, using default.
a lot of those",1.0,False,1.0,0,0.0,0,0,False
"XD-1549","Improvement","Sprint 26","2014-04-11T08:59:51.000+0000","pperalta","pperalta","Proactively handle failed deployments","When a deployment fails on a container due to a misconfiguration, the container does not notify the admin. Instead the admin waits for the container to write out an ephemeral node to the definition path {{/xd/streams}} or {{/xd/jobs}} to indicate a successful deployment and if the path isn't written in 10 seconds the deployment is considered failed.

This ""timeout"" should be considered a heuristic failure, meaning that the container was not able to write out a response of success or failure. If the deployment fails, the container needs to indicate this by writing a node to ZK.",10.0,0.9995158182764098,0.0,0.1997257047317049,True,True,True,1397206791000.0,1399894562000.0,1399895864000.0,1399894964000.0,1399895864000.0,1397206791000.0,1398853841000.0,1397743868000.0,1397747468000.0,1397206791000.0,1397210391000.0,1399894562000.0,1399895864000.0,"pperalta",1397747468000.0,1397206791000.0,44,1398853841000.0,1398853841000.0,"Proactively handle failed deployments","When a deployment fails on a container due to a misconfiguration, the container does not notify the admin. Instead the admin waits for the container to write out an ephemeral node to the definition path {{/xd/streams}} or {{/xd/jobs}} to indicate a successful deployment and if the path isn't written in 10 seconds the deployment is considered failed.

This ""timeout"" should be considered a heuristic failure, meaning that the container was not able to write out a response of success or failure. If the deployment fails, the container needs to indicate this by writing a node to ZK.",4.0,True,4.0,1,150.0,0,0,False
"XD-1548","Improvement","Sprint 26","2014-04-11T08:51:41.000+0000","pperalta","pperalta","Refactor duplicate code in Listeners","{{ContainerListener}}, {{StreamListener}}, and {{JobListener}} have duplicate code for deploying and undeploying modules. This needs to be consolidated. One possibility is for the deployment code to live in classes named {{StreamDeployer}} and {{JobDeployer}}.",10.0,0.5505791807255793,0.0,0.1998642908266175,True,True,True,1397206301000.0,1398687125000.0,1398690725000.0,1399894976000.0,1399895876000.0,1397206301000.0,1398853758000.0,1397743851000.0,1397747451000.0,1397206301000.0,1397209901000.0,1398687125000.0,1398690725000.0,"pperalta",1397747451000.0,1397206301000.0,44,1398853758000.0,1398853758000.0,"Refactor duplicate code in Listeners","{{ContainerListener}}, {{StreamListener}}, and {{JobListener}} have duplicate code for deploying and undeploying modules. This needs to be consolidated. One possibility is for the deployment code to live in classes named {{StreamDeployer}} and {{JobDeployer}}.",4.0,True,4.0,1,150.0,0,0,False
"XD-1547","Bug","Sprint 25","2014-04-11T07:41:53.000+0000","thomas.risberg","pperalta","clean up dead entries in ZooKeeper /xd/deployments/modules","When starting and stopping xd containers there are entries left in the /xd/deployments/modules directory that will cause 'runtime modules' command to fail.

xd:>runtime modules 
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b/test.sink.hdfs-1/metadata

here the ""/xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b"" container is no longer running, but there is some data left over.
",3.0,0.5494230501876824,0.081537606005839,0.9984012234116502,True,True,True,1397202113000.0,1397210017000.0,1397213617000.0,1397215599000.0,1397216499000.0,1397202113000.0,-1.0,1397216476000.0,1397216499000.0,1397203286000.0,1397206886000.0,1397210017000.0,1397213617000.0,"thomas.risberg",1397216499000.0,1397202113000.0,44,1397202113000.0,-1.0,"clean up dead entries in ZooKeeper /xd/deployments/modules","When starting and stopping xd containers there are entries left in the /xd/deployments/modules directory that will cause 'runtime modules' command to fail.

xd:>runtime modules 
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b/test.sink.hdfs-1/metadata

here the ""/xd/deployments/modules/5201ac3f-e952-48a2-a807-4f0bb2dab82b"" container is no longer running, but there is some data left over.
",3.0,False,3.0,0,0.0,0,1,False
"XD-1546","Bug","Sprint 25","2014-04-11T07:31:10.000+0000","grenfro","","Batch Notification Sample fails to execute","* payment_with_error.txt is not present in the project
* --makeUnique is not available for this command anymore
* Syntax for http post needs to be updated
* Need to add a job deploy command or --deploy
* Throws Exception when deployed (stacktrace attached).",3.0,0.9990746909467764,0.9987785920497446,0.0158042786290621,True,True,True,1397201470000.0,1397228463000.0,1397228488000.0,1397227588000.0,1397228488000.0,1397201470000.0,-1.0,1397201897000.0,1397205497000.0,1397228455000.0,1397228488000.0,1397228463000.0,1397228488000.0,"grenfro",1397205497000.0,1397201470000.0,44,1397201470000.0,-1.0,"Batch Notification Sample fails to execute","* payment_with_error.txt is not present in the project
* --makeUnique is not available for this command anymore
* Syntax for http post needs to be updated
* Need to add a job deploy command or --deploy
* Throws Exception when deployed (stacktrace attached).",3.0,False,3.0,0,0.0,0,0,False
"XD-1545","Story","Sprint 31","2014-04-11T06:46:53.000+0000","grenfro","grenfro","Support deploying to multiple containers in EC2 acceptance tests","need the ability to support the --group option.",5.0,0.9082086387797328,0.1370716010133805,0.910037525361412,True,True,True,1397198813000.0,1404204719000.0,1404208319000.0,1404911896000.0,1404912796000.0,1397198813000.0,-1.0,1404218827000.0,1404222427000.0,1398256181000.0,1398259781000.0,1404204719000.0,1404208319000.0,"grenfro",1404222427000.0,1397198813000.0,44,1397198813000.0,-1.0,"Support deploying to multiple containers in EC2 acceptance tests","need the ability to support the --group option.",5.0,False,5.0,0,0.0,0,2,False
"XD-1543","Story","","2014-04-11T06:27:36.000+0000","grenfro","","Update instructions to how to setup admin to use RDBMS.","Need to update instructions to discuss the setup of the relational database requirement for the xd-admin.",2.0,0.9740896425443836,2.547567868800255e-05,-1.0,True,False,True,1397197656000.0,1397809433000.0,1397813033000.0,1397824806000.0,1397825706000.0,1397197656000.0,-1.0,-1.0,-1.0,1397197672000.0,1397201272000.0,1397809433000.0,1397813033000.0,"grenfro",-1.0,-1.0,0,1397197656000.0,-1.0,"Update instructions to how to setup admin to use RDBMS.","Need to update instructions to discuss the setup of the relational database requirement for the xd-admin.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1541","Bug","Sprint 25","2014-04-10T19:29:08.000+0000","grenfro","","Batch Basic Fails to launch job when rabbit is data transport","Deployed the batch basic as instructions prescribe.
Tests work for both singlenode and redis as a data transport.  However, while the job does deploy using rabbit  it does not launch.",3.0,-1.0,0.9058729293067012,0.0854352716613266,False,True,True,1397158148000.0,-1.0,-1.0,1397215924000.0,1397216824000.0,1397158148000.0,-1.0,1397163161000.0,1397166761000.0,1397211301000.0,1397214901000.0,-1.0,-1.0,"grenfro",1397166761000.0,1397158148000.0,44,1397158148000.0,-1.0,"Batch Basic Fails to launch job when rabbit is data transport","Deployed the batch basic as instructions prescribe.
Tests work for both singlenode and redis as a data transport.  However, while the job does deploy using rabbit  it does not launch.",3.0,False,3.0,0,0.0,0,0,False
"XD-1540","Story","Sprint 25","2014-04-10T18:43:35.000+0000","mark.fisher","pperalta","stack trace on xd-admin restart when redploying streams","(the trace below occurs in the admin console but the Admin continues to handle subsequent deployment requests fine it seems)

To reproduce:

* start xd-admin and xd-container (just 1 of each)
* deploy 'time | log'
* kill both xd-admin and xd-container
* start xd-container by itself
* wait 10 seconds or so, then start xd-admin

result:
{code}
21:35:01,575 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:550 -
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/deployments/modules/f89e5fdc-7dc1-49fb-93cc-d536ab853f12/s.source.time-0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:411)
	at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:319)
	at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:255)
	at org.springframework.xd.dirt.server.ContainerListener.onChildAdded(ContainerListener.java:272)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:152)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}
",2.0,-1.0,0.0007160956263082,0.9997429400315816,False,True,True,1397155415000.0,-1.0,-1.0,1397208977000.0,1397209877000.0,1397155415000.0,-1.0,1397209863000.0,1397209877000.0,1397155454000.0,1397159054000.0,-1.0,-1.0,"mark.fisher",1397209877000.0,1397155415000.0,44,1397155415000.0,-1.0,"stack trace on xd-admin restart when redploying streams","(the trace below occurs in the admin console but the Admin continues to handle subsequent deployment requests fine it seems)

To reproduce:

* start xd-admin and xd-container (just 1 of each)
* deploy 'time | log'
* kill both xd-admin and xd-container
* start xd-container by itself
* wait 10 seconds or so, then start xd-admin

result:
{code}
21:35:01,575 ERROR ContainersPathChildrenCache-0 cache.PathChildrenCache:550 -
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /xd/deployments/modules/f89e5fdc-7dc1-49fb-93cc-d536ab853f12/s.source.time-0
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:119)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676)
	at org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656)
	at org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431)
	at org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:411)
	at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:319)
	at org.apache.curator.framework.imps.CreateBuilderImpl$4.forPath(CreateBuilderImpl.java:255)
	at org.springframework.xd.dirt.server.ContainerListener.onChildAdded(ContainerListener.java:272)
	at org.springframework.xd.dirt.server.ContainerListener.childEvent(ContainerListener.java:152)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}
",2.0,False,2.0,0,0.0,0,0,False
"XD-1539","Story","Sprint 25","2014-04-10T17:17:45.000+0000","mark.fisher","mark.fisher","Eliminate stack trace on xd-container shutdown when active module running","After deploying stream (such as ""time | log""), the xd-container emits the following stacktrace(s) if the stream is in a deployed state when that xd-container process is halted via CTRL-C:

{code}
20:15:31,415  INFO main-EventThread module.ModuleDeployer:215 - removed SimpleModule [name=log, type=sink, group=s, index=1 @128936ff]
20:15:31,417 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception
java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@7bf4bc83 has been closed already
	at org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:956)
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:978)
	at org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:214)
	at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.unbindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:140)
	at org.springframework.xd.dirt.plugins.stream.StreamPlugin.beforeShutdown(StreamPlugin.java:74)
	at org.springframework.xd.dirt.module.ModuleDeployer.beforeShutdown(ModuleDeployer.java:267)
	at org.springframework.xd.dirt.module.ModuleDeployer.destroyModule(ModuleDeployer.java:217)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleUndeploy(ModuleDeployer.java:197)
	at org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:169)
	at org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:252)
	at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:580)
	at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
20:15:31,420  INFO main-EventThread server.ContainerRegistrar:250 - Undeploying module time-0: time type=source, deploymentProperties={count=1}
20:15:31,420  INFO main-EventThread module.ModuleDeployer:215 - removed SimpleModule [name=time, type=source, group=s, index=0 @51a42578]
20:15:31,420 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception
java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@6d223732 has been closed already
	at org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:956)
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:978)
	at org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:214)
	at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.unbindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:144)
	at org.springframework.xd.dirt.plugins.stream.StreamPlugin.beforeShutdown(StreamPlugin.java:74)
	at org.springframework.xd.dirt.module.ModuleDeployer.beforeShutdown(ModuleDeployer.java:267)
	at org.springframework.xd.dirt.module.ModuleDeployer.destroyModule(ModuleDeployer.java:217)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleUndeploy(ModuleDeployer.java:197)
	at org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:169)
	at org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:252)
	at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:580)
	at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
{code}
",4.0,0.0001596400841738,0.0,0.0,True,True,True,1397150265000.0,1397150276000.0,1397153876000.0,1397218270000.0,1397219170000.0,1397150265000.0,-1.0,1397150265000.0,1397153865000.0,1397150265000.0,1397153865000.0,1397150276000.0,1397153876000.0,"mark.fisher",1397153865000.0,1397150265000.0,44,1397150265000.0,-1.0,"Eliminate stack trace on xd-container shutdown when active module running","After deploying stream (such as ""time | log""), the xd-container emits the following stacktrace(s) if the stream is in a deployed state when that xd-container process is halted via CTRL-C:

{code}
20:15:31,415  INFO main-EventThread module.ModuleDeployer:215 - removed SimpleModule [name=log, type=sink, group=s, index=1 @128936ff]
20:15:31,417 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception
java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@7bf4bc83 has been closed already
	at org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:956)
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:978)
	at org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:214)
	at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.unbindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:140)
	at org.springframework.xd.dirt.plugins.stream.StreamPlugin.beforeShutdown(StreamPlugin.java:74)
	at org.springframework.xd.dirt.module.ModuleDeployer.beforeShutdown(ModuleDeployer.java:267)
	at org.springframework.xd.dirt.module.ModuleDeployer.destroyModule(ModuleDeployer.java:217)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleUndeploy(ModuleDeployer.java:197)
	at org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:169)
	at org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:252)
	at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:580)
	at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
20:15:31,420  INFO main-EventThread server.ContainerRegistrar:250 - Undeploying module time-0: time type=source, deploymentProperties={count=1}
20:15:31,420  INFO main-EventThread module.ModuleDeployer:215 - removed SimpleModule [name=time, type=source, group=s, index=0 @51a42578]
20:15:31,420 ERROR main-EventThread imps.CuratorFrameworkImpl:512 - Watcher exception
java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@6d223732 has been closed already
	at org.springframework.context.support.AbstractApplicationContext.assertBeanFactoryActive(AbstractApplicationContext.java:956)
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:978)
	at org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:214)
	at org.springframework.xd.dirt.plugins.AbstractMessageBusBinderPlugin.unbindConsumerAndProducers(AbstractMessageBusBinderPlugin.java:144)
	at org.springframework.xd.dirt.plugins.stream.StreamPlugin.beforeShutdown(StreamPlugin.java:74)
	at org.springframework.xd.dirt.module.ModuleDeployer.beforeShutdown(ModuleDeployer.java:267)
	at org.springframework.xd.dirt.module.ModuleDeployer.destroyModule(ModuleDeployer.java:217)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleUndeploy(ModuleDeployer.java:197)
	at org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:169)
	at org.springframework.xd.dirt.server.ContainerRegistrar.undeployModule(ContainerRegistrar.java:252)
	at org.springframework.xd.dirt.server.ContainerRegistrar$StreamModuleWatcher.process(ContainerRegistrar.java:580)
	at org.apache.curator.framework.imps.NamespaceWatcher.process(NamespaceWatcher.java:56)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:522)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)
{code}
",4.0,False,4.0,0,0.0,0,0,False
"XD-1538","Story","Sprint 25","2014-04-10T17:06:10.000+0000","mark.fisher","mark.fisher","Update docs and samples now that deploy is false on job/stream creation","",4.0,0.4290572659462924,0.0,0.0,True,True,True,1397149570000.0,1397228547000.0,1397232147000.0,1397332741000.0,1397333641000.0,1397149570000.0,-1.0,1397149570000.0,1397153170000.0,1397149570000.0,1397153170000.0,1397228547000.0,1397232147000.0,"mark.fisher",1397153170000.0,1397149570000.0,44,1397149570000.0,-1.0,"Update docs and samples now that deploy is false on job/stream creation","",4.0,False,4.0,0,0.0,0,0,False
"XD-1537","Story","","2014-04-10T14:32:26.000+0000","iperumal","iperumal","Create documentation for Job listeners support","",1.0,7.739707677194654e-05,0.0,-1.0,True,False,True,1397140346000.0,1397140359000.0,1397143959000.0,1397307411000.0,1397308311000.0,1397140346000.0,-1.0,-1.0,-1.0,1397140346000.0,1397143946000.0,1397140359000.0,1397143959000.0,"iperumal",-1.0,-1.0,0,1397140346000.0,-1.0,"Create documentation for Job listeners support","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1536","Improvement","Sprint 25","2014-04-10T14:10:31.000+0000","mminella","mminella","Add ability to inject delimiter on pre-packaged jobs that deal with files.","",4.0,0.9789358983214754,9.048836084824204e-05,0.11303515921807,True,True,True,1397139031000.0,1399973442000.0,1399977042000.0,1400033531000.0,1400034431000.0,1397139031000.0,1398839156000.0,1397466313000.0,1397469913000.0,1397139293000.0,1397142893000.0,1399973442000.0,1399977042000.0,"mminella",1397469913000.0,1397139031000.0,44,1398839156000.0,1398839156000.0,"Add ability to inject delimiter on pre-packaged jobs that deal with files.","",1.0,True,1.0,1,300.0,0,0,False
"XD-1534","Bug","","2014-04-10T13:50:26.000+0000","grenfro","","Twitter Search results are not deserialized","Data is returned, but not human readable i.e. ""org.springframework.social.twitter.api.Tweet@4b20a104"". 
Needs to be readable in log and or file.  ",3.0,-1.0,-1.0,-1.0,False,False,False,1397137826000.0,-1.0,-1.0,1397139192000.0,1397140092000.0,1397137826000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1397137826000.0,-1.0,"Twitter Search results are not deserialized","Data is returned, but not human readable i.e. ""org.springframework.social.twitter.api.Tweet@4b20a104"". 
Needs to be readable in log and or file.  ",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1533","Bug","Sprint 25","2014-04-10T12:20:34.000+0000","pperalta","pperalta","Admin needs to clean up failed deployment attempts","If a container fails to deploy a module, the admin needs to clean up the {{/xd/deployments/modules/CONTAINER-ID/module}} path so that another attempt can be made to deploy that module to that container.",2.0,9.505251651537474e-05,0.0,0.9998217765315336,True,True,True,1397132434000.0,1397132442000.0,1397136042000.0,1397215698000.0,1397216598000.0,1397132434000.0,-1.0,1397216583000.0,1397216598000.0,1397132434000.0,1397136034000.0,1397132442000.0,1397136042000.0,"pperalta",1397216598000.0,1397132434000.0,44,1397132434000.0,-1.0,"Admin needs to clean up failed deployment attempts","If a container fails to deploy a module, the admin needs to clean up the {{/xd/deployments/modules/CONTAINER-ID/module}} path so that another attempt can be made to deploy that module to that container.",2.0,False,2.0,0,0.0,0,0,False
"XD-1532","Bug","","2014-04-10T12:19:15.000+0000","pperalta","","Clean up MBean registration for failed module deployments","When a module fails to deploy (for instance an http module configured with a port that is already bound) subsequent attempts to deploy the module fail due to a JMX exception:

{noformat}
java.lang.RuntimeException: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#beafb1fc-4423-4e4f-a88c-1655ea0fdcc5'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=output, sends=0]] with key 'org.springframework.integration:type=MessageChannel,name=output'; nested exception is javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:447)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:346)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$700(ContainerRegistrar.java:92)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:655)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#beafb1fc-4423-4e4f-a88c-1655ea0fdcc5'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=output, sends=0]] with key 'org.springframework.integration:type=MessageChannel,name=output'; nested exception is javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:176)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:346)
	at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:149)
	at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:112)
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:773)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:485)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:648)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:311)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:240)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:184)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:174)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:164)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployModule(ContainerRegistrar.java:227)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:429)
	... 18 more
Caused by: org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=output, sends=0]] with key 'org.springframework.integration:type=MessageChannel,name=output'; nested exception is javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output
	at org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:610)
	at org.springframework.integration.monitor.IntegrationMBeanExporter.registerChannels(IntegrationMBeanExporter.java:837)
	at org.springframework.integration.monitor.IntegrationMBeanExporter.doStart(IntegrationMBeanExporter.java:459)
	at org.springframework.integration.monitor.IntegrationMBeanExporter.start(IntegrationMBeanExporter.java:410)
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:173)
	... 33 more
Caused by: javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.springframework.jmx.support.MBeanRegistrationSupport.doRegister(MBeanRegistrationSupport.java:195)
	at org.springframework.jmx.export.MBeanExporter.registerBeanInstance(MBeanExporter.java:663)
	at org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:606)
	... 37 more
{noformat}",2.0,0.0137580269364096,0.0137239302153776,-1.0,True,False,True,1397132355000.0,1397134776000.0,1397138376000.0,1397307425000.0,1397308325000.0,1397132355000.0,-1.0,-1.0,-1.0,1397134770000.0,1397138370000.0,1397134776000.0,1397138376000.0,"pperalta",-1.0,-1.0,0,1397132355000.0,-1.0,"Clean up MBean registration for failed module deployments","When a module fails to deploy (for instance an http module configured with a port that is already bound) subsequent attempts to deploy the module fail due to a JMX exception:

{noformat}
java.lang.RuntimeException: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#beafb1fc-4423-4e4f-a88c-1655ea0fdcc5'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=output, sends=0]] with key 'org.springframework.integration:type=MessageChannel,name=output'; nested exception is javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:447)
	at org.springframework.xd.dirt.server.ContainerRegistrar.onChildAdded(ContainerRegistrar.java:346)
	at org.springframework.xd.dirt.server.ContainerRegistrar.access$700(ContainerRegistrar.java:92)
	at org.springframework.xd.dirt.server.ContainerRegistrar$DeploymentListener.childEvent(ContainerRegistrar.java:655)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:494)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$5.apply(PathChildrenCache.java:488)
	at org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)
	at com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:83)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache.callListeners(PathChildrenCache.java:485)
	at org.apache.curator.framework.recipes.cache.EventOperation.invoke(EventOperation.java:35)
	at org.apache.curator.framework.recipes.cache.PathChildrenCache$11.run(PathChildrenCache.java:755)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#beafb1fc-4423-4e4f-a88c-1655ea0fdcc5'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=output, sends=0]] with key 'org.springframework.integration:type=MessageChannel,name=output'; nested exception is javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:176)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:346)
	at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:149)
	at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:112)
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:773)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:485)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:648)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:311)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:240)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:184)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:174)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:164)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployModule(ContainerRegistrar.java:227)
	at org.springframework.xd.dirt.server.ContainerRegistrar.deployStreamModule(ContainerRegistrar.java:429)
	... 18 more
Caused by: org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=output, sends=0]] with key 'org.springframework.integration:type=MessageChannel,name=output'; nested exception is javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output
	at org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:610)
	at org.springframework.integration.monitor.IntegrationMBeanExporter.registerChannels(IntegrationMBeanExporter.java:837)
	at org.springframework.integration.monitor.IntegrationMBeanExporter.doStart(IntegrationMBeanExporter.java:459)
	at org.springframework.integration.monitor.IntegrationMBeanExporter.start(IntegrationMBeanExporter.java:410)
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:173)
	... 33 more
Caused by: javax.management.InstanceAlreadyExistsException: xd.foo:module=http.0,component=MessageChannel,name=output
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.springframework.jmx.support.MBeanRegistrationSupport.doRegister(MBeanRegistrationSupport.java:195)
	at org.springframework.jmx.export.MBeanExporter.registerBeanInstance(MBeanExporter.java:663)
	at org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:606)
	... 37 more
{noformat}",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1531","Story","Sprint 26","2014-04-10T11:53:23.000+0000","thomas.risberg","thomas.risberg","Rename xd-config.yml to servers.yml and add modules/modules.yml to spring-xd-yarn","Make changes to XD on YARN config that correspond to XD-1499 changes",3.0,0.6853747778006218,0.1190621139371091,0.2175969171371441,True,True,True,1397130803000.0,1399014278000.0,1399017878000.0,1399877998000.0,1399878898000.0,1397130803000.0,-1.0,1397728780000.0,1397732380000.0,1397457997000.0,1397461597000.0,1399014278000.0,1399017878000.0,"thomas.risberg",1397732380000.0,1397130803000.0,44,1397130803000.0,-1.0,"Rename xd-config.yml to servers.yml and add modules/modules.yml to spring-xd-yarn","Make changes to XD on YARN config that correspond to XD-1499 changes",3.0,False,3.0,0,0.0,0,0,False
"XD-1530","Bug","Sprint 25","2014-04-10T09:53:38.000+0000","thomas.risberg","thomas.risberg","Error when removing HDFS files in shell","I get this:

{code}
xd:>hadoop fs rm /xd/test/time-3.log
Error: run HDFS shell failed. Message is: org.apache.hadoop.fs.FileStatus.isDirectory()Z
{code}

so far I have seen this with --hadoopDistro hdp13 and hadoop12

same command works fine using shell from M5 release
",3.0,0.3191265707869463,0.1568636819723828,0.0374599837545988,True,True,True,1397123618000.0,1397130297000.0,1397133897000.0,1397143647000.0,1397144547000.0,1397123618000.0,-1.0,1397124402000.0,1397128002000.0,1397126901000.0,1397130501000.0,1397130297000.0,1397133897000.0,"thomas.risberg",1397128002000.0,1397123618000.0,44,1397123618000.0,-1.0,"Error when removing HDFS files in shell","I get this:

{code}
xd:>hadoop fs rm /xd/test/time-3.log
Error: run HDFS shell failed. Message is: org.apache.hadoop.fs.FileStatus.isDirectory()Z
{code}

so far I have seen this with --hadoopDistro hdp13 and hadoop12

same command works fine using shell from M5 release
",3.0,False,3.0,0,0.0,-1,2,False
"XD-1528","Story","","2014-04-10T09:34:38.000+0000","iperumal","","Support external datasource for single node application","Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts.

We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.",3.0,0.0005399276496949,0.0005399276496949,-1.0,True,False,True,1397122478000.0,1397122488000.0,1397126088000.0,1397140099000.0,1397140999000.0,1397122478000.0,-1.0,-1.0,-1.0,1397122488000.0,1397126088000.0,1397122488000.0,1397126088000.0,"iperumal",-1.0,-1.0,0,1397122478000.0,-1.0,"Support external datasource for single node application","Currently, single node application assumes to use hsqldb server by default and starts the hsqldb as it starts.

We need a way to provide external datasource and not to start hsqldb by default when the single node starts up.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1527","Improvement","","2014-04-09T22:42:31.000+0000","iperumal","iperumal","Modify module metadata to exclude default module properties","When the runtime modules command displays the the modules metadata properties, we can exclude the default module properties such as 'xd.stream.name', 'xd.module.index' and show only the other properties such as resolved module options etc.,",2.0,0.0023758060770618,0.0,-1.0,True,False,True,1397083351000.0,1397083484000.0,1397087084000.0,1397138432000.0,1397139332000.0,1397083351000.0,-1.0,-1.0,-1.0,1397083351000.0,1397086951000.0,1397083484000.0,1397087084000.0,"iperumal",-1.0,-1.0,0,1397083351000.0,-1.0,"Modify module metadata to exclude default module properties","When the runtime modules command displays the the modules metadata properties, we can exclude the default module properties such as 'xd.stream.name', 'xd.module.index' and show only the other properties such as resolved module options etc.,",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1526","Bug","Sprint 25","2014-04-09T14:04:44.000+0000","thomas.risberg","thomas.risberg","Exception when accessing CDH4 namenode","Get exception when accessing cdh4 from shell -

java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.
	at com.google.protobuf.GeneratedMessage.getUnknownFields

most likely due to protobuf-java-2.5.0.jar being on the main classpath now


Full stack trace:
{code}
trisberg@carbon:~/Test$ ./spring-xd-1.0.0.BUILD-SNAPSHOT/shell/bin/xd-shell --hadoopDistro cdh4
16:55:22,680  WARN main conf.Configuration:824 - fs.default.name is deprecated. Instead, use fs.defaultFS
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
eXtreme Data
1.0.0.BUILD-SNAPSHOT | Admin Server Target: http://localhost:9393
Welcome to the Spring XD shell. For assistance hit TAB or type ""help"".
xd:>hadoop config fs --namenode hdfs://cdh4:8020
xd:>hadoop fs ls /
Hadoop configuration changed, re-initializing shell...
16:55:28,853  WARN Spring Shell util.NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
-ls: Fatal internal error
java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.
	at com.google.protobuf.GeneratedMessage.getUnknownFields(GeneratedMessage.java:180)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto.getSerializedSize(ClientNamenodeProtocolProtos.java:30108)
	at com.google.protobuf.AbstractMessageLite.toByteString(AbstractMessageLite.java:49)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.constructRpcRequest(ProtobufRpcEngine.java:149)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:193)
	at com.sun.proxy.$Proxy43.getFileInfo(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at com.sun.proxy.$Proxy43.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:629)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1545)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:819)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1646)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1592)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1567)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:271)
	at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:224)
	at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:207)
	at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:254)
	at org.springframework.xd.shell.hadoop.FsShellCommands.run(FsShellCommands.java:412)
	at org.springframework.xd.shell.hadoop.FsShellCommands.runCommand(FsShellCommands.java:407)
	at org.springframework.xd.shell.hadoop.FsShellCommands.ls(FsShellCommands.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:196)
	at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)
	at org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:48)
	at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)
	at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:530)
	at org.springframework.shell.core.JLineShell.run(JLineShell.java:178)
	at java.lang.Thread.run(Thread.java:744)
{code}",3.0,0.9863859988708766,0.9863859988708766,0.9861117832083232,True,True,True,1397052284000.0,1397113435000.0,1397114279000.0,1397113379000.0,1397114279000.0,1397052284000.0,-1.0,1397113418000.0,1397114279000.0,1397113435000.0,1397114279000.0,1397113435000.0,1397114279000.0,"thomas.risberg",1397114279000.0,1397052284000.0,44,1397052284000.0,-1.0,"Exception when accessing CDH4 namenode","Get exception when accessing cdh4 from shell -

java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.
	at com.google.protobuf.GeneratedMessage.getUnknownFields

most likely due to protobuf-java-2.5.0.jar being on the main classpath now


Full stack trace:
{code}
trisberg@carbon:~/Test$ ./spring-xd-1.0.0.BUILD-SNAPSHOT/shell/bin/xd-shell --hadoopDistro cdh4
16:55:22,680  WARN main conf.Configuration:824 - fs.default.name is deprecated. Instead, use fs.defaultFS
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
eXtreme Data
1.0.0.BUILD-SNAPSHOT | Admin Server Target: http://localhost:9393
Welcome to the Spring XD shell. For assistance hit TAB or type ""help"".
xd:>hadoop config fs --namenode hdfs://cdh4:8020
xd:>hadoop fs ls /
Hadoop configuration changed, re-initializing shell...
16:55:28,853  WARN Spring Shell util.NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
-ls: Fatal internal error
java.lang.UnsupportedOperationException: This is supposed to be overridden by subclasses.
	at com.google.protobuf.GeneratedMessage.getUnknownFields(GeneratedMessage.java:180)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetFileInfoRequestProto.getSerializedSize(ClientNamenodeProtocolProtos.java:30108)
	at com.google.protobuf.AbstractMessageLite.toByteString(AbstractMessageLite.java:49)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.constructRpcRequest(ProtobufRpcEngine.java:149)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:193)
	at com.sun.proxy.$Proxy43.getFileInfo(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:164)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:83)
	at com.sun.proxy.$Proxy43.getFileInfo(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:629)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1545)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:819)
	at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1646)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1592)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1567)
	at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:271)
	at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:224)
	at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:207)
	at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)
	at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
	at org.apache.hadoop.fs.FsShell.run(FsShell.java:254)
	at org.springframework.xd.shell.hadoop.FsShellCommands.run(FsShellCommands.java:412)
	at org.springframework.xd.shell.hadoop.FsShellCommands.runCommand(FsShellCommands.java:407)
	at org.springframework.xd.shell.hadoop.FsShellCommands.ls(FsShellCommands.java:110)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:196)
	at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)
	at org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:48)
	at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)
	at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:530)
	at org.springframework.shell.core.JLineShell.run(JLineShell.java:178)
	at java.lang.Thread.run(Thread.java:744)
{code}",3.0,False,3.0,0,0.0,-1,1,False
"XD-1525","Improvement","","2014-04-09T12:56:03.000+0000","dturanski","","Need better error handling for module info shell command","Would be better to provide a more useful message, e.g. ""The module name must be of the form <module-type>:<module-name>""
xd:>module info --name time
java.lang.StringIndexOutOfBoundsException: Failed to convert 'time' to type QualifiedModuleName for option 'name,'
String index out of range: -1
xd:>module info --name sink/time
java.lang.StringIndexOutOfBoundsException: Failed to convert 'sink/time' to type QualifiedModuleName for option 'name,'
String index out of range: -1
xd:>module info --name sink:time
Command failed org.springframework.xd.rest.client.impl.SpringXDException: NullPointerException

xd:>module info --name source:time
Information about source module 'time':",1.0,-1.0,0.0004651025662059,-1.0,False,False,True,1397048163000.0,-1.0,-1.0,1426352627000.0,1426353527000.0,1397048163000.0,-1.0,-1.0,-1.0,1397061793000.0,1397065393000.0,-1.0,-1.0,"dturanski",-1.0,-1.0,0,1397048163000.0,-1.0,"Need better error handling for module info shell command","Would be better to provide a more useful message, e.g. ""The module name must be of the form <module-type>:<module-name>""
xd:>module info --name time
java.lang.StringIndexOutOfBoundsException: Failed to convert 'time' to type QualifiedModuleName for option 'name,'
String index out of range: -1
xd:>module info --name sink/time
java.lang.StringIndexOutOfBoundsException: Failed to convert 'sink/time' to type QualifiedModuleName for option 'name,'
String index out of range: -1
xd:>module info --name sink:time
Command failed org.springframework.xd.rest.client.impl.SpringXDException: NullPointerException

xd:>module info --name source:time
Information about source module 'time':",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1524","Story","Sprint 25","2014-04-09T11:19:54.000+0000","mark.pollack","","Create small documentation section on jmx/monitoring functionalty","Should mention jolokia, how to turn on/off boot/jolokia http metric/monitoring and jmx.

Mention the naming strategy to identify modules running in a stream.",1.0,0.8719063786277954,0.1339236557166452,0.0,True,True,True,1397042394000.0,1397308451000.0,1397312051000.0,1397346638000.0,1397347538000.0,1397042394000.0,-1.0,1397042394000.0,1397045994000.0,1397083260000.0,1397086860000.0,1397308451000.0,1397312051000.0,"mark.pollack",1397045994000.0,1397042394000.0,44,1397042394000.0,-1.0,"Create small documentation section on jmx/monitoring functionalty","Should mention jolokia, how to turn on/off boot/jolokia http metric/monitoring and jmx.

Mention the naming strategy to identify modules running in a stream.",1.0,False,1.0,0,0.0,-1,0,False
"XD-1523","Story","Sprint 25","2014-04-09T10:33:49.000+0000","mark.fisher","mark.fisher","stream list should show ""undeployed"" rather than blank if a stream is not deployed","",2.0,0.0003011011699931,0.0,0.0,True,True,True,1397039629000.0,1397039636000.0,1397043236000.0,1397061977000.0,1397062877000.0,1397039629000.0,-1.0,1397039629000.0,1397043229000.0,1397039629000.0,1397043229000.0,1397039636000.0,1397043236000.0,"mark.fisher",1397043229000.0,1397039629000.0,44,1397039629000.0,-1.0,"stream list should show ""undeployed"" rather than blank if a stream is not deployed","",2.0,False,2.0,0,0.0,0,0,False
"XD-1522","Story","Sprint 26","2014-04-09T07:28:59.000+0000","mark.fisher","eric.bottard","parser should only allow one label instance per module","",2.0,-1.0,0.0,0.5395778984683031,False,True,True,1397028539000.0,-1.0,-1.0,1398324436000.0,1398325336000.0,1397028539000.0,-1.0,1397728262000.0,1397731862000.0,1397028539000.0,1397032139000.0,-1.0,-1.0,"mark.fisher",1397731862000.0,1397028539000.0,44,1397028539000.0,-1.0,"parser should only allow one label instance per module","",2.0,False,2.0,0,0.0,0,0,False
"XD-1521","Story","Sprint 25","2014-04-09T05:48:11.000+0000","eric.bottard","eric.bottard","Create a dedicated plugin for ${xd.stream.name} and similar","The setting of the xd.stream.name property is currently duplicated in StreamPlugin and JobPlugin, etc
Moreover, there is no strong String constant to reference it

Create a plugin dedicated to those matters (namely making bits of DeploymentMetadata available to the module environment)",4.0,0.9995649250484516,0.9995649250484516,0.999911007396274,True,True,True,1397022491000.0,1397123579000.0,1397123623000.0,1397122723000.0,1397123623000.0,1397022491000.0,-1.0,1397123614000.0,1397123623000.0,1397123579000.0,1397123623000.0,1397123579000.0,1397123623000.0,"eric.bottard",1397123623000.0,1397022491000.0,44,1397022491000.0,-1.0,"Create a dedicated plugin for ${xd.stream.name} and similar","The setting of the xd.stream.name property is currently duplicated in StreamPlugin and JobPlugin, etc
Moreover, there is no strong String constant to reference it

Create a plugin dedicated to those matters (namely making bits of DeploymentMetadata available to the module environment)",4.0,False,4.0,0,0.0,0,0,False
"XD-1520","Story","Sprint 27","2014-04-09T05:45:40.000+0000","eric.bottard","eric.bottard","Push ${xd.stream.name} into POJO defaults","See XD-1283.
We've been waiting for 1283 to change constructs like
{noformat}
attr=""${name:${xd.stream.name}}""

to just
{noformat}
attr=""${name}""
{noformat}


Turns out we can simply push down the ${xd.stream.name} bit in the default value (most likely initialization of a field in a POJO metadata class) and it will work just fine.

We can also consider:
- providing a fake value for those placeholders to use when doing ""module info"" (ie user will see  ""<name of the stream>"" instead of ""${xd.stream.name}""


",8.0,-1.0,0.3284514420173511,0.973581418444364,False,True,True,1397022340000.0,-1.0,-1.0,1400775313000.0,1400776213000.0,1397022340000.0,-1.0,1400677041000.0,1400680641000.0,1398255305000.0,1398258905000.0,-1.0,-1.0,"eric.bottard",1400680641000.0,1397022340000.0,44,1397022340000.0,-1.0,"Push ${xd.stream.name} into POJO defaults","See XD-1283.
We've been waiting for 1283 to change constructs like
{noformat}
attr=""${name:${xd.stream.name}}""

to just
{noformat}
attr=""${name}""
{noformat}


Turns out we can simply push down the ${xd.stream.name} bit in the default value (most likely initialization of a field in a POJO metadata class) and it will work just fine.

We can also consider:
- providing a fake value for those placeholders to use when doing ""module info"" (ie user will see  ""<name of the stream>"" instead of ""${xd.stream.name}""


",8.0,False,8.0,0,0.0,0,0,False
"XD-1519","Story","Sprint 36","2014-04-08T18:20:51.000+0000","mark.pollack","","Provide job repository creation schema for additonal databases","oracle, gemfire xd (derby should be the dialect), and sybase",3.0,0.9738634887725076,0.9738634887725076,0.952561610504627,True,True,True,1396981251000.0,1412929854000.0,1412933454000.0,1413356982000.0,1413357882000.0,1396981251000.0,-1.0,1412581001000.0,1412584601000.0,1412929854000.0,1412933454000.0,1412929854000.0,1412933454000.0,"mark.pollack",1412584601000.0,1396981251000.0,44,1396981251000.0,-1.0,"Provide job repository creation schema for additonal databases","oracle, gemfire xd (derby should be the dialect), and sybase",3.0,False,3.0,0,0.0,0,0,False
"XD-1518","Story","Sprint 26","2014-04-08T16:11:41.000+0000","mark.fisher","Mark Fisher","Add test cases for DefaultContainerMatcher","",4.0,0.9924178993910588,0.9900862676036812,0.4334927677827817,True,True,True,1396973501000.0,1398763285000.0,1398766885000.0,1398776059000.0,1398776959000.0,1396973501000.0,-1.0,1397755287000.0,1397758887000.0,1398759080000.0,1398762680000.0,1398763285000.0,1398766885000.0,"mark.fisher",1397758887000.0,1396973501000.0,44,1396973501000.0,-1.0,"Add test cases for DefaultContainerMatcher","",4.0,False,4.0,0,0.0,0,0,False
"XD-1517","Story","Sprint 30","2014-04-08T16:06:29.000+0000","mark.pollack","dturanski","Change request mapping for removing a stream deployment in XDController","Currently _deployments - with an understore in XDController should be something else.  Need to segment up the url space better for stream/jobs to avoid a clash.",3.0,0.9451627558518788,0.9330430437575844,0.932497833502228,True,True,True,1396973189000.0,1402640245000.0,1402643845000.0,1402968141000.0,1402969041000.0,1396973189000.0,-1.0,1402564308000.0,1402567908000.0,1402567577000.0,1402571177000.0,1402640245000.0,1402643845000.0,"mark.pollack",1402567908000.0,1396973189000.0,44,1396973189000.0,-1.0,"Change request mapping for removing a stream deployment in XDController","Currently _deployments - with an understore in XDController should be something else.  Need to segment up the url space better for stream/jobs to avoid a clash.",3.0,False,3.0,0,0.0,0,0,False
"XD-1516","Improvement","","2014-04-08T12:25:07.000+0000","pperalta","pperalta","Deploy ""orphaned"" jobs to new containers","When a container joins the cluster, the leader admin will assign modules to it that are unassigned. It needs to also check for job modules and deploy them if required.",1.0,0.0004996763460031,0.0,-1.0,True,False,True,1396959907000.0,1396959951000.0,1396963551000.0,1397047064000.0,1397047964000.0,1396959907000.0,-1.0,-1.0,-1.0,1396959907000.0,1396963507000.0,1396959951000.0,1396963551000.0,"pperalta",-1.0,-1.0,0,1396959907000.0,-1.0,"Deploy ""orphaned"" jobs to new containers","When a container joins the cluster, the leader admin will assign modules to it that are unassigned. It needs to also check for job modules and deploy them if required.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1514","Story","Sprint 25","2014-04-08T10:07:33.000+0000","mark.pollack","Mark Fisher","Create documentation for the deployment manifest","",3.0,0.3769314779201125,0.3669098068665029,0.0,True,True,True,1396951653000.0,1397149415000.0,1397153015000.0,1397475416000.0,1397476316000.0,1396951653000.0,-1.0,1396951653000.0,1396955253000.0,1397144157000.0,1397147757000.0,1397149415000.0,1397153015000.0,"mark.pollack",1396955253000.0,1396951653000.0,44,1396951653000.0,-1.0,"Create documentation for the deployment manifest","",3.0,False,3.0,0,0.0,0,0,False
"XD-1513","Story","Sprint 25","2014-04-08T10:06:19.000+0000","mark.pollack","","Change the default deploy option to false for stream/job deploy commands","With the introduction of the deployment manifest setting this to false makes it easier to use the deployment manifest, which is the main use-case scenario.
",2.0,0.7286887084473554,0.6898927818402045,0.0,True,True,True,1396951579000.0,1397032795000.0,1397036395000.0,1397062134000.0,1397063034000.0,1396951579000.0,-1.0,1396951579000.0,1396955179000.0,1397028471000.0,1397032071000.0,1397032795000.0,1397036395000.0,"mark.pollack",1396955179000.0,1396951579000.0,44,1396951579000.0,-1.0,"Change the default deploy option to false for stream/job deploy commands","With the introduction of the deployment manifest setting this to false makes it easier to use the deployment manifest, which is the main use-case scenario.
",2.0,False,2.0,0,0.0,0,0,False
"XD-1512","Story","Sprint 25","2014-04-08T09:14:19.000+0000","mark.pollack","dturanski","Create documentation for how to extend the XD containers","https://jira.spring.io/browse/XD-1343 and related issues.",4.0,0.0243423656437754,0.0,0.0,True,True,True,1396948459000.0,1397022019000.0,1397025619000.0,1399969451000.0,1399970351000.0,1396948459000.0,1398413045000.0,1396948459000.0,1396952059000.0,1396948459000.0,1396952059000.0,1397022019000.0,1397025619000.0,"mark.pollack",1396952059000.0,1396948459000.0,44,1398413045000.0,1398413045000.0,"Create documentation for how to extend the XD containers","https://jira.spring.io/browse/XD-1343 and related issues.",1.0,True,1.0,1,300.0,0,0,False
"XD-1511","Story","Sprint 25","2014-04-08T09:11:01.000+0000","mark.pollack","dturanski","Create documentation for ZK runtime","",3.0,0.1396684651789958,0.0,0.0,True,True,True,1396948261000.0,1397022010000.0,1397025610000.0,1397475390000.0,1397476290000.0,1396948261000.0,-1.0,1396948261000.0,1396951861000.0,1396948261000.0,1396951861000.0,1397022010000.0,1397025610000.0,"mark.pollack",1396951861000.0,1396948261000.0,44,1396948261000.0,-1.0,"Create documentation for ZK runtime","",3.0,False,3.0,0,0.0,0,0,False
"XD-1510","Story","Sprint 25","2014-04-07T21:04:54.000+0000","mark.pollack","","Do not log ZK client environment java.class.path on application startup"," INFO main zookeeper.ZooKeeper:100 - Client environment:java.class.path

produces a huge amount of output and is rather distracting, if that specific entry could be at a log level of debug it would make the startup of the server look cleaner.",1.0,0.9797412401529932,0.991943294638446,0.0,True,True,True,1396904694000.0,1405329050000.0,1405332650000.0,1405502346000.0,1405503246000.0,1396904694000.0,1397031716000.0,1396904694000.0,1396908294000.0,1405433970000.0,1405437570000.0,1405329050000.0,1405332650000.0,"mark.pollack",1396908294000.0,1396904694000.0,44,1397031716000.0,1397031716000.0,"Reduce amount of logging at server startup"," INFO main zookeeper.ZooKeeper:100 - Client environment:java.class.path

produces a huge amount of output and is rather distracting, if that specific entry could be at a log level of debug it would make the startup of the server look cleaner.",2.0,True,2.0,1,-50.0,0,0,True
"XD-1509","Story","Sprint 32","2014-04-07T19:48:41.000+0000","mark.pollack","","Clean up publishing to maven repositories of empty module projects","See the various module.xyz directories here 

https://repo.spring.io/libs-snapshot/org/springframework/xd/",4.0,0.9896322264517248,0.9896322264517248,0.9620950185001084,True,True,True,1396900121000.0,1406189771000.0,1406193371000.0,1406286193000.0,1406287093000.0,1396900121000.0,-1.0,1405931280000.0,1405934880000.0,1406189771000.0,1406193371000.0,1406189771000.0,1406193371000.0,"mark.pollack",1405934880000.0,1396900121000.0,44,1396900121000.0,-1.0,"Clean up publishing to maven repositories of empty module projects","See the various module.xyz directories here 

https://repo.spring.io/libs-snapshot/org/springframework/xd/",4.0,False,4.0,0,0.0,0,0,False
"XD-1508","Bug","Sprint 25","2014-04-07T15:47:31.000+0000","thomas.risberg","pperalta","All jobs end up on the same container node","The jobs aren't spread evenly across available container nodes as they are created/deployed. I had 3 nodes but only one has the job modules.

[zk: localhost:2181(CONNECTED) 56] ls /xd/deployments/modules/621230e0-a089-4fbe-afc8-611ae527fcbc
[myjob9.job.jdbchdfs-0, myjob5.job.jdbchdfs-0, myjob8.job.jdbchdfs-0, myjob4.job.jdbchdfs-0, myjob6.job.jdbchdfs-0, myjob7.job.jdbchdfs-0]
[zk: localhost:2181(CONNECTED) 57] ls /xd/deployments/modules/6969579c-0cf4-4cc1-8e21-e01d73a70965
[]
[zk: localhost:2181(CONNECTED) 58] ls /xd/deployments/modules/d0667cd1-a57a-4279-b7fb-dd63e4dd40d4
[]

",3.0,0.6404608294930876,0.6404032258064516,0.7472695852534562,True,True,True,1396885651000.0,1396941243000.0,1396944843000.0,1396971551000.0,1396972451000.0,1396885651000.0,-1.0,1396950514000.0,1396954114000.0,1396941238000.0,1396944838000.0,1396941243000.0,1396944843000.0,"thomas.risberg",1396954114000.0,1396885651000.0,44,1396885651000.0,-1.0,"All jobs end up on the same container node","The jobs aren't spread evenly across available container nodes as they are created/deployed. I had 3 nodes but only one has the job modules.

[zk: localhost:2181(CONNECTED) 56] ls /xd/deployments/modules/621230e0-a089-4fbe-afc8-611ae527fcbc
[myjob9.job.jdbchdfs-0, myjob5.job.jdbchdfs-0, myjob8.job.jdbchdfs-0, myjob4.job.jdbchdfs-0, myjob6.job.jdbchdfs-0, myjob7.job.jdbchdfs-0]
[zk: localhost:2181(CONNECTED) 57] ls /xd/deployments/modules/6969579c-0cf4-4cc1-8e21-e01d73a70965
[]
[zk: localhost:2181(CONNECTED) 58] ls /xd/deployments/modules/d0667cd1-a57a-4279-b7fb-dd63e4dd40d4
[]

",3.0,False,3.0,0,0.0,-1,0,False
"XD-1507","Bug","Sprint 26","2014-04-07T15:45:16.000+0000","thomas.risberg","Gunnar Hillert","Able to submit jobs that are not currently deployed using Admin UI","Job modules ""Launch"" and ""Schedule"" command buttons are active even if the job module isn't deployed or has been destroyed.

Get errors like:
 ""Yikes, something bad happened while launching job myjob4""
""The job named 'myjob4' is not currently deployed""",3.0,0.3084147194295211,0.8425875281633376,0.1961187853053866,True,True,True,1396885516000.0,1398237682000.0,1398241282000.0,1401268862000.0,1401269762000.0,1396885516000.0,-1.0,1397745349000.0,1397748949000.0,1400579627000.0,1400583227000.0,1398237682000.0,1398241282000.0,"thomas.risberg",1397748949000.0,1396885516000.0,44,1396885516000.0,-1.0,"Prevent submiting jobs that are not currently deployed using Admin UI","Job modules ""Launch"" and ""Schedule"" command buttons are active even if the job module isn't deployed or has been destroyed.

Get errors like:
 ""Yikes, something bad happened while launching job myjob4""
""The job named 'myjob4' is not currently deployed""",3.0,False,3.0,0,0.0,0,0,True
"XD-1506","Story","","2014-04-07T15:08:30.000+0000","thomas.risberg","thomas.risberg","Add admin-ui to YARN zip packaging","",1.0,-1.0,0.0,-1.0,False,False,True,1396883310000.0,-1.0,-1.0,1397725242000.0,1397726142000.0,1396883310000.0,-1.0,-1.0,-1.0,1396883310000.0,1396886910000.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1396883310000.0,-1.0,"Add admin-ui to YARN zip packaging","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1505","Bug","Sprint 25","2014-04-07T14:58:58.000+0000","dbeauregard","mark.pollack","Documentation typo in JSON SPEL filter","In the JSON SPEL Filter twitter example here:
http://docs.spring.io/spring-xd/docs/1.0.0.M5/reference/html/#filter

""hashTags"" should not have a capital 'T'.  Should be ""hashtags"".",1.0,-1.0,0.9999365028970552,0.999904754345583,False,True,True,1396882738000.0,-1.0,-1.0,1396944833000.0,1396945733000.0,1396882738000.0,-1.0,1396945727000.0,1396945733000.0,1396945729000.0,1396945733000.0,-1.0,-1.0,"dbeauregard",1396945733000.0,1396882738000.0,44,1396882738000.0,-1.0,"Documentation typo in JSON SPEL filter","In the JSON SPEL Filter twitter example here:
http://docs.spring.io/spring-xd/docs/1.0.0.M5/reference/html/#filter

""hashTags"" should not have a capital 'T'.  Should be ""hashtags"".",1.0,False,1.0,0,0.0,-1,0,False
"XD-1504","Story","Sprint 26","2014-04-07T13:26:47.000+0000","mark.fisher","pperalta","Avoid duplication when loading streams for deployment","See: ContainerListener.loadStream() and StreamListener.onChildAdded(). Both require the stream definition as well as stream deployment manifest.",10.0,0.9978161749675464,0.2866731854487838,0.2866573343068576,True,True,True,1396877207000.0,1399898767000.0,1399902367000.0,1399904480000.0,1399905380000.0,1396877207000.0,1398853823000.0,1397745255000.0,1397748855000.0,1397745303000.0,1397748903000.0,1399898767000.0,1399902367000.0,"mark.fisher",1397748855000.0,1396877207000.0,44,1398853823000.0,1398853823000.0,"Avoid duplication when loading streams for deployment","See: ContainerListener.loadStream() and StreamListener.onChildAdded(). Both require the stream definition as well as stream deployment manifest.",2.0,True,2.0,1,400.0,0,0,False
"XD-1503","Improvement","Sprint 26","2014-04-07T10:54:34.000+0000","pperalta","pperalta","Refactor duplicate code for module deployment","The admin leader performs module deployment requests by listening to stream deployment requests under {{/xd/deployments/streams}} and by listening to containers joining and leaving the cluster under {{/xd/containers}}. Refactor any duplicate code into a common class/component that can be used by both listeners.",10.0,0.7800349043594159,0.366280508323601,0.3658681222246415,True,True,True,1396868074000.0,1398759590000.0,1398763190000.0,1399292086000.0,1399292986000.0,1396868074000.0,1398762842000.0,1397755272000.0,1397758872000.0,1397756272000.0,1397759872000.0,1398759590000.0,1398763190000.0,"pperalta",1397758872000.0,1396868074000.0,44,1398853709000.0,1398762842000.0,"Refactor duplicate code for modules","The admin leader performs module deployment requests by listening to stream deployment requests under {{/xd/deployments/streams}} and by listening to containers joining and leaving the cluster under {{/xd/containers}}. Refactor any duplicate code into a common class/component that can be used by both listeners.

*Edit:* the listener duplication will be handled in XD-1548. This issue is for the duplicate code in {{ModuleDeploymentRequest}} and {{ModuleDescriptor}}.",1.0,True,1.0,2,900.0,0,0,True
"XD-1502","Bug","Sprint 25","2014-04-07T10:51:45.000+0000","pperalta","Patrick Peralta","Investigate failing LocalSingleNodeStreamDeploymentIntegrationTests","Investigate the failing test LocalSingleNodeStreamDeploymentIntegrationTests.moduleChannelsRegisteredWithMessageBus:

{noformat}
java.lang.AssertionError: expected:<3> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.springframework.xd.dirt.stream.AbstractSingleNodeStreamDeploymentIntegrationTests.moduleChannelsRegisteredWithMessageBus(AbstractSingleNodeStreamDeploymentIntegrationTests.java:270)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
{noformat}

This can be most easily reproduced on Ubuntu.",1.0,0.0053168371338336,1.0441717613652826,1.1629096282560194,True,True,True,1396867905000.0,1396868272000.0,1396871872000.0,1396936031000.0,1396936931000.0,1396867905000.0,-1.0,1396948176000.0,1396936931000.0,1396939980000.0,1396936931000.0,1396868272000.0,1396871872000.0,"pperalta",1396936931000.0,1396867905000.0,44,1396867905000.0,-1.0,"Investigate failing LocalSingleNodeStreamDeploymentIntegrationTests","Investigate the failing test LocalSingleNodeStreamDeploymentIntegrationTests.moduleChannelsRegisteredWithMessageBus:

{noformat}
java.lang.AssertionError: expected:<3> but was:<0>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:743)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:555)
	at org.junit.Assert.assertEquals(Assert.java:542)
	at org.springframework.xd.dirt.stream.AbstractSingleNodeStreamDeploymentIntegrationTests.moduleChannelsRegisteredWithMessageBus(AbstractSingleNodeStreamDeploymentIntegrationTests.java:270)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
{noformat}

This can be most easily reproduced on Ubuntu.",1.0,False,1.0,0,0.0,0,0,False
"XD-1501","Improvement","Sprint 26","2014-04-07T10:44:19.000+0000","pperalta","iperumal","IP address used as default data when creating paths","Invoking {{Paths.ensurePath}} is creating a default value of the host IP address instead of the expected ""empty"" value. ",1.0,0.9969227368843196,0.9906292154289256,0.9906394691749608,True,True,True,1396867459000.0,1397742486000.0,1397745187000.0,1397744287000.0,1397745187000.0,1396867459000.0,-1.0,1397736971000.0,1397740571000.0,1397736962000.0,1397740562000.0,1397742486000.0,1397745187000.0,"pperalta",1397740571000.0,1396867459000.0,44,1396867459000.0,-1.0,"IP address used as default data when creating paths","Invoking {{Paths.ensurePath}} is creating a default value of the host IP address instead of the expected ""empty"" value. ",1.0,False,1.0,0,0.0,0,0,False
"XD-1500","Bug","Sprint 26","2014-04-07T10:41:27.000+0000","pperalta","pperalta","Stream deployment race condition","When a container is started, the leader admin will scan the deployed streams to determine if any have modules that need to be deployed on the new container. 

When a stream is deployed, the leader admin will select containers to deploy modules to.

If a new container and stream are deployed at the same time, there is the window for a race condition where both attempt to deploy a module to a container. This can be solved by (at least one) of the following:

* Consider using a single thread in the admin leader to handle all ZooKeeper updates. This means that the handling of new containers and stream deployment requests will not happen concurrently.
* Trap the {{NodeExists}} exception when creating the {{/xd/deployments/modules/...}} node in ZooKeeper",5.0,0.9995707563570158,0.0,0.2931952005277716,True,True,True,1396867287000.0,1399894570000.0,1399895870000.0,1399894970000.0,1399895870000.0,1396867287000.0,1398853795000.0,1397755253000.0,1397758853000.0,1396867287000.0,1396870887000.0,1399894570000.0,1399895870000.0,"pperalta",1397758853000.0,1396867287000.0,44,1398853795000.0,1398853795000.0,"Stream deployment race condition","When a container is started, the leader admin will scan the deployed streams to determine if any have modules that need to be deployed on the new container. 

When a stream is deployed, the leader admin will select containers to deploy modules to.

If a new container and stream are deployed at the same time, there is the window for a race condition where both attempt to deploy a module to a container. This can be solved by (at least one) of the following:

* Consider using a single thread in the admin leader to handle all ZooKeeper updates. This means that the handling of new containers and stream deployment requests will not happen concurrently.
* Trap the {{NodeExists}} exception when creating the {{/xd/deployments/modules/...}} node in ZooKeeper",1.0,True,1.0,1,400.0,0,0,False
"XD-1499","Story","Sprint 25","2014-04-07T10:28:12.000+0000","mark.pollack","Mark Pollack","change xd-config.yml and xd-modules-config.yml to servers.yml and modules.yml","",1.0,0.9949636011269808,0.6705983421005488,0.0,True,True,True,1396866492000.0,1397110867000.0,1397112104000.0,1397111204000.0,1397112104000.0,1396866492000.0,-1.0,1396866492000.0,1396870092000.0,1397031199000.0,1397034799000.0,1397110867000.0,1397112104000.0,"mark.pollack",1396870092000.0,1396866492000.0,44,1396866492000.0,-1.0,"change xd-config.yml and xd-modules-config.yml to servers.yml and modules.yml","",1.0,False,1.0,0,0.0,0,0,False
"XD-1498","Story","Sprint 25","2014-04-07T10:27:23.000+0000","mark.pollack","mark.pollack","Creae documentation for module property configuration","Describe the algorithm as defined in https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit?usp=sharing

",2.0,1.3113012862226491e-05,0.0,0.0,True,True,True,1396866443000.0,1396866451000.0,1396870051000.0,1397475624000.0,1397476524000.0,1396866443000.0,-1.0,1396866443000.0,1396870043000.0,1396866443000.0,1396870043000.0,1396866451000.0,1396870051000.0,"mark.pollack",1396870043000.0,1396866443000.0,44,1396866443000.0,-1.0,"Create documentation for module property configuration","Describe the algorithm as defined in https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit?usp=sharing

",2.0,False,2.0,0,0.0,0,0,True
"XD-1497","Story","Sprint 27","2014-04-07T08:46:22.000+0000","grenfro","iperumal","Move JMX Endpoints from /jolokia to management/jolokia","Update code to use the management/jolokia endpoint.",3.0,1.8423845036541456e-05,0.9999905391066028,0.9993496880643858,True,True,True,1396860382000.0,1396860419000.0,1396864019000.0,1398867749000.0,1398868649000.0,1396860382000.0,-1.0,1398867343000.0,1398868649000.0,1398868630000.0,1398868649000.0,1396860419000.0,1396864019000.0,"grenfro",1398868649000.0,1396860382000.0,44,1396860382000.0,-1.0,"Move JMX Endpoints from /jolokia to management/jolokia","Update code to use the management/jolokia endpoint.",3.0,False,3.0,0,0.0,0,1,False
"XD-1496","Bug","Sprint 26","2014-04-07T08:38:28.000+0000","grenfro","iperumal","Exception thrown when accessing Jolokia via the management context path","When trying to access Jolokia via the management/jolokia (http://localhost:9393/management/jolokia) I get the following exception.   

{""error_type"":""java.lang.IllegalArgumentException"",""error"":""java.lang.IllegalArgumentException : No type with name 'management' exists"",""status"":400,""stacktrace"":""java.lang.IllegalArgumentException: No type with name 'management' exists\n\tat org.jolokia.util.RequestType.getTypeByName(RequestType.java:69)\n\tat org.jolokia.request.JmxRequestFactory.createGetRequest(JmxRequestFactory.java:94)\n\tat org.jolokia.http.HttpRequestHandler.handleGetRequest(HttpRequestHandler.java:78)\n\tat org.jolokia.http.AgentServlet$3.handleRequest(AgentServlet.java:298)\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:229)\n\tat org.jolokia.http.AgentServlet.doGet(AgentServlet.java:194)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:158)\n\tat org.springframework.web.servlet.mvc.AbstractController.handleRequest(AbstractController.java:154)\n\tat org.springframework.boot.actuate.endpoint.mvc.JolokiaMvcEndpoint.handle(JolokiaMvcEndpoint.java:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:115)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:137)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:85)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)\n\tat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)\n\tat org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)\n\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)\n\tat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)\n\tat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n""}",5.0,0.016293031257849,0.0002579869779499,0.9988170353206196,True,True,True,1396859908000.0,1396890980000.0,1396894580000.0,1398766081000.0,1398766981000.0,1396859908000.0,-1.0,1398764725000.0,1398766981000.0,1396860400000.0,1396864000000.0,1396890980000.0,1396894580000.0,"grenfro",1398766981000.0,1396859908000.0,44,1396859908000.0,-1.0,"Exception thrown when accessing Jolokia via the management context path","When trying to access Jolokia via the management/jolokia (http://localhost:9393/management/jolokia) I get the following exception.   

{""error_type"":""java.lang.IllegalArgumentException"",""error"":""java.lang.IllegalArgumentException : No type with name 'management' exists"",""status"":400,""stacktrace"":""java.lang.IllegalArgumentException: No type with name 'management' exists\n\tat org.jolokia.util.RequestType.getTypeByName(RequestType.java:69)\n\tat org.jolokia.request.JmxRequestFactory.createGetRequest(JmxRequestFactory.java:94)\n\tat org.jolokia.http.HttpRequestHandler.handleGetRequest(HttpRequestHandler.java:78)\n\tat org.jolokia.http.AgentServlet$3.handleRequest(AgentServlet.java:298)\n\tat org.jolokia.http.AgentServlet.handle(AgentServlet.java:229)\n\tat org.jolokia.http.AgentServlet.doGet(AgentServlet.java:194)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.springframework.web.servlet.mvc.ServletWrappingController.handleRequestInternal(ServletWrappingController.java:158)\n\tat org.springframework.web.servlet.mvc.AbstractController.handleRequest(AbstractController.java:154)\n\tat org.springframework.boot.actuate.endpoint.mvc.JolokiaMvcEndpoint.handle(JolokiaMvcEndpoint.java:120)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:215)\n\tat org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)\n\tat org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)\n\tat org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:689)\n\tat org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:938)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:870)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:852)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:621)\n\tat org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:728)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:115)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:137)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilterInternal(MetricFilterAutoConfiguration.java:85)\n\tat org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)\n\tat org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)\n\tat org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)\n\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)\n\tat org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)\n\tat org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)\n\tat org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n""}",5.0,False,5.0,0,0.0,0,1,False
"XD-1495","Bug","Sprint 26","2014-04-06T19:02:06.000+0000","dbeauregard","mark.fisher","xd:>runtime modules gives error from CLI","xd:>runtime modules
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/4dc55d87-125b-4e4a-a76e-82bb6980820d/TickTock.sink.log-1/metadata

This is on OSX running in distributed mode with --transport rabbit --hadoopDistro hadoop22, redis 2.8.8, rabbit 3.2.3, hadoop 2.2.0, and zookeeper 3.4.5.",1.0,-1.0,0.6924011554552587,0.6923922650582951,False,True,True,1396810926000.0,-1.0,-1.0,1398159797000.0,1398160697000.0,1396810926000.0,-1.0,1397745497000.0,1397749097000.0,1397745509000.0,1397749109000.0,-1.0,-1.0,"dbeauregard",1397749097000.0,1396810926000.0,44,1396810926000.0,-1.0,"xd:>runtime modules gives error from CLI","xd:>runtime modules
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.RuntimeException: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /xd/deployments/modules/4dc55d87-125b-4e4a-a76e-82bb6980820d/TickTock.sink.log-1/metadata

This is on OSX running in distributed mode with --transport rabbit --hadoopDistro hadoop22, redis 2.8.8, rabbit 3.2.3, hadoop 2.2.0, and zookeeper 3.4.5.",1.0,False,1.0,0,0.0,0,0,False
"XD-1494","Bug","Sprint 26","2014-04-06T16:51:16.000+0000","dbeauregard","mark.fisher","OS commands no longer supports whitespace/arguments in M6","OS commands, i.e., ""!"" doesn't support arguments in M6; it did in M5.  

The following gives an error:
xd:>! ls /
You cannot specify option '' more than once in a single command

No arguments or whitespace works:
xd:>! ls
command is:ls
spring-shell.log
xd-shell
xd-shell.bat",1.0,-1.0,0.6947517314377198,0.6947657362553771,False,True,True,1396803076000.0,-1.0,-1.0,1398158852000.0,1398159752000.0,1396803076000.0,-1.0,1397745648000.0,1397749248000.0,1397745629000.0,1397749229000.0,-1.0,-1.0,"dbeauregard",1397749248000.0,1396803076000.0,44,1396803076000.0,-1.0,"OS commands no longer supports whitespace/arguments in M6","OS commands, i.e., ""!"" doesn't support arguments in M6; it did in M5.  

The following gives an error:
xd:>! ls /
You cannot specify option '' more than once in a single command

No arguments or whitespace works:
xd:>! ls
command is:ls
spring-shell.log
xd-shell
xd-shell.bat",1.0,False,1.0,0,0.0,0,0,False
"XD-1493","Bug","","2014-04-06T16:48:50.000+0000","dbeauregard","","xd-shell tab completion missing for http post/get","xd-shell tab completion missing for 'http post' and 'http get' cli commands.  Typing ""xd:>http post"" <tab> <tab> gives no suggestions event though --file or --data are required.  ",1.0,-1.0,0.2762303627873215,-1.0,False,False,True,1396802930000.0,-1.0,-1.0,1402059077000.0,1402059977000.0,1396802930000.0,-1.0,-1.0,-1.0,1398255086000.0,1398258686000.0,-1.0,-1.0,"dbeauregard",-1.0,-1.0,0,1396802930000.0,-1.0,"xd-shell tab completion missing for http post/get","xd-shell tab completion missing for 'http post' and 'http get' cli commands.  Typing ""xd:>http post"" <tab> <tab> gives no suggestions event though --file or --data are required.  ",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1492","Story","Sprint 25","2014-04-04T14:55:51.000+0000","dturanski","dturanski","Support groups container attribute","A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups=<comma delimited string> or an environment variable or --groups command line argument. ",2.0,0.0309088108441158,0.0309088108441158,0.0307979051139864,True,True,True,1396623351000.0,1396633384000.0,1396636984000.0,1396947051000.0,1396947951000.0,1396623351000.0,1396633437000.0,1396633348000.0,1396636948000.0,1396633384000.0,1396636984000.0,1396633384000.0,1396636984000.0,"dturanski",1396636948000.0,1396633437000.0,44,1396633437000.0,-1.0,"Support groups container attribute","A container instance may be designated as belonging to a group. The end user may define this attribute using yaml config as xd.container.groups=<comma delimited string> or an environment variable or --groups command line argument. ",2.0,False,2.0,0,0.0,0,0,False
"XD-1491","Story","Sprint 25","2014-04-04T14:53:23.000+0000","dturanski","dturanski","Allow users to set attribute values on a container","Allow users to configure arbitrary key value pairs for a container instance that may be referenced in deployment descriptors to target specific container instances. ",2.0,0.0313160115040923,0.0,0.0312143961275303,True,True,True,1396623203000.0,1396633373000.0,1396636973000.0,1396947057000.0,1396947957000.0,1396623203000.0,1396633406000.0,1396633340000.0,1396636940000.0,1396623203000.0,1396626803000.0,1396633373000.0,1396636973000.0,"dturanski",1396636940000.0,1396633406000.0,44,1396633406000.0,-1.0,"Allow users to set attribute values on a container","Allow users to configure arbitrary key value pairs for a container instance that may be referenced in deployment descriptors to target specific container instances. ",2.0,False,2.0,0,0.0,0,0,False
"XD-1490","Bug","Sprint 26","2014-04-04T08:46:31.000+0000","dbeauregard","dturanski","SXD's gemfire-server wrapper script can't handle absolute paths w/o extra slash","Absolute paths fail: gemfire-server /Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml  (This failse, see error below)

You have to add an extra forward slash at the beginning (//Users) to get it to work: gemfire-server //Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml (This works)

---------------- The log and error ----------------------

dbeauregard-mbp:~ dbeauregard$ gemfire-server /Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml
09:39:33,772  INFO main gemfire.CacheServer:50 - Starting Cache Server
09:39:33,884  INFO main support.FileSystemXmlApplicationContext:513 - Refreshing org.springframework.context.support.FileSystemXmlApplicationContext@2ba119b3: startup date [Fri Apr 04 09:39:33 MDT 2014]; root of context hierarchy
09:39:33,949  INFO main xml.XmlBeanDefinitionReader:316 - Loading XML bean definitions from file [/Users/dbeauregard/Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml]
Exception in thread ""main"" org.springframework.beans.factory.BeanDefinitionStoreException: IOException parsing XML document from file [/Users/dbeauregard/Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml]; nested exception is java.io.FileNotFoundException: Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml (No such file or directory)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:343)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:251)
	at org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:127)
	at org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:93)
	at org.springframework.context.support.AbstractRefreshableApplicationContext.refreshBeanFactory(AbstractRefreshableApplicationContext.java:129)
	at org.springframework.context.support.AbstractApplicationContext.obtainFreshBeanFactory(AbstractApplicationContext.java:540)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:454)
	at org.springframework.context.support.FileSystemXmlApplicationContext.<init>(FileSystemXmlApplicationContext.java:140)
	at org.springframework.context.support.FileSystemXmlApplicationContext.<init>(FileSystemXmlApplicationContext.java:84)
	at org.springframework.xd.gemfire.CacheServer.main(CacheServer.java:52)
Caused by: java.io.FileNotFoundException: Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.springframework.core.io.FileSystemResource.getInputStream(FileSystemResource.java:114)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:329)
	... 13 more",1.0,0.999959748198732,0.9772411065395176,0.999976373942734,True,True,True,1396601191000.0,1397743951000.0,1397743997000.0,1397743097000.0,1397743997000.0,1396601191000.0,-1.0,1397743970000.0,1397743997000.0,1397717988000.0,1397721588000.0,1397743951000.0,1397743997000.0,"dbeauregard",1397743997000.0,1396601191000.0,44,1396601191000.0,-1.0,"SXD's gemfire-server wrapper script can't handle absolute paths w/o extra slash","Absolute paths fail: gemfire-server /Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml  (This failse, see error below)

You have to add an extra forward slash at the beginning (//Users) to get it to work: gemfire-server //Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml (This works)

---------------- The log and error ----------------------

dbeauregard-mbp:~ dbeauregard$ gemfire-server /Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml
09:39:33,772  INFO main gemfire.CacheServer:50 - Starting Cache Server
09:39:33,884  INFO main support.FileSystemXmlApplicationContext:513 - Refreshing org.springframework.context.support.FileSystemXmlApplicationContext@2ba119b3: startup date [Fri Apr 04 09:39:33 MDT 2014]; root of context hierarchy
09:39:33,949  INFO main xml.XmlBeanDefinitionReader:316 - Loading XML bean definitions from file [/Users/dbeauregard/Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml]
Exception in thread ""main"" org.springframework.beans.factory.BeanDefinitionStoreException: IOException parsing XML document from file [/Users/dbeauregard/Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml]; nested exception is java.io.FileNotFoundException: Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml (No such file or directory)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:343)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:303)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:180)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:216)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:187)
	at org.springframework.beans.factory.support.AbstractBeanDefinitionReader.loadBeanDefinitions(AbstractBeanDefinitionReader.java:251)
	at org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:127)
	at org.springframework.context.support.AbstractXmlApplicationContext.loadBeanDefinitions(AbstractXmlApplicationContext.java:93)
	at org.springframework.context.support.AbstractRefreshableApplicationContext.refreshBeanFactory(AbstractRefreshableApplicationContext.java:129)
	at org.springframework.context.support.AbstractApplicationContext.obtainFreshBeanFactory(AbstractApplicationContext.java:540)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:454)
	at org.springframework.context.support.FileSystemXmlApplicationContext.<init>(FileSystemXmlApplicationContext.java:140)
	at org.springframework.context.support.FileSystemXmlApplicationContext.<init>(FileSystemXmlApplicationContext.java:84)
	at org.springframework.xd.gemfire.CacheServer.main(CacheServer.java:52)
Caused by: java.io.FileNotFoundException: Users/dbeauregard/Software/spring-xd/spring-xd-1.0.0.M5/gemfire/config/my-demo.xml (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at org.springframework.core.io.FileSystemResource.getInputStream(FileSystemResource.java:114)
	at org.springframework.beans.factory.xml.XmlBeanDefinitionReader.loadBeanDefinitions(XmlBeanDefinitionReader.java:329)
	... 13 more",1.0,False,1.0,0,0.0,0,0,False
"XD-1489","Story","Sprint 25","2014-04-03T23:27:20.000+0000","mark.pollack","eric.bottard","Add documentation for new module property configuration support","the algorithm and approach in 

https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit#

needs to be added to the section on application configuration",3.0,0.9996945980367964,0.9942748197187868,0.9996780001040138,True,True,True,1396567640000.0,1397471091000.0,1397471367000.0,1397470467000.0,1397471367000.0,1396567640000.0,-1.0,1397471076000.0,1397471367000.0,1397466193000.0,1397469793000.0,1397471091000.0,1397471367000.0,"mark.pollack",1397471367000.0,1396567640000.0,44,1396567640000.0,-1.0,"Add documentation for new module property configuration support","the algorithm and approach in 

https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit#

needs to be added to the section on application configuration",3.0,False,3.0,0,0.0,-1,0,False
"XD-1488","Story","Sprint 25","2014-04-03T22:18:41.000+0000","mark.pollack","eric.bottard","Create a test case for insulating environment variables in module property lookup","For PR https://github.com/spring-projects/spring-xd/pull/682

see if one can have a test case such that a test module would have a 'PATH' property that overlaps with the environment variable.  It should never resolve to the real unix/windows path.",2.0,0.9999091349648656,3.028834504482675e-05,0.9999821833264442,True,True,True,1396563521000.0,1397124742000.0,1397124793000.0,1397123893000.0,1397124793000.0,1396563521000.0,-1.0,1397124783000.0,1397124793000.0,1396563538000.0,1396567138000.0,1397124742000.0,1397124793000.0,"mark.pollack",1397124793000.0,1396563521000.0,44,1396563521000.0,-1.0,"Create a test case for insulating environment variables in module property lookup","For PR https://github.com/spring-projects/spring-xd/pull/682

see if one can have a test case such that a test module would have a 'PATH' property that overlaps with the environment variable.  It should never resolve to the real unix/windows path.",2.0,False,2.0,0,0.0,-1,0,False
"XD-1487","Story","Sprint 25","2014-04-03T21:41:38.000+0000","mark.pollack","","Test scripts on windows that use XD_MODULE_CONFIG_LOCATION/NAME","",2.0,0.9541791354640908,0.9426337455463588,0.0074403856921467,True,True,True,1396561298000.0,1397471312000.0,1397474912000.0,1397514112000.0,1397515012000.0,1396561298000.0,-1.0,1396568394000.0,1396571994000.0,1397460301000.0,1397463901000.0,1397471312000.0,1397474912000.0,"mark.pollack",1396571994000.0,1396561298000.0,44,1396561298000.0,-1.0,"Test scripts on windows that use XD_MODULE_CONFIG_LOCATION/NAME","",2.0,False,2.0,0,0.0,0,0,False
"XD-1486","Improvement","Sprint 25","2014-04-03T19:59:11.000+0000","mark.fisher","Mark Fisher","eliminate package tangle","see:
https://sonar.springsource.org/drilldown/measures/7173?metric=package_tangle_index&rids%5B%5D=7717",4.0,0.9830249293240813,0.9826651246466204,0.000899511693652,True,True,True,1396555151000.0,1396631650000.0,1396632971000.0,1396632071000.0,1396632971000.0,1396555151000.0,-1.0,1396555221000.0,1396558821000.0,1396631622000.0,1396632971000.0,1396631650000.0,1396632971000.0,"mark.fisher",1396558821000.0,1396555151000.0,44,1396555151000.0,-1.0,"eliminate package tangle","see:
https://sonar.springsource.org/drilldown/measures/7173?metric=package_tangle_index&rids%5B%5D=7717",4.0,False,4.0,0,0.0,0,0,False
"XD-1485","Story","Sprint 25","2014-04-03T08:37:10.000+0000","mark.fisher","pperalta","The Container's DeploymentListener should watch /xd/deployments/modules","Currently it watches /xd/deployments/[containerid], but due to the reuse of that top level node for XD-1483 and XD-1484, we should instead use /xd/deployments/modules/[containerid]",4.0,0.0565188905874645,0.0160733784669141,0.9767198078183008,True,True,True,1396514230000.0,1396515524000.0,1396519124000.0,1396536225000.0,1396537125000.0,1396514230000.0,-1.0,1396536592000.0,1396537125000.0,1396514598000.0,1396518198000.0,1396515524000.0,1396519124000.0,"mark.fisher",1396537125000.0,1396514230000.0,44,1396514230000.0,-1.0,"The Container's DeploymentListener should watch /xd/deployments/modules","Currently it watches /xd/deployments/[containerid], but due to the reuse of that top level node for XD-1483 and XD-1484, we should instead use /xd/deployments/modules/[containerid]",4.0,False,4.0,0,0.0,-1,0,False
"XD-1484","Story","Sprint 25","2014-04-03T08:35:34.000+0000","mark.fisher","pperalta","JobListener should watch /xd/deployments/jobs","",4.0,0.9775021758050478,0.976544821583986,0.9814621409921672,True,True,True,1396514134000.0,1396536597000.0,1396537114000.0,1396536214000.0,1396537114000.0,1396514134000.0,-1.0,1396536688000.0,1396537114000.0,1396536575000.0,1396537114000.0,1396536597000.0,1396537114000.0,"mark.fisher",1396537114000.0,1396514134000.0,44,1396514134000.0,-1.0,"JobListener should watch /xd/deployments/jobs","",4.0,False,4.0,0,0.0,0,0,False
"XD-1483","Story","Sprint 25","2014-04-03T08:35:10.000+0000","mark.fisher","pperalta","StreamListener should watch /xd/deployments/streams","",4.0,0.1293099699986956,0.1293099699986956,0.3384060176529414,True,True,True,1396514110000.0,1396517084000.0,1396520684000.0,1396536209000.0,1396537109000.0,1396514110000.0,-1.0,1396521893000.0,1396525493000.0,1396517084000.0,1396520684000.0,1396517084000.0,1396520684000.0,"mark.fisher",1396525493000.0,1396514110000.0,44,1396514110000.0,-1.0,"StreamListener should watch /xd/deployments/streams","",4.0,False,4.0,0,0.0,0,0,False
"XD-1482","Story","Sprint 25","2014-04-03T08:33:34.000+0000","mark.fisher","mark.fisher","Add initial support for DeploymentManifest","The REST API for deploy should accept parameters, which provide manifest key/value pairs (e.g. ?http.instances=5). Ultimately we will want to support passing a named manifest which had been stored previously.

The 'stream deploy' shell command should support passing these as --options.

Initially we should support [modulename].instances and [modulename].group.",5.0,0.0431797066731757,0.0431683535513511,0.0431475394946725,True,True,True,1396514014000.0,1396536834000.0,1396540434000.0,1397041603000.0,1397042503000.0,1396514014000.0,-1.0,1396536817000.0,1396540417000.0,1396536828000.0,1396540428000.0,1396536834000.0,1396540434000.0,"mark.fisher",1396540417000.0,1396514014000.0,44,1396514014000.0,-1.0,"Add initial support for DeploymentManifest","The REST API for deploy should accept parameters, which provide manifest key/value pairs (e.g. ?http.instances=5). Ultimately we will want to support passing a named manifest which had been stored previously.

The 'stream deploy' shell command should support passing these as --options.

Initially we should support [modulename].instances and [modulename].group.",5.0,False,5.0,0,0.0,-1,2,False
"XD-1481","Story","Sprint 25","2014-04-03T06:47:05.000+0000","mark.pollack","Thomas Darimont","Remove spring-xd-analytics-ml-pmml project and module xml - to be put in seperate repository","AGPL license issues",2.0,0.5329935516571397,0.6598297337125372,0.1140784126000807,True,True,True,1396507625000.0,1396791548000.0,1396795148000.0,1397039420000.0,1397040320000.0,1396507625000.0,-1.0,1396568394000.0,1396571994000.0,1396859113000.0,1396862713000.0,1396791548000.0,1396795148000.0,"mark.pollack",1396571994000.0,1396507625000.0,44,1396507625000.0,-1.0,"Remove spring-xd-analytics-ml-pmml project and module xml - to be put in seperate repository","AGPL license issues",2.0,False,2.0,0,0.0,0,0,False
"XD-1480","Story","Sprint 26","2014-04-02T06:25:44.000+0000","mark.fisher","Mark Fisher","Merge Module.Type and ModuleType","Also likely rename, remove, or replace that Module (maybe can be supplanted by ModuleDescriptor when used in the refactored parser).

Also, considering the ""url"" property is not necessary (vestige of the prototype), all we'd be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself (e.g. getDescriptor(moduleKey)).",2.0,0.9995590608468472,0.9991610345219925,0.5257431694785067,True,True,True,1396419944000.0,1398958857000.0,1398959977000.0,1398959077000.0,1398959977000.0,1396419944000.0,-1.0,1397755349000.0,1397758949000.0,1398957846000.0,1398959977000.0,1398958857000.0,1398959977000.0,"mark.fisher",1397758949000.0,1396419944000.0,44,1396419944000.0,-1.0,"Merge Module.Type and ModuleType","Also likely rename, remove, or replace that Module (maybe can be supplanted by ModuleDescriptor when used in the refactored parser).

Also, considering the ""url"" property is not necessary (vestige of the prototype), all we'd be left with here is the Module name and type, which are used to identify a Module uniquely. Therefore this Module could be renamed to ModuleKey or something. It could be used within the StreamDefinition itself (e.g. getDescriptor(moduleKey)).",2.0,False,2.0,0,0.0,0,1,False
"XD-1479","Story","Sprint 26","2014-04-02T05:57:39.000+0000","mark.fisher","","DefaultContainerMatcher should make a better attempt at round-robin distribution","Currently the index is used globally but applied to a range of candidates that can differ based on the match criteria per invocation.",2.0,0.9942162467418162,0.9924461859110788,0.5673373536679052,True,True,True,1396418259000.0,1398763293000.0,1398766893000.0,1398776035000.0,1398776935000.0,1396418259000.0,-1.0,1397756424000.0,1397760024000.0,1398759118000.0,1398762718000.0,1398763293000.0,1398766893000.0,"mark.fisher",1397760024000.0,1396418259000.0,44,1396418259000.0,-1.0,"DefaultContainerMatcher should make a better attempt at round-robin distribution","Currently the index is used globally but applied to a range of candidates that can differ based on the match criteria per invocation.",2.0,False,2.0,0,0.0,0,0,False
"XD-1477","Story","Sprint 26","2014-04-02T05:40:11.000+0000","mark.fisher","","Consider merging Container and ContainerMetadata as well as their ""repositories""","The two classes in question are:
* org.springframework.xd.dirt.cluster.Container
* org.springframework.xd.dirt.container.ContainerMetadata

The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis/InMemory Repositories to use the data that is now available in ZooKeeper instead.

The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd-shell's 'runtime containers' command. Perhaps those can also be merged.

In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders (e.g. for attribute key/values such as ""group""==""foo"") might be convenient for various Module deployment strategies.",4.0,0.8866376998851886,0.3684259382561866,0.2006723424522609,True,True,True,1396417211000.0,1402329602000.0,1402333202000.0,1403084639000.0,1403085539000.0,1396417211000.0,-1.0,1397755360000.0,1397758960000.0,1398873996000.0,1398877596000.0,1402329602000.0,1402333202000.0,"mark.fisher",1397758960000.0,1396417211000.0,44,1396417211000.0,-1.0,"Merge Container and ContainerMetadata as well as their ""repositories""","The two classes in question are:
* org.springframework.xd.dirt.cluster.Container
* org.springframework.xd.dirt.container.ContainerMetadata

The former is currently used by the Admin when making decisions about Module deployment. That latter was a replacement for RuntimeContainerInforEntity as we migrated the various Redis/InMemory Repositories to use the data that is now available in ZooKeeper instead.

The ContainerRepository is currently used by the Admin leader, and the ContainerMetadataRepository is used by the REST endpoint that supports the xd-shell's 'runtime containers' command. Perhaps those can also be merged.

In any case, if not addressed by a larger refactoring, the ContainerRepository should probably support an Iterable return rather than an Iterator. Having finders (e.g. for attribute key/values such as ""group""==""foo"") might be convenient for various Module deployment strategies.",4.0,False,4.0,0,0.0,0,1,True
"XD-1476","Improvement","Sprint 26","2014-04-02T05:34:52.000+0000","mark.fisher","dturanski","Create an under-the-hood document for the distributed runtime","This should be included on the wiki, providing a thorough overview of how the distributed runtime works under-the-hood, including details of the various ZooKeeper nodes and associated watchers, etc. For example, it should describe the role of *any* Admin as a provider of the REST API as well as the specific roles allocated to the single *leader* Admin. It should describe Module recovery after Container failure, as well as some description of various failure scenarios (crashed JVM, network partition, etc) and how the recovery should be expected to occur (e.g. in some cases it will be nearly immediate, and in other cases it will be after a timed out connection). Intentional Container shutdown should be included in that discussion as well. At many points, this doc could link back to items that we also need to add to the main User Guide, such as the description of ContainerMatcher as the strategy for an Admin to determine which Container(s) should deploy a given Module, since that ContainerMatcher is used not only for the initial deployment, but also any redeployment that may be necessary.",6.0,0.2179114479175332,0.0197945099446907,0.1462446295001988,True,True,True,1396416892000.0,1398412909000.0,1398416509000.0,1405575754000.0,1405576654000.0,1396416892000.0,1402054863000.0,1397756458000.0,1397760058000.0,1396598205000.0,1396601805000.0,1398412909000.0,1398416509000.0,"mark.fisher",1397760058000.0,1396416892000.0,44,1402054863000.0,1402054863000.0,"Update document for the distributed runtime based on RC1 changes","This should be included on the wiki, providing a thorough overview of how the distributed runtime works under-the-hood, including details of the various ZooKeeper nodes and associated watchers, etc. For example, it should describe the role of *any* Admin as a provider of the REST API as well as the specific roles allocated to the single *leader* Admin. It should describe Module recovery after Container failure, as well as some description of various failure scenarios (crashed JVM, network partition, etc) and how the recovery should be expected to occur (e.g. in some cases it will be nearly immediate, and in other cases it will be after a timed out connection). Intentional Container shutdown should be included in that discussion as well. At many points, this doc could link back to items that we also need to add to the main User Guide, such as the description of ContainerMatcher as the strategy for an Admin to determine which Container(s) should deploy a given Module, since that ContainerMatcher is used not only for the initial deployment, but also any redeployment that may be necessary.",8.0,True,8.0,1,-25.0,0,0,True
"XD-1475","Story","Sprint 26","2014-04-02T05:23:18.000+0000","mark.fisher","","Improve Exception handling for ZooKeeper data access","Currently we have many catch(Exception) blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top-level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access.

Also, we should not be re-wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler (and although I'm typically hesitant to recommend it, this might be a case where a static util method is the right approach).
",8.0,0.7069723261392538,0.706965197122237,0.3294709630220345,True,True,True,1396416198000.0,1399292078000.0,1399295678000.0,1400483180000.0,1400484080000.0,1396416198000.0,-1.0,1397756447000.0,1397760047000.0,1399292049000.0,1399295649000.0,1399292078000.0,1399295678000.0,"mark.fisher",1397760047000.0,1396416198000.0,44,1396416198000.0,-1.0,"Improve Exception handling for ZooKeeper data access","Currently we have many catch(Exception) blocks that simply wrap and rethrow RuntimeExceptions. We should create at least a top-level RuntimeException of our own, within the XD Exception hierarchy, and possibly a hierarchy of RuntimeExceptions extending from that, and mapping to the various checked Exceptions that can occur in ZooKeeper data access.

Also, we should not be re-wrapping those Exceptions that are already RuntimeExceptions, so we should consider a ZooKeeperExceptionHandler (and although I'm typically hesitant to recommend it, this might be a case where a static util method is the right approach).
",8.0,False,8.0,0,0.0,-1,0,False
"XD-1472","Improvement","Sprint 27","2014-04-02T04:47:34.000+0000","prietzler","dturanski","Let modules define a default value for --inputType option","If a module e.g. always expects the payload of the message to be of a certain java type if would be good for documentation and convenience reasons in order to specify a default value for the --inputType option. 

documentation = output for module info
convenience = we could e.g. support to always accept a Json payload (or automatic message payload conversion once it is extensible)

currently, adding
options.inputType.default
to the module's property file has no effect

I've also tried to ""redefine"" it using
options.inputType.description
This leads to the following exception:

Command failed org.springframework.xd.rest.client.impl.SpringXDException: Module option named 'outputType' is present in several delegates: [org.springframework.xd.module.options.SimpleModuleOptionsMetadata@3c1d635a, FlattenedCompositeModuleOptionsMetadata
[outputType] => PojoModuleOptionsMetadata backed by class org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPluginMetadataResolver$OutputOptionsMetadata, defining options [[ModuleOption [name=outputType, type=class org.springframework.util.MimeType, defaultValue=null, description=how this module should emit messages it produces]]]
[inputType] => PojoModuleOptionsMetadata backed by class org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPluginMetadataResolver$InputOptionsMetadata, defining options [[ModuleOption [name=inputType, type=class org.springframework.util.MimeType, defaultValue=null, description=how this module should interpret messages it consumes]]]]",4.0,0.724347308943501,0.7056037219724751,0.7055165853007624,True,True,True,1396414054000.0,1398932824000.0,1398936424000.0,1399890450000.0,1399891350000.0,1396414054000.0,-1.0,1398867344000.0,1398870944000.0,1398867647000.0,1398871247000.0,1398932824000.0,1398936424000.0,"prietzler",1398870944000.0,1398839524000.0,44,1398839524000.0,-1.0,"Let modules define a default value for --inputType option","If a module e.g. always expects the payload of the message to be of a certain java type if would be good for documentation and convenience reasons in order to specify a default value for the --inputType option. 

documentation = output for module info
convenience = we could e.g. support to always accept a Json payload (or automatic message payload conversion once it is extensible)

currently, adding
options.inputType.default
to the module's property file has no effect

I've also tried to ""redefine"" it using
options.inputType.description
This leads to the following exception:

Command failed org.springframework.xd.rest.client.impl.SpringXDException: Module option named 'outputType' is present in several delegates: [org.springframework.xd.module.options.SimpleModuleOptionsMetadata@3c1d635a, FlattenedCompositeModuleOptionsMetadata
[outputType] => PojoModuleOptionsMetadata backed by class org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPluginMetadataResolver$OutputOptionsMetadata, defining options [[ModuleOption [name=outputType, type=class org.springframework.util.MimeType, defaultValue=null, description=how this module should emit messages it produces]]]
[inputType] => PojoModuleOptionsMetadata backed by class org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPluginMetadataResolver$InputOptionsMetadata, defining options [[ModuleOption [name=inputType, type=class org.springframework.util.MimeType, defaultValue=null, description=how this module should interpret messages it consumes]]]]",4.0,False,4.0,0,0.0,0,0,False
"XD-1471","Story","Sprint 24","2014-04-01T19:18:35.000+0000","mark.fisher","mark.fisher","Migrate repositories to ZooKeeper","all state about the running system (containers, streams, and jobs) should be available via ZK

ultimately the --store option should not be needed",10.0,0.0002095102697444,0.0001795802312095,8.979011560477385e-05,True,True,True,1396379915000.0,1396379943000.0,1396383543000.0,1396512660000.0,1396513560000.0,1396379915000.0,-1.0,1396379927000.0,1396383527000.0,1396379939000.0,1396383539000.0,1396379943000.0,1396383543000.0,"mark.fisher",1396383527000.0,1396379915000.0,44,1396379915000.0,-1.0,"Migrate repositories to ZooKeeper","all state about the running system (containers, streams, and jobs) should be available via ZK, and ultimately the --store option should not be needed

1. Refactor ModuleDefinitionRepository to use ZooKeeper
   * remove RedisModuleDefinitionRepository
   * remove InMemoryModuleDefinitionRepository

2. Refactor ModuleDependencyRepository to use ZooKeeper
   * remove RedisModuleDependencyRepository
   * remove InMemoryModuleDependencyRepository

3. Refactor RuntimeModuleInfoRepository to use ZooKeeper (rename ModuleMetadata...)
   * remove RedisRuntimeModuleInfoRepository
   * remove AbstractRedisRuntimeModuleInfoRepository
   * remove InMemoryRuntimeModuleInfoRepository

4. Refactor RuntimeContainerModuleInfoRepository to use ZooKeeper (rename ContainerMetadata...)
   * remove RedisRuntimeContainerModuleInfoRepository
   * remove InMemoryRuntimeContainerModuleInfoRepository

5. Remove support for --store
   * remove the memory-store.xml and the redis-store.xml
   * instead include just one repositories.xml in shared server config
   * remove the associated property key and the *Options properties

6. Remove the events and listeners that were being used",10.0,False,10.0,0,0.0,0,0,True
"XD-1470","Story","","2014-04-01T14:26:23.000+0000","mark.pollack","luke","Rationalize inclusion of JSON jars in xd/lib and in modules","To avoid CL conflicts, a short term solution to this problem is to make sure that there is only one copy of jackson related classes and have them live in xd/lib.  

using 
jackson-core-2.3.2.jar
jackson-databind-2.3.2.jar
jackson-core-asl-1.9.13.jar
jackson-mapper-asl-1.9.13.jar

are the latest versions and the ones to use.



There maybe other libraries that we duplicate in the same manner, we will need to investigate as well as part of another issue.",3.0,-1.0,0.0,-1.0,False,False,True,1396362383000.0,-1.0,-1.0,1396516499000.0,1396517399000.0,1396362383000.0,-1.0,-1.0,-1.0,1396362383000.0,1396365983000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1396362383000.0,-1.0,"Rationalize inclusion of JSON jars in xd/lib and in modules","To avoid CL conflicts, a short term solution to this problem is to make sure that there is only one copy of jackson related classes and have them live in xd/lib.  

using 
jackson-core-2.3.2.jar
jackson-databind-2.3.2.jar
jackson-core-asl-1.9.13.jar
jackson-mapper-asl-1.9.13.jar

are the latest versions and the ones to use.

There maybe other libraries that we duplicate in the same manner, we will need to investigate as well as part of another issue.

Typical error is:

java.lang.LinkageError: loader constraint violation: when resolving method ""org.springframework.http.converter.json.MappingJackson2HttpMessageConverter.setObjectMapper(Lcom/fasterxml/jackson/databind/ObjectMapper;)V"" the class loader (instance of org/springframework/xd/module/support/ParentLastURLClassLoader) of the current class, org/springframework/social/twitter/api/impl/TwitterTemplate, and the class loader (instance of sun/misc/Launcher$AppClassLoader) for resolved class, org/springframework/http/converter/json/MappingJackson2HttpMessageConverter, have different Class objects for the type com/fasterxml/jackson/databind/ObjectMapper used in the signature
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1469","Story","Sprint 25","2014-04-01T13:19:20.000+0000","mark.pollack","","Error in correlation strategy in aggregator.xml","	correlation-strategy-expression=""${correlation:'${xd.stream.name}}'""

should be

	correlation-strategy-expression=""${correlation:'${xd.stream.name}'}""


Add a test to make sure correlation expressions here work.",2.0,0.9736370770161556,0.9735771748962196,0.419416673156063,True,True,True,1396358360000.0,1396845974000.0,1396849574000.0,1396858277000.0,1396859177000.0,1396358360000.0,-1.0,1396568411000.0,1396572011000.0,1396845944000.0,1396849544000.0,1396845974000.0,1396849574000.0,"mark.pollack",1396572011000.0,1396358360000.0,44,1396358360000.0,-1.0,"Error in correlation strategy in aggregator.xml","{noformat}
	correlation-strategy-expression=""${correlation:'${xd.stream.name}}'""
{noformat}
should be
{noformat}
	correlation-strategy-expression=""${correlation:'${xd.stream.name}'}""
{noformat}
Add a test to make sure correlation expressions here work.",2.0,False,2.0,0,0.0,0,0,True
"XD-1468","Story","Sprint 24","2014-04-01T07:19:01.000+0000","mark.pollack","jbrisbin","Update to Reactor 1.1.0 M3","",1.0,-1.0,0.0,0.0,False,True,True,1396336741000.0,-1.0,-1.0,1396342241000.0,1396343141000.0,1396336741000.0,-1.0,1396336741000.0,1396340341000.0,1396336741000.0,1396340341000.0,-1.0,-1.0,"mark.pollack",1396340341000.0,1396336741000.0,44,1396336741000.0,-1.0,"Update to Reactor 1.1.0 M3","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1467","Story","","2014-04-01T05:16:53.000+0000","eric.bottard","eric.bottard","Do not eagerly use repositories in completion's *Strategy","",4.0,-1.0,0.0,-1.0,False,False,True,1396329413000.0,-1.0,-1.0,1396573736000.0,1396574636000.0,1396329413000.0,-1.0,-1.0,-1.0,1396329413000.0,1396333013000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1396329413000.0,-1.0,"Do not eagerly use repositories in completion's *Strategy","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1466","Story","Sprint 24","2014-04-01T00:18:32.000+0000","mark.pollack","Mark Pollack","Update XdEc2Validation to reference <root>/management endpoint","change 

""/jolokia/list"";

to 

""/management/jolokia/list"";

etc.",2.0,0.2343114935290828,0.9383461107710644,0.0,True,True,True,1396311512000.0,1396950038000.0,1396953638000.0,1399035728000.0,1399036628000.0,1396311512000.0,-1.0,1396311512000.0,1396315112000.0,1398868614000.0,1398872214000.0,1396950038000.0,1396953638000.0,"mark.pollack",1396315112000.0,1396311512000.0,44,1396311512000.0,-1.0,"Update XdEc2Validation to reference <root>/management endpoint","change 

""/jolokia/list"";

to 

""/management/jolokia/list"";

etc.",2.0,False,2.0,0,0.0,0,0,False
"XD-1465","Story","Sprint 24","2014-03-31T23:55:34.000+0000","mark.pollack","iperumal","Update management context path to <root>/management","",1.0,-1.0,0.0,0.0,False,False,False,1396310134000.0,-1.0,-1.0,1396310134000.0,1396310897000.0,1396310134000.0,-1.0,1396310134000.0,1396310897000.0,1396310134000.0,1396310897000.0,-1.0,-1.0,"mark.pollack",1396310897000.0,1396310134000.0,44,1396310134000.0,-1.0,"Update management context path to <root>/management","",1.0,False,1.0,0,0.0,0,0,False
"XD-1464","Story","Sprint 24","2014-03-31T21:51:51.000+0000","mark.pollack","mark.pollack","Upgrade to Spring Integration 4.0.0.M4","Currently on snapshots, which is oddly pulling in groovy 2.1.0",1.0,-1.0,0.0,0.0,False,True,True,1396302711000.0,-1.0,-1.0,1396336172000.0,1396337072000.0,1396302711000.0,-1.0,1396302711000.0,1396306311000.0,1396302711000.0,1396306311000.0,-1.0,-1.0,"mark.pollack",1396306311000.0,1396302711000.0,44,1396302711000.0,-1.0,"Upgrade to Spring Integration 4.0.0.M4","Currently on snapshots, which is oddly pulling in groovy 2.1.0",1.0,False,1.0,0,0.0,-1,0,False
"XD-1463","Story","Sprint 30","2014-03-31T12:55:17.000+0000","mark.pollack","liujiong","Delete post module and CF profile","This would get rid of the CF specific post module, keeping the general abstraction of 'http' source across CF and non-CF environments.",1.0,0.9761895534827918,0.9695896086935748,1.0057952741693166,True,True,True,1396270517000.0,1403625871000.0,1403629471000.0,1403804377000.0,1403805277000.0,1396270517000.0,-1.0,1403848943000.0,1403805277000.0,1403576142000.0,1403579742000.0,1403625871000.0,1403629471000.0,"mark.pollack",1403805277000.0,1402056961000.0,44,1402056961000.0,-1.0,"Delete post module and CF profile","This would get rid of the CF specific post module, keeping the general abstraction of 'http' source across CF and non-CF environments.",1.0,False,1.0,0,0.0,0,2,False
"XD-1462","Story","Sprint 32","2014-03-31T12:31:05.000+0000","mark.pollack","eric.bottard","Create test case that uses AggregateCounter with date-time fields.","AggregateCounter needs a test case along the lines of of org.springframework.xd.analytics.metrics.integration.FieldValueCounterHandlerTests that will demonstrate the base functionality of the module as well as use of the  timeField and dateFormat options.",3.0,0.9991259884586636,0.9542880486857412,0.9990403760658768,True,True,True,1396269065000.0,1405932109000.0,1405935709000.0,1405939662000.0,1405940562000.0,1396269065000.0,-1.0,1405931281000.0,1405934881000.0,1405498459000.0,1405502059000.0,1405932109000.0,1405935709000.0,"mark.pollack",1405934881000.0,1396269065000.0,44,1396269065000.0,-1.0,"Validate time field processing with AggregateCounter","AggregateCounter needs a test case along the lines of of org.springframework.xd.analytics.metrics.integration.FieldValueCounterHandlerTests that will demonstrate the base functionality of the module as well as use of the  timeField and dateFormat options.",3.0,False,3.0,0,0.0,0,0,True
"XD-1461","Story","Sprint 25","2014-03-31T11:38:00.000+0000","mark.pollack","jvalkeal","Use Hadoop mini-cluster test support in XD tests","The tests that use HDFS currently require an external Hadoop installation and is hard to set up/update version in all the environments where we want to run tests, e.g. bamboo, travis.

See if the mini-cluster described in 

http://docs.spring.io/spring-hadoop/docs/2.0.0.RC1/reference/html/testing.html#testing:yarn:minicluster

can be used in the test cases instead.",3.0,0.2035725791940805,5.876303543704407e-05,0.0134431333837338,True,True,True,1396265880000.0,1400845680000.0,1400849280000.0,1418762116000.0,1418763016000.0,1396265880000.0,-1.0,1396568312000.0,1396571912000.0,1396267202000.0,1396270802000.0,1400845680000.0,1400849280000.0,"mark.pollack",1396571912000.0,1396265880000.0,44,1396265880000.0,-1.0,"Use Hadoop mini-cluster test support in XD tests","The tests that use HDFS currently require an external Hadoop installation and is hard to set up/update version in all the environments where we want to run tests, e.g. bamboo, travis.

See if the mini-cluster described in 

http://docs.spring.io/spring-hadoop/docs/2.0.0.RC1/reference/html/testing.html#testing:yarn:minicluster

can be used in the test cases instead.",3.0,False,3.0,0,0.0,0,1,False
"XD-1460","Story","Sprint 24","2014-03-31T09:28:57.000+0000","iperumal","iperumal","Remove jmxEnabled as a cmdLine option and enable JMX by default","After some discussion and voting, we decided to remove ""jmxEnabled"" as a command line option and have JMX enabled by default.
This can be disabled from xd-config.yml externally.",3.0,0.0002639218791237,0.0,0.2524311265403902,True,True,True,1396258137000.0,1396258150000.0,1396261750000.0,1396306494000.0,1396307394000.0,1396258137000.0,-1.0,1396270571000.0,1396274171000.0,1396258137000.0,1396261737000.0,1396258150000.0,1396261750000.0,"iperumal",1396274171000.0,1396261780000.0,44,1396261780000.0,-1.0,"Remove jmxEnabled as a cmdLine option and enable JMX by default","After some discussion and voting, we decided to remove ""jmxEnabled"" as a command line option and have JMX enabled by default.
This can be disabled from xd-config.yml externally.",3.0,False,3.0,0,0.0,0,0,False
"XD-1459","Story","","2014-03-31T07:03:17.000+0000","eric.bottard","eric.bottard","Prevent accidental pickup of ENV var as module option","Typical case is with a module that contains
{noformat}
... user=""${username}"" ... />
{noformat}

(say, as part of a jdbc connection configuration)
and no value has been given at deployment time.

The module may pickup a value from the environment by mistake (typically from an environment variable of the same name).
This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.",5.0,-1.0,0.0,-1.0,False,False,True,1396249397000.0,-1.0,-1.0,1396567402000.0,1396568302000.0,1396249397000.0,-1.0,-1.0,-1.0,1396249397000.0,1396252997000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1396249397000.0,-1.0,"Prevent accidental pickup of ENV var as module option","Typical case is with a module that contains
{noformat}
... user=""${username}"" ... />
{noformat}

(say, as part of a jdbc connection configuration)
and no value has been given at deployment time.

The module may pickup a value from the environment by mistake (typically from an environment variable of the same name).
This was even more problematic when ordering of property sources was unclear, but should be prevented entirely anyway.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1458","Story","Sprint 24","2014-03-28T08:24:51.000+0000","mark.pollack","thomas.risberg","Upgrade to Spring Hadoop 2.0 RC2","",2.0,0.9597737644100008,0.0,0.0,True,True,True,1395995091000.0,1396416782000.0,1396420382000.0,1396433556000.0,1396434456000.0,1395995091000.0,-1.0,1395995091000.0,1395998691000.0,1395995091000.0,1395998691000.0,1396416782000.0,1396420382000.0,"mark.pollack",1395998691000.0,1395995091000.0,44,1395995091000.0,-1.0,"Upgrade to Spring Hadoop 2.0 RC2","",2.0,False,2.0,0,0.0,-1,0,False
"XD-1457","Story","","2014-03-28T07:28:24.000+0000","grenfro","grenfro","Remove the S3 XD Jar Cache","Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD.  And to allow for faster downloads.  However XD Jars are already placed on S3, so this feature is no longer needed.",3.0,-1.0,0.0,-1.0,False,False,True,1395991704000.0,-1.0,-1.0,1398424417000.0,1398425317000.0,1395991704000.0,-1.0,-1.0,-1.0,1395991704000.0,1395995304000.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1395991704000.0,-1.0,"Remove the S3 XD Jar Cache","Originally it was placed in the code to prevent overtasking our servers with downloads when people want to install XD.  And to allow for faster downloads.  However XD Jars are already placed on S3, so this feature is no longer needed.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1456","Story","Sprint 26","2014-03-28T07:13:51.000+0000","grenfro","grenfro","Allow user to configure tests with DI ","With the addition of sinks and sources that require connections with external entities (hadoop, JMS, JDBC, ...)  the environment setup is getting unwieldy.

* Integrate SpringJUnit4ClassRunner.class into acceptance tests.
* Retrieve environment variables via Dependency injection from application.properties.
* Utilize profiles for 
  --local single node
  --local cluster
  --ec2 single node
  --ec2 cluster",3.0,0.6244788513919092,0.0,0.6293924735798622,True,True,True,1395990831000.0,1397714698000.0,1397718298000.0,1398750420000.0,1398751320000.0,1395990831000.0,-1.0,1397728262000.0,1397731862000.0,1395990831000.0,1395994431000.0,1397714698000.0,1397718298000.0,"grenfro",1397731862000.0,1395990831000.0,44,1395990831000.0,-1.0,"Allow user to configure tests with DI ","With the addition of sinks and sources that require connections with external entities (hadoop, JMS, JDBC, ...)  the environment setup is getting unwieldy.

* Integrate SpringJUnit4ClassRunner.class into acceptance tests.
* Retrieve environment variables via Dependency injection from application.properties.
* Utilize profiles for 
  --local single node
  --local cluster
  --ec2 single node
  --ec2 cluster",3.0,False,3.0,0,0.0,0,3,False
"XD-1455","Story","Sprint 26","2014-03-28T07:11:05.000+0000","grenfro","grenfro","Environment checkers in acceptance tests should use Asserts","Replace tests and throws in the XdEc2Validation with asserts in the following methods:
verifyTestContent
verifyLogContent
verifySendCounts

Also verifySendCounts should check for the exact number of Jmx events instead of >0.",4.0,0.7849423338839143,0.0,0.6293999391463097,True,True,True,1395990665000.0,1398157671000.0,1398161271000.0,1398750485000.0,1398751385000.0,1395990665000.0,-1.0,1397728262000.0,1397731862000.0,1395990665000.0,1395994265000.0,1398157671000.0,1398161271000.0,"grenfro",1397731862000.0,1396160768000.0,44,1396160768000.0,-1.0,"Environment checkers in acceptance tests should use Asserts","Replace tests and throws in the XdEc2Validation with asserts in the following methods:
verifyTestContent
verifyLogContent
verifySendCounts

VerifySendCounts should check for the exact number of Jmx events instead of >0.",4.0,False,4.0,0,0.0,-1,2,True
"XD-1454","Story","","2014-03-28T06:22:16.000+0000","grenfro","grenfro","User should not be required to specify a control channel","With the addition of zookeeper, a user does not have to specify rabbit or redis for the control channel.  
This story should allow a user to specify redis, rabbit or no control channel.",2.0,1.817681955644016e-05,0.0,-1.0,True,False,True,1395987736000.0,1395987752000.0,1395991352000.0,1396867078000.0,1396867978000.0,1395987736000.0,-1.0,-1.0,-1.0,1395987736000.0,1395991336000.0,1395987752000.0,1395991352000.0,"grenfro",-1.0,-1.0,0,1395987736000.0,-1.0,"User should not be required to specify a control channel","With the addition of zookeeper, a user does not have to specify rabbit or redis for the control channel.  
This story should allow a user to specify redis, rabbit or no control channel.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1453","Story","","2014-03-27T14:38:48.000+0000","grussell","grussell","Update Spring-AMQP to 1.3.0.RELEASE","",1.0,-1.0,0.0,-1.0,False,False,True,1395931128000.0,-1.0,-1.0,1396566948000.0,1396567848000.0,1395931128000.0,-1.0,-1.0,-1.0,1395931128000.0,1395934728000.0,-1.0,-1.0,"grussell",-1.0,-1.0,0,1395931128000.0,-1.0,"Update Spring-AMQP to 1.3.1.RELEASE ","",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-1452","Story","Sprint 25","2014-03-27T07:19:07.000+0000","eric.bottard","luke","http module leaks threads","Attempts to create/use/undeploy a stream involving the http module will result in an OOME stating that VM could not create native thread.

Other modules should be checked as well.
{noformat}
	at java.lang.Thread.run(Thread.java:724)
Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.x.http.NettyHttpInboundChannelAdapter#0'; nested exception is java.lang.OutOfMemoryError: unable to create new native thread
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:176)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:346)
	at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:149)
	at org.springframework.context.support.DefaultLifecycleProcessor.start(DefaultLifecycleProcessor.java:91)
	at org.springframework.context.support.AbstractApplicationContext.start(AbstractApplicationContext.java:1180)
	at org.springframework.xd.module.core.SimpleModule.start(SimpleModule.java:273)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:251)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:239)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:179)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:150)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 80 more
Caused by: java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.$$YJP$$start0(Native Method)
	at java.lang.Thread.start0(Thread.java)
	at java.lang.Thread.start(Thread.java:693)
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
	at org.jboss.netty.util.internal.DeadLockProofWorker.start(DeadLockProofWorker.java:38)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:343)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:95)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:53)
	at org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:28)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.newWorker(AbstractNioWorkerPool.java:99)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:69)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:115)
	at org.springframework.integration.x.http.NettyHttpInboundChannelAdapter.doStart(NettyHttpInboundChannelAdapter.java:114)
	at org.springframework.integration.endpoint.AbstractEndpoint.start(AbstractEndpoint.java:84)
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:173)
	... 91 more
{noformat}
",5.0,0.9866728125951326,0.9521536596993664,0.9521536596993664,True,True,True,1395904747000.0,1396591862000.0,1396595462000.0,1396600243000.0,1396601143000.0,1395904747000.0,-1.0,1396567823000.0,1396571423000.0,1396567823000.0,1396571423000.0,1396591862000.0,1396595462000.0,"eric.bottard",1396571423000.0,1395904747000.0,44,1395904747000.0,-1.0,"http module leaks threads","Attempts to create/use/undeploy a stream involving the http module will result in an OOME stating that VM could not create native thread.

Other modules should be checked as well.
{noformat}
	at java.lang.Thread.run(Thread.java:724)
Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.x.http.NettyHttpInboundChannelAdapter#0'; nested exception is java.lang.OutOfMemoryError: unable to create new native thread
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:176)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:346)
	at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:149)
	at org.springframework.context.support.DefaultLifecycleProcessor.start(DefaultLifecycleProcessor.java:91)
	at org.springframework.context.support.AbstractApplicationContext.start(AbstractApplicationContext.java:1180)
	at org.springframework.xd.module.core.SimpleModule.start(SimpleModule.java:273)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:251)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:239)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:179)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:150)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 80 more
Caused by: java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.$$YJP$$start0(Native Method)
	at java.lang.Thread.start0(Thread.java)
	at java.lang.Thread.start(Thread.java:693)
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
	at org.jboss.netty.util.internal.DeadLockProofWorker.start(DeadLockProofWorker.java:38)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.openSelector(AbstractNioSelector.java:343)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.<init>(AbstractNioSelector.java:95)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.<init>(AbstractNioWorker.java:53)
	at org.jboss.netty.channel.socket.nio.NioWorker.<init>(NioWorker.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:45)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.createWorker(NioWorkerPool.java:28)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.newWorker(AbstractNioWorkerPool.java:99)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorkerPool.init(AbstractNioWorkerPool.java:69)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:39)
	at org.jboss.netty.channel.socket.nio.NioWorkerPool.<init>(NioWorkerPool.java:33)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:149)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:131)
	at org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory.<init>(NioServerSocketChannelFactory.java:115)
	at org.springframework.integration.x.http.NettyHttpInboundChannelAdapter.doStart(NettyHttpInboundChannelAdapter.java:114)
	at org.springframework.integration.endpoint.AbstractEndpoint.start(AbstractEndpoint.java:84)
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:173)
	... 91 more
{noformat}
",5.0,False,5.0,0,0.0,0,0,False
"XD-1451","Story","Sprint 24","2014-03-26T15:38:23.000+0000","mark.pollack","mark.pollack","Support CI build in Travis","Get minimal compilation going in Travis.  Redis/Rabbit services can be done in a separate story.",1.0,3.9234320439773136e-05,0.0,0.0,True,True,True,1395848303000.0,1395848321000.0,1395851921000.0,1396306185000.0,1396307085000.0,1395848303000.0,-1.0,1395848303000.0,1395851903000.0,1395848303000.0,1395851903000.0,1395848321000.0,1395851921000.0,"mark.pollack",1395851903000.0,1395848303000.0,44,1395848303000.0,-1.0,"Support CI build in Travis","Get minimal compilation going in Travis.  Redis/Rabbit services can be done in a separate story.",1.0,False,1.0,0,0.0,-1,0,False
"XD-1449","Story","Sprint 24","2014-03-25T19:49:29.000+0000","mark.pollack","mark.pollack","Update to Spring Shell 1.1 RC1","",2.0,0.9912442997794592,0.0,0.0,True,True,True,1395776969000.0,1396513635000.0,1396517235000.0,1396519242000.0,1396520142000.0,1395776969000.0,-1.0,1395776969000.0,1395780569000.0,1395776969000.0,1395780569000.0,1396513635000.0,1396517235000.0,"mark.pollack",1395780569000.0,1395776969000.0,44,1395776969000.0,-1.0,"Update to Spring Shell 1.1 RC1","",2.0,False,2.0,0,0.0,-1,0,False
"XD-1448","Improvement","Sprint 26","2014-03-25T13:44:55.000+0000","dbeauregard","mark.fisher","SpringXD logs error and large stack trace when metric can't be found. Distracting.","When a REST client of SpringXD (i.e., a dashboard) attempts to query (GET) a metric (e.g., counter, gauge, etc.) that does not exist the admin sever logs an ERROR and a large stack trace (attached).  In usage of Spring XD we see this frequently because a dashboard is running but the streams and counters have not been created quite yet, or initialized by messages flowing through the streams.  With a polling dashboard this results in a lot of distracting and large stack traces in the logs that are not actually issues.  

I would suggest logging a one line warning or info message instead of the error and stack trace. ",1.0,0.970006270776056,0.9698623640090724,0.9700016286222822,True,True,True,1395755095000.0,1398262568000.0,1398266168000.0,1398339202000.0,1398340102000.0,1395755095000.0,1398262579000.0,1398262556000.0,1398266156000.0,1398262196000.0,1398265796000.0,1398262568000.0,1398266168000.0,"dbeauregard",1398266156000.0,1398262579000.0,44,1398262579000.0,-1.0,"SpringXD logs error and large stack trace when metric can't be found. Distracting.","When a REST client of SpringXD (i.e., a dashboard) attempts to query (GET) a metric (e.g., counter, gauge, etc.) that does not exist the admin sever logs an ERROR and a large stack trace (attached).  In usage of Spring XD we see this frequently because a dashboard is running but the streams and counters have not been created quite yet, or initialized by messages flowing through the streams.  With a polling dashboard this results in a lot of distracting and large stack traces in the logs that are not actually issues.  

I would suggest logging a one line warning or info message instead of the error and stack trace. ",1.0,False,1.0,0,0.0,0,0,False
"XD-1447","Story","Sprint 24","2014-03-25T11:03:23.000+0000","mark.pollack","luke","Add documentation for a 'stdin' source module","In order to support ingestion from stdin, the suggested approach is to do the following.

xd:>stream create --definition ""tcp --decoder=LF | log"" --name foo

$ cat my.log | netcat localhost 1234

So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin/err in order to achieve the same goal.  It should appear as a source module in the docs on par with other source modules in its own section.
",2.0,-1.0,0.0,0.0,False,True,True,1395745403000.0,-1.0,-1.0,1396591050000.0,1396591950000.0,1395745403000.0,-1.0,1395745403000.0,1395749003000.0,1395745403000.0,1395749003000.0,-1.0,-1.0,"mark.pollack",1395749003000.0,1395745403000.0,44,1395745403000.0,-1.0,"Add documentation for a 'stdin' source module","In order to support ingestion from stdin, the suggested approach is to do the following.

xd:>stream create --definition ""tcp --decoder=LF | log"" --name foo

$ cat my.log | netcat localhost 1234

So while this is really a tcp based ingestion case, once can use pipe or redirect of stdin/err in order to achieve the same goal.  It should appear as a source module in the docs on par with other source modules in its own section.
",2.0,False,2.0,0,0.0,0,0,False
"XD-1446","Story","Sprint 24","2014-03-25T09:29:10.000+0000","mark.pollack","thomas.risberg","Update to Spring Hadoop 2.0 RC1","Update yarn deployment scripts as well.",8.0,0.0266028451246285,0.0,0.0,True,True,True,1395739750000.0,1395841029000.0,1395844629000.0,1399545924000.0,1399546824000.0,1395739750000.0,1395989083000.0,1395739750000.0,1395743350000.0,1395739750000.0,1395743350000.0,1395841029000.0,1395844629000.0,"mark.pollack",1395743350000.0,1395739750000.0,44,1395989083000.0,1395989083000.0,"Update spring-data-hadoop dependency and add new Hadoop distros","Update to Spring for Apache Hadoop 2.0 RC3
Add support for new hadoop distros: 
- Pivotal HD 2.0 (phd20)
- Hortonworks HDP 2.1 (hdp21)
- Cloudera CDH5 (cdh5) 
",3.0,True,3.0,1,166.66666666666669,0,0,True
"XD-1445","Story","Sprint 24","2014-03-25T09:02:47.000+0000","eric.bottard","eric.bottard","Don't swallow unexpected exceptions in StacktraceFingerprintingCompletionRecoveryStrategy","",4.0,-1.0,0.1740577232245387,0.1746463538078104,False,True,True,1395738167000.0,-1.0,-1.0,1396325072000.0,1396325972000.0,1395738167000.0,-1.0,1395840825000.0,1395844425000.0,1395840479000.0,1395844079000.0,-1.0,-1.0,"eric.bottard",1395844425000.0,1395738167000.0,44,1395738167000.0,-1.0,"Don't swallow unexpected exceptions in StacktraceFingerprintingCompletionRecoveryStrategy","",4.0,False,4.0,0,0.0,0,0,False
"XD-1444","Story","Sprint 24","2014-03-25T03:53:54.000+0000","mark.pollack","luke","Update to Spring Boot 1.0 GA","",2.0,-1.0,0.0,0.0,False,True,True,1395719634000.0,-1.0,-1.0,1396516387000.0,1396517287000.0,1395719634000.0,-1.0,1395719634000.0,1395723234000.0,1395719634000.0,1395723234000.0,-1.0,-1.0,"mark.pollack",1395723234000.0,1395719634000.0,44,1395719634000.0,-1.0,"Update to Spring Boot 1.0 GA","",2.0,False,2.0,0,0.0,0,0,False
"XD-1443","Story","Sprint 24","2014-03-25T03:53:07.000+0000","mark.pollack","luke","Update Spring Framework dependency to 4.0.3 GA","",2.0,-1.0,0.0,0.0,False,True,True,1395719587000.0,-1.0,-1.0,1396239886000.0,1396240786000.0,1395719587000.0,-1.0,1395719587000.0,1395723187000.0,1395719587000.0,1395723187000.0,-1.0,-1.0,"mark.pollack",1395723187000.0,1395719587000.0,44,1395719587000.0,-1.0,"Update Spring Framework dependency to 4.0.3 GA","",2.0,False,2.0,0,0.0,0,0,False
"XD-1442","Story","Sprint 30","2014-03-24T11:11:16.000+0000","iperumal","eric.bottard","Remove Hadoop distro Enum options","Please see the discussion here:

https://github.com/spring-projects/spring-xd/pull/655/files#r10892925",3.0,-1.0,0.9999915620890454,0.9999979849764884,False,True,True,1395659476000.0,-1.0,-1.0,1403598930000.0,1403599830000.0,1395659476000.0,-1.0,1403599814000.0,1403599830000.0,1403599763000.0,1403599830000.0,-1.0,-1.0,"iperumal",1403599830000.0,1395659476000.0,44,1395659476000.0,-1.0,"Remove Hadoop distro Enum options","Please see the discussion here:

https://github.com/spring-projects/spring-xd/pull/655/files#r10892925",3.0,False,3.0,0,0.0,-1,0,False
"XD-1441","Story","Sprint 24","2014-03-24T07:55:54.000+0000","eric.bottard","eric.bottard","Implement XD_MODULE_CONFIG_LOCATION & NAME","The description in the google doc 

https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit?usp=sharing

describes the usage of XD_MODULE_CONFIG_LOCATION and XD_MODULE_CONFIG_NAME",5.0,0.7273960270999003,0.2109103268689277,0.2096799940057922,True,True,True,1395647754000.0,1396317605000.0,1396321205000.0,1396567743000.0,1396568643000.0,1395647754000.0,-1.0,1395840846000.0,1395844446000.0,1395841979000.0,1395845579000.0,1396317605000.0,1396321205000.0,"eric.bottard",1395844446000.0,1395647754000.0,44,1395647754000.0,-1.0,"Implement XD_MODULE_CONFIG_LOCATION & NAME","The description in the google doc 

https://docs.google.com/a/gopivotal.com/document/d/12Cboa7nyVVKVxDIsHLJ-68m5f78ayXn14EJLrclJYVg/edit?usp=sharing

describes the usage of XD_MODULE_CONFIG_LOCATION and XD_MODULE_CONFIG_NAME",5.0,False,5.0,0,0.0,0,4,False
"XD-1439","Story","Sprint 24","2014-03-24T04:16:15.000+0000","eric.bottard","eric.bottard","Investigate module classloader leakage","See report at https://github.com/spring-projects/spring-xd/issues/661

This should not happen as the module holds the classes that hold the classloader, but who knows. An integration test that verifies this would be nice, albeit tricky.",5.0,-1.0,0.7063276470287496,0.7029158642785324,False,True,True,1395634575000.0,-1.0,-1.0,1395927070000.0,1395927970000.0,1395634575000.0,-1.0,1395840807000.0,1395844407000.0,1395841808000.0,1395845408000.0,-1.0,-1.0,"eric.bottard",1395844407000.0,1395634575000.0,44,1395634575000.0,-1.0,"Investigate module classloader leakage","See report at https://github.com/spring-projects/spring-xd/issues/661

This should not happen as the module holds the classes that hold the classloader, but who knows. An integration test that verifies this would be nice, albeit tricky.",5.0,False,5.0,0,0.0,0,0,False
"XD-1437","Story","Sprint 24","2014-03-21T15:28:34.000+0000","mark.pollack","grussell","Exclude slf4j Transitive Dependencies","",3.0,0.9603660860807844,0.0,0.0,True,True,True,1395415714000.0,1395655067000.0,1395658667000.0,1395664045000.0,1395664945000.0,1395415714000.0,-1.0,1395415714000.0,1395419314000.0,1395415714000.0,1395419314000.0,1395655067000.0,1395658667000.0,"mark.pollack",1395419314000.0,1395415714000.0,44,1395415714000.0,-1.0,"Exclude slf4j Transitive Dependencies","",3.0,False,3.0,0,0.0,-1,0,False
"XD-1436","Story","Sprint 24","2014-03-21T15:17:29.000+0000","mark.pollack","Gunnar Hillert","Misc cleanup in UI","0. Remove home page with sign in and upper right hand corner with user login info.
1. Change the word template to modules in the tab
2. Different text for each of the tabs, modules, definition, deployments, scheduled
3. Definitions tab to have text along the lines ""allows you to deploy  and undeploy batch job definitions"" add links to help on how to do that in the CLI.
4. Deployments tab
 a   creating new definitions, - parameters needs to be space on parameters,  Job Parameters for Job XYZ after clicking launch.
 b. comment out scheduler button
 c. add quick filter
5. Scheduler tab
 a. comment out tab
",5.0,0.761113475571017,0.7610888029459114,0.0,True,True,True,1395415049000.0,1395661837000.0,1395665437000.0,1395738395000.0,1395739295000.0,1395415049000.0,-1.0,1395415049000.0,1395418649000.0,1395661829000.0,1395665429000.0,1395661837000.0,1395665437000.0,"mark.pollack",1395418649000.0,1395415049000.0,44,1395415049000.0,-1.0,"Misc cleanup in UI","0. Remove home page with sign in and upper right hand corner with user login info.
1. Change the word template to modules in the tab
2. Different text for each of the tabs, modules, definition, deployments, scheduled
3. Definitions tab to have text along the lines ""allows you to deploy  and undeploy batch job definitions"" add links to help on how to do that in the CLI.
4. Deployments tab
 a   creating new definitions, - parameters needs to be space on parameters,  Job Parameters for Job XYZ after clicking launch.
 b. comment out scheduler button
 c. add quick filter
5. Scheduler tab
 a. comment out tab
",5.0,False,5.0,0,0.0,0,1,False
"XD-1435","Story","Sprint 24","2014-03-21T15:14:34.000+0000","mark.pollack","Gunnar Hillert","Improvements to Executions Tab","1. Add quick filter
2. The table should have columns for          
                                name | instance | execution id

Getting the name might require a bit of extra work given some limitations with JSON serialization and cycles in the current object returned from spring batch.

3. The restart action should appear only if the job is restartable and the status was failed.
",1.0,0.728135335228802,0.7278104861289434,0.0,True,True,True,1395414874000.0,1395650227000.0,1395653827000.0,1395737201000.0,1395738101000.0,1395414874000.0,1395654565000.0,1395414874000.0,1395418474000.0,1395650122000.0,1395653722000.0,1395650227000.0,1395653827000.0,"mark.pollack",1395418474000.0,1395414874000.0,44,1395654565000.0,1395654565000.0,"Improvements to Executions Tab","1. Add quick filter
2. The table should have columns for          
                                name | instance | execution id

Getting the name might require a bit of extra work given some limitations with JSON serialization and cycles in the current object returned from spring batch.

3. The restart action should appear only if the job is restartable and the status was failed.
",5.0,True,5.0,1,-80.0,0,0,False
"XD-1434","Story","Sprint 24","2014-03-21T15:11:15.000+0000","mark.pollack","hillert","Improvements to Modules Tab","1. Get listing of job modules
2. Remove version and action column
3. Text to say creating definitions from available modules in the UI is forthcoming, link to https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs#creating-a-job for how to do this in the command line.

 4. Hardcode an association between spring xd out of the box module names and a description.
 5. Add button to display the XML file that defines the job module
",8.0,0.3970244076919146,0.0,0.0,True,True,True,1395414675000.0,1395647772000.0,1395651372000.0,1396000885000.0,1396001785000.0,1395414675000.0,-1.0,1395414675000.0,1395418275000.0,1395414675000.0,1395418275000.0,1395647772000.0,1395651372000.0,"mark.pollack",1395418275000.0,1395414675000.0,44,1395414675000.0,-1.0,"Improvements to Modules Tab","1. Get listing of job modules
2. Remove version and action column
3. Text to say creating definitions from available modules in the UI is forthcoming, link to https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs#creating-a-job for how to do this in the command line.

 4. Hardcode an association between spring xd out of the box module names and a description.
 5. Add button to display the XML file that defines the job module
",8.0,False,8.0,0,0.0,0,0,False
"XD-1433","Story","Sprint 24","2014-03-21T15:02:26.000+0000","mark.pollack","luke","Update to Spring Boot RC5","",2.0,-1.0,0.0,0.0,False,True,True,1395414146000.0,-1.0,-1.0,1395421086000.0,1395421986000.0,1395414146000.0,-1.0,1395414146000.0,1395417746000.0,1395414146000.0,1395417746000.0,-1.0,-1.0,"mark.pollack",1395417746000.0,1395414146000.0,44,1395414146000.0,-1.0,"Update to Spring Boot RC5","",2.0,False,2.0,0,0.0,-1,0,False
"XD-1432","Story","Sprint 24","2014-03-21T13:22:19.000+0000","mark.pollack","iperumal","Configure servers to use VanillaHealthEndpoint","The standard SimpleHealthIndicator that boot performs a database test that fails in xd-container since it does not require the use of a database.  ",3.0,0.0048523440226783,0.0,0.0,True,True,True,1395408139000.0,1395409279000.0,1395412879000.0,1395642177000.0,1395643077000.0,1395408139000.0,-1.0,1395408139000.0,1395411739000.0,1395408139000.0,1395411739000.0,1395409279000.0,1395412879000.0,"mark.pollack",1395411739000.0,1395408139000.0,44,1395408139000.0,-1.0,"Configure servers to use VanillaHealthEndpoint","The standard SimpleHealthIndicator that boot performs a database test that fails in xd-container since it does not require the use of a database.  ",3.0,False,3.0,0,0.0,0,0,False
"XD-1431","Story","Sprint 25","2014-03-20T17:11:05.000+0000","iperumal","iperumal","Support multiple admin servers on a same host","By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ",5.0,4.439838288404284e-06,0.0,0.7540805285183498,True,True,True,1395335465000.0,1395335472000.0,1395339072000.0,1396911199000.0,1396912099000.0,1395335465000.0,1396568918000.0,1396524374000.0,1396527974000.0,1395335465000.0,1395339065000.0,1395335472000.0,1395339072000.0,"iperumal",1396527974000.0,1395335465000.0,44,1396568918000.0,1396568918000.0,"Support multiple admin servers on a same host","By default, XD admin server uses hsqldb as data source for batch job repository and uses the default port 9101, specific database location and dbname. This makes the existence of multiple admin server on a same host not possible. ",2.0,True,2.0,1,150.0,0,0,False
"XD-1430","Story","Sprint 26","2014-03-20T11:29:46.000+0000","grenfro","grenfro","Add JMS Acceptance Tests","",5.0,2.200964169036983e-06,0.0,0.590170445110766,True,True,True,1395314986000.0,1395314995000.0,1395318595000.0,1399403203000.0,1399404103000.0,1395314986000.0,-1.0,1397728262000.0,1397731862000.0,1395314986000.0,1395318586000.0,1395314995000.0,1395318595000.0,"grenfro",1397731862000.0,1395314986000.0,44,1395314986000.0,-1.0,"Add JMS Acceptance Tests","",5.0,False,5.0,0,0.0,0,0,False
"XD-1429","Improvement","Sprint 24","2014-03-20T09:58:28.000+0000","dturanski","dturanski","Create Shared Server Context","Create a Shared Server Context to the Container hierarchy. This is a child of Global Beans which contains beans that are only shared by the Admin and Container contexts but not Modules. The ZooKeeper components go here to support the ZK deployment architecture. The MessageBus also goes here to support a clean way to launch jobs from the Admin process. ",4.0,0.0509811886158362,0.0,0.9999501837453392,True,True,True,1395309508000.0,1395322812000.0,1395326412000.0,1395569567000.0,1395570467000.0,1395309508000.0,-1.0,1395570454000.0,1395570467000.0,1395309508000.0,1395313108000.0,1395322812000.0,1395326412000.0,"dturanski",1395570467000.0,1395309508000.0,44,1395309508000.0,-1.0,"Create Shared Server Context","Create a Shared Server Context to the Container hierarchy. This is a child of Global Beans which contains beans that are only shared by the Admin and Container contexts but not Modules. The ZooKeeper components go here to support the ZK deployment architecture. The MessageBus also goes here to support a clean way to launch jobs from the Admin process. ",4.0,False,4.0,0,0.0,-1,0,False
"XD-1428","Improvement","Sprint 24","2014-03-20T08:35:59.000+0000","iperumal","iperumal","Log Hadoop Distro and ZK client connect info on Container startup","It would be nice to display container config logging with the hadoop distro and zookeeper client connect being used when the container starts up.
",3.0,9.463057951563388e-05,0.0,0.0069487336291641,True,True,True,1395304559000.0,1395304652000.0,1395308252000.0,1396286428000.0,1396287328000.0,1395304559000.0,1395410855000.0,1395311388000.0,1395314988000.0,1395304559000.0,1395308159000.0,1395304652000.0,1395308252000.0,"iperumal",1395314988000.0,1395304559000.0,44,1395410855000.0,1395410855000.0,"Log Hadoop Distro and ZK client connect info on Container startup","It would be nice to display container config logging with the hadoop distro and zookeeper client connect being used when the container starts up.
",1.0,True,1.0,1,200.0,0,0,False
"XD-1427","Story","Sprint 24","2014-03-20T04:59:57.000+0000","mark.fisher","mark.fisher","Upgrade to ZooKeeper 3.4.6","3.4.6 was released on 2014.03.10:
http://zookeeper.apache.org/doc/r3.4.6/releasenotes.html

Especially relevant for us, they updated Netty from 3.2.2 to 3.6.6:
https://issues.apache.org/jira/browse/ZOOKEEPER-1715
",2.0,0.8577268073844512,1.4730276364535058e-05,3.191559878982596e-05,True,True,True,1395291597000.0,1396339716000.0,1396343316000.0,1396512670000.0,1396513570000.0,1395291597000.0,-1.0,1395291636000.0,1395295236000.0,1395291615000.0,1395295215000.0,1396339716000.0,1396343316000.0,"mark.fisher",1395295236000.0,1395291597000.0,44,1395291597000.0,-1.0,"Upgrade to ZooKeeper 3.4.6","3.4.6 was released on 2014.03.10:
http://zookeeper.apache.org/doc/r3.4.6/releasenotes.html

Especially relevant for us, they updated Netty from 3.2.2 to 3.6.6:
https://issues.apache.org/jira/browse/ZOOKEEPER-1715
",2.0,False,2.0,0,0.0,0,0,False
"XD-1425","Story","Sprint 24","2014-03-19T13:44:19.000+0000","iperumal","Ilayaperumal Gopinathan","Fix Karma unit tests in XD admin UI","The grunt build for karma unit tests is currently broken with requireJS support on the XD admin app. ",4.0,0.2172249967116699,0.1329347244765284,0.0502041980639212,True,True,True,1395236659000.0,1395912116000.0,1395915716000.0,1398345240000.0,1398346140000.0,1395236659000.0,1395650017000.0,1395392768000.0,1395396368000.0,1395650017000.0,1395653617000.0,1395912116000.0,1395915716000.0,"iperumal",1395396368000.0,1395236659000.0,44,1395650017000.0,1395650017000.0,"Fix existing Karma unit tests + Migrate E2E tests to Protractor","The grunt build for karma unit tests is currently broken with requireJS support on the XD admin app. ",3.0,True,3.0,1,33.33333333333333,0,0,True
"XD-1423","Story","Sprint 24","2014-03-19T05:18:18.000+0000","mark.pollack","eric.bottard","Create documentation for how module properties are resolved.","The ordering of the lookup should be described, in particular detail on how environment variables can overrride properties.

Some details will necessarily change based on outcome of current discussion, but the overall ordering is going to remain.",4.0,-1.0,0.0,0.0,False,True,True,1395206298000.0,-1.0,-1.0,1396325008000.0,1396325908000.0,1395206298000.0,-1.0,1395206298000.0,1395209898000.0,1395206298000.0,1395209898000.0,-1.0,-1.0,"mark.pollack",1395209898000.0,1395206298000.0,44,1395206298000.0,-1.0,"Create documentation for how module properties are resolved.","The ordering of the lookup should be described, in particular detail on how environment variables can overrride properties.

Some details will necessarily change based on outcome of current discussion, but the overall ordering is going to remain.",4.0,False,4.0,0,0.0,0,0,False
"XD-1422","Story","Sprint 24","2014-03-19T03:56:56.000+0000","mark.pollack","thomasd","Create a simple sample application for the jppml module","The sample application should primarily show the 'round trip' that is possible in that ""offline"" analysis in R can generate a pmml flle that can be imported and evaluated in an ""online"" stream definition.  The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo.

The sample application resides in https://github.com/spring-projects/spring-xd-samples",4.0,0.7162064058216748,0.0,0.0,True,True,True,1395201416000.0,1396518463000.0,1396522063000.0,1397039437000.0,1397040337000.0,1395201416000.0,-1.0,1395201416000.0,1395205016000.0,1395201416000.0,1395205016000.0,1396518463000.0,1396522063000.0,"mark.pollack",1395205016000.0,1395201416000.0,44,1395201416000.0,-1.0,"Create a simple sample application for the jpmml module","The sample application should primarily show the 'round trip' that is possible in that ""offline"" analysis in R can generate a pmml flle that can be imported and evaluated in an ""online"" stream definition.  The specific use case can be as simple as the IRIS data set or other existing examples, such as the fraud demo.

The sample application resides in https://github.com/spring-projects/spring-xd-samples",4.0,False,4.0,0,0.0,-1,1,True
"XD-1421","Story","Sprint 24","2014-03-19T03:54:39.000+0000","mark.pollack","thomasd","Create documentation for the core analtyical model abstractions and use of jppml processor","",5.0,0.7162336899663523,0.0,0.0,True,True,True,1395201279000.0,1396518470000.0,1396522070000.0,1397039431000.0,1397040331000.0,1395201279000.0,-1.0,1395201279000.0,1395204879000.0,1395201279000.0,1395204879000.0,1396518470000.0,1396522070000.0,"mark.pollack",1395204879000.0,1395201279000.0,44,1395201279000.0,-1.0,"Create documentation for the core analytical model abstractions and use of jpmml processor","",5.0,False,5.0,0,0.0,-1,0,True
"XD-1420","Story","Sprint 24","2014-03-19T03:52:28.000+0000","mark.pollack","thomasd","Create a jppml module that will evaluate a model.","A analytical model should be evaluated as a processor in a stream.  The model evaluation will take the input variables from a Tuple and output variable will be placed into the tuple as well.

A strawman of the stream definition can be 

stream create --definition "" SOURCE | jpmml fraud-detection | PROC1  | PROCN "" --name stream1

Using profiles and playing all implementations of the analytical model in the same module lib directory, it maybe possible to select one of multiple implementations in the form

 "" SOURCE | analytic --library=jpmml --name=fraud-detection | PROC1  | PROCN ""

such that the core module name is the same but parameterized by what library type to use.  This may be problematic in that different libraries may have incompatible dependencies.

The analytical model can define the names of input and output fields, so at a minimum a name is required, however to easily adapt a given analytic model evaluation to a specific source modules output, it seems desirable to specify which fields are to be used as input, overriding the names of the input fields could be done in a manner such as 

jpmml name=linear-regresssion inputFields=a,b,c ",8.0,0.8126642071031177,0.0,0.0,True,True,True,1395201148000.0,1395669449000.0,1395673049000.0,1395776502000.0,1395777402000.0,1395201148000.0,-1.0,1395201148000.0,1395204748000.0,1395201148000.0,1395204748000.0,1395669449000.0,1395673049000.0,"mark.pollack",1395204748000.0,1395201148000.0,44,1395201148000.0,-1.0,"Create a JPMML module that will evaluate a model.","A analytical model should be evaluated as a processor in a stream.  The model evaluation will take the input variables from a Tuple and output variable will be placed into the tuple as well.

A strawman of the stream definition can be 

stream create --definition "" SOURCE | jpmml fraud-detection | PROC1  | PROCN "" --name stream1

Using profiles and playing all implementations of the analytical model in the same module lib directory, it maybe possible to select one of multiple implementations in the form

 "" SOURCE | analytic --library=jpmml --name=fraud-detection | PROC1  | PROCN ""

such that the core module name is the same but parameterized by what library type to use.  This may be problematic in that different libraries may have incompatible dependencies.

The analytical model can define the names of input and output fields, so at a minimum a name is required, however to easily adapt a given analytic model evaluation to a specific source modules output, it seems desirable to specify which fields are to be used as input, overriding the names of the input fields could be done in a manner such as 

jpmml name=linear-regresssion inputFields=a,b,c ",8.0,False,8.0,0,0.0,0,0,True
"XD-1419","Story","Sprint 24","2014-03-19T03:42:22.000+0000","mark.pollack","thomasd","Create subproject spring-xd-machine-learning-analytics-jppml","Using the jppml evaluator, provide an implementation of the core abstractions in the spring-xd-machine-learning.

The initial code for this has been developed in a separate github repo and is located here

https://github.com/thomasdarimont/spring-xd/tree/feature/advanced-analytics-support/spring-xd-analytics-jpmml/src/main/java/org/springframework/xd/analytics/model/jpmml ",4.0,0.8128847383995575,0.0,0.0,True,True,True,1395200542000.0,1395669457000.0,1395673057000.0,1395776495000.0,1395777395000.0,1395200542000.0,-1.0,1395200542000.0,1395204142000.0,1395200542000.0,1395204142000.0,1395669457000.0,1395673057000.0,"mark.pollack",1395204142000.0,1395200542000.0,44,1395200542000.0,-1.0,"Create subproject spring-xd-machine-learning-analytics-jpmml","Using the jppml evaluator, provide an implementation of the core abstractions in the spring-xd-machine-learning.

The initial code for this has been developed in a separate github repo and is located here

https://github.com/thomasdarimont/spring-xd/tree/feature/advanced-analytics-support/spring-xd-analytics-jpmml/src/main/java/org/springframework/xd/analytics/model/jpmml ",4.0,False,4.0,0,0.0,-1,0,True
"XD-1418","Story","Sprint 24","2014-03-19T03:37:44.000+0000","mark.pollack","thomasd","Create subproject spring-xd-machine-learning-analytics","This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations.  

The initial code for this has been developed in a separate github repo and is located here 

https://github.com/thomasdarimont/spring-xd/tree/feature/advanced-analytics-support/spring-xd-analytics/src/main/java/org/springframework/xd/analytics/model

The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple.  Note, it maybe useful to consider Message<Tuple> in case any metadata outside the core 'input data' is required to help guide the evaluation.

The build.gradle file should be updated such that there is a new build artifact spring-xd-machine-learning-analytics.jar along the lines of our other build artifacts.  Open to other naming suggestions.",8.0,0.8129632902575192,0.0003448144413139,0.0,True,True,True,1395200264000.0,1395669443000.0,1395673043000.0,1395776486000.0,1395777386000.0,1395200264000.0,-1.0,1395200264000.0,1395203864000.0,1395200463000.0,1395204063000.0,1395669443000.0,1395673043000.0,"mark.pollack",1395203864000.0,1395200264000.0,44,1395200264000.0,-1.0,"Create subproject spring-xd-machine-learning-analytics","This project contains core abstractions that will allow for multiple implementations of a machine learning algorithm to be implemented via integration with various existing libraries or custom code implementations.  

The initial code for this has been developed in a separate github repo and is located here 

https://github.com/thomasdarimont/spring-xd/tree/feature/advanced-analytics-support/spring-xd-analytics/src/main/java/org/springframework/xd/analytics/model

The model can assume its use in evaluation of the model inside a stream where the data structure is a Tuple.  Note, it maybe useful to consider Message<Tuple> in case any metadata outside the core 'input data' is required to help guide the evaluation.

The build.gradle file should be updated such that there is a new build artifact spring-xd-machine-learning-analytics.jar along the lines of our other build artifacts.  Open to other naming suggestions.",8.0,False,8.0,0,0.0,0,0,False
"XD-1417","Story","Sprint 24","2014-03-19T03:26:00.000+0000","mark.pollack","kparikh","Create RPM for distribution","Package SpringXD into an RPM
install path = /opt/pivotal/spring-xd-1.0.0.M5
with symlink /opt/pivotal/spring-xd -> current version
init.d scripts to start/stop/status
service springxd-admin start|stop|status
service springxd-container start|stop|status
user/group = springxd/pivotal
Host springxd rpm in Pivotal repo
yum install springxd
Support RHEL/CentOS version 5 and 6? (tested on latest updates)
Support for 32 and 64 bits
Support Java 1.6 and 1.7
",8.0,0.4305510912393864,0.0,0.0,True,True,True,1395199560000.0,1395676416000.0,1395680016000.0,1396306208000.0,1396307108000.0,1395199560000.0,-1.0,1395199560000.0,1395203160000.0,1395199560000.0,1395203160000.0,1395676416000.0,1395680016000.0,"mark.pollack",1395203160000.0,1395199560000.0,44,1395199560000.0,-1.0,"Create RPM for distribution","Package SpringXD into an RPM
install path = /opt/pivotal/spring-xd-1.0.0.M5
with symlink /opt/pivotal/spring-xd -> current version
init.d scripts to start/stop/status
service springxd-admin start|stop|status
service springxd-container start|stop|status
user/group = springxd/pivotal
Host springxd rpm in Pivotal repo
yum install springxd
Support RHEL/CentOS version 5 and 6? (tested on latest updates)
Support for 32 and 64 bits
Support Java 1.6 and 1.7
",8.0,False,8.0,0,0.0,-1,0,False
"XD-1416","Story","Sprint 26","2014-03-18T17:11:35.000+0000","cblack","mark.fisher","When there are no wiretap listeners don't publish messages","Being able to listen to a stream at any point has a significant performance impact.  The reason for the impact is the message needs to be ""serialized + transported + deserialized"" to other members even if there is no one listening.  This ""serialized + transported + deserialized"" processes happens for each step in a flow - source | process | sink.

Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data.  Likewise we need to deregister the listener if the wiretap is deleted.",8.0,0.6152416813245848,0.0010714766244263,0.4199427236326887,True,True,True,1395162695000.0,1398921407000.0,1398925007000.0,1401271121000.0,1401272021000.0,1395162695000.0,1397745682000.0,1397728262000.0,1397731862000.0,1395169241000.0,1395172841000.0,1398921407000.0,1398925007000.0,"cblack",1397731862000.0,1395162695000.0,44,1397745682000.0,1397745682000.0,"When there are no wiretap listeners don't publish messages","Being able to listen to a stream at any point has a significant performance impact.  The reason for the impact is the message needs to be ""serialized + transported + deserialized"" to other members even if there is no one listening.  This ""serialized + transported + deserialized"" processes happens for each step in a flow - source | process | sink.

Recommend creating some kind of protocol for wiretaps that allows members to know if there is someone listening in the grid so they will emit the data.  Likewise we need to deregister the listener if the wiretap is deleted.",1.0,True,1.0,1,700.0,0,0,False
"XD-1413","Story","Sprint 24","2014-03-18T11:03:56.000+0000","grenfro","grenfro","Create spring-xd-test-fixtures project","Acceptance tests has a direct library reference to spring-xd-shell.  This causes problems with eclipse.  It needs a intermediate main project to resolve the dependencies.  This is based on the conversation I had with Mark Fisher.

you cant depend on another projects' src/test
what is needed is an intermediate project some test support project
that project would depend on spring-xd-shell (and others)  but then the spring-xd-integration-test project would depend on the intermediate one
lib dependencies should only be under src/main
src/test is intended to be scoped to the project it sits in tests FOR that project - not reusable base classes, etc",4.0,1.7577363664280626e-05,0.0,0.5447080244800978,True,True,True,1395140636000.0,1395140653000.0,1395144253000.0,1396106889000.0,1396107789000.0,1395140636000.0,-1.0,1395667452000.0,1395671052000.0,1395140636000.0,1395144236000.0,1395140653000.0,1395144253000.0,"grenfro",1395671052000.0,1395667444000.0,44,1395667444000.0,-1.0,"Create spring-xd-test-fixtures project","Acceptance tests has a direct library reference to spring-xd-shell.  This causes problems with eclipse.  It needs a intermediate main project to resolve the dependencies.  This is based on the conversation I had with Mark Fisher.

you cant depend on another projects' src/test
what is needed is an intermediate project some test support project
that project would depend on spring-xd-shell (and others)  but then the spring-xd-integration-test project would depend on the intermediate one
lib dependencies should only be under src/main
src/test is intended to be scoped to the project it sits in tests FOR that project - not reusable base classes, etc",4.0,False,4.0,0,0.0,-1,1,False
"XD-1412","Story","","2014-03-18T09:31:44.000+0000","eric.bottard","","Composed options does not trigger profile activation","I was in the process of rewriting transform using profiles (see ExpressionOrScriptMixin).

This broke eg ModuleCommandTests.testComposedModulesValuesInDefinition because basically composed module options activate no profile.

The problem is that there is no real way to know what to activate currently, because when deploying a part of a composed module, its metadata is actually a link to the *whole* metadata, but does not really know which part.

Long story short, something we may be able to do is to activate the union of all profiles, but this breaks very easily :
module compose foo --definition ""transform --expr=foo | transform --script=bar"" 
would try to activate both ""script"" and ""expression"" profiles for both modules.",5.0,-1.0,-1.0,-1.0,False,False,False,1395135104000.0,-1.0,-1.0,1397726209000.0,1397727109000.0,1395135104000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1395135104000.0,-1.0,"Composed options does not trigger profile activation","I was in the process of rewriting transform using profiles (see ExpressionOrScriptMixin).

This broke eg ModuleCommandTests.testComposedModulesValuesInDefinition because basically composed module options activate no profile.

The problem is that there is no real way to know what to activate currently, because when deploying a part of a composed module, its metadata is actually a link to the *whole* metadata, but does not really know which part.

Long story short, something we may be able to do is to activate the union of all profiles, but this breaks very easily :
module compose foo --definition ""transform --expr=foo | transform --script=bar"" 
would try to activate both ""script"" and ""expression"" profiles for both modules.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1411","Story","Sprint 24","2014-03-18T09:24:23.000+0000","thomas.risberg","thomas.risberg","Create xd-yarn script","Create an xd-yarn script that is more ""Cloud Foundry"" like - 

xd-yarn push -p <path-to-unzipped-yearn-distro>
xd-yarn start admin
xd-yarn start container
",5.0,0.022988172877547,0.022988172877547,0.1289747124306416,True,True,True,1395134663000.0,1395138632000.0,1395142232000.0,1395306417000.0,1395307317000.0,1395134663000.0,-1.0,1395156931000.0,1395160531000.0,1395138632000.0,1395142232000.0,1395138632000.0,1395142232000.0,"thomas.risberg",1395160531000.0,1395134663000.0,44,1395134663000.0,-1.0,"Create xd-yarn script","Create an xd-yarn script that is more ""Cloud Foundry"" like - 

xd-yarn push -p <path-to-unzipped-yearn-distro>
xd-yarn start admin
xd-yarn start container
",5.0,False,5.0,0,0.0,-1,0,False
"XD-1410","Story","Sprint 24","2014-03-17T14:28:39.000+0000","grenfro","grenfro","XD EC2 needs to bootstrap ZOOKEEPER at installation time.","Startup zookeeper on EC2 cluster instances.",5.0,2.655403820536008e-05,0.0,0.8882089743797789,True,True,True,1395066519000.0,1395066537000.0,1395070137000.0,1395743482000.0,1395744382000.0,1395066519000.0,-1.0,1395668603000.0,1395672203000.0,1395066519000.0,1395070119000.0,1395066537000.0,1395070137000.0,"grenfro",1395672203000.0,1395066519000.0,44,1395066519000.0,-1.0,"XD EC2 needs to bootstrap ZOOKEEPER at installation time.","Startup zookeeper on EC2 cluster instances.",5.0,False,5.0,0,0.0,0,0,False
"XD-1408","Bug","","2014-03-17T12:34:22.000+0000","iperumal","","Container ID is not equal to its application context ID","The ContainerServerApplication has a bean ContainerMetaData whose ID is equal to the ContainerServerApplication context's ID. 

The ContainerMetadata is alway referred (using @Autowired) in ContainerConfiguration class and this injects a new bean whose containerID is different from that of the containerID created at ContainerServerApplication. Also, this is the ID that gets stored as the container ID in znode for /xd/containers by ContainerRegistrar.

We should avoid having duplicate container metadata and the container ID needs to be same as that of the ContainerServerApplication's context ID.",3.0,-1.0,0.2106438199425111,-1.0,False,False,True,1395059662000.0,-1.0,-1.0,1395094595000.0,1395095495000.0,1395059662000.0,-1.0,-1.0,-1.0,1395067210000.0,1395070810000.0,-1.0,-1.0,"iperumal",-1.0,-1.0,0,1395059662000.0,-1.0,"Container ID is not equal to its application context ID","The ContainerServerApplication has a bean ContainerMetaData whose ID is equal to the ContainerServerApplication context's ID. 

The ContainerMetadata is alway referred (using @Autowired) in ContainerConfiguration class and this injects a new bean whose containerID is different from that of the containerID created at ContainerServerApplication. Also, this is the ID that gets stored as the container ID in znode for /xd/containers by ContainerRegistrar.

We should avoid having duplicate container metadata and the container ID needs to be same as that of the ContainerServerApplication's context ID.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1407","Story","Sprint 24","2014-03-17T11:06:23.000+0000","mark.pollack","jbrisbin","Create a throughput sink","The throughput module would expect a payload of the type Message<byte[]> and look for the byte[] to be START or STOP strings to trigger a throughput measurement.

https://github.com/spring-projects/spring-xd/tree/master/extensions would be the place for the module to live.",5.0,-1.0,6.386066763425254e-05,0.9953671988388968,False,True,True,1395054383000.0,-1.0,-1.0,1395225733000.0,1395226633000.0,1395054383000.0,-1.0,1395225835000.0,1395226633000.0,1395054394000.0,1395057994000.0,-1.0,-1.0,"mark.pollack",1395226633000.0,1395054383000.0,44,1395054383000.0,-1.0,"Create a throughput sink","The throughput module would expect a payload of the type Message<byte[]> and look for the byte[] to be START or STOP strings to trigger a throughput measurement.

https://github.com/spring-projects/spring-xd/tree/master/extensions would be the place for the module to live.",5.0,False,5.0,0,0.0,-1,2,False
"XD-1406","Story","","2014-03-17T09:18:32.000+0000","iperumal","iperumal","Upgrade groovy version to 2.2.2","Currently, XD hast ""testCompile"" dependency to use org.codehaus.groovy:groovy-all:2.2.1. 

The spring-integration-groovy uses ""2.2.2"" and it is what spring IO platform uses as well. To keep it all same, we can change this ""testCompile"" dependency to use ""2.2.2""",1.0,0.9992183004549056,0.0,-1.0,True,False,True,1395047912000.0,1395139947000.0,1395140019000.0,1395139119000.0,1395140019000.0,1395047912000.0,-1.0,-1.0,-1.0,1395047912000.0,1395051512000.0,1395139947000.0,1395140019000.0,"iperumal",-1.0,-1.0,0,1395047912000.0,-1.0,"Upgrade groovy version to 2.2.2","Currently, XD hast ""testCompile"" dependency to use org.codehaus.groovy:groovy-all:2.2.1. 

The spring-integration-groovy uses ""2.2.2"" and it is what spring IO platform uses as well. To keep it all same, we can change this ""testCompile"" dependency to use ""2.2.2""",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1404","Story","Sprint 24","2014-03-17T06:05:47.000+0000","mark.pollack","Glenn renfro","Test against Spring Boot Snapshot build","",4.0,-1.0,0.9999890655923872,0.1775185455363405,False,True,True,1395036347000.0,-1.0,-1.0,1395675628000.0,1395676528000.0,1395036347000.0,-1.0,1395149991000.0,1395153591000.0,1395676521000.0,1395676528000.0,-1.0,-1.0,"mark.pollack",1395153591000.0,1395036347000.0,44,1395036347000.0,-1.0,"Test against Spring Boot Snapshot build","",4.0,False,4.0,0,0.0,0,0,False
"XD-1403","Story","Sprint 24","2014-03-14T12:25:47.000+0000","mark.pollack","jbrisbin","Create benchmarking application to demonstrate high performance message processing","The application should live in  in spring-xd-samples repository.

The stream created in https://jira.spring.io/browse/XD-1402 should be documented how to run a benchmark and made easy to execute.  Can use ruby/bash-awk-sed to generate traffic via sendfile in order to saturate the stream.
",10.0,0.6891924682291849,0.0,0.2814113045323784,True,True,True,1394799947000.0,1395842332000.0,1395845932000.0,1396311520000.0,1396312420000.0,1394799947000.0,-1.0,1395225574000.0,1395229174000.0,1394799947000.0,1394803547000.0,1395842332000.0,1395845932000.0,"mark.pollack",1395229174000.0,1394799947000.0,44,1394799947000.0,-1.0,"Create benchmarking application to demonstrate high performance message processing","The application should live in  in spring-xd-samples repository.

The stream created in https://jira.spring.io/browse/XD-1402 should be documented how to run a benchmark and made easy to execute.  Can use ruby/bash-awk-sed to generate traffic via sendfile in order to saturate the stream.
",10.0,False,10.0,0,0.0,-1,2,False
"XD-1402","Story","Sprint 24","2014-03-14T12:18:52.000+0000","mark.pollack","jbrisbin","Create a throughput-sampler module for benchmarking","A module that could be used in a stream definition such as 

reactor --bind tcp://0.0.0.0:3000/length?codec=bytes | do-stuff | throughput-sampler

where throughput-sampler could start measurements once a key 'START' is found in a Message and stop when the key 'STOP' is found in a Message, listing the number of msgs/sec etc.",4.0,-1.0,0.0,0.9975088151419086,False,True,True,1394799532000.0,-1.0,-1.0,1395225738000.0,1395226638000.0,1394799532000.0,-1.0,1395225574000.0,1395226638000.0,1394799532000.0,1394803132000.0,-1.0,-1.0,"mark.pollack",1395226638000.0,1394799532000.0,44,1394799532000.0,-1.0,"Create a throughput-sampler module for benchmarking","A module that could be used in a stream definition such as 

reactor --bind tcp://0.0.0.0:3000/length?codec=bytes | do-stuff | throughput-sampler

where throughput-sampler could start measurements once a key 'START' is found in a Message and stop when the key 'STOP' is found in a Message, listing the number of msgs/sec etc.",4.0,False,4.0,0,0.0,0,0,False
"XD-1401","Story","Sprint 24","2014-03-14T12:13:28.000+0000","mark.pollack","jbrisbin","Add new reactor tcp module","A reactor based TCP module that would support some basic CODECS.

Should evaluate if this new TCP module would subsume the current reactor-syslog module functionality or if the reactor-syslog module should be enhanced/upgradted.",8.0,-1.0,0.0,0.9975340414580506,False,True,True,1394799208000.0,-1.0,-1.0,1395225728000.0,1395226628000.0,1394799208000.0,-1.0,1395225574000.0,1395226628000.0,1394799208000.0,1394802808000.0,-1.0,-1.0,"mark.pollack",1395226628000.0,1394799208000.0,44,1394799208000.0,-1.0,"Add new reactor tcp module","A reactor based TCP module that would support some basic CODECS.

Should evaluate if this new TCP module would subsume the current reactor-syslog module functionality or if the reactor-syslog module should be enhanced/upgradted.",8.0,False,8.0,0,0.0,-1,1,False
"XD-1400","Story","Sprint 23","2014-03-14T11:08:47.000+0000","mark.fisher","mark.fisher","Containers should listen for Module deployment requests and their deletions","The Admin leader will write each Module deployment request to a child node of /xd/deployments for a selected Container (see XD-1399). That Container-specific (persistent) child node needs to be created by the Container at the same time as it creates its ephemeral node under /xd/containers.

The Container should then deploy the Module. If that same node is subsequently deleted, the Container should undeploy the Module.",8.0,0.0109159097813675,0.0,0.0,True,True,True,1394795327000.0,1394814083000.0,1394817683000.0,1396512653000.0,1396513553000.0,1394795327000.0,-1.0,1394795327000.0,1394798927000.0,1394795327000.0,1394798927000.0,1394814083000.0,1394817683000.0,"mark.fisher",1394798927000.0,1394795327000.0,44,1394795327000.0,-1.0,"Containers should listen for Module deployment requests and their deletions","The Admin leader will write each Module deployment request to a child node of /xd/deployments for a selected Container (see XD-1399). That Container-specific (persistent) child node needs to be created by the Container at the same time as it creates its ephemeral node under /xd/containers.

The Container should then deploy the Module. If that same node is subsequently deleted, the Container should undeploy the Module.",8.0,False,8.0,0,0.0,-1,1,False
"XD-1399","Story","Sprint 23","2014-03-14T11:04:03.000+0000","mark.fisher","mark.fisher","Admin leader should watch ZooKeeper for Stream deployment requests","The Stream deployment requests will be written to /xd/streams/streamname and the data on that node will include the state or a boolean indicator (for now) of whether it should be deployed.

When a Stream is deployed, the leader will consult its Container cache and write the modules to the various /xd/deployments child nodes (see XD-1400).",8.0,0.4512319306939335,0.0,0.0,True,True,True,1394795043000.0,1395570495000.0,1395574095000.0,1396512665000.0,1396513565000.0,1394795043000.0,-1.0,1394795043000.0,1394798643000.0,1394795043000.0,1394798643000.0,1395570495000.0,1395574095000.0,"mark.fisher",1394798643000.0,1394795043000.0,44,1394795043000.0,-1.0,"Admin leader should watch ZooKeeper for Stream deployment requests","The Stream deployment requests will be written to /xd/streams/streamname and the data on that node will include the state or a boolean indicator (for now) of whether it should be deployed.

When a Stream is deployed, the leader will consult its Container cache and write the modules to the various /xd/deployments child nodes (see XD-1400).",8.0,False,8.0,0,0.0,-1,0,False
"XD-1398","Story","Sprint 23","2014-03-14T11:01:54.000+0000","mark.fisher","mark.fisher","Admin servers should write streams to and delete them from ZooKeeper","This should also enable removal of any StreamDefinitionRepository code.

The state should be written as a data node at the stream level (e.g. /xd/streams/mystream {state=...})

For now we at least need to support the boolean --deploy=true|false flag. If that is true, then the leader Admin will deploy the modules of the stream across available containers (XD-1399)",8.0,0.4512752569773436,0.0,0.0,True,True,True,1394794914000.0,1395570491000.0,1395574091000.0,1396512648000.0,1396513548000.0,1394794914000.0,-1.0,1394794914000.0,1394798514000.0,1394794914000.0,1394798514000.0,1395570491000.0,1395574091000.0,"mark.fisher",1394798514000.0,1394794914000.0,44,1394794914000.0,-1.0,"Admin servers should write streams to and delete them from ZooKeeper","This should also enable removal of any StreamDefinitionRepository code.

The state should be written as a data node at the stream level (e.g. /xd/streams/mystream {state=...})

For now we at least need to support the boolean --deploy=true|false flag. If that is true, then the leader Admin will deploy the modules of the stream across available containers (XD-1399)",8.0,False,8.0,0,0.0,-1,1,False
"XD-1397","Story","Sprint 23","2014-03-14T10:56:41.000+0000","mark.fisher","mark.fisher","Admin leader should watch Container nodes in ZooKeeper","This will require a ZooKeeperConnection in AdminServerApplication, based on singlenode vs. distributed (via profiles, see: ContainerServerApplication for an example).

Then a PathChildrenCache should be established for the /xd/containers node.",4.0,0.0113232507689327,0.0,0.0,True,True,True,1394794601000.0,1394814065000.0,1394817665000.0,1396512642000.0,1396513542000.0,1394794601000.0,-1.0,1394794601000.0,1394798201000.0,1394794601000.0,1394798201000.0,1394814065000.0,1394817665000.0,"mark.fisher",1394798201000.0,1394794601000.0,44,1394794601000.0,-1.0,"Admin leader should watch Container nodes in ZooKeeper","This will require a ZooKeeperConnection in AdminServerApplication, based on singlenode vs. distributed (via profiles, see: ContainerServerApplication for an example).

Then a PathChildrenCache should be established for the /xd/containers node.",4.0,False,4.0,0,0.0,-1,1,False
"XD-1396","Story","","2014-03-13T10:29:38.000+0000","grenfro","","Container fails to start if JMX is enabled and manage_port is set","   The container will not start with JMX enabled and the management_port set.  The stacktrace is attached and the settings for the container are enumerated below:

    export endpoints_jmx_enabled=true
    export endpoints_jmx_uniqueNames=true
    export endpoints_jolokia_enabled=true
    export XD_JMX_ENABLED=true
    export management_port=15005",5.0,0.6718630231306336,0.6545240826015258,-1.0,True,False,True,1394706578000.0,1395059812000.0,1395063412000.0,1395231431000.0,1395232331000.0,1394706578000.0,-1.0,-1.0,-1.0,1395050696000.0,1395054296000.0,1395059812000.0,1395063412000.0,"grenfro",-1.0,-1.0,0,1394706578000.0,-1.0,"Container fails to start if JMX is enabled and manage_port is set","   The container will not start with JMX enabled and the management_port set.  The stacktrace is attached and the settings for the container are enumerated below:

    export endpoints_jmx_enabled=true
    export endpoints_jmx_uniqueNames=true
    export endpoints_jolokia_enabled=true
    export XD_JMX_ENABLED=true
    export management_port=15005",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1395","Story","","2014-03-13T10:20:27.000+0000","grenfro","","Container and Single Node do not update their associated log","After the container/singlenode is up and running, the application does not update the singlenode.log or containernode.log.

create a stream ""time|log"".  you will see the timestamp in your console, but it will not be in the log.",5.0,-1.0,-1.0,-1.0,False,False,False,1394706027000.0,-1.0,-1.0,1395840345000.0,1395841245000.0,1394706027000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1394706027000.0,-1.0,"Container and Single Node do not update their associated log","After the container/singlenode is up and running, the application does not update the singlenode.log or containernode.log.

create a stream ""time|log"".  you will see the timestamp in your console, but it will not be in the log.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1394","Story","Sprint 23","2014-03-13T05:39:53.000+0000","mark.fisher","","Upgrade to Spring 4.0.2.RELEASE","",1.0,-1.0,0.023147282870331,0.0,False,True,True,1394689193000.0,-1.0,-1.0,1395041423000.0,1395042323000.0,1394689193000.0,-1.0,1394689193000.0,1394692793000.0,1394697367000.0,1394700967000.0,-1.0,-1.0,"mark.fisher",1394692793000.0,1394689193000.0,44,1394689193000.0,-1.0,"Upgrade to Spring 4.0.2.RELEASE","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1393","Story","Sprint 23","2014-03-12T07:29:44.000+0000","mark.fisher","dturanski","Support explicit client connect string for ZooKeeper","This will have an effect on both singlenode and distributed mode as 
described below...

*singlenode:*

Currently, when running in singlenode mode, an embedded ZooKeeper server is started, and it discovers an available port for client connections and exposes it via a getClientPort() method.

With this change, IF a client connect string is provided explicitly, the embedded ZooKeeper server should not be started, but instead the client connection should be established using that explicitly provided connect string. SEE the ""todo"" comment above the `SpringApplicationBuilder admin` line within `SingleNodeApplication`.

*distributed:*

Currently, when running in distributed mode, the client connect string is hardcoded to `localhost:2181`. Obviously that is only a short term solution since a real-world distributed runtime would span multiple machines (including the ZooKeeper ensemble itself). That is also why the connect string must support a comma-delimited list of `host:port` values (for redundancy), but the constructor for `ZooKeeperConnection` that accepts a client connect string already supports that via the underlying `CuratorFramework` builder.

With this change, a command-line argument for the client connect string will be passed into the `ZooKeeperConnection` constructor, within the configuration (e.g. SEE the ""todo"" statement within `ContainerServerApplication.DistributedZooKeeperConnectionConfiguration`).",4.0,0.1335479503699271,0.1315241780527218,0.1315115645133486,True,True,True,1394609384000.0,1394704673000.0,1394708273000.0,1395322003000.0,1395322903000.0,1394609384000.0,-1.0,1394703220000.0,1394706820000.0,1394703229000.0,1394706829000.0,1394704673000.0,1394708273000.0,"mark.fisher",1394706820000.0,1394609384000.0,44,1394609384000.0,-1.0,"Support explicit client connect string for ZooKeeper","This will have an effect on both singlenode and distributed mode as 
described below...

*singlenode:*

Currently, when running in singlenode mode, an embedded ZooKeeper server is started, and it discovers an available port for client connections and exposes it via a getClientPort() method.

With this change, IF a client connect string is provided explicitly, the embedded ZooKeeper server should not be started, but instead the client connection should be established using that explicitly provided connect string. SEE the ""todo"" comment above the `SpringApplicationBuilder admin` line within `SingleNodeApplication`.

*distributed:*

Currently, when running in distributed mode, the client connect string is hardcoded to `localhost:2181`. Obviously that is only a short term solution since a real-world distributed runtime would span multiple machines (including the ZooKeeper ensemble itself). That is also why the connect string must support a comma-delimited list of `host:port` values (for redundancy), but the constructor for `ZooKeeperConnection` that accepts a client connect string already supports that via the underlying `CuratorFramework` builder.

With this change, a command-line argument for the client connect string will be passed into the `ZooKeeperConnection` constructor, within the configuration (e.g. SEE the ""todo"" statement within `ContainerServerApplication.DistributedZooKeeperConnectionConfiguration`).",4.0,False,4.0,0,0.0,0,0,False
"XD-1389","Bug","Sprint 27","2014-03-11T13:35:36.000+0000","thomas.risberg","grussell","Sometimes getting NPE when master step runs for ftphdfs job","Depending in the ftp server used there seems to be an error condition that generates an NullPointerException. 

These are the steps to reproduce this:

{code}
job create --name myftphdfs --definition ""ftphdfs --host=ftp.sunet.se --port=21""
job launch --name myftphdfs --params {""remoteDirectory"":""/pub/music/Abba"",""hdfsDirectory"":""/xd/ftp""}
{code}

Exception:
{code}
16:31:38,385 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 step.AbstractStep:225 - Encountered an error executing the step
java.lang.NullPointerException
	at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:140)
	at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:105)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:144)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:163)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:142)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy40.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)
	at org.springframework.expression.spel.ast.MethodReference.access$100(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:85)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:113)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:163)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.xd.dirt.plugins.job.JobPlugin.launch(JobPlugin.java:176)
	at org.springframework.xd.dirt.module.ModuleDeployer.launchModule(ModuleDeployer.java:380)
	at org.springframework.xd.dirt.module.ModuleDeployer.processLaunchRequest(ModuleDeployer.java:330)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleLaunch(ModuleDeployer.java:316)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:169)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:724)
{code}",5.0,0.9982980330257296,0.8416048762067783,0.9797037775099195,True,True,True,1394544936000.0,1398949381000.0,1398952981000.0,1398955990000.0,1398956890000.0,1394544936000.0,-1.0,1398867344000.0,1398870944000.0,1398258058000.0,1398261658000.0,1398949381000.0,1398952981000.0,"thomas.risberg",1398870944000.0,1394544936000.0,44,1394544936000.0,-1.0,"Sometimes getting NPE when master step runs for ftphdfs job","Depending in the ftp server used there seems to be an error condition that generates an NullPointerException. 

These are the steps to reproduce this:

{code}
job create --name myftphdfs --definition ""ftphdfs --host=ftp.sunet.se --port=21""
job launch --name myftphdfs --params {""remoteDirectory"":""/pub/music/Abba"",""hdfsDirectory"":""/xd/ftp""}
{code}

Exception:
{code}
16:31:38,385 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 step.AbstractStep:225 - Encountered an error executing the step
java.lang.NullPointerException
	at org.springframework.batch.integration.partition.MessageChannelPartitionHandler.handle(MessageChannelPartitionHandler.java:140)
	at org.springframework.batch.core.partition.support.PartitionStep.doExecute(PartitionStep.java:105)
	at org.springframework.batch.core.step.AbstractStep.execute(AbstractStep.java:198)
	at org.springframework.batch.core.job.SimpleStepHandler.handleStep(SimpleStepHandler.java:144)
	at org.springframework.batch.core.job.flow.JobFlowExecutor.executeStep(JobFlowExecutor.java:64)
	at org.springframework.batch.core.job.flow.support.state.StepState.handle(StepState.java:67)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.resume(SimpleFlow.java:163)
	at org.springframework.batch.core.job.flow.support.SimpleFlow.start(SimpleFlow.java:142)
	at org.springframework.batch.core.job.flow.FlowJob.doExecute(FlowJob.java:134)
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:304)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy40.run(Unknown Source)
	at org.springframework.batch.integration.launch.JobLaunchingMessageHandler.launch(JobLaunchingMessageHandler.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:63)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:122)
	at org.springframework.expression.spel.ast.MethodReference.access$100(MethodReference.java:44)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:258)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:85)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:113)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:111)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:163)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:268)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:142)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:75)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:71)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:260)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:241)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:205)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:199)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:177)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.xd.dirt.plugins.job.JobPlugin.launch(JobPlugin.java:176)
	at org.springframework.xd.dirt.module.ModuleDeployer.launchModule(ModuleDeployer.java:380)
	at org.springframework.xd.dirt.module.ModuleDeployer.processLaunchRequest(ModuleDeployer.java:330)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleLaunch(ModuleDeployer.java:316)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:169)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:724)
{code}",5.0,False,5.0,0,0.0,0,1,False
"XD-1388","Story","Sprint 23","2014-03-11T13:11:56.000+0000","mark.pollack","","Upgrade to Spring Batch 3.0.0 M3 and Spring Framework 4.0.2","",2.0,-1.0,0.6242962277400944,0.1032835263506135,False,True,True,1394543516000.0,-1.0,-1.0,1394788967000.0,1394789867000.0,1394543516000.0,-1.0,1394568960000.0,1394572560000.0,1394697312000.0,1394700912000.0,-1.0,-1.0,"mark.pollack",1394572560000.0,1394543516000.0,44,1394543516000.0,-1.0,"Upgrade to Spring Batch 3.0.0 M3","",2.0,False,2.0,0,0.0,-1,0,True
"XD-1387","Story","Sprint 23","2014-03-11T12:09:22.000+0000","mark.pollack","","Add documentation on how to deploy XD to YARN","",5.0,0.2827055010534283,0.2827055010534283,0.034289770014254,True,True,True,1394539762000.0,1394779548000.0,1394783148000.0,1395387045000.0,1395387945000.0,1394539762000.0,-1.0,1394568846000.0,1394572446000.0,1394779548000.0,1394783148000.0,1394779548000.0,1394783148000.0,"mark.pollack",1394572446000.0,1394539762000.0,44,1394539762000.0,-1.0,"Add documentation on how to deploy XD to YARN","",5.0,False,5.0,0,0.0,0,0,False
"XD-1386","Story","Sprint 23","2014-03-11T12:05:09.000+0000","mark.pollack","","Add option to xd-admin and xd-container YARN scripts to allow copying ot HDFS and no execution.","",4.0,0.5464225417354893,0.5464225417354893,0.0001950398917528,True,True,True,1394539509000.0,1394629160000.0,1394632760000.0,1394702678000.0,1394703578000.0,1394539509000.0,-1.0,1394539541000.0,1394543141000.0,1394629160000.0,1394632760000.0,1394629160000.0,1394632760000.0,"mark.pollack",1394543141000.0,1394539509000.0,44,1394539509000.0,-1.0,"Add option to xd-admin and xd-container YARN scripts to allow copying ot HDFS and no execution.","",4.0,False,4.0,0,0.0,-1,0,False
"XD-1385","Story","Sprint 23","2014-03-11T09:07:24.000+0000","grenfro","grenfro","Add JDBC, JMS, File, Tail Sinks and sources to acceptance tests","jdbc, JMS, file, tail",10.0,0.0698672311070492,0.0,0.0253257154148845,True,True,True,1394528844000.0,1394639158000.0,1394642758000.0,1396106853000.0,1396107753000.0,1394528844000.0,-1.0,1394568831000.0,1394572431000.0,1394528844000.0,1394532444000.0,1394639158000.0,1394642758000.0,"grenfro",1394572431000.0,1394568795000.0,44,1394568795000.0,-1.0,"Add JDBC Sink to acceptance tests","",10.0,False,10.0,0,0.0,-1,1,True
"XD-1384","Bug","","2014-03-10T15:20:23.000+0000","thomas.risberg","","Most if not all OOTB batch jobs seem broken in distributed mode","Tried deploying some bartch jobs and they all seem to fail when running admin and one container using redis as transport

xd:>job create mongojob --definition ""hdfsmongodb --resources=/data/*.log --names=col1,col2,col3 --idField=col1 --collectionName=test""

fails with this:
18:19:01,612  WARN redisInboundAdapter-redis:queue-inbound-channel-adapter6 boot.SpringApplication:635 - Error handling failed (Error creating bean with name 'integrationRequestMappingHandlerMapping': Initialization of bean failed; nested exception is java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@25e7506f has not been refreshed yet)
18:19:01,614 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter6 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [moduleDeployer]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)
	at org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:724)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'readResourcesStep': Cannot create inner bean '(inner bean)' of type [org.springframework.batch.core.listener.StepListenerFactoryBean] while setting bean property 'listeners' with key [0]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name '(inner bean)#4': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: interface org.springframework.batch.core.StepListener is not visible from class loader
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:282)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:126)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:351)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:154)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1456)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1197)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:681)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:616)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:306)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:225)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:270)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:260)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:201)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:172)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 18 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name '(inner bean)#4': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: interface org.springframework.batch.core.StepListener is not visible from class loader
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:151)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:110)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:272)
	... 41 more
Caused by: java.lang.IllegalArgumentException: interface org.springframework.batch.core.StepListener is not visible from class loader
	at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:487)
	at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:722)
	at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:121)
	at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:111)
	at org.springframework.aop.framework.ProxyFactory.getProxy(ProxyFactory.java:98)
	at org.springframework.batch.core.listener.AbstractListenerFactoryBean.getObject(AbstractListenerFactoryBean.java:163)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:144)
	... 43 more
",8.0,0.444748476524357,0.9999129950750038,-1.0,True,False,True,1394464823000.0,1395839887000.0,1395843487000.0,1397555702000.0,1397556602000.0,1394464823000.0,-1.0,-1.0,-1.0,1397556333000.0,1397556602000.0,1395839887000.0,1395843487000.0,"thomas.risberg",-1.0,-1.0,0,1394464823000.0,-1.0,"Improve job launch functionality with distributed nodes","When sending a launch request, the message is not targeted to the container node that hosts the deployed job.  With RabbitMQ, the message is not ack'd so it will get picked up eventually by the container that hosts the deployed job.  This should change to a targeted message.

-----
Original description from Thomas below

Tried deploying some batch jobs and they all seem to fail when running admin and one container using redis as transport

xd:>job create mongojob --definition ""hdfsmongodb --resources=/data/*.log --names=col1,col2,col3 --idField=col1 --collectionName=test""

fails with this:
{quote}
18:19:01,612  WARN redisInboundAdapter-redis:queue-inbound-channel-adapter6 boot.SpringApplication:635 - Error handling failed (Error creating bean with name 'integrationRequestMappingHandlerMapping': Initialization of bean failed; nested exception is java.lang.IllegalStateException: org.springframework.context.annotation.AnnotationConfigApplicationContext@25e7506f has not been refreshed yet)
18:19:01,614 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter6 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [moduleDeployer]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:84)
	at org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:170)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	at org.springframework.integration.dispatcher.AbstractDispatcher.tryOptimizedDispatch(AbstractDispatcher.java:128)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:101)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:97)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:255)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:223)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:98)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:211)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:50)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:290)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:724)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'readResourcesStep': Cannot create inner bean '(inner bean)' of type [org.springframework.batch.core.listener.StepListenerFactoryBean] while setting bean property 'listeners' with key [0]; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name '(inner bean)#4': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: interface org.springframework.batch.core.StepListener is not visible from class loader
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:282)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:126)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveManagedList(BeanDefinitionValueResolver.java:351)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:154)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1456)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1197)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:537)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:681)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:616)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:306)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)
	at org.springframework.xd.module.core.SimpleModule.initialize(SimpleModule.java:225)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:270)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:260)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:201)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:172)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:78)
	... 18 more
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name '(inner bean)#4': FactoryBean threw exception on object creation; nested exception is java.lang.IllegalArgumentException: interface org.springframework.batch.core.StepListener is not visible from class loader
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:151)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.getObjectFromFactoryBean(FactoryBeanRegistrySupport.java:110)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:272)
	... 41 more
Caused by: java.lang.IllegalArgumentException: interface org.springframework.batch.core.StepListener is not visible from class loader
	at java.lang.reflect.Proxy.getProxyClass0(Proxy.java:487)
	at java.lang.reflect.Proxy.newProxyInstance(Proxy.java:722)
	at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:121)
	at org.springframework.aop.framework.JdkDynamicAopProxy.getProxy(JdkDynamicAopProxy.java:111)
	at org.springframework.aop.framework.ProxyFactory.getProxy(ProxyFactory.java:98)
	at org.springframework.batch.core.listener.AbstractListenerFactoryBean.getObject(AbstractListenerFactoryBean.java:163)
	at org.springframework.beans.factory.support.FactoryBeanRegistrySupport.doGetObjectFromFactoryBean(FactoryBeanRegistrySupport.java:144)
	... 43 more
{quote}

I'll post more errors as I collect them",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1383","Story","Sprint 23","2014-03-10T11:04:23.000+0000","grenfro","grenfro","XD needs Acceptance tests for basic Sources","Need to be able to test the following sources:
TCP, HTTP, Time,",5.0,9.756021416418214e-05,6.634094563164385e-05,0.3119039070914568,True,True,True,1394449463000.0,1394449488000.0,1394453088000.0,1394704815000.0,1394705715000.0,1394449463000.0,-1.0,1394529389000.0,1394532989000.0,1394449480000.0,1394453080000.0,1394449488000.0,1394453088000.0,"grenfro",1394532989000.0,1394449463000.0,44,1394449463000.0,-1.0,"Add acceptance tests for stream with sources of TCP, HTTP, and Time and sinks of File and Log","Need to be able to test the following sources:
TCP, HTTP, Time,",5.0,False,5.0,0,0.0,0,1,True
"XD-1379","Story","Sprint 23","2014-03-07T06:58:50.000+0000","mark.fisher","iperumal","Avoid false negative test failures related to HSQLDB","When executing tests and failures occur (should be easy to simulate with a forced failure), several other side-effect failures occur with the following error:

{code}
Caused by: java.lang.IllegalArgumentException: HSQLDB could not be started. Maybe another instance is already running on 0.0.0.0:13583 ?
{code}

It seems that the failing tests are not properly cleaning up, so the HSQLDB instance they started is still running thereby causing the other failures.

In the case I last witnessed this on a dev branch, I had a valid failure in LocalControlRedisDataInitializationTests (the environmentMatchesTransport method, specifically) and TypeConvertingStreamTests. So, perhaps a good way to start exploring this issue is to simulate a failure in one or both of those classes. The false negatives I'm seeing are in the following:

{code}
LocalSingleNodeInitializationTests.environmentMatchesTransport
RabbitSingleNodeInitializationTests.environmentMatchesTransport
RedisSingleNodeInitializationTests.environmentMatchesTransport
FileSourceModuleTests.classMethod
LocalSingleNodeStreamDeploymentIntegrationTests.classMethod
RabbitSingleNodeStreamDeploymentIntegrationTests.classMethod
RabbitSingleNodeStreamDeploymentIntegrationTests.classMethod
{code}

They all seem to be tests that start the SingleNodeApplication.",4.0,0.0191805905241474,0.0189288166628519,0.0190890363927672,True,True,True,1394175530000.0,1394178044000.0,1394181644000.0,1394305700000.0,1394306600000.0,1394175530000.0,-1.0,1394178032000.0,1394181632000.0,1394178011000.0,1394181611000.0,1394178044000.0,1394181644000.0,"mark.fisher",1394181632000.0,1394175530000.0,44,1394175530000.0,-1.0,"Avoid false negative test failures related to HSQLDB","When executing tests and failures occur (should be easy to simulate with a forced failure), several other side-effect failures occur with the following error:

{code}
Caused by: java.lang.IllegalArgumentException: HSQLDB could not be started. Maybe another instance is already running on 0.0.0.0:13583 ?
{code}

It seems that the failing tests are not properly cleaning up, so the HSQLDB instance they started is still running thereby causing the other failures.

In the case I last witnessed this on a dev branch, I had a valid failure in LocalControlRedisDataInitializationTests (the environmentMatchesTransport method, specifically) and TypeConvertingStreamTests. So, perhaps a good way to start exploring this issue is to simulate a failure in one or both of those classes. The false negatives I'm seeing are in the following:

{code}
LocalSingleNodeInitializationTests.environmentMatchesTransport
RabbitSingleNodeInitializationTests.environmentMatchesTransport
RedisSingleNodeInitializationTests.environmentMatchesTransport
FileSourceModuleTests.classMethod
LocalSingleNodeStreamDeploymentIntegrationTests.classMethod
RabbitSingleNodeStreamDeploymentIntegrationTests.classMethod
RabbitSingleNodeStreamDeploymentIntegrationTests.classMethod
{code}

They all seem to be tests that start the SingleNodeApplication.",4.0,False,4.0,0,0.0,0,0,False
"XD-1378","Story","Sprint 23","2014-03-07T06:24:39.000+0000","mark.pollack","eric.bottard","Change module placeholder names and remove context:property-placeholder usage","e.g. rabbit.xml source.

	<context:property-placeholder
location=""${xd.config.home}/${configProperties:rabbit}.properties""
ignore-resource-not-found=""true"" />

would be removed and

	<rabbit:connection-factory id=""rabbitConnectionFactory"" host=""${host:${spring.rabbitmq.host:localhost}}""
port=""${port:${spring.rabbitmq.port:5672}}"" virtual-host=""${vhost:${spring.rabbitmq.virtualHost:/}}""
username=""${username:${spring.rabbitmq.username:guest}}"" password=""${password:${spring.rabbitmq.password:guest}}""/>

would look like
port=""${port}"" and a property file in the module directory or POJO in the module lib would specify the default value of the port.

For POJO it would be
options_class = org.springframework.xd.dirt.modules.metadata.RabbitSourceOptionsMetadata

For property file it would be
option.port.default=5672
option.port.description=""cool port number""

This needs to be consistently done across all the modules.






",3.0,0.9912730086062944,0.1045847935147126,0.1079190461563158,True,True,True,1394173479000.0,1397458643000.0,1397462243000.0,1397486665000.0,1397487565000.0,1394173479000.0,-1.0,1394531132000.0,1394534732000.0,1394520082000.0,1394523682000.0,1397458643000.0,1397462243000.0,"mark.pollack",1394534732000.0,1394173479000.0,44,1394173479000.0,-1.0,"Change module placeholder names and remove context:property-placeholder usage","e.g. rabbit.xml source.

	<context:property-placeholder
location=""${xd.config.home}/${configProperties:rabbit}.properties""
ignore-resource-not-found=""true"" />

would be removed and

	<rabbit:connection-factory id=""rabbitConnectionFactory"" host=""${host:${spring.rabbitmq.host:localhost}}""
port=""${port:${spring.rabbitmq.port:5672}}"" virtual-host=""${vhost:${spring.rabbitmq.virtualHost:/}}""
username=""${username:${spring.rabbitmq.username:guest}}"" password=""${password:${spring.rabbitmq.password:guest}}""/>

would look like
port=""${port}"" and a property file in the module directory or POJO in the module lib would specify the default value of the port.

For POJO it would be
options_class = org.springframework.xd.dirt.modules.metadata.RabbitSourceOptionsMetadata

For property file it would be
option.port.default=5672
option.port.description=""cool port number""

This needs to be consistently done across all the modules.






",3.0,False,3.0,0,0.0,0,0,False
"XD-1377","Story","Sprint 23","2014-03-07T05:40:25.000+0000","mark.fisher","mark.fisher","Rename ""node"" references to ""container""","This applies to a few places (when addressing the issue, a search should be done to uncover any others), e.g.:

ContainerServerApplication:
    public static final String NODE_PROFILE = ""node"";

*Options classes:
     ""The transport to use for data messages (from node to node)""
     ""The transport to use for control messages (between admin and nodes)""
",4.0,0.9997550821062652,1.775114998666265e-05,1.007497701945718e-05,True,True,True,1394170825000.0,1398338548000.0,1398339569000.0,1398338669000.0,1398339569000.0,1394170825000.0,-1.0,1394170867000.0,1394174467000.0,1394170899000.0,1394174499000.0,1398338548000.0,1398339569000.0,"mark.fisher",1394174467000.0,1394170825000.0,44,1394170825000.0,-1.0,"Rename ""node"" references to ""container""","This applies to a few places (when addressing the issue, a search should be done to uncover any others), e.g.:

ContainerServerApplication:
    public static final String NODE_PROFILE = ""node"";

*Options classes:
     ""The transport to use for data messages (from node to node)""
     ""The transport to use for control messages (between admin and nodes)""
",4.0,False,4.0,0,0.0,0,0,False
"XD-1376","Story","Sprint 24","2014-03-06T16:48:59.000+0000","iperumal","eric.bottard","XD Shell crashes when the stream DSL has ""!""","The XD shell crashes when the following command issued:

stream create test --definition ""http | filter --expression=!payload.contains('test') | log""

It looks like the JLine ConsoleReader's expandEvents is set to true by default and this causes the issue:

Exception in thread ""Spring Shell"" java.lang.IllegalArgumentException: !payload.contains('test') | log"": event not found
	at jline.console.ConsoleReader.expandEvents(ConsoleReader.java:734)
	at jline.console.ConsoleReader.finishBuffer(ConsoleReader.java:604)
	at jline.console.ConsoleReader.accept(ConsoleReader.java:1912)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2537)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2162)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2150)
	at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:517)
	at org.springframework.shell.core.JLineShell.run(JLineShell.java:178)
	at java.lang.Thread.run(Thread.java:722)
",2.0,-1.0,0.276319071836265,0.959010871086642,False,True,True,1394124539000.0,-1.0,-1.0,1395731114000.0,1395732014000.0,1394124539000.0,-1.0,1395666125000.0,1395669725000.0,1394568715000.0,1394572315000.0,-1.0,-1.0,"iperumal",1395669725000.0,1394124539000.0,44,1394124539000.0,-1.0,"XD Shell crashes when the stream DSL has ""!""","The XD shell crashes when the following command issued:

stream create test --definition ""http | filter --expression=!payload.contains('test') | log""

It looks like the JLine ConsoleReader's expandEvents is set to true by default and this causes the issue:

Exception in thread ""Spring Shell"" java.lang.IllegalArgumentException: !payload.contains('test') | log"": event not found
	at jline.console.ConsoleReader.expandEvents(ConsoleReader.java:734)
	at jline.console.ConsoleReader.finishBuffer(ConsoleReader.java:604)
	at jline.console.ConsoleReader.accept(ConsoleReader.java:1912)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2537)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2162)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2150)
	at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:517)
	at org.springframework.shell.core.JLineShell.run(JLineShell.java:178)
	at java.lang.Thread.run(Thread.java:722)
",2.0,False,2.0,0,0.0,0,0,False
"XD-1375","Story","Sprint 23","2014-03-06T08:57:59.000+0000","mark.pollack","hillert","Create document that maps UI design docs to MVC components in Angluar","",10.0,0.2593598214016351,0.0,6.787944610371979e-06,True,True,True,1394096279000.0,1394440159000.0,1394443759000.0,1395421259000.0,1395422159000.0,1394096279000.0,1394440154000.0,1394096288000.0,1394099888000.0,1394096279000.0,1394099879000.0,1394440159000.0,1394443759000.0,"mark.pollack",1394099888000.0,1394096279000.0,44,1394440154000.0,1394440154000.0,"Create spike of web app that maps UI design docs to MVC components in Angluar","",4.0,True,4.0,1,150.0,0,0,True
"XD-1373","Bug","Sprint 25","2014-03-06T05:56:54.000+0000","thomas.risberg","iperumal","HSQL always started, even when using other database","I set the config/xd-config.yml properties to use MySQL including this

profiles:
active: default,mysql

When XD ADmin starts I still see HSQL server started and localhost:9393/env shows:

""profiles"": [
""adminServer"",
""hsqldb"",
""default""
],
",5.0,0.9999985600753828,0.9999985600753828,0.9999933763467614,True,True,True,1394085414000.0,1397557813000.0,1397557818000.0,1397556918000.0,1397557818000.0,1394085414000.0,-1.0,1397557795000.0,1397557818000.0,1397557813000.0,1397557818000.0,1397557813000.0,1397557818000.0,"thomas.risberg",1397557818000.0,1394085414000.0,44,1394085414000.0,-1.0,"HSQL always started, even when using other database","I set the config/xd-config.yml properties to use MySQL including this

profiles:
active: default,mysql

When XD ADmin starts I still see HSQL server started and localhost:9393/env shows:

""profiles"": [
""adminServer"",
""hsqldb"",
""default""
],
",5.0,False,5.0,0,0.0,-1,0,False
"XD-1371","Improvement","","2014-03-05T09:32:22.000+0000","anas4120","","API or syntax for managing deployment parameters","Suppose we have 3 environements of Spring XD :
- Dev environment 
- Test environment 
- Prod environement (

Suppose whe develop the script bellow:
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
stream1 = http | filter --expression=payload.contains('toto') | file --dir=/tmp/toto 
stream2 = http | filter --expression=payload.contains('titi') | file --dir=/tmp/titi //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

When we need to deploy the script in Test and Prod environements , we must modify ""dir"" option of ""file"" sink. This is very easy when there is not a lot of options and when we have a small factory team. But in a big factory environment this will be problematic. 

In order to industrialize deployment, it would be convenient to implement in DSL a directory interface API or something equivalent like below:

Suppose we call this directory interface XDDI ... like ""XD Directory Interface"" :-)

The script can be like that:
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
stream1 = http | filter --expression=payload.contains('toto') | file --dir=XDDI('totoKey') 
stream2 = http | filter --expression=payload.contains('titi') | file --dir=XDDI('titiKey') 
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The XDDI keys are defined in a centralized directory interface (admin console or XDDI.properties)

The XDDI keys/values in Dev environment:
/////////////////////////////////////
totoKey=/tmp/toto
titiKey=/tmp/titi
/////////////////////////////////////

The XDDI keys/values in Test environment:
//////////////////////////////////////////
totoKey=/tartempion/toto
titiKey=/petaouchnok/titi
/////////////////////////////////////////

The XDDI keys/values in Prod environment:
/////////////////////////////////////////////////////
totoKey=/vavoirlabasijysuis/toto
titiKey=/vavoirlabasijysuis/titi
/////////////////////////////////////////////////////

When the script is deployed in Test or Prod environement, if the script contain a key that is not defined in centralized directory, the deployment fail. 

This will reduce errors risks in a big factory environnement (several hundred parameters and signifiant team turnover). 
",3.0,-1.0,0.8039598897575012,-1.0,False,False,True,1394011942000.0,-1.0,-1.0,1394703334000.0,1394704234000.0,1394011942000.0,-1.0,-1.0,-1.0,1394568517000.0,1394572117000.0,-1.0,-1.0,"anas4120",-1.0,-1.0,0,1394011942000.0,-1.0,"Clarify API or syntax for managing deployment parameters","Suppose we have 3 environements of Spring XD :
- Dev environment 
- Test environment 
- Prod environement (

Suppose whe develop the script bellow:
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
stream1 = http | filter --expression=payload.contains('toto') | file --dir=/tmp/toto 
stream2 = http | filter --expression=payload.contains('titi') | file --dir=/tmp/titi //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

When we need to deploy the script in Test and Prod environements , we must modify ""dir"" option of ""file"" sink. This is very easy when there is not a lot of options and when we have a small factory team. But in a big factory environment this will be problematic. 

In order to industrialize deployment, it would be convenient to implement in DSL a directory interface API or something equivalent like below:

Suppose we call this directory interface XDDI ... like ""XD Directory Interface"" :-)

The script can be like that:
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
stream1 = http | filter --expression=payload.contains('toto') | file --dir=XDDI('totoKey') 
stream2 = http | filter --expression=payload.contains('titi') | file --dir=XDDI('titiKey') 
//////////////////////////////////////////////////////////////////////////////////////////////////////////////////////

The XDDI keys are defined in a centralized directory interface (admin console or XDDI.properties)

The XDDI keys/values in Dev environment:
/////////////////////////////////////
totoKey=/tmp/toto
titiKey=/tmp/titi
/////////////////////////////////////

The XDDI keys/values in Test environment:
//////////////////////////////////////////
totoKey=/tartempion/toto
titiKey=/petaouchnok/titi
/////////////////////////////////////////

The XDDI keys/values in Prod environment:
/////////////////////////////////////////////////////
totoKey=/vavoirlabasijysuis/toto
titiKey=/vavoirlabasijysuis/titi
/////////////////////////////////////////////////////

When the script is deployed in Test or Prod environement, if the script contain a key that is not defined in centralized directory, the deployment fail. 

This will reduce errors risks in a big factory environnement (several hundred parameters and signifiant team turnover). 
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1370","Bug","","2014-03-05T05:50:56.000+0000","dmcintyre","","Analytics Counter Example Fails - ClassNotFoundException during Redis deserialization","Given:

  the counter example from the guide is run with redis enabled:

xd-singlenode --transport redis --store redis

When:

 a stream is created

stream create --name springtweets --definition ""twittersearch --consumerKey=<your_key> --consumerSecret=<your_secret> --query=spring | file --dir=/tweets/""

Then:

An exception is thrown:

Exception in thread ""inbound.springtweets.0-redis:queue-inbound-channel-adapter35"" org.springframework.integration.MessageHandlingException: error occurred in message handler [springtweets.0.convert.bridge]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:94)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:42)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:86)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:207)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:286)
	at java.lang.Thread.run(Thread.java:724)
Caused by: org.springframework.integration.x.bus.serializer.SerializationException: unable to deserialize [null]. Class not found.
	at org.springframework.integration.x.bus.MessageBusSupport.deserializeConsumerPayload(MessageBusSupport.java:247)
	at org.springframework.integration.x.bus.MessageBusSupport.transformPayloadForConsumer(MessageBusSupport.java:191)
	at org.springframework.integration.x.bus.MessageBusSupport.transformPayloadForConsumerIfNecessary(MessageBusSupport.java:168)
	at org.springframework.integration.x.redis.RedisMessageBus.access$300(RedisMessageBus.java:57)
	at org.springframework.integration.x.redis.RedisMessageBus$ReceivingHandler.handleRequestMessage(RedisMessageBus.java:176)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:142)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	... 13 more
Caused by: java.lang.ClassNotFoundException: org.springframework.social.twitter.api.Tweet
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.springframework.integration.x.bus.MessageBusSupport.deserializeConsumerPayload(MessageBusSupport.java:241)
	... 19 more",3.0,-1.0,0.7173846504630834,-1.0,False,False,True,1393998656000.0,-1.0,-1.0,1398839648000.0,1398840548000.0,1393998656000.0,-1.0,-1.0,-1.0,1397472155000.0,1397475755000.0,-1.0,-1.0,"dmcintyre",-1.0,-1.0,0,1393998656000.0,-1.0,"Serialization over data transport fails for classes that are module specific","Given:

  the counter example from the guide is run with redis enabled:

xd-singlenode --transport redis --store redis

When:

 a stream is created

stream create --name springtweets --definition ""twittersearch --consumerKey=<your_key> --consumerSecret=<your_secret> --query=spring | file --dir=/tweets/""

Then:

An exception is thrown:

Exception in thread ""inbound.springtweets.0-redis:queue-inbound-channel-adapter35"" org.springframework.integration.MessageHandlingException: error occurred in message handler [springtweets.0.convert.bridge]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:94)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:42)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:86)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:207)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:286)
	at java.lang.Thread.run(Thread.java:724)
Caused by: org.springframework.integration.x.bus.serializer.SerializationException: unable to deserialize [null]. Class not found.
	at org.springframework.integration.x.bus.MessageBusSupport.deserializeConsumerPayload(MessageBusSupport.java:247)
	at org.springframework.integration.x.bus.MessageBusSupport.transformPayloadForConsumer(MessageBusSupport.java:191)
	at org.springframework.integration.x.bus.MessageBusSupport.transformPayloadForConsumerIfNecessary(MessageBusSupport.java:168)
	at org.springframework.integration.x.redis.RedisMessageBus.access$300(RedisMessageBus.java:57)
	at org.springframework.integration.x.redis.RedisMessageBus$ReceivingHandler.handleRequestMessage(RedisMessageBus.java:176)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:142)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	... 13 more
Caused by: java.lang.ClassNotFoundException: org.springframework.social.twitter.api.Tweet
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.springframework.integration.x.bus.MessageBusSupport.deserializeConsumerPayload(MessageBusSupport.java:241)
	... 19 more",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1368","Story","Sprint 23","2014-03-04T14:24:40.000+0000","dturanski","dturanski","Refactor container to remove shared module context as a separate context ","The main container context becomes the shared context for modules.",8.0,0.000136031727781,0.0,9.554609451290116e-05,True,True,True,1393943080000.0,1393943164000.0,1393946764000.0,1394559683000.0,1394560583000.0,1393943080000.0,-1.0,1393943139000.0,1393946739000.0,1393943080000.0,1393946680000.0,1393943164000.0,1393946764000.0,"dturanski",1393946739000.0,1393943080000.0,44,1393943080000.0,-1.0,"Refactor container to remove shared module context as a separate context ","The main container context becomes the shared context for modules.",8.0,False,8.0,0,0.0,0,0,False
"XD-1367","Story","","2014-03-04T09:47:55.000+0000","luke","","Exclude commons-logging from final distro1","xd/lib includes jcl-over-slf4j so we don't need additional commons-logging jars in the modules or extensions:

https://github.com/spring-projects/spring-xd/pull/610",1.0,-1.0,0.000320166486573,-1.0,False,False,True,1393926475000.0,-1.0,-1.0,1394000536000.0,1394001436000.0,1393926475000.0,-1.0,-1.0,-1.0,1393926499000.0,1393930099000.0,-1.0,-1.0,"luke",-1.0,-1.0,0,1393926475000.0,-1.0,"Exclude commons-logging from final distro1","xd/lib includes jcl-over-slf4j so we don't need additional commons-logging jars in the modules or extensions:

https://github.com/spring-projects/spring-xd/pull/610",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1366","Bug","","2014-03-04T07:15:56.000+0000","grenfro","","Unable to destroy stream when using http source","When destroying a stream that contains an http source, an exception is thrown.  Thus even though the stream is destroyed all resources are not released i.e. port 9000 is still in use.  

NettyHttpInboundChannelAdapter is currently setup with child.tcpNoDelay set to true.  And when running on my system and on EC2 the http needs more time to release the port.  

My recommendation is to set bootstrap.setOption(""child.tcpNoDelay"", false)
instead of true. ",3.0,-1.0,-1.0,-1.0,False,False,False,1393917356000.0,-1.0,-1.0,1393938017000.0,1393938917000.0,1393917356000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1393917356000.0,-1.0,"Unable to destroy stream when using http source","When destroying a stream that contains an http source, an exception is thrown.  Thus even though the stream is destroyed all resources are not released i.e. port 9000 is still in use.  

NettyHttpInboundChannelAdapter is currently setup with child.tcpNoDelay set to true.  And when running on my system and on EC2 the http needs more time to release the port.  

My recommendation is to set bootstrap.setOption(""child.tcpNoDelay"", false)
instead of true. ",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1365","Bug","Sprint 26","2014-03-04T03:16:08.000+0000","eric.bottard","Mark Fisher","StreamDeployer.deleteAll() does not handle dependency tracking","create a composed module, use it in a stream, delete ALL streams.
Try to delete the composed module => fails thinking that it's still used by the stream

",2.0,0.966817757710661,0.9666224709917852,0.8708553558286757,True,True,True,1393902968000.0,1398180425000.0,1398184025000.0,1398326332000.0,1398327232000.0,1393902968000.0,1398251679000.0,1397755862000.0,1397759462000.0,1398179561000.0,1398183161000.0,1398180425000.0,1398184025000.0,"eric.bottard",1397759462000.0,1393902968000.0,44,1398251679000.0,1398251679000.0,"StreamDeployer.deleteAll() does not handle dependency tracking","create a composed module, use it in a stream, delete ALL streams.
Try to delete the composed module => fails thinking that it's still used by the stream

",4.0,True,4.0,1,-50.0,0,0,False
"XD-1364","Story","Sprint 23","2014-03-03T08:43:58.000+0000","thomas.risberg","","Upgrade to SHDP 2.0 M6","The YARN support in M6 changes most of the config properties, need to update XD to use new ones.",5.0,0.9793651375422552,0.9793651375422552,0.7739638163922964,True,True,True,1393836238000.0,1394183610000.0,1394187210000.0,1394190029000.0,1394190929000.0,1393836238000.0,-1.0,1394110756000.0,1394114356000.0,1394183610000.0,1394187210000.0,1394183610000.0,1394187210000.0,"thomas.risberg",1394114356000.0,1393836238000.0,44,1393836238000.0,-1.0,"Upgrade to SHDP 2.0 M6","The YARN support in M6 changes most of the config properties, need to update XD to use new ones.",5.0,False,5.0,0,0.0,-1,0,False
"XD-1363","Improvement","Sprint 22","2014-03-03T08:16:18.000+0000","iperumal","iperumal","Tap XD batch Job output","Currently, job module only has an input channel that receives the job launching requests. If we have an output channel for the job module, we get the following benefits:

1) The output channel reflects the JobExecution from the job launching message handler's reply channel
2) This can be different from JobExecution listener's notification channel where we would get both the beforeJob() and afterJob() notification.
3) We can tap the output channel to send the job results to some other sinks.

Along with this improvement, planning to do some more refactoring on the Plugins so that some of the common implementation methods are handled in the base class.
",5.0,0.0043383767655822,0.0,0.0001113682924498,True,True,True,1393834578000.0,1393839798000.0,1393843398000.0,1395036893000.0,1395037793000.0,1393834578000.0,-1.0,1393834712000.0,1393838312000.0,1393834578000.0,1393838178000.0,1393839798000.0,1393843398000.0,"iperumal",1393838312000.0,1393834578000.0,44,1393834578000.0,-1.0,"Tap XD batch Job output","Currently, job module only has an input channel that receives the job launching requests. If we have an output channel for the job module, we get the following benefits:

1) The output channel reflects the JobExecution from the job launching message handler's reply channel
2) This can be different from JobExecution listener's notification channel where we would get both the beforeJob() and afterJob() notification.
3) We can tap the output channel to send the job results to some other sinks.

Along with this improvement, planning to do some more refactoring on the Plugins so that some of the common implementation methods are handled in the base class.
",5.0,False,5.0,0,0.0,0,0,False
"XD-1362","Story","Sprint 22","2014-03-03T08:06:36.000+0000","iperumal","iperumal","Make Job notification channels subscribable","Currently, the job notification channels are direct channels. We need to make these pub/sub channels.

With XD-885 (allowing automatic job listeners registration),  this would allow us to create named channel syntax like:

topic:job:myjobname-jobExecution > log
topic:job:myjobname-stepExecution > log 

",5.0,0.000127092034569,0.0,0.0001138013642873,True,True,True,1393833996000.0,1393834149000.0,1393837749000.0,1395036948000.0,1395037848000.0,1393833996000.0,-1.0,1393834133000.0,1393837733000.0,1393833996000.0,1393837596000.0,1393834149000.0,1393837749000.0,"iperumal",1393837733000.0,1393833996000.0,44,1393833996000.0,-1.0,"Make Job notification channels subscribable","Currently, the job notification channels are direct channels. We need to make these pub/sub channels.

With XD-885 (allowing automatic job listeners registration),  this would allow us to create named channel syntax like:

topic:job:myjobname-jobExecution > log
topic:job:myjobname-stepExecution > log 

",5.0,False,5.0,0,0.0,0,0,False
"XD-1360","Bug","Sprint 30","2014-02-28T11:06:33.000+0000","karllopes","fbiville","Json information returned by curl does not reflect deployed status correctly","The Json information returned by curl does not reflect deployed status correctly.
To recreate:
1. Start xd-singlenode
2. start xd-shell
In the xd-shell
    (i). stream create --definition ""time | log"" --name ticktock
   (ii). stream list
Note the status of the ticktock stream is deployed
3. open a new command prompt & type curl http://localhost:9393/streams/ticktock
4. Note the returned json stream:
  {""name"":""ticktock"",""deployed"":null,""definition"":""time | log"",""links"":[{""rel"":""self"",""href"":""http://localhost:9393/streams/ticktock""}]}
5. I would expect the json attribute ""deployed"" to be ""true"", but it is null.",3.0,0.9259073562491318,0.925989957318656,0.9258338299537876,True,True,True,1393585593000.0,1402564312000.0,1402567912000.0,1403281904000.0,1403282804000.0,1393595302000.0,-1.0,1402563599000.0,1402567199000.0,1402565113000.0,1402568713000.0,1402564312000.0,1402567912000.0,"karllopes",1402567199000.0,1393595302000.0,44,1393595302000.0,-1.0,"Json information returned by curl does not reflect deployed status correctly","The Json information returned by curl does not reflect deployed status correctly.
To recreate:
1. Start xd-singlenode
2. start xd-shell
In the xd-shell
    (i). stream create --definition ""time | log"" --name ticktock
   (ii). stream list
Note the status of the ticktock stream is deployed
3. open a new command prompt & type curl http://localhost:9393/streams/ticktock
4. Note the returned json stream:
  {""name"":""ticktock"",""deployed"":null,""definition"":""time | log"",""links"":[{""rel"":""self"",""href"":""http://localhost:9393/streams/ticktock""}]}
5. I would expect the json attribute ""deployed"" to be ""true"", but it is null.",3.0,False,3.0,0,0.0,0,0,False
"XD-1359","Story","Sprint 22","2014-02-28T13:42:56.000+0000","mark.fisher","","Create an embedded ZooKeeper server process","This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string. It will still be recommended that user's run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs.

It should implement SmartLifecycle so that it can be managed as a bean within an ApplicationContext.",5.0,0.0075477352471936,0.0074420784799256,3.746012657680719e-05,True,True,True,1393594976000.0,1393602834000.0,1393606434000.0,1394635183000.0,1394636083000.0,1393594976000.0,-1.0,1393595015000.0,1393598615000.0,1393602724000.0,1393606324000.0,1393602834000.0,1393606434000.0,"mark.fisher",1393598615000.0,1393594976000.0,44,1393594976000.0,-1.0,"Create an embedded ZooKeeper server process","This will be used by SingleNodeApplication if no ZooKeeper process is available based on the provided client connect string. It will still be recommended that user's run ZooKeeper externally, at least in standalone mode, when running SingleNodeApplication. We should clarify that in the documentation and logs.

It should implement SmartLifecycle so that it can be managed as a bean within an ApplicationContext.",5.0,False,5.0,0,0.0,0,0,False
"XD-1358","Improvement","Sprint 22","2014-02-27T12:39:40.000+0000","dturanski","mark.fisher","Remove launcher.xml and [transport]-launcher.xml configuration.","These are no longer used post boot.
",2.0,0.192011991190146,0.0,0.0377477771433232,True,True,True,1393504780000.0,1393523611000.0,1393527211000.0,1393601952000.0,1393602852000.0,1393504780000.0,1393524443000.0,1393508482000.0,1393512082000.0,1393504780000.0,1393508380000.0,1393523611000.0,1393527211000.0,"dturanski",1393512082000.0,1393504780000.0,44,1393524443000.0,1393524443000.0,"Remove launcher.xml and [transport]-launcher.xml configuration.","These are no longer used post boot.
",1.0,True,1.0,1,100.0,0,0,False
"XD-1357","Story","Sprint 22","2014-02-27T12:04:29.000+0000","mark.fisher","mark.fisher","Container nodes should write attributes to ZooKeeper","The /xd/containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container's ID, and the node data should be the Container's attributes (host, pid, and much more to be added later).

When a Container shuts down cleanly, it should eagerly delete the ephemeral node so that watchers are notified immediately. For any other case (including a network partition), the ephemeral node will disappear after the timeout elapses.",4.0,0.0008486675918807,0.0,0.0006589654242838,True,True,True,1393502669000.0,1393502754000.0,1393506354000.0,1393601926000.0,1393602826000.0,1393502669000.0,-1.0,1393502735000.0,1393506335000.0,1393502669000.0,1393506269000.0,1393502754000.0,1393506354000.0,"mark.fisher",1393506335000.0,1393502669000.0,44,1393502669000.0,-1.0,"Container nodes should write attributes to ZooKeeper","The /xd/containers node is the parent where each Container will write an ephemeral child node. The node name should be the Container's ID, and the node data should be the Container's attributes (host, pid, and much more to be added later).

When a Container shuts down cleanly, it should eagerly delete the ephemeral node so that watchers are notified immediately. For any other case (including a network partition), the ephemeral node will disappear after the timeout elapses.",4.0,False,4.0,0,0.0,0,0,False
"XD-1356","Bug","Sprint 22","2014-02-27T11:58:49.000+0000","thomas.risberg","thomas.risberg","Hadoop distro option hdp20 is broken","Starting the shell with --hadoopDistro hdp20 causes this:

Exception in thread ""main"" org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Unable to locate Spring NamespaceHandler for XML schema namespace [http://www.springframework.org/schema/hadoop]
Offending resource: URL [jar:file:/Users/trisberg/Demo/spring-xd-1.0.0.BUILD-SNAPSHOT/shell/lib/spring-xd-shell-1.0.0.BUILD-SNAPSHOT.jar!/META-INF/spring/spring-shell-plugin.xml]

Creating a stream ""time | hdfs"" in xd-singlenode started with --hadoopDistro hdp20 causes this:

java.lang.IllegalStateException: Can't find class used for type of option 'codec': org.springframework.data.hadoop.store.codec.Codecs
",3.0,0.0066337169783062,0.0066337169783062,0.0067650548969458,True,True,True,1393502329000.0,1393505208000.0,1393508808000.0,1393935424000.0,1393936324000.0,1393502329000.0,-1.0,1393505265000.0,1393508865000.0,1393505208000.0,1393508808000.0,1393505208000.0,1393508808000.0,"thomas.risberg",1393508865000.0,1393502329000.0,44,1393502329000.0,-1.0,"Hadoop distro option hdp20 is broken","Starting the shell with --hadoopDistro hdp20 causes this:

Exception in thread ""main"" org.springframework.beans.factory.parsing.BeanDefinitionParsingException: Configuration problem: Unable to locate Spring NamespaceHandler for XML schema namespace [http://www.springframework.org/schema/hadoop]
Offending resource: URL [jar:file:/Users/trisberg/Demo/spring-xd-1.0.0.BUILD-SNAPSHOT/shell/lib/spring-xd-shell-1.0.0.BUILD-SNAPSHOT.jar!/META-INF/spring/spring-shell-plugin.xml]

Creating a stream ""time | hdfs"" in xd-singlenode started with --hadoopDistro hdp20 causes this:

java.lang.IllegalStateException: Can't find class used for type of option 'codec': org.springframework.data.hadoop.store.codec.Codecs
",3.0,False,3.0,0,0.0,-1,0,False
"XD-1354","Story","Sprint 22","2014-02-27T08:28:31.000+0000","mark.fisher","mark.fisher","Remove XDContainer and rename LauncherApplication","Post-boot refactoring. XDContainer lifecycle methods are not being used. Refactor by merging relevant functionality into LauncherApplication. Rename LauncherApplication to ContainerServerApplication (consistent with AdminServerApplication).  ",5.0,0.404548130703562,0.3240359140418016,0.4024874889608478,True,True,True,1393489711000.0,1393495208000.0,1393498808000.0,1393502399000.0,1393503299000.0,1393489711000.0,-1.0,1393495180000.0,1393498780000.0,1393494114000.0,1393497714000.0,1393495208000.0,1393498808000.0,"mark.fisher",1393498780000.0,1393489711000.0,44,1393489711000.0,-1.0,"Remove XDContainer and rename LauncherApplication","Post-boot refactoring. XDContainer lifecycle methods are not being used. Refactor by merging relevant functionality into LauncherApplication. Rename LauncherApplication to ContainerServerApplication (consistent with AdminServerApplication).  ",5.0,False,5.0,0,0.0,-1,0,False
"XD-1353","Story","Sprint 26","2014-02-27T06:52:14.000+0000","thomas.risberg","","Switch driver for Redis","The Spring Data team recommends using the Jedis driver since the Lettuce driver hasn't had any update activity for several months. Jedis is actively maintained.

We might also want to investigate Redisson which is a fork of Lettuce - https://github.com/mrniko/redisson",4.0,-1.0,0.5654232220192428,0.5052733983432698,False,True,True,1393483934000.0,-1.0,-1.0,1401937639000.0,1401938539000.0,1393483934000.0,-1.0,1397755821000.0,1397759421000.0,1398264364000.0,1398267964000.0,-1.0,-1.0,"thomas.risberg",1397759421000.0,1394567222000.0,44,1394567222000.0,-1.0,"Switch to use Jedis driver for Redis","The Spring Data team recommends using the Jedis driver since the Lettuce driver hasn't had any update activity for several months. Jedis is actively maintained.

We might also want to investigate Redisson which is a fork of Lettuce - https://github.com/mrniko/redisson",4.0,False,4.0,0,0.0,0,2,True
"XD-1352","Bug","Sprint 22","2014-02-25T14:00:20.000+0000","iperumal","iperumal","Job Launch should throw exception if the job is not deployed in the container","While processing the ""Job Launch"" request, the ModuleDeployer checks if the job is already deployed, if not it deploys the job before launching it.
This approach causes issue in case of multiple containers environment where the job is deployed in one container(1) but the ""job launch"" request is picked up by other container(2). Because the container(2) that processes ""job launch"" request deploys the job again, it conflicts with an existing job that is deployed in conatiner(1) with the same name in the JobRepository.

Initially, the idea behind 'deploy before launch' was to enable launch shell command to also deploy. Because of the issue mentioned above, it is ok to assume that job launch needs to be performed on an existing deployed job.",1.0,0.0001261397628572,0.0,0.0001061175782767,True,True,True,1393336820000.0,1393336883000.0,1393340483000.0,1393835366000.0,1393836266000.0,1393336820000.0,-1.0,1393336873000.0,1393340473000.0,1393336820000.0,1393340420000.0,1393336883000.0,1393340483000.0,"iperumal",1393340473000.0,1393336820000.0,44,1393336820000.0,-1.0,"Job Launch should throw exception if the job is not deployed in the container","While processing the ""Job Launch"" request, the ModuleDeployer checks if the job is already deployed, if not it deploys the job before launching it.
This approach causes issue in case of multiple containers environment where the job is deployed in one container(1) but the ""job launch"" request is picked up by other container(2). Because the container(2) that processes ""job launch"" request deploys the job again, it conflicts with an existing job that is deployed in conatiner(1) with the same name in the JobRepository.

Initially, the idea behind 'deploy before launch' was to enable launch shell command to also deploy. Because of the issue mentioned above, it is ok to assume that job launch needs to be performed on an existing deployed job.",1.0,False,1.0,0,0.0,0,0,False
"XD-1351","Story","Sprint 22","2014-02-25T10:29:38.000+0000","dturanski","dturanski","Replace BeanDefinitionAddingBeanPostProcessor with Ordered Plugins","This will allow us to control the order of plugins and use plugin(s) to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser",8.0,0.8598804614743085,0.0,0.8597414631886207,True,True,True,1393324178000.0,1393416972000.0,1393420572000.0,1393431193000.0,1393432093000.0,1393324178000.0,-1.0,1393416957000.0,1393420557000.0,1393324178000.0,1393327778000.0,1393416972000.0,1393420572000.0,"dturanski",1393420557000.0,1393324178000.0,44,1393324178000.0,-1.0,"Replace BeanDefinitionAddingBeanPostProcessor with Ordered Plugins","This will allow us to control the order of plugins and use plugin(s) to manage the common module context, replacing BeanDefinitionAddingBeanPostProcesser",8.0,False,8.0,0,0.0,-1,0,False
"XD-1350","Story","Sprint 22","2014-02-25T10:02:34.000+0000","mark.fisher","eric.bottard","Remove enum for transport options","The enum unnecessarily restricts the ability to add new transport (MessageBus implementations) whereas the config location path is open for extensions since it uses wildcards (in message-bus.xml):

{code}
<import resource=""../../transports/${XD_TRANSPORT}-bus.xml""/>
{code}
",4.0,0.4698129907209136,0.4114987254002243,0.4115101458142143,True,True,True,1393322554000.0,1393898486000.0,1393902086000.0,1394547529000.0,1394548429000.0,1393322554000.0,-1.0,1393827014000.0,1393830614000.0,1393827000000.0,1393830600000.0,1393898486000.0,1393902086000.0,"mark.fisher",1393830614000.0,1393322554000.0,44,1393322554000.0,-1.0,"Remove enum for transport options","The enum unnecessarily restricts the ability to add new transport (MessageBus implementations) whereas the config location path is open for extensions since it uses wildcards (in message-bus.xml):

{code}
<import resource=""../../transports/${XD_TRANSPORT}-bus.xml""/>
{code}
",4.0,False,4.0,0,0.0,0,0,False
"XD-1349","Story","Sprint 22","2014-02-25T08:18:42.000+0000","thomas.risberg","thomas.risberg","Make hdfs configurable via application.yml","We currently pull all hdfs config properties from hdfs.properties - change that to use application.yml",3.0,0.00014194189415,0.00014194189415,0.0001161342770318,True,True,True,1393316322000.0,1393316410000.0,1393320010000.0,1393935394000.0,1393936294000.0,1393316322000.0,-1.0,1393316394000.0,1393319994000.0,1393316410000.0,1393320010000.0,1393316410000.0,1393320010000.0,"thomas.risberg",1393319994000.0,1393316322000.0,44,1393316322000.0,-1.0,"Make hdfs configurable via application.yml","We currently pull all hdfs config properties from hdfs.properties - change that to use application.yml",3.0,False,3.0,0,0.0,-1,0,False
"XD-1348","Story","Sprint 26","2014-02-25T07:38:50.000+0000","dturanski","grussell","Allow end users to configure Rabbit MQ properties on the MessageBus (for acks, txs, etc).","This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ",1.0,0.98868923464587,0.8036752976857608,0.8036117957321259,True,True,True,1393313930000.0,1398778800000.0,1398782400000.0,1398840419000.0,1398841319000.0,1393313930000.0,1398778828000.0,1397755805000.0,1397759405000.0,1397756156000.0,1397759756000.0,1398778800000.0,1398782400000.0,"dturanski",1397759405000.0,1393313930000.0,44,1398778828000.0,1398778828000.0,"Allow end users to configure Rabbit MQ properties on the MessageBus (for acks, txs, etc).","This requires exposing properties to the ListenerContainer. Probably cleaner to inject the ListenerContainer into the RabbitMessageBus and expose property placeholders on the LC. Maybe do the same for RabbitAdmin as well. ",8.0,True,8.0,1,-87.5,0,0,False
"XD-1345","Story","Sprint 22","2014-02-25T07:10:50.000+0000","eric.bottard","eric.bottard","Use dot as the composed module option separator","Following merge of https://github.com/spring-projects/spring-xd/pull/601, use dot as the separator for a composed module option.

Need change to the parser to accept dots",4.0,0.2170012261920176,0.0,0.2169916669346155,True,True,True,1393312250000.0,1393561957000.0,1393565557000.0,1394462067000.0,1394462967000.0,1393312250000.0,-1.0,1393561946000.0,1393565546000.0,1393312250000.0,1393315850000.0,1393561957000.0,1393565557000.0,"eric.bottard",1393565546000.0,1393312250000.0,44,1393312250000.0,-1.0,"Use dot as the composed module option separator","Following merge of https://github.com/spring-projects/spring-xd/pull/601, use dot as the separator for a composed module option.

Need change to the parser to accept dots",4.0,False,4.0,0,0.0,0,0,False
"XD-1344","Bug","Sprint 27","2014-02-24T11:53:17.000+0000","eserrano","aclement","Cannot undeploy stream that was created and deployed with a ""."" in the name","eserrano-mbp:spring-xd-1.0.0.M5 eserrano$ ./shell/bin/xd-shell
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
eXtreme Data
1.0.0.M5 | Admin Server Target: http://localhost:9393
Welcome to the Spring XD shell. For assistance hit TAB or type ""help"".
xd:>stream list
  Stream Name    Stream Definition                                          Status
  -------------  ---------------------------------------------------------  --------
  eesstream.log  http | transform --expression=payload.toUpperCase() | log  deployed
  httptest       http | file
  tictac         time | log

xd:>stream 

stream all         stream create      stream deploy      stream destroy     stream list        stream undeploy    

xd:>stream undeploy --name 
stream undeploy --name 
required --name: the name of the stream to un-deploy; no default value
xd:>stream undeploy --name eesstream.log 
Command failed org.springframework.xd.rest.client.impl.SpringXDException: The stream named 'eesstream' is not currently deployed

xd:>
",1.0,0.9999192457942562,0.8787119126116874,0.9852619195222154,True,True,True,1393242797000.0,1398951017000.0,1398951478000.0,1398950578000.0,1398951478000.0,1393242797000.0,-1.0,1398867343000.0,1398870943000.0,1398259083000.0,1398262683000.0,1398951017000.0,1398951478000.0,"eserrano",1398870943000.0,1393242797000.0,44,1393242797000.0,-1.0,"Cannot undeploy stream that was created and deployed with a ""."" in the name","eserrano-mbp:spring-xd-1.0.0.M5 eserrano$ ./shell/bin/xd-shell
 _____                           __   _______
/  ___|          (-)             \ \ / /  _  \
\ `--. _ __  _ __ _ _ __   __ _   \ V /| | | |
 `--. \ '_ \| '__| | '_ \ / _` |  / ^ \| | | |
/\__/ / |_) | |  | | | | | (_| | / / \ \ |/ /
\____/| .__/|_|  |_|_| |_|\__, | \/   \/___/
      | |                  __/ |
      |_|                 |___/
eXtreme Data
1.0.0.M5 | Admin Server Target: http://localhost:9393
Welcome to the Spring XD shell. For assistance hit TAB or type ""help"".
xd:>stream list
  Stream Name    Stream Definition                                          Status
  -------------  ---------------------------------------------------------  --------
  eesstream.log  http | transform --expression=payload.toUpperCase() | log  deployed
  httptest       http | file
  tictac         time | log

xd:>stream 

stream all         stream create      stream deploy      stream destroy     stream list        stream undeploy    

xd:>stream undeploy --name 
stream undeploy --name 
required --name: the name of the stream to un-deploy; no default value
xd:>stream undeploy --name eesstream.log 
Command failed org.springframework.xd.rest.client.impl.SpringXDException: The stream named 'eesstream' is not currently deployed

xd:>
",1.0,False,1.0,0,0.0,0,0,False
"XD-1343","Story","Sprint 23","2014-02-24T07:33:08.000+0000","dturanski","dturanski","Provide a conventional way to extend XD Container configuration","Provide an easy way for users to add beans (e.g., Gemfire cache configuration) or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath*:META-INF/spring/xd/extensions or include this path in an extensible @Configuration base class.  In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.",8.0,0.6690058552082533,0.0,0.3416257066116471,True,True,True,1393227188000.0,1394629251000.0,1394632851000.0,1395322029000.0,1395322929000.0,1393227188000.0,-1.0,1393943147000.0,1393946747000.0,1393227188000.0,1393230788000.0,1394629251000.0,1394632851000.0,"dturanski",1393946747000.0,1393227188000.0,44,1393227188000.0,-1.0,"Provide a conventional way to extend XD Container configuration","Provide an easy way for users to add beans (e.g., Gemfire cache configuration) or modify default XD configuration such as serializers, and message converters. A simple approach is to add a well known resource selector such as classpath*:META-INF/spring/xd/extensions or include this path in an extensible @Configuration base class.  In addition, we should adopt conventional names for beans that are meant to be extended, e.g. use an xd. prefix.",8.0,False,8.0,0,0.0,0,0,False
"XD-1342","Improvement","Sprint 22","2014-02-21T18:04:42.000+0000","iperumal","Ilayaperumal Gopinathan","Configuration for RabbitMQ message bus concurrent consumers","By having the configuration option for concurrent consumers would help improve the performance of message consumption by the consumer modules when the ordering of the incoming messages don't matter.",1.0,1.3195225590374956e-05,0.9005391877636046,1.1138826797069767e-05,True,True,True,1393005882000.0,1393005959000.0,1393009559000.0,1398840426000.0,1398841326000.0,1393005882000.0,-1.0,1393005947000.0,1393009547000.0,1398260928000.0,1398264528000.0,1393005959000.0,1393009559000.0,"iperumal",1393009547000.0,1393005882000.0,44,1393005882000.0,-1.0,"Configuration for RabbitMQ message bus concurrent consumers","By having the configuration option for concurrent consumers would help improve the performance of message consumption by the consumer modules when the ordering of the incoming messages don't matter.",1.0,False,1.0,0,0.0,0,0,False
"XD-1341","Story","Sprint 50","2014-02-21T15:24:58.000+0000","iperumal","","Support oracle jdbc configuration for XD batch job repository","Currently, hsqldb, postgres and mysql job repositories are supported. We need to add configurable oracle jdbc settings.",2.0,0.9886474474919862,0.9969936335295708,0.9886483556422168,True,True,True,1392996298000.0,1431098651000.0,1431102251000.0,1431535277000.0,1431536177000.0,1392996298000.0,1431333844000.0,1431098686000.0,1431102286000.0,1431420312000.0,1431423912000.0,1431098651000.0,1431102251000.0,"iperumal",1431102286000.0,1392996298000.0,44,1431333885000.0,1431333844000.0,"Support oracle jdbc configuration for XD batch job repository","Currently, hsqldb, postgres and mysql job repositories are supported. We need to add configurable oracle jdbc settings.",3.0,True,3.0,2,-33.33333333333333,-1,0,False
"XD-1339","Improvement","Sprint 26","2014-02-20T10:19:43.000+0000","cblack","pperalta","Need some kind of hint that a given deployment is to be run on a group of servers","Need some kind of hint that a given deployment is to be run on a group of servers.  That deployment would then be part of a partitioned work flow.

Another item on this would be the ""group"" that the server is running in can be added to removed dynamically.  The use case on this would be if the group of servers are running a max CPU capacity we can easily add another compute node.  Likewise we can remove a server from the group if the servers are not being fully utilized. 


This issue is lightly linked to:
https://jira.springsource.org/browse/XD-1337",8.0,-1.0,0.2042870775363956,0.8178966329341382,False,True,True,1392891583000.0,-1.0,-1.0,1398838651000.0,1398839551000.0,1392891583000.0,1398839119000.0,1397756406000.0,1397760006000.0,1394106676000.0,1394110276000.0,-1.0,-1.0,"cblack",1397760006000.0,1392891583000.0,44,1398839119000.0,1398839119000.0,"Deployment manifest to support directing deployment to run on a group of servers","Need some kind of hint that a given deployment is to be run on a group of servers.  That deployment would then be part of a partitioned work flow.

Another item on this would be the ""group"" that the server is running in can be added to removed dynamically.  The use case on this would be if the group of servers are running a max CPU capacity we can easily add another compute node.  Likewise we can remove a server from the group if the servers are not being fully utilized. 


This issue is lightly linked to:
https://jira.springsource.org/browse/XD-1337",1.0,True,1.0,1,700.0,0,0,True
"XD-1338","Story","Sprint 26","2014-02-20T10:14:16.000+0000","cblack","Mark Fisher","Hints for partitioning a stream","We need a methodology for providing partitioning hints.

A current proposal uses message headers to provide:

* partition_key  the item to partition on
* destination_region  what to target

In the proposal the developer used ""partition_key"" to route the stream message to the node where data was stored in process.  This was done so downstream stream operations could work on the data with out suffering any network IO.

The ""destination_region"" was used to target the type of data the downstream streams were going to use in their stream processing.


",16.0,0.8298454295906976,0.6275952385885353,0.5694822381157559,True,True,True,1392891256000.0,1399980706000.0,1399984306000.0,1401433453000.0,1401434353000.0,1392891256000.0,1398870895000.0,1397756398000.0,1397759998000.0,1398252863000.0,1398256463000.0,1399980706000.0,1399984306000.0,"cblack",1397759998000.0,1392891256000.0,44,1398870895000.0,1398870895000.0,"Deployment manifest to support partitioning a stream","We need a methodology for providing partitioning hints.

A current proposal uses message headers to provide:

* partition_key  the item to partition on
* destination_region  what to target

In the proposal the developer used ""partition_key"" to route the stream message to the node where data was stored in process.  This was done so downstream stream operations could work on the data with out suffering any network IO.

The ""destination_region"" was used to target the type of data the downstream streams were going to use in their stream processing.


",1.0,True,1.0,1,1500.0,0,0,True
"XD-1337","Improvement","Sprint 47","2014-02-20T10:04:44.000+0000","cblack","liujiong","Partitioning metadata should allow updating at runtime - dynamically / anytime","In a running system some times the algorithm for partitioning the data might overload a given server with work.  When that happens we might need to ""rebalance"" the partitioned work / data to achieve a even balance of stream throughput across servers in a given compute group.

We can think of this dynamic rebalancing behavior as an extension of a failure use case.   In the failure scenario we need to re-partition the stream to other servers in the group.

We should allow third parties to plug-in to help with this capability.  As an example GemFire will report the new partitioning meta-data when this type failure / rebalance happens.
",8.0,-1.0,0.2691007385912509,0.6030298975228632,False,True,True,1392890684000.0,-1.0,-1.0,1448375819000.0,1448376719000.0,1392890684000.0,1428062698000.0,1426350422000.0,1426354022000.0,1407822017000.0,1407825617000.0,-1.0,-1.0,"cblack",1426354022000.0,1392890684000.0,44,1428062698000.0,1428062698000.0,"Stream partitioning metadata should allow updating at runtime - dynamically / anytime","In a running system some times the algorithm for partitioning the data might overload a given server with work.  When that happens we might need to ""rebalance"" the partitioned work / data to achieve a even balance of stream throughput across servers in a given compute group.

We can think of this dynamic rebalancing behavior as an extension of a failure use case.   In the failure scenario we need to re-partition the stream to other servers in the group.

We should allow third parties to plug-in to help with this capability.  As an example GemFire will report the new partitioning meta-data when this type failure / rebalance happens.
",1.0,True,1.0,1,700.0,0,0,True
"XD-1335","Story","Sprint 32","2014-02-20T07:24:32.000+0000","grussell","","STS Gradle Import Broken","When importing XD as a gradle project into STS, it fails with

Missing directories spring-xd-hadoop/hdp20 and spring-xd-yarn

mkdir on these directories solves the problem.

The hdp20 case relates to XD-599 - it is not clear why spring-xd-yarn is needed.
",1.0,0.9998026693080916,0.999765277978044,0.9739813810581044,True,True,True,1392881072000.0,1406277256000.0,1406279900000.0,1406279000000.0,1406279900000.0,1392881072000.0,-1.0,1405931281000.0,1405934881000.0,1406276755000.0,1406279900000.0,1406277256000.0,1406279900000.0,"grussell",1405934881000.0,1392881072000.0,44,1392881072000.0,-1.0,"STS Gradle Import Broken","When importing XD as a gradle project into STS, it fails with

Missing directories spring-xd-hadoop/hdp20 and spring-xd-yarn

mkdir on these directories solves the problem.

The hdp20 case relates to XD-599 - it is not clear why spring-xd-yarn is needed.
",1.0,False,1.0,0,0.0,0,0,False
"XD-1334","Improvement","Sprint 32","2014-02-20T05:07:09.000+0000","dturanski","","Ensure XD Samples share common version dependencies","Currently samples use separate build scripts, so the XD versions, etc. may all be different. There should be a top level build script or at least a way to ensure the same version dependencies",3.0,0.9930825642342492,0.9930821110825794,0.9862432216062716,True,True,True,1392872829000.0,1406021838000.0,1406025438000.0,1406112529000.0,1406113429000.0,1392872829000.0,-1.0,1405931281000.0,1405934881000.0,1406021832000.0,1406025432000.0,1406021838000.0,1406025438000.0,"dturanski",1405934881000.0,1392872829000.0,44,1392872829000.0,-1.0,"Ensure XD Samples share common version dependencies","Currently samples use separate build scripts, so the XD versions, etc. may all be different. There should be a top level build script or at least a way to ensure the same version dependencies",3.0,False,3.0,0,0.0,0,0,False
"XD-1332","Story","","2014-02-19T11:49:35.000+0000","grenfro","grenfro","XD Integration pauses should be configurable","While the current setting work while running from your laptop to local deployments.  Or running from your laptop to ec2, they are not long enough for CI to Ec2.
This should have good defaults and have CI set them to what it needs.",5.0,1.1844481951970624e-05,0.0,-1.0,True,False,True,1392810575000.0,1392810583000.0,1392814183000.0,1393485095000.0,1393485995000.0,1392810575000.0,-1.0,-1.0,-1.0,1392810575000.0,1392814175000.0,1392810583000.0,1392814183000.0,"grenfro",-1.0,-1.0,0,1392810575000.0,-1.0,"XD Integration pauses should be configurable","While the current setting work while running from your laptop to local deployments.  Or running from your laptop to ec2, they are not long enough for CI to Ec2.
This should have good defaults and have CI set them to what it needs.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1331","Story","Sprint 22","2014-02-19T07:59:32.000+0000","hillert","hillert","Make Batch Job Restarts Work using Single Node","See also XD-1320.",4.0,0.0007373032279251,0.0007373032279251,0.0024729392348466,True,True,True,1392796772000.0,1392798173000.0,1392801773000.0,1394696040000.0,1394696940000.0,1392796772000.0,-1.0,1392801471000.0,1392805071000.0,1392798173000.0,1392801773000.0,1392798173000.0,1392801773000.0,"hillert",1392805071000.0,1392796772000.0,44,1392796772000.0,-1.0,"Make Batch Job Restarts Work using Single Node","See also XD-1320.",4.0,False,4.0,0,0.0,0,0,False
"XD-1329","Improvement","Sprint 33","2014-02-18T10:00:57.000+0000","david_geary","iperumal","Add a Kafka Source","This would use the Kafka Spring Integration Extension. We have a version of this working but had to modify the adapter code as its not currently compatible with Spring Integration 4. See INTEXT-97

",8.0,0.7851438395886124,0.7849071936867567,0.803076020392944,True,True,True,1392717657000.0,1406904571000.0,1406908171000.0,1410785948000.0,1410786848000.0,1392717657000.0,1409140190000.0,1407228591000.0,1407232191000.0,1406900295000.0,1406903895000.0,1406904571000.0,1406908171000.0,"david_geary",1407232191000.0,1392717657000.0,44,1409140190000.0,1409140190000.0,"Add a Kafka Source","This would use the Kafka Spring Integration Extension. We have a version of this working but had to modify the adapter code as its not currently compatible with Spring Integration 4. See INTEXT-97

",5.0,True,5.0,1,60.0,0,0,False
"XD-1328","Story","Sprint 22","2014-02-14T19:49:28.000+0000","iperumal","iperumal","Modularize XD UI","From https://jira.springsource.org/browse/XD-1231 we understand the importance of modularizing client side javascript code. This story tracks modularization of XD UI.",8.0,0.3063248247801182,0.3063104888585997,0.3063211489028057,True,True,True,1392407368000.0,1393240706000.0,1393244306000.0,1395126907000.0,1395127807000.0,1392407368000.0,-1.0,1393240696000.0,1393244296000.0,1393240667000.0,1393244267000.0,1393240706000.0,1393244306000.0,"iperumal",1393244296000.0,1392407368000.0,44,1392407368000.0,-1.0,"Modularize XD UI","From https://jira.springsource.org/browse/XD-1231 we understand the importance of modularizing client side javascript code. This story tracks modularization of XD UI.",8.0,False,8.0,0,0.0,0,0,False
"XD-1327","Bug","Sprint 22","2014-02-14T18:02:33.000+0000","iperumal","iperumal","Rabbit source module with outputType fails to deploy","To replicate the issue:

Create stream: 
stream create rabbittest --definition ""rabbit --queues=test --outputType=text/plain | log""

Stacktrace thrown:

17:59:56,436 ERROR http-nio-9393-exec-3 rest.RestControllerAdvice:191 - Caught exception while handling a request
java.lang.IllegalArgumentException: Module option named outputType is already present
	at org.springframework.xd.module.options.FlattenedCompositeModuleOptionsMetadata.<init>(FlattenedCompositeModuleOptionsMetadata.java:56)
	at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:49)
	at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:117)
	at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:73)
	at org.springframework.xd.dirt.rest.XDController.save(XDController.java:227)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:214)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:690)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:945)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:876)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:863)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:647)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:728)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:114)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:131)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:97)
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:82)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)
	at org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
",1.0,0.1595813204508856,0.0,0.1573268921095008,True,True,True,1392400953000.0,1392401944000.0,1392405544000.0,1392406263000.0,1392407163000.0,1392400953000.0,-1.0,1392401930000.0,1392405530000.0,1392400953000.0,1392404553000.0,1392401944000.0,1392405544000.0,"iperumal",1392405530000.0,1392400953000.0,44,1392400953000.0,-1.0,"Rabbit source module with outputType fails to deploy","To replicate the issue:

Create stream: 
stream create rabbittest --definition ""rabbit --queues=test --outputType=text/plain | log""

Stacktrace thrown:

17:59:56,436 ERROR http-nio-9393-exec-3 rest.RestControllerAdvice:191 - Caught exception while handling a request
java.lang.IllegalArgumentException: Module option named outputType is already present
	at org.springframework.xd.module.options.FlattenedCompositeModuleOptionsMetadata.<init>(FlattenedCompositeModuleOptionsMetadata.java:56)
	at org.springframework.xd.module.options.DelegatingModuleOptionsMetadataResolver.resolve(DelegatingModuleOptionsMetadataResolver.java:49)
	at org.springframework.xd.dirt.stream.XDStreamParser.parse(XDStreamParser.java:117)
	at org.springframework.xd.dirt.stream.AbstractDeployer.save(AbstractDeployer.java:73)
	at org.springframework.xd.dirt.rest.XDController.save(XDController.java:227)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.springframework.web.method.support.InvocableHandlerMethod.invoke(InvocableHandlerMethod.java:214)
	at org.springframework.web.method.support.InvocableHandlerMethod.invokeForRequest(InvocableHandlerMethod.java:132)
	at org.springframework.web.servlet.mvc.method.annotation.ServletInvocableHandlerMethod.invokeAndHandle(ServletInvocableHandlerMethod.java:104)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.invokeHandleMethod(RequestMappingHandlerAdapter.java:749)
	at org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter.handleInternal(RequestMappingHandlerAdapter.java:690)
	at org.springframework.web.servlet.mvc.method.AbstractHandlerMethodAdapter.handle(AbstractHandlerMethodAdapter.java:83)
	at org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:945)
	at org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:876)
	at org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:961)
	at org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:863)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:647)
	at org.springframework.web.servlet.FrameworkServlet.service(FrameworkServlet.java:837)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:728)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:305)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.boot.actuate.trace.WebRequestTraceFilter.doFilter(WebRequestTraceFilter.java:114)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.boot.actuate.autoconfigure.EndpointWebMvcAutoConfiguration$ApplicationContextFilterConfiguration$1.doFilterInternal(EndpointWebMvcAutoConfiguration.java:131)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.web.filter.HiddenHttpMethodFilter.doFilterInternal(HiddenHttpMethodFilter.java:77)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.web.filter.HttpPutFormContentFilter.doFilterInternal(HttpPutFormContentFilter.java:88)
	at org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:108)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:97)
	at org.springframework.boot.actuate.autoconfigure.MetricFilterAutoConfiguration$MetricsFilter.doFilter(MetricFilterAutoConfiguration.java:82)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:243)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:210)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:222)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:123)
	at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:472)
	at org.apache.catalina.valves.RemoteIpValve.invoke(RemoteIpValve.java:680)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:171)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:99)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:118)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:407)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)
	at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run(NioEndpoint.java:1680)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
",1.0,False,1.0,0,0.0,0,0,False
"XD-1325","Story","Sprint 23","2014-02-14T14:16:52.000+0000","mark.pollack","","Deploy XD on YARN for a distribution other than Apache Hadoop 2.2","Suggest trying with Hortonworks 2.0",8.0,0.934336465042635,0.9343330098399852,0.8962981390710428,True,True,True,1392387412000.0,1394550727000.0,1394554327000.0,1394701861000.0,1394702761000.0,1392387412000.0,-1.0,1394462655000.0,1394466255000.0,1394550719000.0,1394554319000.0,1394550727000.0,1394554327000.0,"mark.pollack",1394466255000.0,1392387412000.0,44,1392387412000.0,-1.0,"Deploy XD on YARN for a distribution other than Apache Hadoop 2.2","Suggest trying with Hortonworks 2.0",8.0,False,8.0,0,0.0,0,0,False
"XD-1324","Story","Sprint 24","2014-02-14T14:15:56.000+0000","mark.pollack","","Make Hadoop22 the default for the build","",4.0,0.8713663456781496,0.8435870909852307,0.7012462294356445,True,True,True,1392387356000.0,1395828969000.0,1395832569000.0,1396336130000.0,1396337030000.0,1392387356000.0,-1.0,1395157050000.0,1395160650000.0,1395719250000.0,1395722850000.0,1395828969000.0,1395832569000.0,"mark.pollack",1395160650000.0,1392387356000.0,44,1392387356000.0,-1.0,"Make Hadoop22 the default for the build","",4.0,False,4.0,0,0.0,-1,0,False
"XD-1322","Story","Sprint 22","2014-02-13T11:47:07.000+0000","thomas.risberg","","Create shell scripts to deploy XD to YARN","",5.0,0.5152953514396027,0.5152953514396027,8.788187114174174e-06,True,True,True,1392292027000.0,1392819742000.0,1392823342000.0,1393315229000.0,1393316129000.0,1392292027000.0,-1.0,1392292036000.0,1392295636000.0,1392819742000.0,1392823342000.0,1392819742000.0,1392823342000.0,"thomas.risberg",1392295636000.0,1392292027000.0,44,1392292027000.0,-1.0,"Add way to provide module config options for XD on YARN","There seems to be some intersection with the work for this issue and the rationalization of how module properties are handled.  There will be changes to configuration/property management support such that each module (source, sink, etc) will be able to also be overridden in spring-xd.yml (or wherever -Dspring.config.location points to.  The HDFS sink module for example, will have default values based on it's OptionsMetadata and will be of the form <type>.<module>.<option>


That means in the configuration for hdfs.xml sink, there would be a config section such as

{code:xml}
    <configuration>
      fs.default.name=${sink.hdfs.hd.fs}
      mapred.job.tracker=${sink.hdfs.hd.jt}
      yarn.resourcemanager.address=${sink.hdfs.hd.rm}
      mapreduce.framework.name=${sink.hdfs.mr.fw}
    </configuration>
{code}

With default values defined by a HdfsSinkOptionsMetadata class.  The hdfs.xml module file would not contain any references to a properties file.

A file specified by -Dspring.config.location could override the values in a config section such as

sink:
  hdfs:
    hd.fs : hdfs://foobarhost:8020
    hd.jt : 10.123.123.123:9000

etc.

",5.0,False,5.0,0,0.0,-1,0,True
"XD-1321","Story","Sprint 22","2014-02-13T11:45:14.000+0000","thomas.risberg","thomas.risberg","Add YARN Client and AppMaster implementations","Add YARN specifci code based on Janne's prototyping",8.0,0.000146454626404,0.000146454626404,3.417274616093606e-05,True,True,True,1392291914000.0,1392292064000.0,1392295664000.0,1393315222000.0,1393316122000.0,1392291914000.0,-1.0,1392291949000.0,1392295549000.0,1392292064000.0,1392295664000.0,1392292064000.0,1392295664000.0,"thomas.risberg",1392295549000.0,1392291914000.0,44,1392291914000.0,-1.0,"Add XD deployment for YARN","Add YARN specific code based on Janne's prototyping

Add YARN Client and AppMaster implementations and startup config files

This includes shell scripts to deploy XD to YARN

Test working on Apache 2.2 distribution

We can modify config files, everything should be possible to override by providing command-line args or env variables.
./xd-yarn-deploy --zipFile /tmp/spring-xd-yarn.zip --config /tmp/spring-xd-yarn.yml

",8.0,False,8.0,0,0.0,-1,0,True
"XD-1319","Story","","2014-02-13T03:27:49.000+0000","eric.bottard","","Allow mixins of ModuleOptionsMetadata","A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains (eg jdbc + hdfs).

I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like:

public class JdbcHdfsOptionsMetadata {

  @OptionsMixin
  private JdbcOptionsMetadata jdbc;

  @OptionsMixin
  private HdfsOptionsMetadata hdfs;
}

this would expose eg ""driverClass"" as well as ""rolloverSize"" as top level options. Values could be actually injected into the fields, so that eg custom validation could occur (default validation for the mixin class would occur by default)",5.0,-1.0,-1.0,-1.0,False,False,False,1392262069000.0,-1.0,-1.0,1396248150000.0,1396249050000.0,1392262069000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1392262069000.0,-1.0,"Allow mixins of ModuleOptionsMetadata","A lot of modules have similar options. Moreover, job modules often have options that belong to at least two domains (eg jdbc + hdfs).

I think that by using FlattenedCompositeModuleOptionsMetadata, we could come up with a way to combine several options POJOs into one. Something like:

public class JdbcHdfsOptionsMetadata {

  @OptionsMixin
  private JdbcOptionsMetadata jdbc;

  @OptionsMixin
  private HdfsOptionsMetadata hdfs;
}

this would expose eg ""driverClass"" as well as ""rolloverSize"" as top level options. Values could be actually injected into the fields, so that eg custom validation could occur (default validation for the mixin class would occur by default)",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1318","Bug","Sprint 22","2014-02-12T16:03:13.000+0000","iperumal","iperumal","XD container can not be started before the admin server","The job single partitioned step support (from singlestep-partitioner-support.xml) has the batch job DAOs (loaded from batch.xml).

During container startup, when the jobExecutionDao bean is initialized it makes the db connection to the underlying batch database (which admin server initializes). 

Here is the exception:

15:30:03,600  INFO main xml.XmlBeanDefinitionReader:316 - Loading XML bean definitions from class path resource [META-INF/spring-xd/batch/batch.xml]
15:30:06,154  WARN main annotation.ConfigurationClassEnhancer:318 - @Bean method StepScopeConfiguration.stepScope is non-static and returns an object assignable to Spring's BeanFactoryPostProcessor interface. This will result in a failure to process annotations such as @Autowired, @Resource and @PostConstruct within the method's declaring @Configuration class. Add the 'static' modifier to this method to avoid these container lifecycle issues; see @Bean Javadoc for complete details
15:30:06,705  INFO main support.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:309 - Bean 'org.springframework.xd.dirt.server.ParentConfiguration$JmxConfiguration' of type [class org.springframework.xd.dirt.server.ParentConfiguration$JmxConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
15:30:07,266  INFO main annotation.AnnotationMBeanExporter:416 - Registering beans for JMX exposure on startup
15:30:07,291  INFO main annotation.AnnotationMBeanExporter:896 - Bean with name 'XDParentConfigMBeanExporter' has been autodetected for JMX exposure
15:30:07,299  INFO main annotation.AnnotationMBeanExporter:659 - Located managed bean 'XDParentConfigMBeanExporter': registering with JMX server as MBean [org.springframework.integration.monitor:name=XDParentConfigMBeanExporter,type=IntegrationMBeanExporter]
15:30:09,637  INFO main concurrent.ThreadPoolTaskScheduler:165 - Initializing ExecutorService  'scheduler'
15:56:08,788  INFO main concurrent.ThreadPoolTaskScheduler:203 - Shutting down ExecutorService 'scheduler'
15:56:08,806  INFO main annotation.AnnotationMBeanExporter:434 - Unregistering JMX-exposed beans on shutdown
15:56:08,853  INFO main autoconfigure.AutoConfigurationReportLoggingInitializer:118 - 

Error starting ApplicationContext. To display the auto-configuration report enabled debug logging (start with --debug)


15:56:08,854  INFO main listener.ClasspathLoggingApplicationListener:54 - Application failed to start with classpath: [file:/Users/iperumal/workspace/spring-xd/modules/processor/scripts/, file:/Users/iperumal/workspace/spring-xd/spring-xd-dirt/bin/, file:/Users/iperumal/workspace/spring-xd/spring-xd-test/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-test/4.0.0.M3/74e696bad60aab349c74f52839eb43ed0e1ce0e2/spring-integration-test-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-amqp/4.0.0.M3/32dd5001acffd82391d756cf3b5ba73ca4075aed/spring-integration-amqp-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-redis/4.0.0.M3/ed5e47b6844212bb88c112c559556b4cb3d6b087/spring-integration-redis-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop/2.0.0.M4/9f1acbf66f3a97d42a8f5b00eb0c0cad11562730/spring-data-hadoop-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-redis/1.1.1.RELEASE/e2d5e9cfdaaa3fbcc2a8d4bdbe06daf771cb4e39/spring-data-redis-1.1.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/4.0.1.RELEASE/cb996939c8d48ae55ec933041f17e7fba4d9e27d/spring-context-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context-support/4.0.1.RELEASE/94dc23c49a74f3f4b894b29416b08202e5976f49/spring-context-support-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/4.0.1.RELEASE/b93b2c39b09ff858a42db85a0a9a8ce232a6779/spring-tx-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/4.0.1.RELEASE/367212c3b84c63a48220efa0fe8e9a3a937fcf68/spring-test-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.lambdaworks/lettuce/2.3.3/1366615be02807a568c5f2d3a4475a3d27a879a6/lettuce-2.3.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hsqldb/hsqldb/2.3.0/93306187b1a782f2b929d12536022185487037d2/hsqldb-2.3.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-jdbc/7.0.42/3827da9ca05ff115f239a2372bd44cfd729c692d/tomcat-jdbc-7.0.42.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/junit/junit/4.11/4e031bb61df09069aeb2bffb4019e7a5034a4ee0/junit-4.11.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.4/b1b6ea3b7e4aa4f492509a4952029cd8e48019ad/commons-io-2.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.groovy/groovy-all/2.1.0/a14306a090eec2fa91017b77ac079361f68e1830/groovy-all-2.1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-library/1.3/4785a3c21320980282f9f33d0d1264a69040538f/hamcrest-library-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-core/1.3/42a25dc3219429f0e5d060061f71acb49bf010a0/hamcrest-core-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/1.0/9b473564e792c2bdf1449da1f0b1b5bff9805704/objenesis-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/1.9.5/c3264abeea62c4d2f367e21484fbb40c7e256393/mockito-core-1.9.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-logging/commons-logging/1.1.1/5043bfebc3db072ed80fbd362e7caf00e885d8ae/commons-logging-1.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/4.0.1.RELEASE/e39774d97c9dadfe49e6dfd16e3868bc1e390554/spring-core-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/4.0.1.RELEASE/605582e95fb62b43fb4a843babdcf739f3497e92/spring-beans-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/aopalliance/aopalliance/1.0/235ba8b489512805ac13a8f9ea77a1ca5ebe3e8/aopalliance-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/4.0.1.RELEASE/ff68e4cfdbb2be3e8d8a7f34e7cbacc1860dfe75/spring-aop-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/4.0.1.RELEASE/452cb22401e868a1e79677dd22b6a3097fc603fa/spring-expression-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.0.3.RELEASE/33b967f6abaa0a496318bff2ce96e6da6285a54d/spring-retry-1.0.3.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/4.0.1.RELEASE/829829afd9135368faa1e3a5261404f602a2e939/spring-messaging-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-core/4.0.0.M3/12b445cfa896b906facd2be289adcdfe839f6104/spring-integration-core-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-all/1.3/63a21ebc981131004ad02e0434e799fd7f3a8d5a/hamcrest-all-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.amqp/spring-amqp/1.3.0.M2/e668db16a4206e96531b978e5978868ba0ebf4e9/spring-amqp-1.3.0.M2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.rabbitmq/amqp-client/3.2.2/9e4485e734415e84ea3caea25650f8651f388a3a/amqp-client-3.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.amqp/spring-rabbit/1.3.0.M2/ceb54c437d2d00c3a22d59982922f24fbf78c8a/spring-rabbit-1.3.0.M2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.6.6/ce53b0a0e2cfbb27e8a59d38f79a18a5c6a8d2b0/slf4j-api-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/jcl-over-slf4j/1.6.6/ec497945fdcaf7fd970ae9931b9bbfaf735d385e/jcl-over-slf4j-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jdbc/4.0.1.RELEASE/7d46d07d44f56af7cdcbba53ff671c5487f9547/spring-jdbc-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.2/2bf96b7aa8b611c177d329452af1dc933e14501c/commons-cli-1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/xmlenc/xmlenc/0.52/d82554efbe65906d83b3d97bd7509289e9db561a/xmlenc-0.52.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-codec/commons-codec/1.4/4216af16d38465bbab0f3dff8efa14204f7a399a/commons-codec-1.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.0.1/d6364bcc1b2b2aa69d008602d36a700453648560/commons-httpclient-3.0.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-math/2.1/b3c4bdc2778ddccceb8da2acec3e37bfa41303e9/commons-math-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2.1/761ea405b9b37ced573d2df0d1e3a4e0f9edc668/commons-collections-3.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-lang/commons-lang/2.4/16313e02a793435009f1e458fa4af5d879f6fb11/commons-lang-2.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.7.0/5675fd96b29656504b86029551973d60fb41339b/commons-beanutils-1.7.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-digester/commons-digester/1.8/dc6a73fdbd1fa3f0944e8497c6c872fa21dca37e/commons-digester-1.8.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils-core/1.8.0/175dc721f87e4bc5cc0573f990e28c3cf9117508/commons-beanutils-core-1.8.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-configuration/commons-configuration/1.6/32cadde23955d7681b0d94a2715846d20b425235/commons-configuration-1.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/oro/oro/2.0.8/5592374f834645c4ae250f4c9fbb314c9369d698/oro-2.0.8.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-net/commons-net/1.4.1/abb932adb2c10790c1eaa4365d3ac2a1ac7cb700/commons-net-1.4.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-el/commons-el/1.0/1df2c042b3f2de0124750241ac6c886dbfa2cc2c/commons-el-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.eclipse.jdt/core/3.1.1/88c83ce444cf46d02494da37c9fa1eebc9ce9cea/core-3.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.hadoop/hadoop-core/1.2.1/3e5874122a26a735162a380627210779b41bfd59/hadoop-core-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.hadoop/hadoop-streaming/1.2.1/4baac190cf4cd4a6d085780cbcab1a89493f932b/hadoop-streaming-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.hadoop/hadoop-tools/1.2.1/b08c16bd0448fbcadab67c4f8df837c094fdc91e/hadoop-tools-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop-core/2.0.0.M4/ff4cefb0870d61fdc9efe26d118310c02b5eafbb/spring-data-hadoop-core-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop-batch/2.0.0.M4/47de250d5d9b48ed1319a747e3b06fdc46d939ef/spring-data-hadoop-batch-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/io.netty/netty/3.6.6.Final/e4e40738ce9bee0a92389cb739c94d7839778647/netty-3.6.6.Final.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-juli/7.0.42/f0049ac94514d69231c41ed96238efb94ffdd9cf/tomcat-juli-7.0.42.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-analytics/bin/, file:/Users/iperumal/workspace/spring-xd/spring-xd-tuple/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.2.2/3c8f6018eaa72d43b261181e801e6f8676c16ef6/jackson-databind-2.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-infrastructure/3.0.0.BUILD-SNAPSHOT/cfaea737589c43c54ff338ae27e1bee477620176/spring-batch-infrastructure-3.0.0.BUILD-SNAPSHOT.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.eaio.uuid/uuid/3.2/77ba5105d949cd589aff75400d9f7d3676691a46/uuid-3.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.2.2/285cb9c666f0f0f3dd8a1be04e1f457eb7b15113/jackson-annotations-2.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.2.2/d20be6a5ddd6f8cfd36ebf6dea329873a1c41f1b/jackson-core-2.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-commons/1.6.2.RELEASE/e96a0458cdc3179ca70c880f42315bb75df4faf5/spring-data-commons-1.6.2.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/joda-time/joda-time/2.1/8f79e353ef77da6710e1f10d34fc3698eaaacbca/joda-time-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/log4j/log4j/1.2.17/5af35056b4d257e4b64b9e8069c0746e8b08629f/log4j-1.2.17.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/jcl-over-slf4j/1.7.5/cd5970bd13fa85f7bed41ca606d6daf7cbf1365/jcl-over-slf4j-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.5/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/nl.jqno.equalsverifier/equalsverifier/1.1.3/60cd685f314a9cebfd0595d88fea45fba2f47918/equalsverifier-1.1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.5/6b262da268f8ad9eff941b25503a9198f0a0ac93/slf4j-api-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/1.1/63db1176f16448172611266154e4f6d39a0e1e68/objenesis-1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/cglib/cglib-nodep/2.2/59afed7ab65e7ec6585d5bc60556c3cbd203532b/cglib-nodep-2.2.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-rest-domain/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.hateoas/spring-hateoas/0.8.0.RELEASE/819c25e1ff12b7fca483d76b4e7d20221f621fcd/spring-hateoas-0.8.0.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-admin-manager/1.3.0.M1/5afc7442417af8c46ae51480ed2b83943283d449/spring-batch-admin-manager-1.3.0.M1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-core/3.0.0.BUILD-SNAPSHOT/8168f58716cd305040eaa87c82dc61822b03415c/spring-batch-core-3.0.0.BUILD-SNAPSHOT.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.jackson/jackson-core-asl/1.9.13/3c304d70f42f832e0a86d45bd437f692129299a4/jackson-core-asl-1.9.13.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/4.0.1.RELEASE/2ace92025f042e1d3ddfdbba093172e3572ac130/spring-web-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/4.0.1.RELEASE/2dbc91a6413115f7ffbe94f0fa9bc9fda3281d90/spring-webmvc-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/1.3/dc13ae4faca6df981fc7aeb5a522d9db446d5d50/objenesis-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.2/81d61b7f33ebeab314e07de0cc596f8e858d97/slf4j-api-1.7.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjrt/1.6.6/ff58f520e1a304b8a02b8cea8b96b1b8e5b25b0/aspectjrt-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjweaver/1.6.6/c0383be877cfa4ec6b62202c942a89a6264a2be6/aspectjweaver-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-pool/commons-pool/1.3/3231230c1d7631b66a74d1c4653cfd65a6f9ea0/commons-pool-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-dbcp/commons-dbcp/1.2.2/4fd4c6110e9bca3a655b717eb2e5920febb8403d/commons-dbcp-1.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/1.4/a8762d07e76cfde2395257a5da47ba7c1dbd3dce/commons-io-1.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-lang/commons-lang/2.1/4763ecc9d78781c915c07eb03e90572c7ff04205/commons-lang-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-fileupload/commons-fileupload/1.2.1/384faa82e193d4e4b0546059ca09572654bc3970/commons-fileupload-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/net.sf.ehcache/ehcache-core/2.3.0/e59473c71a31e8e19da4fbc7028585c8ed51d69f/ehcache-core-2.3.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2/f951934aa5ae5a88d7e6dfaa6d32307d834a88be/commons-collections-3.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.freemarker/freemarker/2.3.15/c8cfe522476fcec8da5c980d58bf62d6ab0cf27c/freemarker-2.3.15.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-admin-resources/1.3.0.M1/bdf7d5afc02397385fce8731409f606e54d4d033/spring-batch-admin-resources-1.3.0.M1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.0.2.RELEASE/d673c90a9fd8f0de5f20d53d61047849f707f42b/spring-retry-1.0.2.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/javax.batch/javax.batch-api/1.0/65392d027a6eb369fd9fcd1b75cae150e25ac03c/javax.batch-api-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.ibm.jbatch/com.ibm.jbatch-tck-spi/1.0/8ac869b0a60bff1a15eba0fb6398942410396938/com.ibm.jbatch-tck-spi-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/xpp3/xpp3_min/1.1.4c/19d4e90b43059058f6e056f794f0ea4030d60b86/xpp3_min-1.1.4c.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.thoughtworks.xstream/xstream/1.3/3f755b1a46744302712b1b962c4ab64de392f477/xstream-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.jettison/jettison/1.1/1a01a2a1218fcf9faa2cc2a6ced025bdea687262/jettison-1.1.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-module/bin/, file:/Users/iperumal/workspace/spring-xd/spring-xd-module-spi/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/1.0.0.RC1/7830d0dd26f75841d8b5c2c72c42b864b1192ddb/spring-boot-autoconfigure-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/javax.validation/validation-api/1.0.0.GA/b6bd7f9d78f6fdaa3c37dae18a4bd298915f328e/validation-api-1.0.0.GA.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hibernate/hibernate-validator/4.3.1.Final/49b31d8ea51fa21cc78a89e9d4ddb11d6bfb4669/hibernate-validator-4.3.1.Final.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/1.0.0.RC1/7e53b72a368c495a482d3a213ad6338f8f7afcfa/spring-boot-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jboss.logging/jboss-logging/3.1.0.CR2/28725380c07f917ace4e511db21cc45e9ae5a72b/jboss-logging-3.1.0.CR2.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-hadoop/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop-store/2.0.0.M4/1d3d691c0e6952ba26724339668e17040c368683/spring-data-hadoop-store-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.0/71c46e2313e92887de4247f2aabce4d09d65d82/snappy-java-1.1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.1/fd51f906669f49a4ffd06650666c3b8147a6106e/commons-io-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-codec/commons-codec/1.7/9cd61d269c88f9fb0eb36cea1efcd596ab74772f/commons-codec-1.7.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.google.code.findbugs/jsr305/1.3.9/40719ea6961c0cb6afaeb6a921eaa1f6afd4cfdf/jsr305-1.3.9.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.google.guava/guava/11.0.2/35a3c69e19d72743cac83778aecbee68680f63eb/guava-11.0.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.jackson/jackson-mapper-asl/1.9.13/1ee2f2bed0e5dd29d1cb155a166e6f8d50bbddb7/jackson-mapper-asl-1.9.13.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.3/4a85963a752c0a2f715c3924bfc686865e7e1bc6/paranamer-2.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.tukaani/xz/1.0/ecff5cb8b1189514c9d1d8d68eb77ac372e000c9/xz-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-compress/1.4.1/b02e84a993d88568417536240e970c4b809126fd/commons-compress-1.4.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.6.4/2396d74b12b905f780ed7966738bb78438e8371a/slf4j-api-1.6.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.avro/avro/1.7.5/8343a5b33f56fa16306ed27fa7b1a79278c26c2d/avro-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-common/1.2.5/fd1e6d42b3cb63f05ea5a1d22ded5ab63837b70f/parquet-common-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-generator/1.2.5/f16fd52b93d6677be340e7ac61609d6b91ba5ab0/parquet-generator-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-encoding/1.2.5/e96b9cd40f0eebc4ae483b0187401aa444ba36c0/parquet-encoding-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-column/1.2.5/1b1c618df93b16eb684c120662d56cde1b2caa99/parquet-column-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-format/1.0.0/ffaa2809ce56db3d7288ae118d876be30e9d1eb0/parquet-format-1.0.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-hadoop/1.2.5/6c87620226b4e156dac44af45171055b9ea78c8/parquet-hadoop-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-avro/1.2.5/8e7ad3c7f06b40e6d09417f82b0e4c703d8918ad/parquet-avro-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-jexl/2.1.1/6ecc181debade00230aa1e17666c4ea0371beaaa/commons-jexl-2.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/net.sf.opencsv/opencsv/2.3/c23708cdb9e80a144db433e23344a788a1fd6599/opencsv-2.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/0.10.0/916792f62bb1177ec4e9f3dedc53f19d9ae869a/kite-data-core-0.10.0.jar, file:/Users/iperumal/workspace/spring-xd/extensions/spring-xd-extension-reactor/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.projectreactor/reactor-spring/1.0.0.RC1/1652e63c473f1824ca7185d12be70981f97913e0/reactor-spring-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.projectreactor/reactor-tcp/1.0.0.RC1/d275a1bdb5a449a1e9638dcd99f9a97a97c26b45/reactor-tcp-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/0.8.1/fbcad75a02160def33c4359b24015cca2d91ccce/json-path-0.8.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.lmax/disruptor/3.2.0/ac62995678dd4b906e85b26354aa2ebfda130c32/disruptor-3.2.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.projectreactor/reactor-core/1.0.0.RC1/202718fd7bbe443de0004e305edb075355b0b340/reactor-core-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/io.netty/netty-all/4.0.6.Final/1ce815d4d1ab73b17e20a5f9c206f422014deb4a/netty-all-4.0.6.Final.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/1.1.1/24a2f903d25e004de30ac602c5b47f2d4e420a59/json-smart-1.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-lang/commons-lang/2.6/ce1edb914c94ebc388f086c6827e8bdeec71ac2/commons-lang-2.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.cloud/spring-service-connector/0.9.2/75e716e825fe66db6047cf1e026ca7581a5c933a/spring-service-connector-0.9.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.cloud/cloudfoundry-connector/0.9.2/d0f4df0a67f13704db02e688fc9d85d1c9ed5498/cloudfoundry-connector-0.9.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator/1.0.0.RC1/88bf421071c1a38a9fda606c5f3cab609c74bf71/spring-boot-actuator-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-event/4.0.0.M3/d0abb5b6a6d8050a71cbb6e95c69d484bb26c005/spring-integration-event-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-file/4.0.0.M3/ba6b17b85f2847326bf5ea0444872c62de0f5e8f/spring-integration-file-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-ftp/4.0.0.M3/6ddfabd427c6edb5114595903eead0cd042a6ec7/spring-integration-ftp-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-groovy/4.0.0.M3/53203058a9f6d703ae0e1d86ebb59c6ee2515c14/spring-integration-groovy-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-http/4.0.0.M3/b78a8227736b84aa857dbec75a6ef4c47a0563be/spring-integration-http-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-jmx/4.0.0.M3/d2b332cc945281327fc91b8641fa71b7340dfdcf/spring-integration-jmx-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-spring/1.1.5/e9a8d8710c19381327afe4a7e5af9cc4aa5e7ef4/jolokia-spring-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-client-java/1.1.5/e0f24b062d3f359737a24a0130986e20f0a77e5c/jolokia-client-java-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.esotericsoftware.kryo/kryo/2.22/1b624c2db8eb3a40d84362f22c2078ebde3e8c81/kryo-2.22.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/args4j/args4j/2.0.16/9f00fb12820743b9e05c686eba543d64dd43f2b1/args4j-2.0.16.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-integration/3.0.0.BUILD-SNAPSHOT/e11f30ff7b6da07e7f630752c5d92c41eorg.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jobExecutionDao' defined in class path resource [META-INF/spring-xd/batch/batch.xml]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not inspect meta data for database type.  You have to supply it explicitly.
84de90a/spring-batch-integration-3.0.0.BUILD-SNAPSHOT.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.plugin/spring-plugin-core/0.8.0.RELEASE/4390edad56632d97fec6abe8dba96531234b9603/spring-plugin-core-0.8.0.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/7.0.35/3efe5c4c44215a514fa804c5ef829569f6652b8b/tomcat-embed-core-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-logging-juli/7.0.35/5e92715a81731f6615053a399e1d3e9a2c05d4b9/tomcat-embed-logging-juli-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-jsp-api/7.0.35/30a8560fad20cf681da4b26a5100d474559fda8a/tomcat-jsp-api-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.sun.mail/javax.mail/1.5.0/ec2410fdf7e0a3022e7c2a2e6241039d1abc1e98/javax.mail-1.5.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.12/ebe66a6b88caab31d7a19571ad23656377523545/snakeyaml-1.12.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.postgresql/postgresql/9.2-1002-jdbc4/a535facdcc0b4c488798e0c19499c08b1bc3a2e/postgresql-9.2-1002-jdbc4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/mysql/mysql-connector-java/5.1.23/c264e2114579474d13dd808a510fc74e762dda8c/mysql-connector-java-5.1.23.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.activemq/activemq-core/5.6.0/9996dd23872b8588d63cf8a6cc8522b6ad8caa85/activemq-core-5.6.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jms/4.0.1.RELEASE/7d74a1414cd45349ed61ee3f3e1fca019090358c/spring-jms-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/1.0.0.RC1/15b007421d5419e435cf508057d458a065126d00/spring-boot-1.0.0.RC1-tests.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.social/spring-social-twitter/1.0.5.RELEASE/6c4b7e69b060862d98c87588d827d4feabbcd68/spring-social-twitter-1.0.5.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.3.1/c4096a8323bbbcbeda072e3def123a9b66783361/jackson-databind-2.3.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hsqldb/hsqldb/2.3.1/bd7153caa30958e1c64a3dd38b93e5a7442d9a0e/hsqldb-2.3.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.cloud/core/0.9.2/32631d9bd70d432e77df0ca64db21b49d28f9589/core-0.9.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.3.0/f5e853a20b60758922453d56f9ae1e64af5cb3da/jackson-annotations-2.3.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.3.1/f9f7185c92ca5fefe2fb3efdeb477a67c96ea2d0/jackson-core-2.3.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-net/commons-net/3.0.1/643cc426b9f75aa111fac0fac0e52ff5d991a337/commons-net-3.0.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-scripting/4.0.0.M3/6b0260ca4709da0bdd230d7bca41f3764a91ed4c/spring-integration-scripting-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.googlecode.json-simple/json-simple/1.1/5e303a03d04e6788dddfa3655272580ae0fc13bb/json-simple-1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-core/1.1.5/56bdd8956fb2bb3a9241267a7cb22d43f27d0a71/jolokia-core-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-jvm/1.1.5/c59f18d004d64bea982d5128fbf08b4a177c1720/jolokia-jvm-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-jmx/1.1.5/c7026c2cf9111a138538cc6705ca4c41f70be8fa/jolokia-jmx-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpcore/4.1.2/39e30fb2ddfcf32b9dd663039ca6e847b698aa6f/httpcore-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient/4.1.2/9003011c1b6d28a7331f04251a25e7ccf63c98f8/httpclient-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpmime/4.1.2/c78eaaea68d0f36f73b646f08cdb34f1a784239b/httpmime-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient-cache/4.1.2/6906f34467097e240ca451652f5738245a2face0/httpclient-cache-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient-osgi/4.1.2/c7577b4f8877a0a29ae5cc65331dcbe9960f8cce/httpclient-osgi-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-el-api/7.0.35/5e117f3f62c49d52cb72d028e76c5d16a8d148ca/tomcat-el-api-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-servlet-api/7.0.35/87bd606eef7d42ff3ec324d198549ccd8ccb56fc/tomcat-servlet-api-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/javax.activation/activation/1.1/e6cb541461c2834bdea3eb920f1884d1eb508b50/activation-1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.geronimo.specs/geronimo-jms_1.1_spec/1.1.1/c872b46c601d8dc03633288b81269f9e42762cea/geronimo-jms_1.1_spec-1.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.activemq/kahadb/5.6.0/ab90275013deb4a1652a49dc6b01a894b8138a48/kahadb-5.6.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.activemq.protobuf/activemq-protobuf/1.1/26682eb801f70563511f7c424dc10e8b3e66340e/activemq-protobuf-1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.fusesource.hawtdispatch/hawtdispatch/1.9/cbfeed0be226850799a7ef819b0dfa3346198af6/hawtdispatch-1.9.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.fusesource.hawtdispatch/hawtdispatch-transport/1.9/5e29bb84478f8f2526e0cae2cb74b9978f3c01d0/hawtdispatch-transport-1.9.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.fusesource.hawtbuf/hawtbuf/1.9/4a42b835d1df77db8c9a144a11ebb4600a372f5f/hawtbuf-1.9.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.fusesource.mqtt-client/mqtt-client/1.0/4af519e42c765b78aedd63c872706f5fa1621a9c/mqtt-client-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.osgi/org.osgi.core/4.1.0/b88cd082b5b6774e9db939e28c0e3dc526c92d89/org.osgi.core-4.1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.geronimo.specs/geronimo-j2ee-management_1.1_spec/1.0.1/5372615b0c04c1913c95c34a0414cef720ca2855/geronimo-j2ee-management_1.1_spec-1.0.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jasypt/jasypt/1.8/b14e87ad376dee74bb2d088279fe2df179cfc9e0/jasypt-1.8.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.social/spring-social-core/1.0.3.RELEASE/44e648f23b45162c698e255a16759832dfcfc004/spring-social-core-1.0.3.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.security/spring-security-crypto/3.1.3.RELEASE/a0db20c52e8281bffc200429b7fcd7c2cbeeb0e0/spring-security-crypto-3.1.3.RELEASE.jar, file:/Users/iperumal/workspace/spring-xd-samples/batch-notifications/target/batch-notifications-1.0.0.BUILD-SNAPSHOT.jar]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1553)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:658)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:355)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:116)
	at org.springframework.xd.dirt.server.LauncherApplication.run(LauncherApplication.java:76)
	at org.springframework.xd.dirt.server.LauncherApplication.main(LauncherApplication.java:56)
Caused by: java.lang.IllegalArgumentException: Could not inspect meta data for database type.  You have to supply it explicitly.
	at org.springframework.batch.item.database.support.SqlPagingQueryProviderFactoryBean.getObject(SqlPagingQueryProviderFactoryBean.java:159)
	at org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao.getPagingQueryProvider(JdbcSearchableJobExecutionDao.java:134)
	at org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao.getPagingQueryProvider(JdbcSearchableJobExecutionDao.java:113)
	at org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao.getPagingQueryProvider(JdbcSearchableJobExecutionDao.java:104)
	at org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao.afterPropertiesSet(JdbcSearchableJobExecutionDao.java:92)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1612)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1549)
	... 15 more
Caused by: org.springframework.jdbc.support.MetaDataAccessException: Could not get Connection for extracting meta data; nested exception is org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused
	at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:293)
	at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:320)
	at org.springframework.batch.support.DatabaseType.fromMetaData(DatabaseType.java:95)
	at org.springframework.batch.item.database.support.SqlPagingQueryProviderFactoryBean.getObject(SqlPagingQueryProviderFactoryBean.java:155)
	... 21 more
Caused by: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused
	at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:80)
	at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:280)
	... 24 more
Caused by: java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused
	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source)
	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source)
	at org.hsqldb.jdbc.JDBCConnection.<init>(Unknown Source)
	at org.hsqldb.jdbc.JDBCDriver.getConnection(Unknown Source)
	at org.hsqldb.jdbc.JDBCDriver.connect(Unknown Source)
	at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:278)
	at org.apache.tomcat.jdbc.pool.PooledConnection.connect(PooledConnection.java:182)
	at org.apache.tomcat.jdbc.pool.ConnectionPool.createConnection(ConnectionPool.java:702)
	at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:634)
	at org.apache.tomcat.jdbc.pool.ConnectionPool.init(ConnectionPool.java:488)
	at org.apache.tomcat.jdbc.pool.ConnectionPool.<init>(ConnectionPool.java:144)
	at org.apache.tomcat.jdbc.pool.DataSourceProxy.pCreatePool(DataSourceProxy.java:116)
	at org.apache.tomcat.jdbc.pool.DataSourceProxy.createPool(DataSourceProxy.java:103)
	at org.apache.tomcat.jdbc.pool.DataSourceProxy.getConnection(DataSourceProxy.java:127)
	at org.springframework.jdbc.datasource.DataSourceUtils.doGetConnection(DataSourceUtils.java:111)
	at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:77)
	... 25 more
Caused by: org.hsqldb.HsqlException: java.net.ConnectException: Connection refused
	at org.hsqldb.ClientConnection.openConnection(Unknown Source)
	at org.hsqldb.ClientConnection.initConnection(Unknown Source)
	at org.hsqldb.ClientConnection.<init>(Unknown Source)
	... 39 more
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at java.net.Socket.<init>(Socket.java:425)
	at java.net.Socket.<init>(Socket.java:208)
	at org.hsqldb.server.HsqlSocketFactory.createSocket(Unknown Source)
	... 42 more


",3.0,0.0890781462285513,0.0881975544708901,0.0889146077592713,True,True,True,1392220993000.0,1392228074000.0,1392231674000.0,1392299585000.0,1392300485000.0,1392220993000.0,-1.0,1392228061000.0,1392231661000.0,1392228004000.0,1392231604000.0,1392228074000.0,1392231674000.0,"iperumal",1392231661000.0,1392220993000.0,44,1392220993000.0,-1.0,"XD container can not be started before the admin server","The job single partitioned step support (from singlestep-partitioner-support.xml) has the batch job DAOs (loaded from batch.xml).

During container startup, when the jobExecutionDao bean is initialized it makes the db connection to the underlying batch database (which admin server initializes). 

Here is the exception:

15:30:03,600  INFO main xml.XmlBeanDefinitionReader:316 - Loading XML bean definitions from class path resource [META-INF/spring-xd/batch/batch.xml]
15:30:06,154  WARN main annotation.ConfigurationClassEnhancer:318 - @Bean method StepScopeConfiguration.stepScope is non-static and returns an object assignable to Spring's BeanFactoryPostProcessor interface. This will result in a failure to process annotations such as @Autowired, @Resource and @PostConstruct within the method's declaring @Configuration class. Add the 'static' modifier to this method to avoid these container lifecycle issues; see @Bean Javadoc for complete details
15:30:06,705  INFO main support.PostProcessorRegistrationDelegate$BeanPostProcessorChecker:309 - Bean 'org.springframework.xd.dirt.server.ParentConfiguration$JmxConfiguration' of type [class org.springframework.xd.dirt.server.ParentConfiguration$JmxConfiguration] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying)
15:30:07,266  INFO main annotation.AnnotationMBeanExporter:416 - Registering beans for JMX exposure on startup
15:30:07,291  INFO main annotation.AnnotationMBeanExporter:896 - Bean with name 'XDParentConfigMBeanExporter' has been autodetected for JMX exposure
15:30:07,299  INFO main annotation.AnnotationMBeanExporter:659 - Located managed bean 'XDParentConfigMBeanExporter': registering with JMX server as MBean [org.springframework.integration.monitor:name=XDParentConfigMBeanExporter,type=IntegrationMBeanExporter]
15:30:09,637  INFO main concurrent.ThreadPoolTaskScheduler:165 - Initializing ExecutorService  'scheduler'
15:56:08,788  INFO main concurrent.ThreadPoolTaskScheduler:203 - Shutting down ExecutorService 'scheduler'
15:56:08,806  INFO main annotation.AnnotationMBeanExporter:434 - Unregistering JMX-exposed beans on shutdown
15:56:08,853  INFO main autoconfigure.AutoConfigurationReportLoggingInitializer:118 - 

Error starting ApplicationContext. To display the auto-configuration report enabled debug logging (start with --debug)


15:56:08,854  INFO main listener.ClasspathLoggingApplicationListener:54 - Application failed to start with classpath: [file:/Users/iperumal/workspace/spring-xd/modules/processor/scripts/, file:/Users/iperumal/workspace/spring-xd/spring-xd-dirt/bin/, file:/Users/iperumal/workspace/spring-xd/spring-xd-test/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-test/4.0.0.M3/74e696bad60aab349c74f52839eb43ed0e1ce0e2/spring-integration-test-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-amqp/4.0.0.M3/32dd5001acffd82391d756cf3b5ba73ca4075aed/spring-integration-amqp-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-redis/4.0.0.M3/ed5e47b6844212bb88c112c559556b4cb3d6b087/spring-integration-redis-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop/2.0.0.M4/9f1acbf66f3a97d42a8f5b00eb0c0cad11562730/spring-data-hadoop-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-redis/1.1.1.RELEASE/e2d5e9cfdaaa3fbcc2a8d4bdbe06daf771cb4e39/spring-data-redis-1.1.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/4.0.1.RELEASE/cb996939c8d48ae55ec933041f17e7fba4d9e27d/spring-context-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context-support/4.0.1.RELEASE/94dc23c49a74f3f4b894b29416b08202e5976f49/spring-context-support-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/4.0.1.RELEASE/b93b2c39b09ff858a42db85a0a9a8ce232a6779/spring-tx-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/4.0.1.RELEASE/367212c3b84c63a48220efa0fe8e9a3a937fcf68/spring-test-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.lambdaworks/lettuce/2.3.3/1366615be02807a568c5f2d3a4475a3d27a879a6/lettuce-2.3.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hsqldb/hsqldb/2.3.0/93306187b1a782f2b929d12536022185487037d2/hsqldb-2.3.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-jdbc/7.0.42/3827da9ca05ff115f239a2372bd44cfd729c692d/tomcat-jdbc-7.0.42.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/junit/junit/4.11/4e031bb61df09069aeb2bffb4019e7a5034a4ee0/junit-4.11.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.4/b1b6ea3b7e4aa4f492509a4952029cd8e48019ad/commons-io-2.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.groovy/groovy-all/2.1.0/a14306a090eec2fa91017b77ac079361f68e1830/groovy-all-2.1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-library/1.3/4785a3c21320980282f9f33d0d1264a69040538f/hamcrest-library-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-core/1.3/42a25dc3219429f0e5d060061f71acb49bf010a0/hamcrest-core-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/1.0/9b473564e792c2bdf1449da1f0b1b5bff9805704/objenesis-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/1.9.5/c3264abeea62c4d2f367e21484fbb40c7e256393/mockito-core-1.9.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-logging/commons-logging/1.1.1/5043bfebc3db072ed80fbd362e7caf00e885d8ae/commons-logging-1.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/4.0.1.RELEASE/e39774d97c9dadfe49e6dfd16e3868bc1e390554/spring-core-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/4.0.1.RELEASE/605582e95fb62b43fb4a843babdcf739f3497e92/spring-beans-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/aopalliance/aopalliance/1.0/235ba8b489512805ac13a8f9ea77a1ca5ebe3e8/aopalliance-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/4.0.1.RELEASE/ff68e4cfdbb2be3e8d8a7f34e7cbacc1860dfe75/spring-aop-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/4.0.1.RELEASE/452cb22401e868a1e79677dd22b6a3097fc603fa/spring-expression-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.0.3.RELEASE/33b967f6abaa0a496318bff2ce96e6da6285a54d/spring-retry-1.0.3.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/4.0.1.RELEASE/829829afd9135368faa1e3a5261404f602a2e939/spring-messaging-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-core/4.0.0.M3/12b445cfa896b906facd2be289adcdfe839f6104/spring-integration-core-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest-all/1.3/63a21ebc981131004ad02e0434e799fd7f3a8d5a/hamcrest-all-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.amqp/spring-amqp/1.3.0.M2/e668db16a4206e96531b978e5978868ba0ebf4e9/spring-amqp-1.3.0.M2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.rabbitmq/amqp-client/3.2.2/9e4485e734415e84ea3caea25650f8651f388a3a/amqp-client-3.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.amqp/spring-rabbit/1.3.0.M2/ceb54c437d2d00c3a22d59982922f24fbf78c8a/spring-rabbit-1.3.0.M2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.6.6/ce53b0a0e2cfbb27e8a59d38f79a18a5c6a8d2b0/slf4j-api-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/jcl-over-slf4j/1.6.6/ec497945fdcaf7fd970ae9931b9bbfaf735d385e/jcl-over-slf4j-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jdbc/4.0.1.RELEASE/7d46d07d44f56af7cdcbba53ff671c5487f9547/spring-jdbc-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.2/2bf96b7aa8b611c177d329452af1dc933e14501c/commons-cli-1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/xmlenc/xmlenc/0.52/d82554efbe65906d83b3d97bd7509289e9db561a/xmlenc-0.52.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-codec/commons-codec/1.4/4216af16d38465bbab0f3dff8efa14204f7a399a/commons-codec-1.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-httpclient/commons-httpclient/3.0.1/d6364bcc1b2b2aa69d008602d36a700453648560/commons-httpclient-3.0.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-math/2.1/b3c4bdc2778ddccceb8da2acec3e37bfa41303e9/commons-math-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2.1/761ea405b9b37ced573d2df0d1e3a4e0f9edc668/commons-collections-3.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-lang/commons-lang/2.4/16313e02a793435009f1e458fa4af5d879f6fb11/commons-lang-2.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils/1.7.0/5675fd96b29656504b86029551973d60fb41339b/commons-beanutils-1.7.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-digester/commons-digester/1.8/dc6a73fdbd1fa3f0944e8497c6c872fa21dca37e/commons-digester-1.8.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-beanutils/commons-beanutils-core/1.8.0/175dc721f87e4bc5cc0573f990e28c3cf9117508/commons-beanutils-core-1.8.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-configuration/commons-configuration/1.6/32cadde23955d7681b0d94a2715846d20b425235/commons-configuration-1.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/oro/oro/2.0.8/5592374f834645c4ae250f4c9fbb314c9369d698/oro-2.0.8.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-net/commons-net/1.4.1/abb932adb2c10790c1eaa4365d3ac2a1ac7cb700/commons-net-1.4.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-el/commons-el/1.0/1df2c042b3f2de0124750241ac6c886dbfa2cc2c/commons-el-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.eclipse.jdt/core/3.1.1/88c83ce444cf46d02494da37c9fa1eebc9ce9cea/core-3.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.hadoop/hadoop-core/1.2.1/3e5874122a26a735162a380627210779b41bfd59/hadoop-core-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.hadoop/hadoop-streaming/1.2.1/4baac190cf4cd4a6d085780cbcab1a89493f932b/hadoop-streaming-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.hadoop/hadoop-tools/1.2.1/b08c16bd0448fbcadab67c4f8df837c094fdc91e/hadoop-tools-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop-core/2.0.0.M4/ff4cefb0870d61fdc9efe26d118310c02b5eafbb/spring-data-hadoop-core-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop-batch/2.0.0.M4/47de250d5d9b48ed1319a747e3b06fdc46d939ef/spring-data-hadoop-batch-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/io.netty/netty/3.6.6.Final/e4e40738ce9bee0a92389cb739c94d7839778647/netty-3.6.6.Final.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-juli/7.0.42/f0049ac94514d69231c41ed96238efb94ffdd9cf/tomcat-juli-7.0.42.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-analytics/bin/, file:/Users/iperumal/workspace/spring-xd/spring-xd-tuple/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.2.2/3c8f6018eaa72d43b261181e801e6f8676c16ef6/jackson-databind-2.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-infrastructure/3.0.0.BUILD-SNAPSHOT/cfaea737589c43c54ff338ae27e1bee477620176/spring-batch-infrastructure-3.0.0.BUILD-SNAPSHOT.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.eaio.uuid/uuid/3.2/77ba5105d949cd589aff75400d9f7d3676691a46/uuid-3.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.2.2/285cb9c666f0f0f3dd8a1be04e1f457eb7b15113/jackson-annotations-2.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.2.2/d20be6a5ddd6f8cfd36ebf6dea329873a1c41f1b/jackson-core-2.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-commons/1.6.2.RELEASE/e96a0458cdc3179ca70c880f42315bb75df4faf5/spring-data-commons-1.6.2.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/joda-time/joda-time/2.1/8f79e353ef77da6710e1f10d34fc3698eaaacbca/joda-time-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/log4j/log4j/1.2.17/5af35056b4d257e4b64b9e8069c0746e8b08629f/log4j-1.2.17.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/jcl-over-slf4j/1.7.5/cd5970bd13fa85f7bed41ca606d6daf7cbf1365/jcl-over-slf4j-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-log4j12/1.7.5/6edffc576ce104ec769d954618764f39f0f0f10d/slf4j-log4j12-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/nl.jqno.equalsverifier/equalsverifier/1.1.3/60cd685f314a9cebfd0595d88fea45fba2f47918/equalsverifier-1.1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.5/6b262da268f8ad9eff941b25503a9198f0a0ac93/slf4j-api-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/1.1/63db1176f16448172611266154e4f6d39a0e1e68/objenesis-1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/cglib/cglib-nodep/2.2/59afed7ab65e7ec6585d5bc60556c3cbd203532b/cglib-nodep-2.2.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-rest-domain/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.hateoas/spring-hateoas/0.8.0.RELEASE/819c25e1ff12b7fca483d76b4e7d20221f621fcd/spring-hateoas-0.8.0.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-admin-manager/1.3.0.M1/5afc7442417af8c46ae51480ed2b83943283d449/spring-batch-admin-manager-1.3.0.M1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-core/3.0.0.BUILD-SNAPSHOT/8168f58716cd305040eaa87c82dc61822b03415c/spring-batch-core-3.0.0.BUILD-SNAPSHOT.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.jackson/jackson-core-asl/1.9.13/3c304d70f42f832e0a86d45bd437f692129299a4/jackson-core-asl-1.9.13.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/4.0.1.RELEASE/2ace92025f042e1d3ddfdbba093172e3572ac130/spring-web-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/4.0.1.RELEASE/2dbc91a6413115f7ffbe94f0fa9bc9fda3281d90/spring-webmvc-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/1.3/dc13ae4faca6df981fc7aeb5a522d9db446d5d50/objenesis-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.2/81d61b7f33ebeab314e07de0cc596f8e858d97/slf4j-api-1.7.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjrt/1.6.6/ff58f520e1a304b8a02b8cea8b96b1b8e5b25b0/aspectjrt-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjweaver/1.6.6/c0383be877cfa4ec6b62202c942a89a6264a2be6/aspectjweaver-1.6.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-pool/commons-pool/1.3/3231230c1d7631b66a74d1c4653cfd65a6f9ea0/commons-pool-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-dbcp/commons-dbcp/1.2.2/4fd4c6110e9bca3a655b717eb2e5920febb8403d/commons-dbcp-1.2.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/1.4/a8762d07e76cfde2395257a5da47ba7c1dbd3dce/commons-io-1.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-lang/commons-lang/2.1/4763ecc9d78781c915c07eb03e90572c7ff04205/commons-lang-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-fileupload/commons-fileupload/1.2.1/384faa82e193d4e4b0546059ca09572654bc3970/commons-fileupload-1.2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/net.sf.ehcache/ehcache-core/2.3.0/e59473c71a31e8e19da4fbc7028585c8ed51d69f/ehcache-core-2.3.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-collections/commons-collections/3.2/f951934aa5ae5a88d7e6dfaa6d32307d834a88be/commons-collections-3.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.freemarker/freemarker/2.3.15/c8cfe522476fcec8da5c980d58bf62d6ab0cf27c/freemarker-2.3.15.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-admin-resources/1.3.0.M1/bdf7d5afc02397385fce8731409f606e54d4d033/spring-batch-admin-resources-1.3.0.M1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.0.2.RELEASE/d673c90a9fd8f0de5f20d53d61047849f707f42b/spring-retry-1.0.2.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/javax.batch/javax.batch-api/1.0/65392d027a6eb369fd9fcd1b75cae150e25ac03c/javax.batch-api-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.ibm.jbatch/com.ibm.jbatch-tck-spi/1.0/8ac869b0a60bff1a15eba0fb6398942410396938/com.ibm.jbatch-tck-spi-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/xpp3/xpp3_min/1.1.4c/19d4e90b43059058f6e056f794f0ea4030d60b86/xpp3_min-1.1.4c.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.thoughtworks.xstream/xstream/1.3/3f755b1a46744302712b1b962c4ab64de392f477/xstream-1.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.jettison/jettison/1.1/1a01a2a1218fcf9faa2cc2a6ced025bdea687262/jettison-1.1.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-module/bin/, file:/Users/iperumal/workspace/spring-xd/spring-xd-module-spi/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/1.0.0.RC1/7830d0dd26f75841d8b5c2c72c42b864b1192ddb/spring-boot-autoconfigure-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/javax.validation/validation-api/1.0.0.GA/b6bd7f9d78f6fdaa3c37dae18a4bd298915f328e/validation-api-1.0.0.GA.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hibernate/hibernate-validator/4.3.1.Final/49b31d8ea51fa21cc78a89e9d4ddb11d6bfb4669/hibernate-validator-4.3.1.Final.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/1.0.0.RC1/7e53b72a368c495a482d3a213ad6338f8f7afcfa/spring-boot-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jboss.logging/jboss-logging/3.1.0.CR2/28725380c07f917ace4e511db21cc45e9ae5a72b/jboss-logging-3.1.0.CR2.jar, file:/Users/iperumal/workspace/spring-xd/spring-xd-hadoop/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-hadoop-store/2.0.0.M4/1d3d691c0e6952ba26724339668e17040c368683/spring-data-hadoop-store-2.0.0.M4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.0/71c46e2313e92887de4247f2aabce4d09d65d82/snappy-java-1.1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-io/commons-io/2.1/fd51f906669f49a4ffd06650666c3b8147a6106e/commons-io-2.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-codec/commons-codec/1.7/9cd61d269c88f9fb0eb36cea1efcd596ab74772f/commons-codec-1.7.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.google.code.findbugs/jsr305/1.3.9/40719ea6961c0cb6afaeb6a921eaa1f6afd4cfdf/jsr305-1.3.9.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.google.guava/guava/11.0.2/35a3c69e19d72743cac83778aecbee68680f63eb/guava-11.0.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.codehaus.jackson/jackson-mapper-asl/1.9.13/1ee2f2bed0e5dd29d1cb155a166e6f8d50bbddb7/jackson-mapper-asl-1.9.13.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.3/4a85963a752c0a2f715c3924bfc686865e7e1bc6/paranamer-2.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.tukaani/xz/1.0/ecff5cb8b1189514c9d1d8d68eb77ac372e000c9/xz-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-compress/1.4.1/b02e84a993d88568417536240e970c4b809126fd/commons-compress-1.4.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.6.4/2396d74b12b905f780ed7966738bb78438e8371a/slf4j-api-1.6.4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.avro/avro/1.7.5/8343a5b33f56fa16306ed27fa7b1a79278c26c2d/avro-1.7.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-common/1.2.5/fd1e6d42b3cb63f05ea5a1d22ded5ab63837b70f/parquet-common-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-generator/1.2.5/f16fd52b93d6677be340e7ac61609d6b91ba5ab0/parquet-generator-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-encoding/1.2.5/e96b9cd40f0eebc4ae483b0187401aa444ba36c0/parquet-encoding-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-column/1.2.5/1b1c618df93b16eb684c120662d56cde1b2caa99/parquet-column-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-format/1.0.0/ffaa2809ce56db3d7288ae118d876be30e9d1eb0/parquet-format-1.0.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-hadoop/1.2.5/6c87620226b4e156dac44af45171055b9ea78c8/parquet-hadoop-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.twitter/parquet-avro/1.2.5/8e7ad3c7f06b40e6d09417f82b0e4c703d8918ad/parquet-avro-1.2.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.commons/commons-jexl/2.1.1/6ecc181debade00230aa1e17666c4ea0371beaaa/commons-jexl-2.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/net.sf.opencsv/opencsv/2.3/c23708cdb9e80a144db433e23344a788a1fd6599/opencsv-2.3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.kitesdk/kite-data-core/0.10.0/916792f62bb1177ec4e9f3dedc53f19d9ae869a/kite-data-core-0.10.0.jar, file:/Users/iperumal/workspace/spring-xd/extensions/spring-xd-extension-reactor/bin/, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.projectreactor/reactor-spring/1.0.0.RC1/1652e63c473f1824ca7185d12be70981f97913e0/reactor-spring-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.projectreactor/reactor-tcp/1.0.0.RC1/d275a1bdb5a449a1e9638dcd99f9a97a97c26b45/reactor-tcp-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/0.8.1/fbcad75a02160def33c4359b24015cca2d91ccce/json-path-0.8.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.lmax/disruptor/3.2.0/ac62995678dd4b906e85b26354aa2ebfda130c32/disruptor-3.2.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.projectreactor/reactor-core/1.0.0.RC1/202718fd7bbe443de0004e305edb075355b0b340/reactor-core-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/io.netty/netty-all/4.0.6.Final/1ce815d4d1ab73b17e20a5f9c206f422014deb4a/netty-all-4.0.6.Final.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/1.1.1/24a2f903d25e004de30ac602c5b47f2d4e420a59/json-smart-1.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-lang/commons-lang/2.6/ce1edb914c94ebc388f086c6827e8bdeec71ac2/commons-lang-2.6.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.cloud/spring-service-connector/0.9.2/75e716e825fe66db6047cf1e026ca7581a5c933a/spring-service-connector-0.9.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.cloud/cloudfoundry-connector/0.9.2/d0f4df0a67f13704db02e688fc9d85d1c9ed5498/cloudfoundry-connector-0.9.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator/1.0.0.RC1/88bf421071c1a38a9fda606c5f3cab609c74bf71/spring-boot-actuator-1.0.0.RC1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-event/4.0.0.M3/d0abb5b6a6d8050a71cbb6e95c69d484bb26c005/spring-integration-event-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-file/4.0.0.M3/ba6b17b85f2847326bf5ea0444872c62de0f5e8f/spring-integration-file-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-ftp/4.0.0.M3/6ddfabd427c6edb5114595903eead0cd042a6ec7/spring-integration-ftp-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-groovy/4.0.0.M3/53203058a9f6d703ae0e1d86ebb59c6ee2515c14/spring-integration-groovy-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-http/4.0.0.M3/b78a8227736b84aa857dbec75a6ef4c47a0563be/spring-integration-http-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-jmx/4.0.0.M3/d2b332cc945281327fc91b8641fa71b7340dfdcf/spring-integration-jmx-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-spring/1.1.5/e9a8d8710c19381327afe4a7e5af9cc4aa5e7ef4/jolokia-spring-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-client-java/1.1.5/e0f24b062d3f359737a24a0130986e20f0a77e5c/jolokia-client-java-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.esotericsoftware.kryo/kryo/2.22/1b624c2db8eb3a40d84362f22c2078ebde3e8c81/kryo-2.22.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/args4j/args4j/2.0.16/9f00fb12820743b9e05c686eba543d64dd43f2b1/args4j-2.0.16.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.batch/spring-batch-integration/3.0.0.BUILD-SNAPSHOT/e11f30ff7b6da07e7f630752c5d92c41eorg.springframework.beans.factory.BeanCreationException: Error creating bean with name 'jobExecutionDao' defined in class path resource [META-INF/spring-xd/batch/batch.xml]: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Could not inspect meta data for database type.  You have to supply it explicitly.
84de90a/spring-batch-integration-3.0.0.BUILD-SNAPSHOT.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.plugin/spring-plugin-core/0.8.0.RELEASE/4390edad56632d97fec6abe8dba96531234b9603/spring-plugin-core-0.8.0.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/7.0.35/3efe5c4c44215a514fa804c5ef829569f6652b8b/tomcat-embed-core-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-logging-juli/7.0.35/5e92715a81731f6615053a399e1d3e9a2c05d4b9/tomcat-embed-logging-juli-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-jsp-api/7.0.35/30a8560fad20cf681da4b26a5100d474559fda8a/tomcat-jsp-api-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.sun.mail/javax.mail/1.5.0/ec2410fdf7e0a3022e7c2a2e6241039d1abc1e98/javax.mail-1.5.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.12/ebe66a6b88caab31d7a19571ad23656377523545/snakeyaml-1.12.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.postgresql/postgresql/9.2-1002-jdbc4/a535facdcc0b4c488798e0c19499c08b1bc3a2e/postgresql-9.2-1002-jdbc4.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/mysql/mysql-connector-java/5.1.23/c264e2114579474d13dd808a510fc74e762dda8c/mysql-connector-java-5.1.23.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.activemq/activemq-core/5.6.0/9996dd23872b8588d63cf8a6cc8522b6ad8caa85/activemq-core-5.6.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jms/4.0.1.RELEASE/7d74a1414cd45349ed61ee3f3e1fca019090358c/spring-jms-4.0.1.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/1.0.0.RC1/15b007421d5419e435cf508057d458a065126d00/spring-boot-1.0.0.RC1-tests.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.social/spring-social-twitter/1.0.5.RELEASE/6c4b7e69b060862d98c87588d827d4feabbcd68/spring-social-twitter-1.0.5.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.3.1/c4096a8323bbbcbeda072e3def123a9b66783361/jackson-databind-2.3.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.hsqldb/hsqldb/2.3.1/bd7153caa30958e1c64a3dd38b93e5a7442d9a0e/hsqldb-2.3.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.cloud/core/0.9.2/32631d9bd70d432e77df0ca64db21b49d28f9589/core-0.9.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.3.0/f5e853a20b60758922453d56f9ae1e64af5cb3da/jackson-annotations-2.3.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.3.1/f9f7185c92ca5fefe2fb3efdeb477a67c96ea2d0/jackson-core-2.3.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/commons-net/commons-net/3.0.1/643cc426b9f75aa111fac0fac0e52ff5d991a337/commons-net-3.0.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.integration/spring-integration-scripting/4.0.0.M3/6b0260ca4709da0bdd230d7bca41f3764a91ed4c/spring-integration-scripting-4.0.0.M3.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/com.googlecode.json-simple/json-simple/1.1/5e303a03d04e6788dddfa3655272580ae0fc13bb/json-simple-1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-core/1.1.5/56bdd8956fb2bb3a9241267a7cb22d43f27d0a71/jolokia-core-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-jvm/1.1.5/c59f18d004d64bea982d5128fbf08b4a177c1720/jolokia-jvm-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jolokia/jolokia-jmx/1.1.5/c7026c2cf9111a138538cc6705ca4c41f70be8fa/jolokia-jmx-1.1.5.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpcore/4.1.2/39e30fb2ddfcf32b9dd663039ca6e847b698aa6f/httpcore-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient/4.1.2/9003011c1b6d28a7331f04251a25e7ccf63c98f8/httpclient-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpmime/4.1.2/c78eaaea68d0f36f73b646f08cdb34f1a784239b/httpmime-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient-cache/4.1.2/6906f34467097e240ca451652f5738245a2face0/httpclient-cache-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.httpcomponents/httpclient-osgi/4.1.2/c7577b4f8877a0a29ae5cc65331dcbe9960f8cce/httpclient-osgi-4.1.2.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-el-api/7.0.35/5e117f3f62c49d52cb72d028e76c5d16a8d148ca/tomcat-el-api-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.tomcat/tomcat-servlet-api/7.0.35/87bd606eef7d42ff3ec324d198549ccd8ccb56fc/tomcat-servlet-api-7.0.35.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/javax.activation/activation/1.1/e6cb541461c2834bdea3eb920f1884d1eb508b50/activation-1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.geronimo.specs/geronimo-jms_1.1_spec/1.1.1/c872b46c601d8dc03633288b81269f9e42762cea/geronimo-jms_1.1_spec-1.1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.activemq/kahadb/5.6.0/ab90275013deb4a1652a49dc6b01a894b8138a48/kahadb-5.6.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.activemq.protobuf/activemq-protobuf/1.1/26682eb801f70563511f7c424dc10e8b3e66340e/activemq-protobuf-1.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.fusesource.hawtdispatch/hawtdispatch/1.9/cbfeed0be226850799a7ef819b0dfa3346198af6/hawtdispatch-1.9.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.fusesource.hawtdispatch/hawtdispatch-transport/1.9/5e29bb84478f8f2526e0cae2cb74b9978f3c01d0/hawtdispatch-transport-1.9.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.fusesource.hawtbuf/hawtbuf/1.9/4a42b835d1df77db8c9a144a11ebb4600a372f5f/hawtbuf-1.9.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.fusesource.mqtt-client/mqtt-client/1.0/4af519e42c765b78aedd63c872706f5fa1621a9c/mqtt-client-1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.osgi/org.osgi.core/4.1.0/b88cd082b5b6774e9db939e28c0e3dc526c92d89/org.osgi.core-4.1.0.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.apache.geronimo.specs/geronimo-j2ee-management_1.1_spec/1.0.1/5372615b0c04c1913c95c34a0414cef720ca2855/geronimo-j2ee-management_1.1_spec-1.0.1.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.jasypt/jasypt/1.8/b14e87ad376dee74bb2d088279fe2df179cfc9e0/jasypt-1.8.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.social/spring-social-core/1.0.3.RELEASE/44e648f23b45162c698e255a16759832dfcfc004/spring-social-core-1.0.3.RELEASE.jar, file:/Users/iperumal/.gradle/caches/modules-2/files-2.1/org.springframework.security/spring-security-crypto/3.1.3.RELEASE/a0db20c52e8281bffc200429b7fcd7c2cbeeb0e0/spring-security-crypto-3.1.3.RELEASE.jar, file:/Users/iperumal/workspace/spring-xd-samples/batch-notifications/target/batch-notifications-1.0.0.BUILD-SNAPSHOT.jar]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1553)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:658)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:355)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:129)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:116)
	at org.springframework.xd.dirt.server.LauncherApplication.run(LauncherApplication.java:76)
	at org.springframework.xd.dirt.server.LauncherApplication.main(LauncherApplication.java:56)
Caused by: java.lang.IllegalArgumentException: Could not inspect meta data for database type.  You have to supply it explicitly.
	at org.springframework.batch.item.database.support.SqlPagingQueryProviderFactoryBean.getObject(SqlPagingQueryProviderFactoryBean.java:159)
	at org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao.getPagingQueryProvider(JdbcSearchableJobExecutionDao.java:134)
	at org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao.getPagingQueryProvider(JdbcSearchableJobExecutionDao.java:113)
	at org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao.getPagingQueryProvider(JdbcSearchableJobExecutionDao.java:104)
	at org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao.afterPropertiesSet(JdbcSearchableJobExecutionDao.java:92)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1612)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1549)
	... 15 more
Caused by: org.springframework.jdbc.support.MetaDataAccessException: Could not get Connection for extracting meta data; nested exception is org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused
	at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:293)
	at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:320)
	at org.springframework.batch.support.DatabaseType.fromMetaData(DatabaseType.java:95)
	at org.springframework.batch.item.database.support.SqlPagingQueryProviderFactoryBean.getObject(SqlPagingQueryProviderFactoryBean.java:155)
	... 21 more
Caused by: org.springframework.jdbc.CannotGetJdbcConnectionException: Could not get JDBC Connection; nested exception is java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused
	at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:80)
	at org.springframework.jdbc.support.JdbcUtils.extractDatabaseMetaData(JdbcUtils.java:280)
	... 24 more
Caused by: java.sql.SQLTransientConnectionException: java.net.ConnectException: Connection refused
	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source)
	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source)
	at org.hsqldb.jdbc.JDBCConnection.<init>(Unknown Source)
	at org.hsqldb.jdbc.JDBCDriver.getConnection(Unknown Source)
	at org.hsqldb.jdbc.JDBCDriver.connect(Unknown Source)
	at org.apache.tomcat.jdbc.pool.PooledConnection.connectUsingDriver(PooledConnection.java:278)
	at org.apache.tomcat.jdbc.pool.PooledConnection.connect(PooledConnection.java:182)
	at org.apache.tomcat.jdbc.pool.ConnectionPool.createConnection(ConnectionPool.java:702)
	at org.apache.tomcat.jdbc.pool.ConnectionPool.borrowConnection(ConnectionPool.java:634)
	at org.apache.tomcat.jdbc.pool.ConnectionPool.init(ConnectionPool.java:488)
	at org.apache.tomcat.jdbc.pool.ConnectionPool.<init>(ConnectionPool.java:144)
	at org.apache.tomcat.jdbc.pool.DataSourceProxy.pCreatePool(DataSourceProxy.java:116)
	at org.apache.tomcat.jdbc.pool.DataSourceProxy.createPool(DataSourceProxy.java:103)
	at org.apache.tomcat.jdbc.pool.DataSourceProxy.getConnection(DataSourceProxy.java:127)
	at org.springframework.jdbc.datasource.DataSourceUtils.doGetConnection(DataSourceUtils.java:111)
	at org.springframework.jdbc.datasource.DataSourceUtils.getConnection(DataSourceUtils.java:77)
	... 25 more
Caused by: org.hsqldb.HsqlException: java.net.ConnectException: Connection refused
	at org.hsqldb.ClientConnection.openConnection(Unknown Source)
	at org.hsqldb.ClientConnection.initConnection(Unknown Source)
	at org.hsqldb.ClientConnection.<init>(Unknown Source)
	... 39 more
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:391)
	at java.net.Socket.connect(Socket.java:579)
	at java.net.Socket.connect(Socket.java:528)
	at java.net.Socket.<init>(Socket.java:425)
	at java.net.Socket.<init>(Socket.java:208)
	at org.hsqldb.server.HsqlSocketFactory.createSocket(Unknown Source)
	... 42 more


",3.0,False,3.0,0,0.0,0,0,False
"XD-1316","Bug","Sprint 22","2014-02-12T11:51:29.000+0000","hillert","hillert","UI:Fix E2E test warning","When running E2E tests the following warning may be observed:

{code}
Running ""karma:e2e"" (karma) task
INFO [karma]: Karma v0.10.9 server started at http://localhost:7070/_karma_/
INFO [launcher]: Starting browser PhantomJS
TypeError: Cannot read property 'verbose' of undefined
    at enableWebsocket (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-connect-proxy/lib/utils.js:101:18)
    at Object.utils.proxyRequest [as handle] (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-connect-proxy/lib/utils.js:109:5)
    at next (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/proto.js:193:15)
    at Object.livereload [as handle] (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect-livereload/index.js:147:5)
    at next (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/proto.js:193:15)
    at Function.app.handle (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/proto.js:201:3)
    at Server.app (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/connect.js:65:37)
    at Server.EventEmitter.emit (events.js:98:17)
    at HTTPParser.parser.onIncoming (http.js:2108:12)
    at HTTPParser.parserOnHeadersComplete [as onHeadersComplete] (http.js:121:23)
    at Socket.socket.ondata (http.js:1966:22)
    at TCP.onread (net.js:525:27)
{code}
",2.0,0.1533228676085818,0.0,0.162218733647305,True,True,True,1392205889000.0,1392206475000.0,1392209711000.0,1392208811000.0,1392209711000.0,1392205889000.0,-1.0,1392206509000.0,1392209711000.0,1392205889000.0,1392209489000.0,1392206475000.0,1392209711000.0,"hillert",1392209711000.0,1392205889000.0,44,1392205889000.0,-1.0,"UI:Fix E2E test warning","When running E2E tests the following warning may be observed:

{code}
Running ""karma:e2e"" (karma) task
INFO [karma]: Karma v0.10.9 server started at http://localhost:7070/_karma_/
INFO [launcher]: Starting browser PhantomJS
TypeError: Cannot read property 'verbose' of undefined
    at enableWebsocket (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-connect-proxy/lib/utils.js:101:18)
    at Object.utils.proxyRequest [as handle] (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-connect-proxy/lib/utils.js:109:5)
    at next (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/proto.js:193:15)
    at Object.livereload [as handle] (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect-livereload/index.js:147:5)
    at next (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/proto.js:193:15)
    at Function.app.handle (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/proto.js:201:3)
    at Server.app (/Users/hillert/dev/git/spring-xd/spring-xd-ui/node_modules/grunt-contrib-connect/node_modules/connect/lib/connect.js:65:37)
    at Server.EventEmitter.emit (events.js:98:17)
    at HTTPParser.parser.onIncoming (http.js:2108:12)
    at HTTPParser.parserOnHeadersComplete [as onHeadersComplete] (http.js:121:23)
    at Socket.socket.ondata (http.js:1966:22)
    at TCP.onread (net.js:525:27)
{code}
",2.0,False,2.0,0,0.0,0,1,False
"XD-1315","Story","","2014-02-12T10:28:59.000+0000","grenfro","","Create a FileSink as a POJO","This is so that we can have an ENUM that can show the possible values and autocomplete.  Using just the XML the user has a greater chance to enter an invalid mode.   ",5.0,-1.0,0.700154693915094,-1.0,False,False,True,1392200939000.0,-1.0,-1.0,1392797994000.0,1392798894000.0,1392200939000.0,-1.0,-1.0,-1.0,1392619600000.0,1392623200000.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1392200939000.0,-1.0,"Create POJO based FileSink module metadata","This is so that we can have an ENUM that can show the possible values and autocomplete.  Using just the XML the user has a greater chance to enter an invalid mode.   ",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1314","Story","Sprint 22","2014-02-12T09:05:27.000+0000","thomas.risberg","thomas.risberg","Create XD .zip distribution for YARN","Create XD .zip distribution for YARN that adds an additional sub-project to the spring-xd repo for building the xd-YARN.zip

Link into main build file

Produce a new artifact spring-xd-v-xyz-yarn.zip as part of the nightly CI process -- will now have 2 artifacts, main xd.zip distribution and xd-yarn.zip

Does not include any Hadoop distribution libraries

Does include spring-hadoop jars for Apache22 unflavored
",3.0,0.0028696784976895,0.0028082292793235,0.0002642316389735,True,True,True,1392195927000.0,1392196394000.0,1392199994000.0,1392357763000.0,1392358663000.0,1392195927000.0,-1.0,1392195970000.0,1392199570000.0,1392196384000.0,1392199984000.0,1392196394000.0,1392199994000.0,"thomas.risberg",1392199570000.0,1392195927000.0,44,1392195927000.0,-1.0,"Create XD .zip distribution for YARN","Create XD .zip distribution for YARN that adds an additional sub-project to the spring-xd repo for building the xd-YARN.zip

Link into main build file

Produce a new artifact spring-xd-v-xyz-yarn.zip as part of the nightly CI process -- will now have 2 artifacts, main xd.zip distribution and xd-yarn.zip

Does not include any Hadoop distribution libraries

Does include spring-hadoop jars for Apache22 unflavored
",3.0,False,3.0,0,0.0,-1,0,False
"XD-1312","Story","","2014-02-12T08:01:19.000+0000","eric.bottard","","Job execution restart fails with NPE","Create a job, launch it but make it fail (eg filejdbc with missing file)

job execution list => it's there, as FAILED. Good

job execution restart <theid> ==> Fails with NPE:

{noformat}
16:59:42,160 ERROR http-nio-9393-exec-7 rest.RestControllerAdvice:191 - Caught exception while handling a request
java.lang.NullPointerException
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:351)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.GeneratedMethodAccessor157.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy39.run(Unknown Source)
	at org.springframework.batch.admin.service.SimpleJobService.restart(SimpleJobService.java:179)
	at org.springframework.xd.dirt.plugins.job.DistributedJobService.restart(DistributedJobService.java:77)
	at org.springframework.xd.dirt.rest.BatchJobExecutionsController.restartJobExecution(BatchJobExecutionsController.java:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springfram
{noformat}",5.0,-1.0,0.0012185830629556,-1.0,False,False,True,1392192079000.0,-1.0,-1.0,1395841320000.0,1395842220000.0,1392192079000.0,-1.0,-1.0,-1.0,1392196527000.0,1392200127000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1392192079000.0,-1.0,"Job execution restart fails with NPE","Create a job, launch it but make it fail (eg filejdbc with missing file)

job execution list => it's there, as FAILED. Good

job execution restart <theid> ==> Fails with NPE:

{noformat}
16:59:42,160 ERROR http-nio-9393-exec-7 rest.RestControllerAdvice:191 - Caught exception while handling a request
java.lang.NullPointerException
	at org.springframework.batch.core.job.AbstractJob.execute(AbstractJob.java:351)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher$1.run(SimpleJobLauncher.java:135)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:50)
	at org.springframework.batch.core.launch.support.SimpleJobLauncher.run(SimpleJobLauncher.java:128)
	at sun.reflect.GeneratedMethodAccessor157.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.batch.core.configuration.annotation.SimpleBatchConfiguration$PassthruAdvice.invoke(SimpleBatchConfiguration.java:117)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy39.run(Unknown Source)
	at org.springframework.batch.admin.service.SimpleJobService.restart(SimpleJobService.java:179)
	at org.springframework.xd.dirt.plugins.job.DistributedJobService.restart(DistributedJobService.java:77)
	at org.springframework.xd.dirt.rest.BatchJobExecutionsController.restartJobExecution(BatchJobExecutionsController.java:146)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.springfram
{noformat}",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1311","Story","Sprint 30","2014-02-12T07:58:43.000+0000","eric.bottard","iperumal","Job execution list should mention jobs that have been deleted","Create a job, execute it a couple of times, destroy it and then invoke job execution list.

The job name column should mention that a job is defunct (even though a job with the same name could have been re-created in the interim)",3.0,0.9385572446529812,0.9153756096492746,0.9623962119694424,True,True,True,1392191923000.0,1402306654000.0,1402310254000.0,1402967916000.0,1402968816000.0,1392191923000.0,-1.0,1402563564000.0,1402567164000.0,1402056828000.0,1402060428000.0,1402306654000.0,1402310254000.0,"eric.bottard",1402567164000.0,1392191923000.0,44,1392191923000.0,-1.0,"Job execution list should mention jobs that have been deleted","Create a job, execute it a couple of times, destroy it and then invoke job execution list.

The job name column should mention that a job is defunct (even though a job with the same name could have been re-created in the interim)",3.0,False,3.0,0,0.0,0,0,False
"XD-1310","Story","Sprint 28","2014-02-12T07:56:20.000+0000","eric.bottard","iperumal","Misleading error message when trying to restart a job exec","Disregard the missing date that is caused by another problem.
Here is the setup:
{noformat}
xd:>job execution list
  Id  Job Name  Start Time                        Step Execution Count  Status
  --  --------  --------------------------------  --------------------  ---------
  13  foo         Europe/Paris                    0                     STARTING
  12  foo       2014-02-12 15:39:46 Europe/Paris  1                     FAILED
  11  foo       2014-02-12 15:39:29 Europe/Paris  1                     COMPLETED
  10  foo       2014-02-12 15:38:36 Europe/Paris  1                     COMPLETED
  9   foo       2014-02-12 15:38:21 Europe/Paris  1                     COMPLETED
  8   foo         Europe/Paris                    0                     STARTING
  7   foo       2014-02-12 15:25:41 Europe/Paris  1                     COMPLETED
  6   foo       2014-02-12 15:25:04 Europe/Paris  1                     FAILED
  5   foo       2014-02-12 15:14:32 Europe/Paris  1                     FAILED
  4   foo       2014-02-12 15:14:13 Europe/Paris  1                     FAILED
  3   foo       2014-02-12 15:13:54 Europe/Paris  1                     FAILED
  2   foo       2014-02-12 15:13:18 Europe/Paris  1                     FAILED
  1   foo       2014-02-12 15:12:58 Europe/Paris  1                     FAILED
  0   foo       2014-02-12 15:11:44 Europe/Paris  1                     FAILED

xd:>job execution restart --id 12
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Job Execution 12 is already running.
{noformat}

while the server exception is a bit better:
{noformat}
Caused by: org.springframework.batch.core.repository.JobExecutionAlreadyRunningException: A job execution for this job is already running: JobInstance: id=11, version=0, Job=[foo]
	at org.springframework.batch.core.repository.support.SimpleJobRepository.createJobExecution(SimpleJobRepository.java:120)
{noformat}

I'd argue we should not speak in terms of execution ids if possible, but rather in terms of job names
",1.0,0.9621877843892,0.9621877843892,0.9630108410094664,True,True,True,1392191780000.0,1400841522000.0,1400845122000.0,1401180541000.0,1401181441000.0,1392191780000.0,-1.0,1400848921000.0,1400852521000.0,1400841522000.0,1400845122000.0,1400841522000.0,1400845122000.0,"eric.bottard",1400852521000.0,1400848749000.0,44,1400848749000.0,-1.0,"Misleading error message when trying to restart a job exec","Disregard the missing date that is caused by another problem.
Here is the setup:
{noformat}
xd:>job execution list
  Id  Job Name  Start Time                        Step Execution Count  Status
  --  --------  --------------------------------  --------------------  ---------
  13  foo         Europe/Paris                    0                     STARTING
  12  foo       2014-02-12 15:39:46 Europe/Paris  1                     FAILED
  11  foo       2014-02-12 15:39:29 Europe/Paris  1                     COMPLETED
  10  foo       2014-02-12 15:38:36 Europe/Paris  1                     COMPLETED
  9   foo       2014-02-12 15:38:21 Europe/Paris  1                     COMPLETED
  8   foo         Europe/Paris                    0                     STARTING
  7   foo       2014-02-12 15:25:41 Europe/Paris  1                     COMPLETED
  6   foo       2014-02-12 15:25:04 Europe/Paris  1                     FAILED
  5   foo       2014-02-12 15:14:32 Europe/Paris  1                     FAILED
  4   foo       2014-02-12 15:14:13 Europe/Paris  1                     FAILED
  3   foo       2014-02-12 15:13:54 Europe/Paris  1                     FAILED
  2   foo       2014-02-12 15:13:18 Europe/Paris  1                     FAILED
  1   foo       2014-02-12 15:12:58 Europe/Paris  1                     FAILED
  0   foo       2014-02-12 15:11:44 Europe/Paris  1                     FAILED

xd:>job execution restart --id 12
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Job Execution 12 is already running.
{noformat}

while the server exception is a bit better:
{noformat}
Caused by: org.springframework.batch.core.repository.JobExecutionAlreadyRunningException: A job execution for this job is already running: JobInstance: id=11, version=0, Job=[foo]
	at org.springframework.batch.core.repository.support.SimpleJobRepository.createJobExecution(SimpleJobRepository.java:120)
{noformat}

I'd argue we should not speak in terms of execution ids if possible, but rather in terms of job names
",1.0,False,1.0,0,0.0,0,0,False
"XD-1309","Story","Sprint 22","2014-02-12T06:08:55.000+0000","eric.bottard","eric.bottard","JSR303 validation of options interferes with dsl completion","When using a JSR303 annotated class for module options, the binding failures should be bypassed, as they interfere with completion proposals.
",5.0,0.0777930495102718,0.0,0.0777843809024791,True,True,True,1392185335000.0,1392266102000.0,1392269702000.0,1393222664000.0,1393223564000.0,1392185335000.0,-1.0,1392266093000.0,1392269693000.0,1392185335000.0,1392188935000.0,1392266102000.0,1392269702000.0,"eric.bottard",1392269693000.0,1392185335000.0,44,1392185335000.0,-1.0,"JSR303 validation of options interferes with dsl completion","When using a JSR303 annotated class for module options, the binding failures should be bypassed, as they interfere with completion proposals.
",5.0,False,5.0,0,0.0,0,0,False
"XD-1308","Story","Sprint 27","2014-02-12T04:28:59.000+0000","eric.bottard","mark.fisher","Weird behavior of the transform module","When trying some of the examples of XD-159, came up with weird behavior of the transform module.

This boils down to:
{noformat}
stream create foo --definition ""http |transform --expression='new java.lang.Integer(payload)' |  transform --expression=payload.getClass() | log""
http post --data 42   ==> Integer (OK)
http post --data WTH => WTH (!)
{noformat}

Seems that when the expression can not be evaluated, the incoming payload is transmitted as is",4.0,0.9996349747917536,0.908905151395848,0.9988913330111234,True,True,True,1392179339000.0,1398872323000.0,1398874767000.0,1398873867000.0,1398874767000.0,1392179339000.0,-1.0,1398867344000.0,1398870944000.0,1398264848000.0,1398268448000.0,1398872323000.0,1398874767000.0,"eric.bottard",1398870944000.0,1392179339000.0,44,1392179339000.0,-1.0,"Weird behavior of the transform module","When trying some of the examples of XD-159, came up with weird behavior of the transform module.

This boils down to:
{noformat}
stream create foo --definition ""http |transform --expression='new java.lang.Integer(payload)' |  transform --expression=payload.getClass() | log""
http post --data 42   ==> Integer (OK)
http post --data WTH => WTH (!)
{noformat}

Seems that when the expression can not be evaluated, the incoming payload is transmitted as is",4.0,False,4.0,0,0.0,0,0,False
"XD-1306","Story","Sprint 31","2014-02-12T02:59:26.000+0000","eric.bottard","dturanski","Move ftp support from .x package to spring-xd-dirt batch package","Commit https://github.com/spring-projects/spring-xd/commit/761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes.

These should not belong to DIRT proper though, and should be added to an extension style project. The job(s) module would then depend on them",1.0,0.9438172914537084,0.9368278797053372,0.944843413439774,True,True,True,1392173966000.0,1404205746000.0,1404209346000.0,1404921063000.0,1404921963000.0,1392173966000.0,-1.0,1404218827000.0,1404222427000.0,1404116645000.0,1404120245000.0,1404205746000.0,1404209346000.0,"eric.bottard",1404222427000.0,1402059182000.0,44,1402059182000.0,-1.0,"Move ftp support from .x package to spring-xd-dirt batch package","Commit https://github.com/spring-projects/spring-xd/commit/761cd5e8250c055878caf3a789ab5b3254ba48e8 introduced support for FTP and added a bunch of .x classes.

These should not belong to DIRT proper though, and should be added to an extension style project. The job(s) module would then depend on them",1.0,False,1.0,0,0.0,0,1,False
"XD-1305","Story","Sprint 22","2014-02-11T13:16:25.000+0000","grenfro","grenfro","File Sink should support Replace as an option","currently the file sink only supports append.  User should support an overwrite feature.",5.0,0.0002131832523236,0.0002131832523236,0.992154856314488,True,True,True,1392124585000.0,1392124600000.0,1392128200000.0,1392194047000.0,1392194947000.0,1392124585000.0,-1.0,1392194395000.0,1392194947000.0,1392124600000.0,1392128200000.0,1392124600000.0,1392128200000.0,"grenfro",1392194947000.0,1392124585000.0,44,1392124585000.0,-1.0,"File Sink should support Replace as an option","currently the file sink only supports append.  User should support an overwrite feature.",5.0,False,5.0,0,0.0,0,1,False
"XD-1304","Story","Sprint 22","2014-02-11T11:53:21.000+0000","mark.pollack","","Create shell command for getting information on a given job instance","",3.0,0.6791733959363536,0.3256258914547987,0.0993073708396463,True,True,True,1392119601000.0,1392609103000.0,1392612703000.0,1392839433000.0,1392840333000.0,1392119601000.0,-1.0,1392191175000.0,1392194775000.0,1392354290000.0,1392357890000.0,1392609103000.0,1392612703000.0,"mark.pollack",1392194775000.0,1392119601000.0,44,1392119601000.0,-1.0,"Create shell command for getting information on a given job instance","",3.0,False,3.0,0,0.0,0,0,False
"XD-1303","Story","Sprint 22","2014-02-11T11:52:33.000+0000","mark.pollack","iperumal","Create REST API for getting information on a given job instance","",3.0,0.0321819262284791,0.0321317073579015,0.2997313290424098,True,True,True,1392119553000.0,1392127243000.0,1392130843000.0,1392357607000.0,1392358507000.0,1392119553000.0,-1.0,1392191175000.0,1392194775000.0,1392127231000.0,1392130831000.0,1392127243000.0,1392130843000.0,"mark.pollack",1392194775000.0,1392119553000.0,44,1392119553000.0,-1.0,"Create REST API for getting information on a given job instance","",3.0,False,3.0,0,0.0,0,0,False
"XD-1302","Story","Sprint 23","2014-02-11T11:32:15.000+0000","mark.pollack","jvalkeal","Add documentation for using FTP->HDFS partitioned jobs","",5.0,-1.0,0.0,0.7716976221607548,False,True,True,1392118335000.0,-1.0,-1.0,1394680308000.0,1394681208000.0,1392118335000.0,-1.0,1394096098000.0,1394099698000.0,1392118335000.0,1392121935000.0,-1.0,-1.0,"mark.pollack",1394099698000.0,1392118335000.0,44,1392118335000.0,-1.0,"Add documentation for using FTP->HDFS partitioned jobs","",5.0,False,5.0,0,0.0,0,0,False
"XD-1301","Bug","","2014-02-11T08:54:05.000+0000","grenfro","","MBeans are not destroyed if stream is created and destroyed with no delay","Problem: The container that the stream was deployed to, will not allow new streams to be deployed.  Once the error occurs, the only solution is to terminate the XD Container and restart it.

To reproduce create a stream foo and destroy the stream, then create the stream  foo again.  This best done programmatically, taking the same steps using the ""shell"" may not reproduce the problem.  i.e. if you put a Sleep of 1-2 seconds between the destroy and the next create, it works fine

",5.0,-1.0,0.9906757045810656,-1.0,False,False,True,1392108845000.0,-1.0,-1.0,1398316457000.0,1398317357000.0,1392108845000.0,-1.0,-1.0,-1.0,1398259467000.0,1398263067000.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1392108845000.0,-1.0,"MBeans are not destroyed if stream is created and destroyed with no delay","Problem: The container that the stream was deployed to, will not allow new streams to be deployed.  Once the error occurs, the only solution is to terminate the XD Container and restart it.

To reproduce create a stream foo and destroy the stream, then create the stream  foo again.  This best done programmatically, taking the same steps using the ""shell"" may not reproduce the problem.  i.e. if you put a Sleep of 1-2 seconds between the destroy and the next create, it works fine

",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1300","Story","Sprint 22","2014-02-10T19:01:06.000+0000","iperumal","luke","Handling boolean type module option properties defaults in option metadata","There are few boolean type module option properties whose default values are specified in the module definitions than their corresponding ModuleOptionsMetaData. 

Also, when using boolean we need to have module option using primitive type boolean than Boolean type.

Currently, these are some of the module options that require this change:

""initializeDatabase"" in modules filejdbc, hdfsjdbc job modules, aggregator processor module, jdbc sink module

""restartable"" in all the job modules

""deleteFiles"" in filejdbc, filepollhdfs job modules






",3.0,0.0283941212458227,0.0165724338843727,0.0736810356927422,True,True,True,1392058866000.0,1392111868000.0,1392115468000.0,1393924620000.0,1393925520000.0,1392058866000.0,-1.0,1392196403000.0,1392200003000.0,1392089801000.0,1392093401000.0,1392111868000.0,1392115468000.0,"iperumal",1392200003000.0,1392058866000.0,44,1392058866000.0,-1.0,"Handling boolean type module option properties defaults in option metadata","There are few boolean type module option properties whose default values are specified in the module definitions than their corresponding ModuleOptionsMetaData. 

Also, when using boolean we need to have module option using primitive type boolean than Boolean type.

Currently, these are some of the module options that require this change:

""initializeDatabase"" in modules filejdbc, hdfsjdbc job modules, aggregator processor module, jdbc sink module

""restartable"" in all the job modules

""deleteFiles"" in filejdbc, filepollhdfs job modules






",3.0,False,3.0,0,0.0,-1,0,False
"XD-1299","Story","Sprint 31","2014-02-10T12:26:26.000+0000","iperumal","eric.bottard","Move --deleteFiles out of ResourcesIntoJdbcJobModuleOptionsMetadata","Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules so that the files are deleted after successful completion of jobs that write the file into hdfs/jdbc.

We have ""--deleteFiles"" option in ResourcesIntoJdbcJobModuleOptionsMetadata (from https://github.com/spring-projects/spring-xd/pull/562)  which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files.

We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported.

",3.0,-1.0,0.9513081464829888,0.9955797375903302,False,True,True,1392035186000.0,-1.0,-1.0,1404272021000.0,1404272921000.0,1392035186000.0,-1.0,1404218827000.0,1404222427000.0,1403677043000.0,1403680643000.0,-1.0,-1.0,"iperumal",1404222427000.0,1392035186000.0,44,1392035186000.0,-1.0,"Move --deleteFiles out of ResourcesIntoJdbcJobModuleOptionsMetadata","Currently, the fileDeletion listeners are added to filepollhdfs and filejdbc OOTB job modules so that the files are deleted after successful completion of jobs that write the file into hdfs/jdbc.

We have ""--deleteFiles"" option in ResourcesIntoJdbcJobModuleOptionsMetadata (from https://github.com/spring-projects/spring-xd/pull/562)  which makes it available for hdfsjdbc job module as well. But it is not supported yet as it involves deletion of HDFS files.

We need the file deletion listeners for the hdfsjdbc and hdfsmongodb job modules so that if opted to delete files, it can be supported.

",3.0,False,3.0,0,0.0,0,2,False
"XD-1298","Bug","","2014-02-10T08:08:57.000+0000","eric.bottard","","Commands that prompt the user are now broken","Most likely due to https://github.com/spring-projects/spring-shell/commit/296e4d2ff0e6e91d91209ab8717335357c587de0

When the user submits with ENTER, the shell appears to hang",5.0,-1.0,0.999516363050137,-1.0,False,False,True,1392019737000.0,-1.0,-1.0,1392087070000.0,1392087970000.0,1392019737000.0,-1.0,-1.0,-1.0,1392087937000.0,1392087970000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1392019737000.0,-1.0,"Commands that prompt the user are now broken","Most likely due to https://github.com/spring-projects/spring-shell/commit/296e4d2ff0e6e91d91209ab8717335357c587de0

When the user submits with ENTER, the shell appears to hang",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1296","Story","Sprint 22","2014-02-10T01:10:41.000+0000","iperumal","luke","Few integration tests fail if JMX is enabled","If JMX is enabled, some of the integration tests fail.

This is similar to what we see in XD-1295.

One example of this case is, the test classes that extend StreamTestSupport.

In StreamTestSupport, the @BeforeClass has this line:

moduleDeployer = containerContext.getBean(ModuleDeployer.class);

When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer (since it is of type MessageHandler) and thereby the above line to get bean by the implementing class type (ModuleDeployer) fails.

There are few other places where we use to refer the implementing classes on getBean().
Looks like we need to fix those as well.

",2.0,0.188048918799247,0.186556393080108,0.1880012739810094,True,True,True,1391994641000.0,1392357755000.0,1392361355000.0,1393924696000.0,1393925596000.0,1391994641000.0,-1.0,1392357663000.0,1392361263000.0,1392354873000.0,1392358473000.0,1392357755000.0,1392361355000.0,"iperumal",1392361263000.0,1391994641000.0,44,1391994641000.0,-1.0,"Few integration tests fail if JMX is enabled","If JMX is enabled, some of the integration tests fail.

This is similar to what we see in XD-1295.

One example of this case is, the test classes that extend StreamTestSupport.

In StreamTestSupport, the @BeforeClass has this line:

moduleDeployer = containerContext.getBean(ModuleDeployer.class);

When JMX is enable, the IntegrationMBeanExporter creates JdkDynamicProxy for the ModuleDeployer (since it is of type MessageHandler) and thereby the above line to get bean by the implementing class type (ModuleDeployer) fails.

There are few other places where we use to refer the implementing classes on getBean().
Looks like we need to fix those as well.

",2.0,False,2.0,0,0.0,0,0,False
"XD-1295","Story","Sprint 22","2014-02-10T00:54:04.000+0000","iperumal","luke","Module message conversion fails to work if JMX is enabled","If JMX is enabled for modules (enabling the IntegrationMBeanExporter), the ModuleTypeConversionPlugin fails to get the reference to input/output channel (for adding the ContentTypeHeaderInterceptor) and results in exception.

It seems like the IntegrationMBeanExporter (when JMX is enabled) creates JdkDynamicAopProxy for all the integration components and thereby the following check on ModuleTypeConversionPlugin to retrieve the AbstractMessageChannel fails.

AbstractMessageChannel channel = null;
			if (isInput) {
				channel = module.getComponent(""input"", AbstractMessageChannel.class);
			}
			else {
				channel = module.getComponent(""output"", AbstractMessageChannel.class);
			}

I see the reason why AbstractMessageChannel is used here (to use some of the methods in the implementing class that didn't exist in the interfaces) but IntegrationMBeanExporter creating JdkDynamicAopProxy for the channel fails to resolve as AbstractMessageChannel here.

Following is the full stack trace:

To replicate,

stream create testing --definition """"http --outputType=text/plain | log""
3:45,238 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [moduleDeployer]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy62.handleMessage(Unknown Source)
	at org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:152)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:121)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:108)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:218)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:188)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy61.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:96)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:212)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.springframework.xd.dirt.plugins.ModuleConfigurationException: Bean named 'output' must be of type [org.springframework.integration.channel.AbstractMessageChannel], but was actually of type [com.sun.proxy.$Proxy70]
	at org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.configureModuleConverters(ModuleTypeConversionPlugin.java:144)
	at org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.postProcessModule(ModuleTypeConversionPlugin.java:70)
	at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:378)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:282)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:271)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:266)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleSingleModuleMessage(ModuleDeployer.java:244)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:171)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	... 42 more
Caused by: org.springframework.beans.factory.BeanNotOfRequiredTypeException: Bean named 'output' must be of type [org.springframework.integration.channel.AbstractMessageChannel], but was actually of type [com.sun.proxy.$Proxy70]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:376)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:979)
	at org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:156)
	at org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.configureModuleConverters(ModuleTypeConversionPlugin.java:100)
	... 50 more",3.0,0.5000139583315886,0.4992825417563489,0.5033486037480912,True,True,True,1391993644000.0,1392351863000.0,1392355463000.0,1392709162000.0,1392710062000.0,1391993644000.0,-1.0,1392354252000.0,1392357852000.0,1392351339000.0,1392354939000.0,1392351863000.0,1392355463000.0,"iperumal",1392357852000.0,1391993644000.0,44,1391993644000.0,-1.0,"Module message conversion fails to work if JMX is enabled","If JMX is enabled for modules (enabling the IntegrationMBeanExporter), the ModuleTypeConversionPlugin fails to get the reference to input/output channel (for adding the ContentTypeHeaderInterceptor) and results in exception.

It seems like the IntegrationMBeanExporter (when JMX is enabled) creates JdkDynamicAopProxy for all the integration components and thereby the following check on ModuleTypeConversionPlugin to retrieve the AbstractMessageChannel fails.

AbstractMessageChannel channel = null;
			if (isInput) {
				channel = module.getComponent(""input"", AbstractMessageChannel.class);
			}
			else {
				channel = module.getComponent(""output"", AbstractMessageChannel.class);
			}

I see the reason why AbstractMessageChannel is used here (to use some of the methods in the implementing class that didn't exist in the interfaces) but IntegrationMBeanExporter creating JdkDynamicAopProxy for the channel fails to resolve as AbstractMessageChannel here.

Following is the full stack trace:

To replicate,

stream create testing --definition """"http --outputType=text/plain | log""
3:45,238 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 handler.LoggingHandler:145 - org.springframework.messaging.MessageHandlingException: error occurred in message handler [moduleDeployer]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.handleMessage(SimpleMessageHandlerMetrics.java:106)
	at org.springframework.integration.monitor.SimpleMessageHandlerMetrics.invoke(SimpleMessageHandlerMetrics.java:86)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy62.handleMessage(Unknown Source)
	at org.springframework.integration.config.ServiceActivatorFactoryBean$1.handleRequestMessage(ServiceActivatorFactoryBean.java:83)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:152)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:121)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:108)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:218)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:188)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)
	at org.springframework.integration.monitor.DirectChannelMetrics.monitorSend(DirectChannelMetrics.java:113)
	at org.springframework.integration.monitor.DirectChannelMetrics.doInvoke(DirectChannelMetrics.java:97)
	at org.springframework.integration.monitor.DirectChannelMetrics.invoke(DirectChannelMetrics.java:91)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:179)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:207)
	at com.sun.proxy.$Proxy61.send(Unknown Source)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:114)
	at org.springframework.messaging.core.GenericMessagingTemplate.doSend(GenericMessagingTemplate.java:44)
	at org.springframework.messaging.core.AbstractMessageSendingTemplate.send(AbstractMessageSendingTemplate.java:92)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:96)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:212)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.springframework.xd.dirt.plugins.ModuleConfigurationException: Bean named 'output' must be of type [org.springframework.integration.channel.AbstractMessageChannel], but was actually of type [com.sun.proxy.$Proxy70]
	at org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.configureModuleConverters(ModuleTypeConversionPlugin.java:144)
	at org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.postProcessModule(ModuleTypeConversionPlugin.java:70)
	at org.springframework.xd.dirt.module.ModuleDeployer.postProcessModule(ModuleDeployer.java:378)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:282)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployAndStore(ModuleDeployer.java:271)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:266)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleSingleModuleMessage(ModuleDeployer.java:244)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:171)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	... 42 more
Caused by: org.springframework.beans.factory.BeanNotOfRequiredTypeException: Bean named 'output' must be of type [org.springframework.integration.channel.AbstractMessageChannel], but was actually of type [com.sun.proxy.$Proxy70]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:376)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:200)
	at org.springframework.context.support.AbstractApplicationContext.getBean(AbstractApplicationContext.java:979)
	at org.springframework.xd.module.core.SimpleModule.getComponent(SimpleModule.java:156)
	at org.springframework.xd.dirt.plugins.stream.ModuleTypeConversionPlugin.configureModuleConverters(ModuleTypeConversionPlugin.java:100)
	... 50 more",3.0,False,3.0,0,0.0,0,0,False
"XD-1294","Story","Sprint 21","2014-02-07T13:15:18.000+0000","iperumal","iperumal","Update spring-xd-extension-reactor dependency","Currently, the reactorEnv bean is defined in module-common context and the spring-xd-dirt has the runtime dependency over spring-xd-extension-reactor project. 

This enables boot's ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications.

Since reactor environment is not being used by container and only used by the reactor-syslog module, we can move the reactorEnv bean definition in reactor-syslog module.

There is one caveat in this approach as the reactor environment gets setup everytime a new reactor-syslog module is deployed.",1.0,0.000131805981595,0.0,0.0001030483128834,True,True,True,1391778918000.0,1391778973000.0,1391782573000.0,1392195298000.0,1392196198000.0,1391778918000.0,-1.0,1391778961000.0,1391782561000.0,1391778918000.0,1391782518000.0,1391778973000.0,1391782573000.0,"iperumal",1391782561000.0,1391778918000.0,44,1391778918000.0,-1.0,"Update spring-xd-extension-reactor dependency","Currently, the reactorEnv bean is defined in module-common context and the spring-xd-dirt has the runtime dependency over spring-xd-extension-reactor project. 

This enables boot's ReactorAutoConfiguration to initialize the reactor environment, we have the reactor setup configured for both admin and container server applications.

Since reactor environment is not being used by container and only used by the reactor-syslog module, we can move the reactorEnv bean definition in reactor-syslog module.

There is one caveat in this approach as the reactor environment gets setup everytime a new reactor-syslog module is deployed.",1.0,False,1.0,0,0.0,0,0,False
"XD-1293","Story","Sprint 21","2014-02-07T09:33:05.000+0000","iperumal","iperumal","Remove the constraint on job module batch job id to be ""job""","Currently, the job module's batch job's bean id should be ""job"". This also causes the job name to be 'actual-job-name + "".job""' and the batch job controllers require to search for job with suffix "".job"". 

Removing this constraint would help us avoiding these.",3.0,0.0001292050366472,0.0,6.656017039403621e-05,True,True,True,1391765585000.0,1391765618000.0,1391769218000.0,1392020093000.0,1392020993000.0,1391765585000.0,-1.0,1391765602000.0,1391769202000.0,1391765585000.0,1391769185000.0,1391765618000.0,1391769218000.0,"iperumal",1391769202000.0,1391765585000.0,44,1391765585000.0,-1.0,"Remove the constraint on job module batch job id to be ""job""","Currently, the job module's batch job's bean id should be ""job"". This also causes the job name to be 'actual-job-name + "".job""' and the batch job controllers require to search for job with suffix "".job"". 

Removing this constraint would help us avoiding these.",3.0,False,3.0,0,0.0,0,0,False
"XD-1292","Story","","2014-02-03T08:29:48.000+0000","eric.bottard","eric.bottard","module delete command should only provide completions with composed modules","There is no point in providing completion with something that will fail when the user tries it. The information about a module being a composed is now available at the REST layer, so should be used",4.0,-1.0,0.0,-1.0,False,False,True,1391416188000.0,-1.0,-1.0,1426348688000.0,1426349588000.0,1391416188000.0,-1.0,-1.0,-1.0,1391416188000.0,1391419788000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1391416188000.0,-1.0,"module delete command should only provide completions with composed modules","There is no point in providing completion with something that will fail when the user tries it. The information about a module being a composed is now available at the REST layer, so should be used",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1291","Story","Sprint 21","2014-01-31T14:53:24.000+0000","iperumal","iperumal","Add SmartLifeCycle support to ModuleDeployer","Currently, ModuleDeployer is a disposable bean. When the container context is closed, the ModuleDeployer bean is destroyed along with its associated common context and deployed modules. Issue arises, if the connectionfactory bean associated with the deployed modules' message bus bindings  is destroyed before the ModuleDeployer bean, there is exception stacktrace (at least in case of Redis MessageBus) saying ""Connection closed"". 

Adding SmartLifecycle support to ModuleDeployer will make sure all the beans are destroyed during Lifecycle processor's stop() method before any of the singletonbeans are destroyed.

The stacktrace (when using Redis MessageBus is):

org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is com.lambdaworks.redis.RedisException: Connection closed
at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:45)
at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:35)
at org.springframework.data.redis.connection.lettuce.LettuceConverters.toDataAccessException(LettuceConverters.java:158)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:237)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1449)
at org.springframework.data.redis.core.DefaultListOperations$12.inRedis(DefaultListOperations.java:154)
at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:50)
at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:181)
at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:149)
at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:84)
at org.springframework.data.redis.core.DefaultListOperations.rightPop(DefaultListOperations.java:151)
at org.springframework.data.redis.core.DefaultBoundListOperations.rightPop(DefaultBoundListOperations.java:92)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:178)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)
at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
at java.lang.Thread.run(Thread.java:722)
Caused by: com.lambdaworks.redis.RedisException: Connection closed
at com.lambdaworks.redis.RedisAsyncConnection.await(RedisAsyncConnection.java:1079)
at com.lambdaworks.redis.RedisConnection.await(RedisConnection.java:820)
at com.lambdaworks.redis.RedisConnection.brpop(RedisConnection.java:101)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1447)
... 12 more
13:19:00,897 WARN Thread-5 support.DefaultLifecycleProcessor:257 - Failed to stop bean 'org.springframework.integration.redis.inbound.RedisInboundChannelAdapter#0'
com.lambdaworks.redis.RedisException: Connection is closed
at com.lambdaworks.redis.RedisAsyncConnection.dispatch(RedisAsyncConnection.java:1065)
at com.lambdaworks.redis.pubsub.RedisPubSubConnection.unsubscribe(RedisPubSubConnection.java:82)
at org.springframework.data.redis.connection.lettuce.LettuceSubscription.doUnsubscribe(LettuceSubscription.java:68)
at org.springframework.data.redis.connection.util.AbstractSubscription.unsubscribe(AbstractSubscription.java:186)
at org.springframework.data.redis.connection.util.AbstractSubscription.unsubscribe(AbstractSubscription.java:146)
at org.springframework.data.redis.listener.RedisMessageListenerContainer$SubscriptionTask.cancel(RedisMessageListenerContainer.java:836)
at org.springframework.data.redis.listener.RedisMessageListenerContainer.stop(RedisMessageListenerContainer.java:210)
at org.springframework.integration.redis.inbound.RedisInboundChannelAdapter.doStop(RedisInboundChannelAdapter.java:127)
at org.springframework.integration.endpoint.AbstractEndpoint.stop(AbstractEndpoint.java:100)
at org.springframework.integration.endpoint.AbstractEndpoint.stop(AbstractEndpoint.java:115)
at org.springframework.context.support.DefaultLifecycleProcessor.doStop(DefaultLifecycleProcessor.java:229)
at org.springframework.context.support.DefaultLifecycleProcessor.access$300(DefaultLifecycleProcessor.java:51)
at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.stop(DefaultLifecycleProcessor.java:363)
at org.springframework.context.support.DefaultLifecycleProcessor.stopBeans(DefaultLifecycleProcessor.java:202)
at org.springframework.context.support.DefaultLifecycleProcessor.onClose(DefaultLifecycleProcessor.java:118)
at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:888)
at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.doClose(EmbeddedWebApplicationContext.java:157)
at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:809)",2.0,3.305071096268807e-05,2.2809645593967823e-05,2.886118422093888e-05,True,True,True,1391180004000.0,1391180075000.0,1391183675000.0,1393327318000.0,1393328218000.0,1391180004000.0,-1.0,1391180066000.0,1391183666000.0,1391180053000.0,1391183653000.0,1391180075000.0,1391183675000.0,"iperumal",1391183666000.0,1391180004000.0,44,1391180004000.0,-1.0,"Handle container shutdown gracefully","Currently, ModuleDeployer is a disposable bean. When the container context is closed, the ModuleDeployer bean is destroyed along with its associated common context and deployed modules. Issue arises, if the connectionfactory bean associated with the deployed modules' message bus bindings  is destroyed before the ModuleDeployer bean, there is exception stacktrace (at least in case of Redis MessageBus) saying ""Connection closed"". 

Adding SmartLifecycle support to ModuleDeployer will make sure all the beans are destroyed during Lifecycle processor's stop() method before any of the singletonbeans are destroyed.

The stacktrace (when using Redis MessageBus is):

org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is com.lambdaworks.redis.RedisException: Connection closed
at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:45)
at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:35)
at org.springframework.data.redis.connection.lettuce.LettuceConverters.toDataAccessException(LettuceConverters.java:158)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:237)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1449)
at org.springframework.data.redis.core.DefaultListOperations$12.inRedis(DefaultListOperations.java:154)
at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:50)
at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:181)
at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:149)
at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:84)
at org.springframework.data.redis.core.DefaultListOperations.rightPop(DefaultListOperations.java:151)
at org.springframework.data.redis.core.DefaultBoundListOperations.rightPop(DefaultBoundListOperations.java:92)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:178)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)
at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
at java.lang.Thread.run(Thread.java:722)
Caused by: com.lambdaworks.redis.RedisException: Connection closed
at com.lambdaworks.redis.RedisAsyncConnection.await(RedisAsyncConnection.java:1079)
at com.lambdaworks.redis.RedisConnection.await(RedisConnection.java:820)
at com.lambdaworks.redis.RedisConnection.brpop(RedisConnection.java:101)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1447)
... 12 more
13:19:00,897 WARN Thread-5 support.DefaultLifecycleProcessor:257 - Failed to stop bean 'org.springframework.integration.redis.inbound.RedisInboundChannelAdapter#0'
com.lambdaworks.redis.RedisException: Connection is closed
at com.lambdaworks.redis.RedisAsyncConnection.dispatch(RedisAsyncConnection.java:1065)
at com.lambdaworks.redis.pubsub.RedisPubSubConnection.unsubscribe(RedisPubSubConnection.java:82)
at org.springframework.data.redis.connection.lettuce.LettuceSubscription.doUnsubscribe(LettuceSubscription.java:68)
at org.springframework.data.redis.connection.util.AbstractSubscription.unsubscribe(AbstractSubscription.java:186)
at org.springframework.data.redis.connection.util.AbstractSubscription.unsubscribe(AbstractSubscription.java:146)
at org.springframework.data.redis.listener.RedisMessageListenerContainer$SubscriptionTask.cancel(RedisMessageListenerContainer.java:836)
at org.springframework.data.redis.listener.RedisMessageListenerContainer.stop(RedisMessageListenerContainer.java:210)
at org.springframework.integration.redis.inbound.RedisInboundChannelAdapter.doStop(RedisInboundChannelAdapter.java:127)
at org.springframework.integration.endpoint.AbstractEndpoint.stop(AbstractEndpoint.java:100)
at org.springframework.integration.endpoint.AbstractEndpoint.stop(AbstractEndpoint.java:115)
at org.springframework.context.support.DefaultLifecycleProcessor.doStop(DefaultLifecycleProcessor.java:229)
at org.springframework.context.support.DefaultLifecycleProcessor.access$300(DefaultLifecycleProcessor.java:51)
at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.stop(DefaultLifecycleProcessor.java:363)
at org.springframework.context.support.DefaultLifecycleProcessor.stopBeans(DefaultLifecycleProcessor.java:202)
at org.springframework.context.support.DefaultLifecycleProcessor.onClose(DefaultLifecycleProcessor.java:118)
at org.springframework.context.support.AbstractApplicationContext.doClose(AbstractApplicationContext.java:888)
at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.doClose(EmbeddedWebApplicationContext.java:157)
at org.springframework.context.support.AbstractApplicationContext$1.run(AbstractApplicationContext.java:809)",2.0,False,2.0,0,0.0,0,0,True
"XD-1288","Story","Sprint 21","2014-01-31T01:04:21.000+0000","eric.bottard","eric.bottard","Remove references to XD-1050 in documentation","",2.0,-1.0,0.0,0.0003535609892706,False,True,True,1391130261000.0,-1.0,-1.0,1391415026000.0,1391415926000.0,1391130261000.0,-1.0,1391130362000.0,1391133962000.0,1391130261000.0,1391133861000.0,-1.0,-1.0,"eric.bottard",1391133962000.0,1391130261000.0,44,1391130261000.0,-1.0,"Remove references to XD-1050 in documentation","",2.0,False,2.0,0,0.0,-1,0,False
"XD-1287","Story","","2014-01-30T21:42:24.000+0000","mark.pollack","Andy Clement","Tap definitions should verify stream name","",4.0,0.8732504423820561,0.8709009147484158,-1.0,True,False,True,1391118144000.0,1395731320000.0,1395734920000.0,1396400008000.0,1396400908000.0,1391118144000.0,-1.0,-1.0,-1.0,1395718908000.0,1395722508000.0,1395731320000.0,1395734920000.0,"mark.pollack",-1.0,-1.0,0,1391118144000.0,-1.0,"Tap definitions should verify stream name","xd:>stream create --name simple --definition ""http | transform | filter | transform | file""
Created new stream 'simple'
xd:>stream create --name tapSimple --definition ""tap:stream:mystream.transform > file""
Created new stream 'tapSimple'

There isn't a stream named ""mystream""... I don't remember if we want to allow for this (set up taps before there is a stream) or if it should be an error.

Otherwise, works as expected

xd:>stream create --name tapSimple2 --definition ""tap:stream:simple.transform > file""
Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD144E:(pos 11): Reference to 'transform' is not unique in the target stream 'http | transform | filter | transform | file', please label the relevant module and use the label, or use a suffix index to indicate which occurrence of the module, e.g. 'transform.0'
tap:simple.transform",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-1285","Story","Sprint 21","2014-01-30T07:32:06.000+0000","eric.bottard","eric.bottard","Support shell completions for module names","see CompletionProviderTests#testUnfinishedModuleNameShouldReturnCompletions()

Ideally, would require a change in the parser so that it knows which kind of module was expected when it failed.",4.0,0.980835064723006,0.9808223960180852,0.9808026891437645,True,True,True,1391067126000.0,1391763923000.0,1391767523000.0,1391776638000.0,1391777538000.0,1391067126000.0,-1.0,1391763900000.0,1391767500000.0,1391763914000.0,1391767514000.0,1391763923000.0,1391767523000.0,"eric.bottard",1391767500000.0,1391067126000.0,44,1391067126000.0,-1.0,"Support shell completions for module names","see CompletionProviderTests#testUnfinishedModuleNameShouldReturnCompletions()

Ideally, would require a change in the parser so that it knows which kind of module was expected when it failed.",4.0,False,4.0,0,0.0,-1,1,False
"XD-1284","Story","","2014-01-30T07:29:09.000+0000","eric.bottard","","Support shell completions for closed set of values in stream definitions","When doing dsl completion in a stream definition and a module option accepts a value that has a closed set of possible values (eg booleans, enums), we can provide completions for those.

",8.0,-1.0,-1.0,-1.0,False,False,False,1391066949000.0,-1.0,-1.0,1448379992000.0,1448380892000.0,1391066949000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1391066949000.0,-1.0,"Support shell completions for closed set of values in stream definitions","When doing dsl completion in a stream definition and a module option accepts a value that has a closed set of possible values (eg booleans, enums), we can provide completions for those.

",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1281","Story","Sprint 27","2014-01-29T19:56:23.000+0000","hillert","hillert","Create Better UI instead of Boot's default ""Whitelabel Error Page""","Create a better UI instead of Boot's default ""Whitelabel Error Page""",6.0,0.9890014419230638,0.8480331396322687,0.9185625046121624,True,True,True,1391025383000.0,1399468696000.0,1399472296000.0,1399561693000.0,1399562593000.0,1391025383000.0,-1.0,1398867344000.0,1398870944000.0,1398265220000.0,1398268820000.0,1399468696000.0,1399472296000.0,"hillert",1398870944000.0,1391025383000.0,44,1391025383000.0,-1.0,"Create Better UI instead of Boot's default ""Whitelabel Error Page""","Create a better UI instead of Boot's default ""Whitelabel Error Page""",6.0,False,6.0,0,0.0,0,0,False
"XD-1279","Story","","2014-01-29T11:21:30.000+0000","mark.pollack","","The HDFS Sink should roll over based on the number of events.","",4.0,-1.0,-1.0,-1.0,False,False,False,1390994490000.0,-1.0,-1.0,1448377748000.0,1448378648000.0,1390994490000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1390994490000.0,-1.0,"The HDFS Sink should roll over based on the number of events.","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1278","Story","Sprint 25","2014-01-29T11:12:30.000+0000","mark.pollack","thomas.risberg","Rename avro sink to hdfs-dataset and add support for parquet format","The kite SDK is used to write Avro records in a Kite specific format and also it support the parquet format for which we will eventually support.",2.0,-1.0,0.8432717826892026,0.9946650596114228,False,True,True,1390993950000.0,-1.0,-1.0,1396596674000.0,1396597574000.0,1390993950000.0,-1.0,1396567679000.0,1396571279000.0,1395719328000.0,1395722928000.0,-1.0,-1.0,"mark.pollack",1396571279000.0,1390993950000.0,44,1390993950000.0,-1.0,"Rename avro sink to hdfs-dataset and add support for parquet format","The kite SDK is used to write Avro records in a Kite specific format and also it support the parquet format for which we will eventually support.",2.0,False,2.0,0,0.0,0,1,False
"XD-1277","Story","Sprint 22","2014-01-29T04:32:26.000+0000","eric.bottard","eric.bottard","Default option values broken for composed modules","May want to fix ""properly"" when tackling the module options for composed modules, but this is currently broken (and wasn't at some point):

module compose upperHttp --definition ""http | transform --expression=payload.toUpperCase()""
stream create foo --definition ""upperHttp | log""

This will fail saying that ${port} can't be resolved

This will work though:
module compose upperHttp --definition ""http --port=9000 | transform --expression=payload.toUpperCase()""
stream create foo --definition ""upperHttp | log""


Note that 
stream create foo --definition ""upperHttp --port=xxx | log"" should work too wut won't, but that's another bug (will create after this one)",4.0,0.96002974926532,0.0,0.9600144145924748,True,True,True,1390969946000.0,1393223732000.0,1393227332000.0,1393316667000.0,1393317567000.0,1390969946000.0,-1.0,1393223696000.0,1393227296000.0,1390969946000.0,1390973546000.0,1393223732000.0,1393227332000.0,"eric.bottard",1393227296000.0,1390969946000.0,44,1390969946000.0,-1.0,"Default option values broken for composed modules","May want to fix ""properly"" when tackling the module options for composed modules, but this is currently broken (and wasn't at some point):

module compose upperHttp --definition ""http | transform --expression=payload.toUpperCase()""
stream create foo --definition ""upperHttp | log""

This will fail saying that ${port} can't be resolved

This will work though:
module compose upperHttp --definition ""http --port=9000 | transform --expression=payload.toUpperCase()""
stream create foo --definition ""upperHttp | log""


Note that 
stream create foo --definition ""upperHttp --port=xxx | log"" should work too wut won't, but that's another bug (will create after this one)",4.0,False,4.0,0,0.0,0,0,False
"XD-1276","Story","Sprint 21","2014-01-28T21:32:05.000+0000","mark.pollack","luke","Out of the box batch jobs should add xdJobExecutionListener and xdStepExecutionListener","To show best practice, our batch jobs should include these listeners so that notifications can be sent.  In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move/delete the file",8.0,0.8397393629874146,0.0,0.8396697396496717,True,True,True,1390944725000.0,1391149765000.0,1391153365000.0,1391187996000.0,1391188896000.0,1390944725000.0,-1.0,1391149748000.0,1391153348000.0,1390944725000.0,1390948325000.0,1391149765000.0,1391153365000.0,"mark.pollack",1391153348000.0,1390944725000.0,44,1390944725000.0,-1.0,"Out of the box batch jobs should add xdJobExecutionListener and xdStepExecutionListener","To show best practice, our batch jobs should include these listeners so that notifications can be sent.  In particular it is desirable that jobs with file item readers can send the files that were processed so another stream can be sent the file name when the job completes to move/delete the file",8.0,False,8.0,0,0.0,-1,0,False
"XD-1274","Story","Sprint 21","2014-01-28T10:24:08.000+0000","grussell","grussell","spring-integration-hadoop.xsd (and reactor) Imports the SI 3.0 Instead of SI 4.0 Schema","",1.0,0.9685758648139536,0.9685758648139536,0.9684773441995004,True,True,True,1390904648000.0,1391150428000.0,1391154028000.0,1391157502000.0,1391158402000.0,1390904648000.0,-1.0,1391150403000.0,1391154003000.0,1391150428000.0,1391154028000.0,1391150428000.0,1391154028000.0,"grussell",1391154003000.0,1390904648000.0,44,1390904648000.0,-1.0,"spring-integration-hadoop.xsd (and reactor) Imports the SI 3.0 Instead of SI 4.0 Schema","",1.0,False,1.0,0,0.0,0,0,False
"XD-1273","Improvement","Sprint 21","2014-01-28T06:02:59.000+0000","dturanski","dturanski","The use of labelled modules and taps needs more explanation","https://github.com/spring-projects/spring-xd/wiki/Taps mentions this but the explanation needs more elaboration and example, e.g. 
mystream -> 
""http | flibble: transform --expression=payload.toUpperCase() | file""

""tap:stream:mystream.flibble > transform --expression=payload.replaceAll('A','.') | log"");",1.0,-1.0,0.999660586460832,0.9996987228135475,False,True,True,1390888979000.0,-1.0,-1.0,1391150296000.0,1391151196000.0,1390888979000.0,-1.0,1391151117000.0,1391151196000.0,1391151107000.0,1391151196000.0,-1.0,-1.0,"dturanski",1391151196000.0,1390888979000.0,44,1390888979000.0,-1.0,"The use of labelled modules and taps needs more explanation","https://github.com/spring-projects/spring-xd/wiki/Taps mentions this but the explanation needs more elaboration and example, e.g. 
mystream -> 
""http | flibble: transform --expression=payload.toUpperCase() | file""

""tap:stream:mystream.flibble > transform --expression=payload.replaceAll('A','.') | log"");",1.0,False,1.0,0,0.0,-1,0,False
"XD-1272","Story","Sprint 21","2014-01-28T05:13:16.000+0000","eric.bottard","eric.bottard","Remove job options that are handled at module level from shell","Since makeUnique, dateFormat and numberFormat now have their own module options now, they are nicely advertised as options to a job (and will benefit from code completion soon), so they can be removed from the shell (where they currently allow for a misconfiguration if set at both the job and shell level)",4.0,0.782131422472938,0.0,0.7809117243482239,True,True,True,1390885996000.0,1390891126000.0,1390892555000.0,1390891655000.0,1390892555000.0,1390885996000.0,-1.0,1390891118000.0,1390892555000.0,1390885996000.0,1390889596000.0,1390891126000.0,1390892555000.0,"eric.bottard",1390892555000.0,1390885996000.0,44,1390885996000.0,-1.0,"Remove job options that are handled at module level from shell","Since makeUnique, dateFormat and numberFormat now have their own module options now, they are nicely advertised as options to a job (and will benefit from code completion soon), so they can be removed from the shell (where they currently allow for a misconfiguration if set at both the job and shell level)",4.0,False,4.0,0,0.0,0,0,False
"XD-1271","Story","Sprint 21","2014-01-27T22:47:44.000+0000","iperumal","Ilayaperumal Gopinathan","Investigate missing boot's actuator endpoints in XD","Currently, few of the boot's actuator endpoints go missing in the EndpointHandler mapping.
They are: BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint.

Also, the EndpointHandler mapping doesn't even happen in case of LauncherApplication.
I think this is because the LauncherApplication context starts with port '0' and the TomcatEmbeddedServletContainer sets the local port for it later. With port '0', the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.",3.0,0.0643075655178309,0.8902360027919306,0.0662535760199385,True,True,True,1390862864000.0,1391172801000.0,1391176401000.0,1395681568000.0,1395682468000.0,1390862864000.0,-1.0,1391182180000.0,1391185780000.0,1395153449000.0,1395157049000.0,1391172801000.0,1391176401000.0,"iperumal",1391185780000.0,1390862864000.0,44,1390862864000.0,-1.0,"Investigate missing boot's actuator endpoints in XD","Currently, few of the boot's actuator endpoints go missing in the EndpointHandler mapping.
They are: BeansEndpoint, dumpEndpoint, traceEndpoint, healthEndpoint, infoEndpoint.

Also, the EndpointHandler mapping doesn't even happen in case of LauncherApplication.
I think this is because the LauncherApplication context starts with port '0' and the TomcatEmbeddedServletContainer sets the local port for it later. With port '0', the Endpointhandler mapping is disabled during the EndpointHandler mapping bean creation.",3.0,False,3.0,0,0.0,0,0,False
"XD-1270","Story","Sprint 26","2014-01-27T20:51:07.000+0000","mark.pollack","pperalta","Add states to the deployment of stream","1) Improve how the state of the stream is managed.




A deploy command moves the stream from the undeployed state to the deploying state.
If all modules in the stream are successfully deployed, the stream state is deployed
If one or more module deployments failed, the stream state is failed.  Any modules that were successfully deployed, are still running.  
Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state.

For the individual modules that failed, we will be able to find out which ones failed.  Not yet sure if we can try to redeploy just those parts of the stream that failed.

The channel that will be used to update the state of the stream is xd.stream.status

TBD: Details of how the state for each module in the stream will be stored.",20.0,-1.0,0.1950161774381992,0.5432978004423449,False,True,True,1390855867000.0,-1.0,-1.0,1403504374000.0,1403505274000.0,1390855867000.0,1402589149000.0,1397728262000.0,1397731862000.0,1393322706000.0,1393326306000.0,-1.0,-1.0,"mark.pollack",1397731862000.0,1390855867000.0,44,1402589149000.0,1402589149000.0,"Add states to the deployment of stream","Improve how the state of the stream is managed.

A deploy command moves the stream from the undeployed state to the deploying state. If all modules in the stream are successfully deployed, the stream state is deployed If one or more module deployments failed, the stream state is failed.  Any modules that were successfully deployed, are still running.  

Sending an undeploy command will stop all modules of the stream and return the stream to the undeployed state.

For the individual modules that failed, we will be able to find out which ones failed.  Not yet sure if we can try to redeploy just those parts of the stream that failed.

See the [design doc|https://docs.google.com/a/gopivotal.com/document/d/1kWtoH_xEF1wMklzQ8AZaiuhBZWIlpCDi8G9_hAP8Fgc/edit#heading=h.2rk74f16ow4i] for more details.

Story points for this issue are the total of all the story points for the subtasks.",10.0,True,10.0,1,100.0,0,0,True
"XD-1268","Story","Sprint 21","2014-01-25T06:40:24.000+0000","mark.pollack","dturanski","Remove unused code related to 'accepted media type' in MessageBus","",4.0,0.9790045178511269,0.0,0.97854236673203,True,True,True,1390632024000.0,1391085354000.0,1391088954000.0,1391094176000.0,1391095076000.0,1390632024000.0,-1.0,1391085140000.0,1391088740000.0,1390632024000.0,1390635624000.0,1391085354000.0,1391088954000.0,"mark.pollack",1391088740000.0,1390632024000.0,44,1390632024000.0,-1.0,"Remove unused code related to 'accepted media type' in MessageBus","",4.0,False,4.0,0,0.0,0,0,False
"XD-1266","Bug","Sprint 21","2014-01-24T08:24:36.000+0000","luke","luke","Twitterstream is broken","",3.0,0.938034721677144,0.0006029555525427,0.9381203485011738,True,True,True,1390551876000.0,1390814794000.0,1390818394000.0,1390831262000.0,1390832162000.0,1390551876000.0,-1.0,1390814818000.0,1390818418000.0,1390552045000.0,1390555645000.0,1390814794000.0,1390818394000.0,"luke",1390818418000.0,1390551876000.0,44,1390551876000.0,-1.0,"Twitterstream is broken","",3.0,False,3.0,0,0.0,0,0,False
"XD-1264","Bug","Sprint 21","2014-01-23T05:14:47.000+0000","eric.bottard","grussell","Update SI to latest 4.0 M3 and Spring AMQP to 1.3.0.M2","The Rabbit endpoint suffers from a problem similar to XD-1067.
Seems like spring-[rabbit/amqp] needs to be bumped to 1.3.0.M1 to fix it.
Sadly, we get this error:
noformat
java.lang.NoSuchMethodError: org.springframework.amqp.core.MessageProperties.getContentLength()J
at org.springframework.integration.amqp.support.DefaultAmqpHeaderMapper.extractStandardHeaders(DefaultAmqpHeaderMapper.java:102)
at org.springframework.integration.amqp.support.DefaultAmqpHeaderMapper.extractStandardHeaders(DefaultAmqpHeaderMapper.java:53)
at org.springframework.integration.mapping.AbstractHeaderMapper.toHeaders(AbstractHeaderMapper.java:205)
at org.springframework.integration.mapping.AbstractHeaderMapper.toHeadersFromRequest(AbstractHeaderMapper.java:148)
at org.springframework.integration.amqp.inbound.AmqpInboundChannelAdapter$1.onMessage(AmqpInboundChannelAdapter.java:75)
at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:584)
at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:482)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.access$001(SimpleMessageListenerContainer.java:69)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$1.invokeListener(SimpleMessageListenerContainer.java:144)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.invokeListener(SimpleMessageListenerContainer.java:920)
at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.executeListener(AbstractMessageListenerContainer.java:454)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doReceiveAndExecute(SimpleMessageListenerContainer.java:728)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.receiveAndExecute(SimpleMessageListenerContainer.java:712)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.access$400(SimpleMessageListenerContainer.java:69)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:812)
at java.lang.Thread.run(Thread.java:724)
noformat
Updating to latest SI snapshot does not help (as of Jan 23rd)",4.0,-1.0,0.999970170157942,0.9999332379725366,False,True,True,1390454087000.0,-1.0,-1.0,1391157180000.0,1391158080000.0,1390454087000.0,-1.0,1391158033000.0,1391158080000.0,1391158059000.0,1391158080000.0,-1.0,-1.0,"eric.bottard",1391158080000.0,1390454087000.0,44,1390454087000.0,-1.0,"Update SI to latest 4.0 M3 and Spring AMQP to 1.3.0.M2","The Rabbit endpoint suffers from a problem similar to XD-1067.
Seems like spring-[rabbit/amqp] needs to be bumped to 1.3.0.M1 to fix it.
Sadly, we get this error:
noformat
java.lang.NoSuchMethodError: org.springframework.amqp.core.MessageProperties.getContentLength()J
at org.springframework.integration.amqp.support.DefaultAmqpHeaderMapper.extractStandardHeaders(DefaultAmqpHeaderMapper.java:102)
at org.springframework.integration.amqp.support.DefaultAmqpHeaderMapper.extractStandardHeaders(DefaultAmqpHeaderMapper.java:53)
at org.springframework.integration.mapping.AbstractHeaderMapper.toHeaders(AbstractHeaderMapper.java:205)
at org.springframework.integration.mapping.AbstractHeaderMapper.toHeadersFromRequest(AbstractHeaderMapper.java:148)
at org.springframework.integration.amqp.inbound.AmqpInboundChannelAdapter$1.onMessage(AmqpInboundChannelAdapter.java:75)
at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:584)
at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:482)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.access$001(SimpleMessageListenerContainer.java:69)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$1.invokeListener(SimpleMessageListenerContainer.java:144)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.invokeListener(SimpleMessageListenerContainer.java:920)
at org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer.executeListener(AbstractMessageListenerContainer.java:454)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.doReceiveAndExecute(SimpleMessageListenerContainer.java:728)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.receiveAndExecute(SimpleMessageListenerContainer.java:712)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer.access$400(SimpleMessageListenerContainer.java:69)
at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:812)
at java.lang.Thread.run(Thread.java:724)
noformat
Updating to latest SI snapshot does not help (as of Jan 23rd)",4.0,False,4.0,0,0.0,0,1,False
"XD-1263","Story","","2014-01-22T13:19:21.000+0000","grenfro","","Copy latest build to S3 after a XD Build","",5.0,0.9996193138912984,33.21316549860519,-1.0,True,False,True,1390396761000.0,1390556937000.0,1390556998000.0,1390556098000.0,1390556998000.0,1390396761000.0,-1.0,-1.0,-1.0,1395718739000.0,1390556998000.0,1390556937000.0,1390556998000.0,"grenfro",-1.0,-1.0,0,1390396761000.0,-1.0,"Copy latest build to S3 after a XD Build","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1262","Improvement","Sprint 21","2014-01-22T10:45:00.000+0000","dturanski","iperumal","Provide a clean way to get a reference to the MessageBus running in SingleNodeApplication","Currently the message bus is only obtained via Module.getComponent(MessageBus.class). Stream testing scenarios that depend on sending and receiving payloads via named channels do not require a deployed module instance per se, but any stream flow control uses the MessageBus directly. Getting a deployed module instance in general is expensive, e.g., you have to wait for the module to deploy asynchronously, whereas the MessageBus implementation could be known a priori when the application starts. An improvement would be to ask the container for its MessageBus.",3.0,0.0355546006590744,0.0355373957371238,0.0355415920595508,True,True,True,1390387500000.0,1390556956000.0,1390560556000.0,1395152678000.0,1395153578000.0,1390387500000.0,1390556953000.0,1390556894000.0,1390560494000.0,1390556874000.0,1390560474000.0,1390556956000.0,1390560556000.0,"dturanski",1390560494000.0,1390556953000.0,44,1390556953000.0,-1.0,"Provide a clean way to get a reference to the MessageBus running in SingleNodeApplication","Currently the message bus is only obtained via Module.getComponent(MessageBus.class). Stream testing scenarios that depend on sending and receiving payloads via named channels do not require a deployed module instance per se, but any stream flow control uses the MessageBus directly. Getting a deployed module instance in general is expensive, e.g., you have to wait for the module to deploy asynchronously, whereas the MessageBus implementation could be known a priori when the application starts. An improvement would be to ask the container for its MessageBus.",3.0,False,3.0,0,0.0,0,1,False
"XD-1261","Story","Sprint 26","2014-01-22T00:59:30.000+0000","eric.bottard","eric.bottard","Support default values for options derived out of ${} placeholders","The logic can be found in DefaultModuleOptionsMetadataCollector but caused problems in the initial PR. Revisit if needed",4.0,-1.0,0.8546853030014097,0.8666405589725459,False,True,True,1390352370000.0,-1.0,-1.0,1398862371000.0,1398863271000.0,1390352370000.0,-1.0,1397728262000.0,1397731862000.0,1397626512000.0,1397630112000.0,-1.0,-1.0,"eric.bottard",1397731862000.0,1390352370000.0,44,1390352370000.0,-1.0,"Support default values for options derived out of ${} placeholders","The logic can be found in DefaultModuleOptionsMetadataCollector but caused problems in the initial PR. Revisit if needed",4.0,False,4.0,0,0.0,0,0,False
"XD-1256","Improvement","Sprint 30","2014-01-21T02:49:06.000+0000","eruiz","","Running XD as service","It is useful to configure operating system so that it will start Spring XD automatically on boot.

For example, in Linux it would be great if Spring XD distro contains init.d script to run it as service. A typical init.d script gets executed with arguments such as ""start"", ""stop"", ""restart"", ""pause"", etc. In order for an init.d script to be started or stopped by init during startup and shutdown, the script needs to handle at least ""start"" and ""stop"" arguments.
",2.0,-1.0,-1.0,0.9999996320622162,False,True,False,1390272546000.0,-1.0,-1.0,1403860900000.0,1403861800000.0,1390272546000.0,-1.0,1403861795000.0,1403861800000.0,-1.0,-1.0,-1.0,-1.0,"eruiz",1403861800000.0,1390272546000.0,44,1390272546000.0,-1.0,"Running XD as service","It is useful to configure operating system so that it will start Spring XD automatically on boot.

For example, in Linux it would be great if Spring XD distro contains init.d script to run it as service. A typical init.d script gets executed with arguments such as ""start"", ""stop"", ""restart"", ""pause"", etc. In order for an init.d script to be started or stopped by init during startup and shutdown, the script needs to handle at least ""start"" and ""stop"" arguments.
",2.0,False,2.0,0,0.0,0,0,False
"XD-1255","Story","","2014-01-20T09:00:31.000+0000","mark.pollack","","Create assertion to get count of messages processed by a specific module in a stream","The modules are exposed via JMX and in turn exposed over http via jolokia.  See https://jira.springsource.org/browse/XD-343.  This issue is to develop a helper method that given a stream id and/or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g.

int originalCount = getCount(""testStream"", ""file"");

//do stuff that generates 100 messages
assertCount(""testStream"", ""file"", 100, originalCount)

For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.",5.0,0.918446574243052,0.5100064261881666,-1.0,True,False,True,1390208431000.0,1398063449000.0,1398067049000.0,1398760035000.0,1398760935000.0,1390208431000.0,-1.0,-1.0,-1.0,1394570263000.0,1394573863000.0,1398063449000.0,1398067049000.0,"mark.pollack",-1.0,-1.0,0,1390208431000.0,-1.0,"Create assertion to get count of messages processed by a specific module in a stream","The modules are exposed via JMX and in turn exposed over http via jolokia.  See https://jira.springsource.org/browse/XD-343.  This issue is to develop a helper method that given a stream id and/or module name, assert that the number of messages processed after sending stimulus messages is as expected. e.g.

int originalCount = getCount(""testStream"", ""file"");

//do stuff that generates 100 messages
assertCount(""testStream"", ""file"", 100, originalCount)

For now we can assume we know the location of where the modules are located by assuming we have only one container deployed.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1254","Improvement","Sprint 21","2014-01-20T08:09:56.000+0000","dturanski","dturanski","Optimize AbstractSingleNodeStreamDeploymentIntegrationTests","XD singlenode currenly initialized in @Before, should be @BeforeClass. In this case must be re-initialized for each transport, but not for each @Test.",2.0,0.0941818684355997,0.0045605306799336,0.0042841348811498,True,True,True,1390205396000.0,1390206759000.0,1390210359000.0,1390218968000.0,1390219868000.0,1390205396000.0,-1.0,1390205458000.0,1390209058000.0,1390205462000.0,1390209062000.0,1390206759000.0,1390210359000.0,"dturanski",1390209058000.0,1390205396000.0,44,1390205396000.0,-1.0,"Optimize AbstractSingleNodeStreamDeploymentIntegrationTests","XD singlenode currenly initialized in @Before, should be @BeforeClass. In this case must be re-initialized for each transport, but not for each @Test.",2.0,False,2.0,0,0.0,0,0,False
"XD-1252","Story","","2014-01-17T04:43:43.000+0000","dturanski","","Allow processor script variables to be passed as module parameters","Currently, if we want to bind values to script variables we need to put them in a properties file like so:

xd:> stream create --name groovyprocessortest --definition ""http --port=9006 | script --location=custom-processor.groovy --properties-location=custom-processor.properties | log

Ideally it should be:

xd:> stream create --name groovyprocessortest --definition ""http --port=9006 | script --location=custom-processor.groovy --foo=bar --baz=boo | log


",5.0,-1.0,-1.0,-1.0,False,False,False,1389933823000.0,-1.0,-1.0,1436451736000.0,1436452636000.0,1389933823000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"dturanski",-1.0,-1.0,0,1389933823000.0,-1.0,"Allow processor script variables to be passed as module parameters","Currently, if we want to bind values to script variables we need to put them in a properties file like so:

xd:> stream create --name groovyprocessortest --definition ""http --port=9006 | script --location=custom-processor.groovy --properties-location=custom-processor.properties | log

Ideally it should be:

xd:> stream create --name groovyprocessortest --definition ""http --port=9006 | script --location=custom-processor.groovy --foo=bar --baz=boo | log


",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1251","Story","Sprint 21","2014-01-16T14:46:10.000+0000","hillert","luke","Access-Control-Allow-Origin header should not be hard-coded","See also XD-451 as reference.",4.0,0.7102415737522954,0.7077961354269869,0.7217728560681774,True,True,True,1389883570000.0,1390538792000.0,1390542392000.0,1390805204000.0,1390806104000.0,1389883570000.0,-1.0,1390549430000.0,1390553030000.0,1390536536000.0,1390540136000.0,1390538792000.0,1390542392000.0,"hillert",1390553030000.0,1389883570000.0,44,1389883570000.0,-1.0,"Access-Control-Allow-Origin header should not be hard-coded","See also XD-451 as reference.",4.0,False,4.0,0,0.0,0,0,False
"XD-1250","Improvement","Sprint 21","2014-01-16T11:03:16.000+0000","dturanski","","Investigate RabbitSingleNodeStreamDeploymentIntegrationTests performance","This test is very slow (2x the Redis version). Lots of stacktraces when running. Could be related. ",5.0,0.908343347507241,0.9259654048807544,0.0002830929931595,True,True,True,1389870196000.0,1390813536000.0,1390817136000.0,1390907824000.0,1390908724000.0,1389870196000.0,-1.0,1389870490000.0,1389874090000.0,1390831837000.0,1390835437000.0,1390813536000.0,1390817136000.0,"dturanski",1389874090000.0,1389870196000.0,44,1389870196000.0,-1.0,"Investigate RabbitSingleNodeStreamDeploymentIntegrationTests performance","This test is very slow (2x the Redis version). Lots of stacktraces when running. Could be related. ",5.0,False,5.0,0,0.0,0,0,False
"XD-1249","Improvement","Sprint 21","2014-01-16T11:00:40.000+0000","dturanski","dturanski","AbstractShellIntegrationTests should start and stop server once.","e.g., make application static and check for initialization. Need to ensure each test restores the initial state of the server",5.0,0.0104387233574973,0.010417935003727,0.0013156058171753,True,True,True,1389870040000.0,1389873555000.0,1389877155000.0,1390205867000.0,1390206767000.0,1389870040000.0,-1.0,1389870483000.0,1389874083000.0,1389873548000.0,1389877148000.0,1389873555000.0,1389877155000.0,"dturanski",1389874083000.0,1389870040000.0,44,1389870040000.0,-1.0,"AbstractShellIntegrationTests should start and stop server once.","e.g., make application static and check for initialization. Need to ensure each test restores the initial state of the server",5.0,False,5.0,0,0.0,-1,0,False
"XD-1248","Improvement","Sprint 21","2014-01-16T10:58:16.000+0000","dturanski","dturanski","Merge AbstractStreamDeploymentIntegrationTests and AbstractSingleNodeStreamDeploymentIntegrationTests","We do not need two base classes for this",2.0,0.3586649271580778,0.0,0.0629484670580561,True,True,True,1389869896000.0,1389873195000.0,1389876795000.0,1389878194000.0,1389879094000.0,1389869896000.0,-1.0,1389870475000.0,1389874075000.0,1389869896000.0,1389873496000.0,1389873195000.0,1389876795000.0,"dturanski",1389874075000.0,1389869896000.0,44,1389869896000.0,-1.0,"Merge AbstractStreamDeploymentIntegrationTests and AbstractSingleNodeStreamDeploymentIntegrationTests","We do not need two base classes for this",2.0,False,2.0,0,0.0,0,0,False
"XD-1247","Improvement","Sprint 21","2014-01-16T10:56:06.000+0000","dturanski","dturanski","Remove RedisStreamDeploymentIntegrationTests and RabbitStreamDeploymentIntegrationTests","These are duplicates of *SingleNodeDeploymentIntegrationTests",1.0,0.3673031538296503,0.0,0.07519845526711,True,True,True,1389869766000.0,1389873190000.0,1389876790000.0,1389878188000.0,1389879088000.0,1389869766000.0,-1.0,1389870467000.0,1389874067000.0,1389869766000.0,1389873366000.0,1389873190000.0,1389876790000.0,"dturanski",1389874067000.0,1389869766000.0,44,1389869766000.0,-1.0,"Remove RedisStreamDeploymentIntegrationTests and RabbitStreamDeploymentIntegrationTests","These are duplicates of *SingleNodeDeploymentIntegrationTests",1.0,False,1.0,0,0.0,0,0,False
"XD-1246","Improvement","Sprint 21","2014-01-15T13:39:19.000+0000","iperumal","iperumal","To be able to run the tests without conflicting with an existing XD admin server/launcher","In line with what we address at https://jira.springsource.org/browse/XD-1223, there are cases where tests fail when there is an existing instance of hsqldb running.

Since hsqldb uses the same port and database, it causes issues.",3.0,0.1464982302821359,0.0,0.1464831972847066,True,True,True,1389793159000.0,1389880865000.0,1389884465000.0,1390390942000.0,1390391842000.0,1389793159000.0,-1.0,1389880856000.0,1389884456000.0,1389793159000.0,1389796759000.0,1389880865000.0,1389884465000.0,"iperumal",1389884456000.0,1389793159000.0,44,1389793159000.0,-1.0,"To be able to run the tests without conflicting with an existing XD admin server/launcher","In line with what we address at https://jira.springsource.org/browse/XD-1223, there are cases where tests fail when there is an existing instance of hsqldb running.

Since hsqldb uses the same port and database, it causes issues.",3.0,False,3.0,0,0.0,0,1,False
"XD-1245","Story","Sprint 21","2014-01-15T07:04:56.000+0000","grenfro","Glenn renfro","Need test scripts to exercise XD-EC2 deployment","Create a round trip test script that will push data to the XD EC2 cluster and get results back that can be checked.  ",5.0,0.2607647360667406,0.3459573270653667,0.0472153344478837,True,True,True,1389769496000.0,1390557175000.0,1390560775000.0,1392789246000.0,1392790146000.0,1389769496000.0,-1.0,1389912117000.0,1389915717000.0,1390814512000.0,1390818112000.0,1390557175000.0,1390560775000.0,"grenfro",1389915717000.0,1389769496000.0,44,1389769496000.0,-1.0,"Develop basic acceptance test application to exercise based XD-EC2 deployment from CI","Create a first pass at an acceptance test app for a stream definition of http | log.  

This will involve creating two new projects in xd

1. spring-xd-integration-test
2. spring-xd-acceptance-tests

#1 will contain generally useful utility methods for acceptance test, such as sending data over http, obtaining and asserting JMX values of specific modules.
#2 will contain tests that use #1 to test the various out of the box modules provides in XD.",5.0,False,5.0,0,0.0,0,0,True
"XD-1244","Story","Sprint 21","2014-01-15T06:49:16.000+0000","grenfro","grenfro","Create CI Plan for XD EC2 deployment","* Use the cleanup app from XD-1243 to stop previous CI runs of XD on EC2.
* Build XD-EC2 deployer from github.
* Use XD-EC2 Deployer to deploy the CI XD-Instance
* Should produce artifact that contains the URL 
   * admin server of the XD cluster.
   * container servers of the XD cluster",8.0,0.4543629317983856,7.547933149465682e-06,0.1083551091204695,True,True,True,1389768556000.0,1390370526000.0,1390374126000.0,1391092522000.0,1391093422000.0,1389768556000.0,-1.0,1389912112000.0,1389915712000.0,1389768566000.0,1389772166000.0,1390370526000.0,1390374126000.0,"grenfro",1389915712000.0,1389768556000.0,44,1389768556000.0,-1.0,"Create CI Plan for XD EC2 deployment","* Use the cleanup app from XD-1243 to stop previous CI runs of XD on EC2.
* Build XD-EC2 deployer from github.
* Use XD-EC2 Deployer to deploy the CI XD-Instance
* Should produce artifact that contains the URL 
   * admin server of the XD cluster.
   * container servers of the XD cluster",8.0,False,8.0,0,0.0,0,1,False
"XD-1243","Story","Sprint 21","2014-01-15T06:37:59.000+0000","grenfro","grenfro","Create App for CI to Shutdown XD EC2 Cluster","Application will shutdown all servers with a specific name.  
Application will take a --cluster-name parameter.   
Generate artifact to state success or failure  ",5.0,1.9250482465216785e-05,0.0,0.2313747571631847,True,True,True,1389767879000.0,1389767891000.0,1389771491000.0,1390390340000.0,1390391240000.0,1389767879000.0,-1.0,1389912109000.0,1389915709000.0,1389767879000.0,1389771479000.0,1389767891000.0,1389771491000.0,"grenfro",1389915709000.0,1389767879000.0,44,1389767879000.0,-1.0,"Create App for CI to Shutdown XD EC2 Cluster","Application will shutdown all servers with a specific name.  
Application will take a --cluster-name parameter.   
Generate artifact to state success or failure  ",5.0,False,5.0,0,0.0,-1,1,False
"XD-1242","Story","Sprint 21","2014-01-14T23:02:51.000+0000","mark.pollack","eric.bottard","Optimize deployment of xd-admin and multiple nodes in spring-xd-ec2 to occur in parallel.","The deployment of nodes is sequential, we can reduce the time to deploy a cluster greatly by having these tasks execute in parallel.",6.0,0.6428419114083551,0.0,0.0001098510548932,True,True,True,1389740571000.0,1389840054000.0,1389843654000.0,1389894426000.0,1389895326000.0,1389740571000.0,-1.0,1389740588000.0,1389744188000.0,1389740571000.0,1389744171000.0,1389840054000.0,1389843654000.0,"mark.pollack",1389744188000.0,1389740571000.0,44,1389740571000.0,-1.0,"Optimize deployment of xd-admin and multiple nodes in spring-xd-ec2 to occur in parallel.","The deployment of nodes is sequential, we can reduce the time to deploy a cluster greatly by having these tasks execute in parallel.",6.0,False,6.0,0,0.0,0,0,False
"XD-1241","Story","Sprint 21","2014-01-14T22:59:58.000+0000","mark.pollack","","Add to Acceptance Test EC2 job a stage that uses XD distributed mode with redis","See https://quickstart.atlassian.com/download/bamboo/get-started/bamboo-elements

""Stages are comprised of one or more Jobs, which run in parallel""

we would like the tests across the rabbit and redis transport to occur in parallel.",8.0,-1.0,-1.0,4.419440761577143e-05,False,True,False,1389740398000.0,-1.0,-1.0,1393925548000.0,1393926448000.0,1389740398000.0,-1.0,1389740583000.0,1389744183000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1389744183000.0,1389740398000.0,44,1389740398000.0,-1.0,"Add to Acceptance Test EC2 job a stage that uses XD distributed mode with redis","See https://quickstart.atlassian.com/download/bamboo/get-started/bamboo-elements

""Stages are comprised of one or more Jobs, which run in parallel""

we would like the tests across the rabbit and redis transport to occur in parallel.",8.0,False,8.0,0,0.0,-1,0,False
"XD-1240","Story","Sprint 21","2014-01-14T22:58:40.000+0000","mark.pollack","","Add to Acceptance Test EC2 job a stage that uses XD distributed mode with rabbit","See https://quickstart.atlassian.com/download/bamboo/get-started/bamboo-elements

""Stages are comprised of one or more Jobs, which run in parallel""

we would like the tests across the rabbit and redis transport to occur in parallel.
",8.0,-1.0,-1.0,7.405423780153512e-06,False,True,False,1389740320000.0,-1.0,-1.0,1393925541000.0,1393926441000.0,1389740320000.0,-1.0,1389740351000.0,1389743951000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1389743951000.0,1389740320000.0,44,1389740320000.0,-1.0,"Add to Acceptance Test EC2 CI build plan a stage that uses XD distributed mode with rabbit","See https://quickstart.atlassian.com/download/bamboo/get-started/bamboo-elements

""Stages are comprised of one or more Jobs, which run in parallel""

we would like the tests across the rabbit and redis transport to occur in parallel.
",8.0,False,8.0,0,0.0,0,1,True
"XD-1239","Story","Sprint 21","2014-01-14T22:54:19.000+0000","mark.pollack","grenfro","Add stage to Acceptance Test EC2 build plan that runs a single integration test against the single-node deployment","Run  basic_stream_tests

This will require some mods to pass in the location of the xd-admin process data via a build artifact from the EC2 deployment stage.

",2.0,0.9889668615477104,0.0,4.771180680708685e-06,True,True,True,1389740059000.0,1392020131000.0,1392023731000.0,1392044668000.0,1392045568000.0,1389740059000.0,-1.0,1389740070000.0,1389743670000.0,1389740059000.0,1389743659000.0,1392020131000.0,1392023731000.0,"mark.pollack",1389743670000.0,1389740059000.0,44,1389740059000.0,-1.0,"Add stage to Acceptance Test EC2 build plan that runs a basic acceptance test application against the single-node deployment","Run test application developed in XD-1245

",2.0,False,2.0,0,0.0,0,1,True
"XD-1238","Story","Sprint 21","2014-01-14T22:50:35.000+0000","mark.pollack","grenfro","Create new CI build plan that deploys XD cluster to EC2 using XD singlenode server","Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit.

https://build.springsource.org/browse/XD-ATEC2 was created as an empty shell.

The running of across different transports will be handled in a separate story along with adding a stage to run a 'hello world' acceptance test.

",8.0,0.2739477650142043,0.0,2.214340328477849e-05,True,True,True,1389739835000.0,1390370783000.0,1390374383000.0,1392042104000.0,1392043004000.0,1389739835000.0,-1.0,1389739886000.0,1389743486000.0,1389739835000.0,1389743435000.0,1390370783000.0,1390374383000.0,"mark.pollack",1389743486000.0,1389739835000.0,44,1389739835000.0,-1.0,"Add to Acceptance Test EC2 CI build plan a stage that stops any existing CI EC2 deployments","Add a stage to the plan that will stop any instances that the CI process may have started before and relaunch a 2 node install based on rabbit.

https://build.springsource.org/browse/XD-ATEC2 was created as an empty shell.

The running of across different transports will be handled in a separate story along with adding a stage to run a 'hello world' acceptance test.

",8.0,False,8.0,0,0.0,0,2,True
"XD-1237","Story","Sprint 21","2014-01-14T22:39:22.000+0000","mark.pollack","grenfro","Add option to stop all running XD EC2 instances that match a given naming pattern","This functionality should be added as a command line option to the main app in the spring-xd-ec2 project",4.0,-1.0,0.0,2.0176122025186008e-05,False,True,True,1389739162000.0,-1.0,-1.0,1390382588000.0,1390383488000.0,1389739162000.0,-1.0,1389739175000.0,1389742775000.0,1389739162000.0,1389742762000.0,-1.0,-1.0,"mark.pollack",1389742775000.0,1389739162000.0,44,1389739162000.0,-1.0,"Add option to stop all running XD EC2 instances that match a given naming pattern","This functionality should be added as a command line option to the main app in the spring-xd-ec2 project",4.0,False,4.0,0,0.0,0,0,False
"XD-1236","Story","Sprint 21","2014-01-14T22:35:14.000+0000","mark.pollack","thomas.risberg","Create EC2 AMI for single-node install of Hortonworks Data Platform 1.3","",5.0,0.272487830677987,0.0,1.461780421976141e-05,True,True,True,1389738914000.0,1389943963000.0,1389947563000.0,1390490521000.0,1390491421000.0,1389738914000.0,-1.0,1389738925000.0,1389742525000.0,1389738914000.0,1389742514000.0,1389943963000.0,1389947563000.0,"mark.pollack",1389742525000.0,1389738914000.0,44,1389738914000.0,-1.0,"Create EC2 AMI for single-node install of Hortonworks Data Platform 1.3","",5.0,False,5.0,0,0.0,0,0,False
"XD-1235","Story","Sprint 21","2014-01-14T22:34:24.000+0000","mark.pollack","thomas.risberg","Create EC2 AMI for single-node install of Cloudera CDH 4.3.1","",5.0,0.9998897084715852,0.0,9.3016951675036e-06,True,True,True,1389738864000.0,1390491332000.0,1390491415000.0,1390490515000.0,1390491415000.0,1389738864000.0,-1.0,1389738871000.0,1389742471000.0,1389738864000.0,1389742464000.0,1390491332000.0,1390491415000.0,"mark.pollack",1389742471000.0,1389738864000.0,44,1389738864000.0,-1.0,"Create EC2 AMI for single-node install of Cloudera CDH 4.5.0","",5.0,False,5.0,0,0.0,0,0,True
"XD-1234","Story","Sprint 21","2014-01-14T22:33:08.000+0000","mark.pollack","jvalkeal","Create EC2 AMI for single-node install of  Pivotal HD 1.1","",8.0,-1.0,0.0,9.300100972524843e-06,False,True,True,1389738788000.0,-1.0,-1.0,1390490568000.0,1390491468000.0,1389738788000.0,-1.0,1389738795000.0,1389742395000.0,1389738788000.0,1389742388000.0,-1.0,-1.0,"mark.pollack",1389742395000.0,1389738788000.0,44,1389738788000.0,-1.0,"Create EC2 AMI for single-node install of  Pivotal HD 1.1","",8.0,False,8.0,0,0.0,0,1,False
"XD-1233","Story","Sprint 21","2014-01-14T22:31:47.000+0000","mark.pollack","jvalkeal","Create EC2 AMI for single-node install of Apache Hadoop 2.2.0","",3.0,-1.0,0.0,1.5941462948054744e-05,False,True,True,1389738707000.0,-1.0,-1.0,1390490561000.0,1390491461000.0,1389738707000.0,-1.0,1389738719000.0,1389742319000.0,1389738707000.0,1389742307000.0,-1.0,-1.0,"mark.pollack",1389742319000.0,1389738707000.0,44,1389738707000.0,-1.0,"Create EC2 AMI for single-node install of Apache Hadoop 2.2.0","",3.0,False,3.0,0,0.0,0,1,False
"XD-1232","Story","Sprint 21","2014-01-14T22:31:11.000+0000","mark.pollack","jvalkeal","Create EC2 AMI for single-node install of Apache Hadoop 1.2.1","",4.0,-1.0,0.0,1.594099705622921e-05,False,True,True,1389738671000.0,-1.0,-1.0,1390490547000.0,1390491447000.0,1389738671000.0,-1.0,1389738683000.0,1389742283000.0,1389738671000.0,1389742271000.0,-1.0,-1.0,"mark.pollack",1389742283000.0,1389738671000.0,44,1389738671000.0,-1.0,"Create EC2 AMI for single-node install of Apache Hadoop 1.2.1","",4.0,False,4.0,0,0.0,0,1,False
"XD-1231","Story","Sprint 21","2014-01-14T22:29:14.000+0000","mark.pollack","iperumal","Investigate if we should use RequreJS with Angular","",6.0,0.2225180882180664,0.0,3.347087383154109e-05,True,True,True,1389738554000.0,1389977886000.0,1389981486000.0,1390813216000.0,1390814116000.0,1389738554000.0,-1.0,1389738590000.0,1389742190000.0,1389738554000.0,1389742154000.0,1389977886000.0,1389981486000.0,"mark.pollack",1389742190000.0,1389738579000.0,44,1389738579000.0,-1.0,"Investigate if we should use RequreJS with Angular","",6.0,False,6.0,0,0.0,0,0,False
"XD-1230","Story","Sprint 21","2014-01-14T22:28:18.000+0000","mark.pollack","Ilayaperumal Gopinathan","Integrate grunt based UI build into the XD's gradle build","Blog post http://naleid.com/blog/2013/01/24/calling-gruntjs-tasks-from-gradle/  seems to be the definitive reference....",5.0,0.65250398982293,0.6527114212651519,4.890660019866198e-05,True,True,True,1389738498000.0,1390899238000.0,1390902838000.0,1391516499000.0,1391517399000.0,1389738498000.0,-1.0,1389738585000.0,1389742185000.0,1390899607000.0,1390903207000.0,1390899238000.0,1390902838000.0,"mark.pollack",1389742185000.0,1389738498000.0,44,1389738498000.0,-1.0,"Integrate grunt based UI build into the XD's gradle build","Blog post http://naleid.com/blog/2013/01/24/calling-gruntjs-tasks-from-gradle/  seems to be the definitive reference....",5.0,False,5.0,0,0.0,0,0,False
"XD-1229","Story","Sprint 21","2014-01-14T22:25:08.000+0000","mark.pollack","hillert","Create build infrastructure for Angular based UI","Create a project that includes build and test automation for a new Angular based UI.  This work is independent of calling the UI build step from gradle.  A super minimal UI, just to have some basic code, is all that is required. 

This should use 

Grunt
Jasmine
Karma
Bower
YUIdoc (separate story?)

UI Components from backbone should be available in the base project.

",8.0,0.021699478661664,0.0,4.49671880050026e-06,True,True,True,1389738308000.0,1389776913000.0,1389780513000.0,1391516483000.0,1391517383000.0,1389738308000.0,-1.0,1389738316000.0,1389741916000.0,1389738308000.0,1389741908000.0,1389776913000.0,1389780513000.0,"mark.pollack",1389741916000.0,1389738308000.0,44,1389738308000.0,-1.0,"Create build infrastructure for Angular based UI","Create a project that includes build and test automation for a new Angular based UI.  This work is independent of calling the UI build step from gradle.  A super minimal UI, just to have some basic code, is all that is required. 

This should use 

Grunt
Jasmine
Karma
Bower
YUIdoc (separate story?)

UI Components from backbone should be available in the base project.

",8.0,False,8.0,0,0.0,0,0,False
"XD-1228","Story","Sprint 21","2014-01-14T22:07:42.000+0000","mark.pollack","","Provide a easy, prescriptive means to perform unit and basic stream integration tests.","AbstractSingleNodeStreamDeploymentIntegrationTests is the basis of 'state of the art' testing for a stream that allows you to get a reference to the input and output channel of the stream

http | filter | transform | file.

One can send messages to the channel after the http module, but before filter and one can retrieve the messages that were sent to the channel after the transform module but before file.

The current implementation inside AbstractSingleNodeStreamDeploymentIntegrationTests can be improved in terms of ease of use for end-users.  

The issue is to create as simple a way as possible for a user to test their processing modules/stream definitions without having to actually do a real integration test by sending data to the input module.

Either as a separate issue or as part of this one, the documentation 

https://github.com/spring-projects/spring-xd/wiki/Creating-a-Processor-Module

should be updated to explicitly show how to use this issue's test functionality.
",8.0,0.2174719116973355,0.0120877437868526,2.68691165031457e-06,True,True,True,1389737262000.0,1390384762000.0,1390388362000.0,1392713758000.0,1392714658000.0,1389737262000.0,-1.0,1389737270000.0,1389740870000.0,1389773252000.0,1389776852000.0,1390384762000.0,1390388362000.0,"mark.pollack",1389740870000.0,1389737262000.0,44,1389737262000.0,-1.0,"Provide a easy, prescriptive means to perform unit and basic stream integration tests.","AbstractSingleNodeStreamDeploymentIntegrationTests is the basis of 'state of the art' testing for a stream that allows you to get a reference to the input and output channel of the stream

http | filter | transform | file.

One can send messages to the channel after the http module, but before filter and one can retrieve the messages that were sent to the channel after the transform module but before file.

The current implementation inside AbstractSingleNodeStreamDeploymentIntegrationTests can be improved in terms of ease of use for end-users.  

The issue is to create as simple a way as possible for a user to test their processing modules/stream definitions without having to actually do a real integration test by sending data to the input module.

Either as a separate issue or as part of this one, the documentation 

https://github.com/spring-projects/spring-xd/wiki/Creating-a-Processor-Module

should be updated to explicitly show how to use this issue's test functionality.
",8.0,False,8.0,0,0.0,0,1,False
"XD-1227","Story","Sprint 21","2014-01-14T21:59:45.000+0000","mark.pollack","","Investigate long running tests and create refactoring issues","https://sonar.springsource.org/dashboard/index/7173?did=3

Shows which of our current tests take the most time to execute.

xd.dirt.stream and xd.shell.command are where the most time is spent.

In xd.dirt.stream it seems likely that time can be reduced by not restarting a new single node server per test, but sharing it across tests, e.g.

 RabbitSingleNodeStreamDeploymentIntegrationTests 	
 LocalSingleNodeStreamDeploymentIntegrationTests
 RedisSingleNodeStreamDeploymentIntegrationTests 	

As a first pass, the test that take longer than 15 seconds in that report should be investigated.
",6.0,0.0372119301572399,0.0339185105950698,0.0001001941725955,True,True,True,1389736785000.0,1389776896000.0,1389780496000.0,1390813792000.0,1390814692000.0,1389736785000.0,-1.0,1389736893000.0,1389740493000.0,1389773346000.0,1389776946000.0,1389776896000.0,1389780496000.0,"mark.pollack",1389740493000.0,1389736785000.0,44,1389736785000.0,-1.0,"Investigate long running tests and create refactoring issues","https://sonar.springsource.org/dashboard/index/7173?did=3

Shows which of our current tests take the most time to execute.

xd.dirt.stream and xd.shell.command are where the most time is spent.

In xd.dirt.stream it seems likely that time can be reduced by not restarting a new single node server per test, but sharing it across tests, e.g.

 RabbitSingleNodeStreamDeploymentIntegrationTests 	
 LocalSingleNodeStreamDeploymentIntegrationTests
 RedisSingleNodeStreamDeploymentIntegrationTests 	

As a first pass, the test that take longer than 15 seconds in that report should be investigated.
",6.0,False,6.0,0,0.0,-1,0,False
"XD-1225","Story","Sprint 21","2014-01-14T21:48:18.000+0000","mark.pollack","","Integrate code coverage reports into the CI process","Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master...

Open question is if we want to fail a build do to code coverage levels.",4.0,0.7412997050747483,0.7413372552394214,1.5645901947132497e-05,True,True,True,1389736098000.0,1390209896000.0,1390213496000.0,1390374343000.0,1390375243000.0,1389736098000.0,-1.0,1389736108000.0,1389739708000.0,1390209920000.0,1390213520000.0,1390209896000.0,1390213496000.0,"mark.pollack",1389739708000.0,1389736098000.0,44,1389736098000.0,-1.0,"Integrate code coverage reports into the CI process","Not sure if this is best done via Sonar our sonar build plan, the nightly one, or the frequent one off master...

Open question is if we want to fail a build do to code coverage levels.",4.0,False,4.0,0,0.0,0,0,False
"XD-1224","Story","Sprint 21","2014-01-14T21:44:09.000+0000","mark.pollack","","Add code coverage to gradle build","Build should be able to generate code coverage reports.

After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle.

http://www.gradle.org/docs/current/userguide/jacoco_plugin.html
",3.0,0.2041234205574776,0.2041162798170542,1.4281480846749e-05,True,True,True,1389735849000.0,1389850192000.0,1389853792000.0,1390295115000.0,1390296015000.0,1389735849000.0,-1.0,1389735857000.0,1389739457000.0,1389850188000.0,1389853788000.0,1389850192000.0,1389853792000.0,"mark.pollack",1389739457000.0,1389735849000.0,44,1389735849000.0,-1.0,"Add code coverage to gradle build","Build should be able to generate code coverage reports.

After a quick tour of the intertubes it seems that JaCoCo is a well maintained project and has first class support inside gradle.

http://www.gradle.org/docs/current/userguide/jacoco_plugin.html
",3.0,False,3.0,0,0.0,0,0,False
"XD-1223","Story","Sprint 21","2014-01-14T10:45:03.000+0000","iperumal","","Unit tests need appropriate cleanup of test data during teardown","Currently, there are test data(e.g: stream name) not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests.

We need to move the cleanup logic at an appropriate level so that the test data is always cleaned up irrespective of the test result.",5.0,0.0146973280419745,0.0146955464396905,0.0048085445645541,True,True,True,1389696303000.0,1389712802000.0,1389716402000.0,1390817988000.0,1390818888000.0,1389696303000.0,1389976318000.0,1389701701000.0,1389705301000.0,1389712800000.0,1389716400000.0,1389712802000.0,1389716402000.0,"iperumal",1389705301000.0,1389696303000.0,44,1389976318000.0,1389976318000.0,"Unit/Integration tests need appropriate cleanup of test data during teardown","Currently, there are test data(e.g: stream name) not being cleaned up during teardown especially when there is a test case failure. This breaks the test suite when the same test data is used by other tests.

We need to move the cleanup logic at an appropriate level so that the test data is always cleaned up irrespective of the test result.",3.0,True,3.0,1,66.66666666666666,0,0,True
"XD-1222","Story","Sprint 21","2014-01-13T11:12:10.000+0000","iperumal","iperumal","Duplicate MBean server definition by MBeanExportingPlugin","The MBeanServer is referred by XD admin/launcher when JMX is enabled (by setting --jmxEnabled option) and this is defined in xd-global-beans.xml.

The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition (from jmx/common.xml) which the MBeanExportingPlugin adds as a component to the module which doesn't seem to be needed.

Also, currently the flag ""jmxEnabled"" is generic and used by adminserver/launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX *only* for modules then, a separate definition of MBeanServer could be necessary.
",1.0,0.0001328933088219,0.0,0.9985935458149682,True,True,True,1389611530000.0,1389611542000.0,1389615142000.0,1389700928000.0,1389701828000.0,1389611530000.0,-1.0,1389701701000.0,1389701828000.0,1389611530000.0,1389615130000.0,1389611542000.0,1389615142000.0,"iperumal",1389701828000.0,1389611530000.0,44,1389611530000.0,-1.0,"Duplicate MBean server definition by MBeanExportingPlugin","The MBeanServer is referred by XD admin/launcher when JMX is enabled (by setting --jmxEnabled option) and this is defined in xd-global-beans.xml.

The MBeans that are exposed by the modules also use the same MBeanServer define above and there is a duplicate MBeanServer definition (from jmx/common.xml) which the MBeanExportingPlugin adds as a component to the module which doesn't seem to be needed.

Also, currently the flag ""jmxEnabled"" is generic and used by adminserver/launcher as well as the module MBeanExportingPlugin. If we have a separate flag to enable the JMX *only* for modules then, a separate definition of MBeanServer could be necessary.
",1.0,False,1.0,0,0.0,0,0,False
"XD-1221","Story","Sprint 21","2014-01-13T10:51:05.000+0000","iperumal","iperumal","XD modules should not have 'build' directory upon running gradle build","Currently, the 'modules' project is marked as java project to enable eclipse/idea metadata files generation. But it generates a 'build' directory with a jar that has empty MANIFEST file.

This build directory also gets copied into the bundle after 'dist'.",2.0,0.0001311962915181,0.9998469376598956,0.9996720092712046,True,True,True,1389610265000.0,1389610277000.0,1389613877000.0,1389700831000.0,1389701731000.0,1389610265000.0,-1.0,1389701701000.0,1389701731000.0,1389701717000.0,1389701731000.0,1389610277000.0,1389613877000.0,"iperumal",1389701731000.0,1389610265000.0,44,1389610265000.0,-1.0,"XD modules should not have 'build' directory upon running gradle build","Currently, the 'modules' project is marked as java project to enable eclipse/idea metadata files generation. But it generates a 'build' directory with a jar that has empty MANIFEST file.

This build directory also gets copied into the bundle after 'dist'.",2.0,False,2.0,0,0.0,0,0,False
"XD-1220","Story","Sprint 21","2014-01-10T12:30:46.000+0000","thomas.risberg","thomas.risberg","Batch jobs should use application.yml provided connection as default","Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch-jdbc.properties. This config needs to account for any changes made to application.yml settings so the data is written to the batch metadata database by default.",5.0,0.8470884358461402,0.8470691628988055,0.8469843619305326,True,True,True,1389357046000.0,1389796568000.0,1389800168000.0,1389875008000.0,1389875908000.0,1389357046000.0,-1.0,1389796514000.0,1389800114000.0,1389796558000.0,1389800158000.0,1389796568000.0,1389800168000.0,"thomas.risberg",1389800114000.0,1389357046000.0,44,1389357046000.0,-1.0,"Batch jobs should use application.yml provided connection as default","Batch jobs should use application.yml provided connection as default. They now have their own configuration in batch-jdbc.properties. This config needs to account for any changes made to application.yml settings so the data is written to the batch metadata database by default.",5.0,False,5.0,0,0.0,0,0,False
"XD-1219","Story","","2014-01-10T12:24:38.000+0000","mark.pollack","mark.pollack","Release 1.0.0.M5","",8.0,-1.0,0.0,-1.0,False,False,True,1389356678000.0,-1.0,-1.0,1389360320000.0,1389361220000.0,1389356678000.0,-1.0,-1.0,-1.0,1389356678000.0,1389360278000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1389356678000.0,-1.0,"Release 1.0.0.M5","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1218","Improvement","","2014-01-10T11:45:36.000+0000","mark.pollack","david_syer","Base server implementation on Spring Boot","Server startup is done using Spring  Boot as well as starting a module application context at runtime.",20.0,-1.0,0.0,-1.0,False,False,False,1389354336000.0,-1.0,-1.0,1389354336000.0,1389354363000.0,1389354336000.0,-1.0,-1.0,-1.0,1389354336000.0,1389354363000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1389354336000.0,-1.0,"Base server implementation on Spring Boot","Server startup is done using Spring  Boot as well as starting a module application context at runtime.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1217","Story","","2014-01-10T07:14:47.000+0000","dturanski","","twittersearch and twitterstream should support compatible formats","Currently twitterstream emits native twitter json whereas twittersearch uses SI/Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions.  This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE: I think it's a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.",5.0,-1.0,0.9595682962864772,-1.0,False,False,True,1389338087000.0,-1.0,-1.0,1401226545000.0,1401227445000.0,1389338087000.0,-1.0,-1.0,-1.0,1400746738000.0,1400750338000.0,-1.0,-1.0,"dturanski",-1.0,-1.0,0,1389338087000.0,-1.0,"twittersearch and twitterstream should support compatible formats","Currently twitterstream emits native twitter json whereas twittersearch uses SI/Spring Social and emits spring social Tweet types. This makes it difficult to replace twitter sources and reuse XD stream definitions.  This requires coordination with SS 1.1.0 and SI 4.0 GA releases. NOTE: I think it's a good idea to continue to support native twitter JSON, keep as an option for twitterstream, but the default should be Tweet types.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1216","Bug","Sprint 20","2014-01-09T13:51:41.000+0000","hillert","hillert","Serialization of ChunkContext fails using Kryo","",2.0,0.000652599521427,0.0,0.0149010224059169,True,True,True,1389275501000.0,1389275507000.0,1389279107000.0,1389283795000.0,1389284695000.0,1389275595000.0,-1.0,1389275638000.0,1389279238000.0,1389275501000.0,1389279101000.0,1389275507000.0,1389279107000.0,"hillert",1389279238000.0,1389275595000.0,44,1389275595000.0,-1.0,"Serialization of ChunkContext fails using Kryo","",2.0,False,2.0,0,0.0,0,0,False
"XD-1215","Story","Sprint 21","2014-01-09T12:20:40.000+0000","iperumal","iperumal","Distributed JobLocator should return a valid job","In distributed batch processing in XD, the JobLocator implementation for getJob(String jobName) should return a valid Job (FlowJob/SimpleJob).

Since we won't we relying on the MapJobRegistry's joblocator implementation which doesn't work in distributed use case, we need to have an appropriate way to return FlowJob/SimpleJob using XD's BatchJobLocator.",5.0,0.6514748309034244,0.5095489425519627,0.6514720740813235,True,True,True,1389270040000.0,1391633177000.0,1391636777000.0,1392896505000.0,1392897405000.0,1389270040000.0,-1.0,1391633167000.0,1391636767000.0,1391118360000.0,1391121960000.0,1391633177000.0,1391636777000.0,"iperumal",1391636767000.0,1389270040000.0,44,1389270040000.0,-1.0,"Distributed JobLocator should return a valid job","In distributed batch processing in XD, the JobLocator implementation for getJob(String jobName) should return a valid Job (FlowJob/SimpleJob).

Since we won't we relying on the MapJobRegistry's joblocator implementation which doesn't work in distributed use case, we need to have an appropriate way to return FlowJob/SimpleJob using XD's BatchJobLocator.",5.0,False,5.0,0,0.0,0,0,False
"XD-1214","Story","Sprint 21","2014-01-09T09:39:51.000+0000","iperumal","","Make job names unique across tests that use the same JobRepository","The job names used by the tests should be unique across the tests when use the same JobRepository.

",3.0,0.999990333021372,0.9917093574540562,0.2879067909920676,True,True,True,1389260391000.0,1390915494000.0,1390915510000.0,1390914610000.0,1390915510000.0,1389260391000.0,-1.0,1389736911000.0,1389740511000.0,1390901788000.0,1390905388000.0,1390915494000.0,1390915510000.0,"iperumal",1389740511000.0,1389260391000.0,44,1389260391000.0,-1.0,"Make job names unique across tests that use the same JobRepository","The job names used by the tests should be unique across the tests when use the same JobRepository.

",3.0,False,3.0,0,0.0,0,0,False
"XD-1213","Story","Sprint 21","2014-01-09T05:30:20.000+0000","eric.bottard","eric.bottard","Allow conditional validation for module options","Using JSR303 groups, which should be derived from injected values",4.0,0.956898416819385,0.0,0.9568872811036448,True,True,True,1389245420000.0,1389846934000.0,1389850534000.0,1389873128000.0,1389874028000.0,1389245420000.0,-1.0,1389846927000.0,1389850527000.0,1389245420000.0,1389249020000.0,1389846934000.0,1389850534000.0,"eric.bottard",1389850527000.0,1389245420000.0,44,1389245420000.0,-1.0,"Allow conditional validation for module options","Using JSR303 groups, which should be derived from injected values",4.0,False,4.0,0,0.0,0,0,False
"XD-1212","Story","Sprint 21","2014-01-09T03:53:41.000+0000","luke","luke","Boot updates post 0.5 M7","Some boot classes we compile against have changed or been replaced.",3.0,-1.0,1.7355672397798955e-05,0.999896733749233,False,True,True,1389239621000.0,-1.0,-1.0,1390391082000.0,1390391982000.0,1389239621000.0,-1.0,1390391863000.0,1390391982000.0,1389239641000.0,1389243241000.0,-1.0,-1.0,"luke",1390391982000.0,1389239621000.0,44,1389239621000.0,-1.0,"Boot updates post 0.5 M7","Some boot classes we compile against have changed or been replaced.",3.0,False,3.0,0,0.0,0,0,False
"XD-1211","Story","","2014-01-08T15:25:57.000+0000","grenfro","","Update hadoop instructions in the xd-samples","batchHashtagCount and batchWordCount  projects need ""hadoop fs ls"" instructions need to be updated.  ",1.0,0.0188841201716738,0.0171673819742489,-1.0,True,False,True,1389194757000.0,1389194779000.0,1389195922000.0,1389195022000.0,1389195922000.0,1389194757000.0,-1.0,-1.0,-1.0,1389194777000.0,1389195922000.0,1389194779000.0,1389195922000.0,"grenfro",-1.0,-1.0,0,1389194757000.0,-1.0,"Update hadoop instructions in the xd-samples","batchHashtagCount and batchWordCount  projects need ""hadoop fs ls"" instructions need to be updated.  ",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1210","Bug","Sprint 20","2014-01-08T12:59:06.000+0000","hillert","hillert","Shell - 'makeUnique' Job Parameter is true by default ","Currently the shell assumes that the 'makeUnique'job parameter is by default *false*. That is not true. As a consequence the parameter has currently no impact/does not work.",2.0,0.9792163205084552,0.0,0.9796561116785836,True,True,True,1389185946000.0,1389254969000.0,1389256434000.0,1389255534000.0,1389256434000.0,1389185946000.0,-1.0,1389255000000.0,1389256434000.0,1389185946000.0,1389189546000.0,1389254969000.0,1389256434000.0,"hillert",1389256434000.0,1389185946000.0,44,1389185946000.0,-1.0,"Shell - 'makeUnique' Job Parameter is true by default ","Currently the shell assumes that the 'makeUnique'job parameter is by default *false*. That is not true. As a consequence the parameter has currently no impact/does not work.",2.0,False,2.0,0,0.0,0,0,False
"XD-1209","Story","Sprint 22","2014-01-08T11:54:44.000+0000","mark.pollack","iperumal","Add support for XD_CONFIG environment variable in windows shell scripts","This was added in bash scripts as part of XD-1186.",4.0,0.871706958122433,0.8717024913073428,0.8717046071671224,True,True,True,1389182084000.0,1392889968000.0,1392893568000.0,1393434774000.0,1393435674000.0,1389182084000.0,1392979416000.0,1392889958000.0,1392893558000.0,1392889949000.0,1392893549000.0,1392889968000.0,1392893568000.0,"mark.pollack",1392893558000.0,1389182084000.0,44,1392979416000.0,1392979416000.0,"Add support for XD_CONFIG environment variable in windows shell scripts","This was added in bash scripts as part of XD-1186.",2.0,True,2.0,1,100.0,0,0,False
"XD-1208","Story","Sprint 24","2014-01-08T07:45:37.000+0000","luke","Eric Bottard","Create Module options metadata for OOTB jobs","",5.0,0.8968757628187073,0.8951889107954774,0.8099676766051047,True,True,True,1389167137000.0,1395854153000.0,1395857753000.0,1396622137000.0,1396623037000.0,1389167137000.0,-1.0,1395206175000.0,1395209775000.0,1395841576000.0,1395845176000.0,1395854153000.0,1395857753000.0,"luke",1395209775000.0,1389167137000.0,44,1389167137000.0,-1.0,"Create Module options metadata for OOTB jobs","",5.0,False,5.0,0,0.0,0,0,False
"XD-1207","Bug","Sprint 20","2014-01-08T07:27:40.000+0000","hillert","hillert","Job Plugin - Notification Channel not correctly bound to MessageBus","",4.0,0.6122441612821716,0.0,0.6153846153846154,True,True,True,1389166060000.0,1389183021000.0,1389186621000.0,1389192863000.0,1389193763000.0,1389166060000.0,-1.0,1389183108000.0,1389186708000.0,1389166060000.0,1389169660000.0,1389183021000.0,1389186621000.0,"hillert",1389186708000.0,1389166060000.0,44,1389166060000.0,-1.0,"Job Plugin - Notification Channel not correctly bound to MessageBus","",4.0,False,4.0,0,0.0,-1,1,False
"XD-1206","Story","","2014-01-07T15:28:42.000+0000","grussell","hillert","Garbage in Job Repo Causes List Failure","For some reason, the partitioned batch jobs are storing  {{StepExecutions}}s in the DB with a null ID.

This causes {{JobCommandTests.testListStepExecutionsForSpecificJobExecution()|| to fail because the HATEOS code asserts not null...

{code}
18:25:26,185 ERROR http-nio-9393-exec-3 rest.RestControllerAdvice:186 - Caught exception while handling a request
java.lang.IllegalArgumentException: [Assertion failed] - this argument is required; it must not be null
	at org.springframework.util.Assert.notNull(Assert.java:112)
	at org.springframework.util.Assert.notNull(Assert.java:123)
	at org.springframework.hateoas.mvc.ResourceAssemblerSupport.createResourceWithId(ResourceAssemblerSupport.java:87)
	at org.springframework.xd.dirt.rest.StepExecutionInfoResourceAssembler.toResource(StepExecutionInfoResourceAssembler.java:39)
	at org.springframework.xd.dirt.rest.BatchStepExecutionsController.list(BatchStepExecutionsController.java:107)
{code}

For some reason, the query for objects with the {{jobExecutionId}} also returned these two objects with null keys.

Blowing away my data directory fixes the problem (until I run another partitioned batch job).

I need to figure out why spring batch is creating these bad records, but we should probably add some defensive code to protect against null IDs.

You can reproduce by building against my XD-1146 branch.
",1.0,-1.0,0.0,-1.0,False,False,True,1389108522000.0,-1.0,-1.0,1392117614000.0,1392118514000.0,1389108522000.0,-1.0,-1.0,-1.0,1389108522000.0,1389112122000.0,-1.0,-1.0,"grussell",-1.0,-1.0,0,1389108522000.0,-1.0,"Garbage in Job Repo Causes List Failure","For some reason, the partitioned batch jobs are storing  {{StepExecutions}}s in the DB with a null ID.

This causes {{JobCommandTests.testListStepExecutionsForSpecificJobExecution()}} to fail because the HATEOS code asserts not null...

{code}
18:25:26,185 ERROR http-nio-9393-exec-3 rest.RestControllerAdvice:186 - Caught exception while handling a request
java.lang.IllegalArgumentException: [Assertion failed] - this argument is required; it must not be null
	at org.springframework.util.Assert.notNull(Assert.java:112)
	at org.springframework.util.Assert.notNull(Assert.java:123)
	at org.springframework.hateoas.mvc.ResourceAssemblerSupport.createResourceWithId(ResourceAssemblerSupport.java:87)
	at org.springframework.xd.dirt.rest.StepExecutionInfoResourceAssembler.toResource(StepExecutionInfoResourceAssembler.java:39)
	at org.springframework.xd.dirt.rest.BatchStepExecutionsController.list(BatchStepExecutionsController.java:107)
{code}

For some reason, the query for objects with the {{jobExecutionId}} also returned these two objects with null keys.

Blowing away my data directory fixes the problem (until I run another partitioned batch job).

I need to figure out why spring batch is creating these bad records, but we should probably add some defensive code to protect against null IDs.

You can reproduce by building against my XD-1146 branch.
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1205","Story","Sprint 20","2014-01-07T11:24:22.000+0000","thomas.risberg","thomas.risberg","Add test for filejdbc to test scripts","The filejdbc jobs isn't included in the test scripts",3.0,0.0207376555050145,0.0208034197402312,0.0206609305639283,True,True,True,1389093862000.0,1389095754000.0,1389099354000.0,1389184197000.0,1389185097000.0,1389093862000.0,-1.0,1389095747000.0,1389099347000.0,1389095760000.0,1389099360000.0,1389095754000.0,1389099354000.0,"thomas.risberg",1389099347000.0,1389093862000.0,44,1389093862000.0,-1.0,"Add test for filejdbc to test scripts","The filejdbc jobs isn't included in the test scripts",3.0,False,3.0,0,0.0,-1,0,False
"XD-1204","Story","Sprint 20","2014-01-07T07:37:06.000+0000","thomas.risberg","thomas.risberg","Update jdbchdfs properties and defaults to better match hdfs sink  ","We should use fileName, fileExtension properties and default to /xd/jobname as directory",3.0,0.0235010151532314,0.0,0.0001655614712315,True,True,True,1389080226000.0,1389082923000.0,1389086523000.0,1389194087000.0,1389194987000.0,1389080226000.0,-1.0,1389080245000.0,1389083845000.0,1389080226000.0,1389083826000.0,1389082923000.0,1389086523000.0,"thomas.risberg",1389083845000.0,1389080226000.0,44,1389080226000.0,-1.0,"Update jdbchdfs properties and defaults to better match hdfs sink  ","We should use fileName, fileExtension properties and default to /xd/jobname as directory",3.0,False,3.0,0,0.0,-1,0,False
"XD-1203","Bug","","2014-01-06T16:02:53.000+0000","hillert","hillert","JobPlugin - Use containsKey when checking for Parameters","Use containsKey when checking for Parameters
Add tests",2.0,0.2068852909834656,0.0,-1.0,True,False,True,1389024173000.0,1389036210000.0,1389039810000.0,1389081455000.0,1389082355000.0,1389024173000.0,-1.0,-1.0,-1.0,1389024173000.0,1389027773000.0,1389036210000.0,1389039810000.0,"hillert",-1.0,-1.0,0,1389024173000.0,-1.0,"JobPlugin - Use containsKey when checking for Parameters","Use containsKey when checking for Parameters
Add tests",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1202","Story","Sprint 20","2014-01-06T14:33:22.000+0000","thomas.risberg","thomas.risberg","Update build script to use correct version of spring-data-hadoop based on distro","",1.0,0.4973717573268288,0.0,0.497282790014902,True,True,True,1389018802000.0,1389085888000.0,1389089488000.0,1389152783000.0,1389153683000.0,1389018802000.0,-1.0,1389085876000.0,1389089476000.0,1389018802000.0,1389022402000.0,1389085888000.0,1389089488000.0,"thomas.risberg",1389089476000.0,1389018802000.0,44,1389018802000.0,-1.0,"Update build script to use correct version of spring-data-hadoop based on distro","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1201","Story","Sprint 42","2014-01-06T12:53:54.000+0000","mark.pollack","","Create custom help command for xd-shell so that hadoopDistro option is listed","xd-shell is using the default spring-shell help command.  Need to create a help command specific to XD so that it can list the --hadoopDistro command line option.  Note, the hadoopDistro command line option is actually processed by the xd-shell bash script",2.0,-1.0,0.562671623043013,0.5618158955169527,False,True,True,1389012834000.0,-1.0,-1.0,1448378962000.0,1448379862000.0,1389012834000.0,1422521229000.0,1422366174000.0,1422369774000.0,1422416976000.0,1422420576000.0,-1.0,-1.0,"mark.pollack",1422369774000.0,1389012834000.0,44,1422521229000.0,1422521229000.0,"Create custom help command for xd-shell so that hadoopDistro option is listed","xd-shell is using the default spring-shell help command.  Need to create a help command specific to XD so that it can list the --hadoopDistro command line option.  Note, the hadoopDistro command line option is actually processed by the xd-shell bash script",1.0,True,1.0,1,100.0,0,0,False
"XD-1200","Story","Sprint 20","2014-01-06T10:15:03.000+0000","thomas.risberg","","Fix hdfsjdbc batch job","hdfsjdbc throws an exception:
{code}
org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'itemReader' defined in URL [file:/Users/trisberg/Projects/spring-xd/build/dist/spring-xd/xd/modules/job/hdfsjdbc/config/hdfsjdbc.xml]: Could not resolve placeholder 'columns' in string value ""${columns}""
{code}

The hdfsjdbc job uses 'columns' instead of 'names' as the parameter for the column-names. Should we make this usage consistent between jobs?
Also, to avoid the above exception we should update the docs.

There is a comment in the docs - ""there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future."" Think this is this solved in Spring Hadoop now?
",5.0,0.0610326012652721,0.0610698882632959,0.0010253924456542,True,True,True,1389003303000.0,1389013124000.0,1389016724000.0,1389163317000.0,1389164217000.0,1389003303000.0,-1.0,1389003468000.0,1389007068000.0,1389013130000.0,1389016730000.0,1389013124000.0,1389016724000.0,"thomas.risberg",1389007068000.0,1389003303000.0,44,1389003303000.0,-1.0,"Fix hdfsjdbc batch job","hdfsjdbc throws an exception:
{code}
org.springframework.beans.factory.BeanDefinitionStoreException: Invalid bean definition with name 'itemReader' defined in URL [file:/Users/trisberg/Projects/spring-xd/build/dist/spring-xd/xd/modules/job/hdfsjdbc/config/hdfsjdbc.xml]: Could not resolve placeholder 'columns' in string value ""${columns}""
{code}

The hdfsjdbc job uses 'columns' instead of 'names' as the parameter for the column-names. Should we make this usage consistent between jobs?

There is a comment in the docs - ""there is also a limitation in that the database table must be created manually. This is due to a bug in Spring Hadoop and will be fixed in the future."" Think this is this solved in Spring Hadoop now?

initializeDatabase should default to false now to be consistent with jdbc sink

Rename batch-jdbc/mongo-import.properties to batch-jdbc/mongo.properties since these aren't just for import",5.0,False,5.0,0,0.0,-1,0,True
"XD-1199","Story","Sprint 20","2014-01-06T10:13:47.000+0000","thomas.risberg","","Fix filejdbc batch job","filejdbc throws an exception: 
{code}
java.lang.IllegalArgumentException: Could not resolve resource location pattern [/mycsvdir/*.csv]: class path resource [mycsvdir/] cannot be resolved to URL because it does not exist
{code}

This can be solved by using a file:// prefix

filejdbc has the resource parameter while the filehdfs creates a stream pushing the files - filejdbc should accept resources as a job parameter.

filejdbc needs a more descriptive namimg
",3.0,0.0811678089656341,0.0812162456709694,0.0028819839674505,True,True,True,1389003227000.0,1389009930000.0,1389013530000.0,1389084909000.0,1389085809000.0,1389003227000.0,-1.0,1389003465000.0,1389007065000.0,1389009934000.0,1389013534000.0,1389009930000.0,1389013530000.0,"thomas.risberg",1389007065000.0,1389003227000.0,44,1389003227000.0,-1.0,"Fix filejdbc batch job","filejdbc throws an exception: 
{code}
java.lang.IllegalArgumentException: Could not resolve resource location pattern [/mycsvdir/*.csv]: class path resource [mycsvdir/] cannot be resolved to URL because it does not exist
{code}

This can be solved by using a file:// prefix

Maybe just update the docs?
",3.0,False,3.0,0,0.0,0,0,True
"XD-1196","Story","Sprint 20","2014-01-06T07:06:32.000+0000","thomas.risberg","thomas.risberg","Align filehdfs batch job defaults with those of corresponding hdfs sink","The batch jobs use different defaults compared to some of the sink/source modules.

filehdfs puts data in a /data directory with files named after the stream using a .log file extension. The hdfs sink puts files in an /xd/<streamname> directory using .txt as the default file extension.

filehdfs needs a more descriptive naming",5.0,0.0596211119741166,0.0596468306834489,0.0589935754664087,True,True,True,1388991992000.0,1389003583000.0,1389007183000.0,1389185503000.0,1389186403000.0,1388991992000.0,-1.0,1389003461000.0,1389007061000.0,1389003588000.0,1389007188000.0,1389003583000.0,1389007183000.0,"thomas.risberg",1389007061000.0,1389003435000.0,44,1389003435000.0,-1.0,"Align filehdfs batch job defaults with those of corresponding hdfs sink","The batch jobs use different defaults compared to some of the sink/source modules.

filehdfs puts data in a /data directory with files named after the stream using a .log file extension. The hdfs sink puts files in an /xd/<streamname> directory using .txt as the default file extension.

filehdfs needs a more descriptive naming",5.0,False,5.0,0,0.0,-1,13,False
"XD-1194","Story","Sprint 21","2014-01-03T09:25:50.000+0000","iperumal","iperumal","CommandLine default values from container & admin options can not be overridden","If we have default values from Container/Admin options, then they can not be overridden by the system properties or system environment. 

Currently, the only default we have for the Container/Admin options is for ""jmxEnabled"" option and since it is a boolean it can never be overridden by sys/env property XD_JMX_ENABLED.

I think we need to make sure there are no default values assigned for the non-boolean Container/Admin options and handle the boolean type option separately.",2.0,0.8272823665526119,0.8272852092356878,0.984125510143167,True,True,True,1388741150000.0,1389614215000.0,1389617815000.0,1389795591000.0,1389796491000.0,1388741150000.0,-1.0,1389779738000.0,1389783338000.0,1389614218000.0,1389617818000.0,1389614215000.0,1389617815000.0,"iperumal",1389783338000.0,1388741150000.0,44,1388741150000.0,-1.0,"CommandLine default values from container & admin options can not be overridden","If we have default values from Container/Admin options, then they can not be overridden by the system properties or system environment. 

Currently, the only default we have for the Container/Admin options is for ""jmxEnabled"" option and since it is a boolean it can never be overridden by sys/env property XD_JMX_ENABLED.

I think we need to make sure there are no default values assigned for the non-boolean Container/Admin options and handle the boolean type option separately.",2.0,False,2.0,0,0.0,0,0,False
"XD-1193","Story","Sprint 20","2014-01-03T07:53:07.000+0000","mark.pollack","mark.pollack","Update to Spring Batch 2.2.4","",1.0,0.6139643861293346,0.0,0.001593252108716,True,True,True,1388735587000.0,1388742138000.0,1388745738000.0,1388745357000.0,1388746257000.0,1388735587000.0,-1.0,1388735604000.0,1388739204000.0,1388735587000.0,1388739187000.0,1388742138000.0,1388745738000.0,"mark.pollack",1388739204000.0,1388735587000.0,44,1388735587000.0,-1.0,"Update to Spring Batch 2.2.4","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1192","Story","Sprint 20","2014-01-02T09:03:45.000+0000","mark.pollack","luke","Add documentation for JDBC to HDFS batch job","Add docs to section 

https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs#pre-packaged-batch-jobs",2.0,-1.0,0.0,0.0009722897423432,False,True,True,1388653425000.0,-1.0,-1.0,1388666924000.0,1388667824000.0,1388653425000.0,-1.0,1388653439000.0,1388657039000.0,1388653425000.0,1388657025000.0,-1.0,-1.0,"mark.pollack",1388657039000.0,1388653425000.0,44,1388653425000.0,-1.0,"Add documentation for JDBC to HDFS batch job","Add docs to section 

https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs#pre-packaged-batch-jobs",2.0,False,2.0,0,0.0,-1,0,False
"XD-1191","Bug","Sprint 20","2013-12-30T08:18:57.000+0000","thomas.risberg","","JDBC sink destroys existing table","The jdbc sink deletes existing table and creates a single column payload one even if properties file has 'initializeDatabase=false'",3.0,0.045070795736752,0.0450567014232437,0.0001437619977843,True,True,True,1388391537000.0,1388407526000.0,1388411126000.0,1388745390000.0,1388746290000.0,1388391537000.0,-1.0,1388391588000.0,1388395188000.0,1388407521000.0,1388411121000.0,1388407526000.0,1388411126000.0,"thomas.risberg",1388395188000.0,1388391537000.0,44,1388391537000.0,-1.0,"JDBC sink destroys existing table","The jdbc sink deletes existing table and creates a single column payload one even if properties file has 'initializeDatabase=false'",3.0,False,3.0,0,0.0,-1,0,False
"XD-1190","Story","Sprint 20","2013-12-27T07:20:33.000+0000","iperumal","Ilayaperumal Gopinathan","Setup precedence order for module properties' property resolver","The PropertyResolver needs to follow the below precedence order on PropertySources when resolving the module properties:

From lowest to the highest order,

0 application.yml
1 applicaiton.yml fragment
2 property placeholders
2a  property placeholder under 'shared' config directory 
2b property placeholder under module/(source/sink/processor)/config directory
3. environment variables
4. system properties
5. command line


",5.0,3.3499342879935462e-06,0.8283508897806143,2.131776365086802e-06,True,True,True,1388128833000.0,1388128855000.0,1388132455000.0,1394695226000.0,1394696126000.0,1388128833000.0,-1.0,1388128847000.0,1388132447000.0,1393568856000.0,1393572456000.0,1388128855000.0,1388132455000.0,"iperumal",1388132447000.0,1388128833000.0,44,1388128833000.0,-1.0,"Setup precedence order for module properties' property resolver","The PropertyResolver needs to follow the below precedence order on PropertySources when resolving the module properties:

From lowest to the highest order,

0 application.yml
1 applicaiton.yml fragment
2 property placeholders
2a  property placeholder under 'shared' config directory 
2b property placeholder under module/(source/sink/processor)/config directory
3. environment variables
4. system properties
5. command line


",5.0,False,5.0,0,0.0,0,0,False
"XD-1189","Bug","Sprint 20","2013-12-26T11:07:05.000+0000","iperumal","iperumal","Could not override rabbit sink module's rabbit connection factory properties","Currently RabbitMQ sink module's connection properties could not be overridden by ""${xd.config.home}/${configProperties:rabbit}.properties"" even if ""local-override"" is set to true.

It looks like the AmqpTemplate used by the AMQP outbound channel adapter doesn't use the connection factory defined in the sink module.",2.0,-1.0,0.0,0.0184147317854283,False,True,True,1388056025000.0,-1.0,-1.0,1388056374000.0,1388057274000.0,1388056025000.0,-1.0,1388056048000.0,1388057274000.0,1388056025000.0,1388057274000.0,-1.0,-1.0,"iperumal",1388057274000.0,1388056025000.0,44,1388056025000.0,-1.0,"Could not override rabbit sink module's rabbit connection factory properties","Currently RabbitMQ sink module's connection properties could not be overridden by ""${xd.config.home}/${configProperties:rabbit}.properties"" even if ""local-override"" is set to true.

It looks like the AmqpTemplate used by the AMQP outbound channel adapter doesn't use the connection factory defined in the sink module.",2.0,False,2.0,0,0.0,-1,0,False
"XD-1188","Bug","Sprint 20","2013-12-23T14:44:35.000+0000","iperumal","Ilayaperumal Gopinathan","Add ModuleOptions support for Rabbit sink","We need to add ModuleOptions support for Rabbit sink.",2.0,0.0082433953170005,0.8997471926999123,0.0082328484959705,True,True,True,1387809875000.0,1387817691000.0,1387821291000.0,1388757128000.0,1388758028000.0,1387809875000.0,-1.0,1387817681000.0,1387821281000.0,1388662973000.0,1388666573000.0,1387817691000.0,1387821291000.0,"iperumal",1387821281000.0,1387809875000.0,44,1387809875000.0,-1.0,"Add ModuleOptions support for Rabbit sink","We need to add ModuleOptions support for Rabbit sink.",2.0,False,2.0,0,0.0,-1,0,False
"XD-1187","Story","Sprint 20","2013-12-23T13:19:15.000+0000","grenfro","grenfro","XD-ADMIN must support --transport as an option.","--transport as an alias for --controlTransport",3.0,5.787962340623031e-06,0.0,0.479392321833858,True,True,True,1387804755000.0,1387804763000.0,1387808363000.0,1389186034000.0,1389186934000.0,1387804755000.0,-1.0,1388467361000.0,1388470961000.0,1387804755000.0,1387808355000.0,1387804763000.0,1387808363000.0,"grenfro",1388470961000.0,1387804755000.0,44,1387804755000.0,-1.0,"xd-admin server to --transport as an option.","--transport as an alias for --controlTransport",3.0,False,3.0,0,0.0,0,0,True
"XD-1186","Bug","Sprint 20","2013-12-23T11:14:14.000+0000","iperumal","Ilayaperumal Gopinathan","XD startup scripts' classpath needs to separate out the properties","Since spring boot (by default) looks for property sources from file:./config/application.yml and file:./application.yml

In XD bundles, the CLASSPATH of our XD startup scripts'(admin, container, singlenode startup scripts) set to use APP_HOME and APP_HOME/config.

But, if we have an application.yml fragment on $APP_HOME/config, then it is considered as classpath resource and the actual ""application.yml"" (from dirt lib/ classpath) is not loaded. 

Also, we need to separate out the properties files we have inside $APP_HOME/config as by default, boot uses ""config"" directory as well.",3.0,0.1851328348558725,0.1709871630938006,0.0017748388880499,True,True,True,1387797254000.0,1388732703000.0,1388736303000.0,1392849207000.0,1392850107000.0,1387797431000.0,-1.0,1387806222000.0,1387809822000.0,1388661227000.0,1388664827000.0,1388732703000.0,1388736303000.0,"iperumal",1387809822000.0,1387797431000.0,44,1387797431000.0,-1.0,"Support use of application.yml fragments","Since spring boot (by default) looks for property sources from file:./config/application.yml and file:./application.yml

In XD bundles, the CLASSPATH of our XD startup scripts'(admin, container, singlenode startup scripts) set to use APP_HOME and APP_HOME/config.

But, if we have an application.yml fragment on $APP_HOME/config, then it is considered as classpath resource and the actual ""application.yml"" (from dirt lib/ classpath) is not loaded. 

Also, we need to separate out the properties files we have inside $APP_HOME/config as by default, boot uses ""config"" directory as well.",3.0,False,3.0,0,0.0,0,0,True
"XD-1185","Improvement","Sprint 21","2013-12-21T06:38:25.000+0000","thomas.risberg","iperumal","Add redisConnectionFactory with connection pool","We need to add a connection pool to the Redis connection factory used for the transport, otherwise we'll see exceptions like these:

12:57:54,842 ERROR inbound.tictoc.0-redis:queue-inbound-channel-adapter1 inbound.RedisQueueMessageDrivenEndpoint:183 - Failed to execute listening task. Will attempt to resubmit in 5000 milliseconds.
org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is com.lambdaworks.redis.RedisException: Unable to connect
at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:46)
at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:36)
at org.springframework.data.redis.connection.lettuce.LettuceConverters.toDataAccessException(LettuceConverters.java:159)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:253)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1508)
at org.springframework.data.redis.core.DefaultListOperations$12.inRedis(DefaultListOperations.java:163)
at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:51)
at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:185)
at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:153)
at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:86)
at org.springframework.data.redis.core.DefaultListOperations.rightPop(DefaultListOperations.java:160)
at org.springframework.data.redis.core.DefaultBoundListOperations.rightPop(DefaultBoundListOperations.java:105)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:178)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)
at java.lang.Thread.run(Thread.java:724)
Caused by: com.lambdaworks.redis.RedisException: Unable to connect
at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)
at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.getAsyncDedicatedConnection(LettuceConnection.java:2924)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.getDedicatedConnection(LettuceConnection.java:2932)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1506)
... 11 more
Caused by: java.net.BindException: Cannot assign requested address
at sun.nio.ch.Net.connect0(Native Method)
at sun.nio.ch.Net.connect(Net.java:465)
at sun.nio.ch.Net.connect(Net.java:457)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)
at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
at org.jboss.netty.channel.Channels.connect(Channels.java:634)
at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:165)
... 15 more
",1.0,0.9994164851319708,0.9994156041331764,0.9994552490789158,True,True,True,1387607905000.0,1391011144000.0,1391013131000.0,1391012231000.0,1391013131000.0,1387607905000.0,1391011613000.0,1391011276000.0,1391013131000.0,1391011141000.0,1391013131000.0,1391011144000.0,1391013131000.0,"thomas.risberg",1391013131000.0,1391011613000.0,44,1391011613000.0,-1.0,"Add redisConnectionFactory with connection pool","We need to add a connection pool to the Redis connection factory used for the transport, otherwise we'll see exceptions like these:

12:57:54,842 ERROR inbound.tictoc.0-redis:queue-inbound-channel-adapter1 inbound.RedisQueueMessageDrivenEndpoint:183 - Failed to execute listening task. Will attempt to resubmit in 5000 milliseconds.
org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is com.lambdaworks.redis.RedisException: Unable to connect
at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:46)
at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:36)
at org.springframework.data.redis.connection.lettuce.LettuceConverters.toDataAccessException(LettuceConverters.java:159)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:253)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1508)
at org.springframework.data.redis.core.DefaultListOperations$12.inRedis(DefaultListOperations.java:163)
at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:51)
at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:185)
at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:153)
at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:86)
at org.springframework.data.redis.core.DefaultListOperations.rightPop(DefaultListOperations.java:160)
at org.springframework.data.redis.core.DefaultBoundListOperations.rightPop(DefaultBoundListOperations.java:105)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:178)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:291)
at java.lang.Thread.run(Thread.java:724)
Caused by: com.lambdaworks.redis.RedisException: Unable to connect
at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)
at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.getAsyncDedicatedConnection(LettuceConnection.java:2924)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.getDedicatedConnection(LettuceConnection.java:2932)
at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1506)
... 11 more
Caused by: java.net.BindException: Cannot assign requested address
at sun.nio.ch.Net.connect0(Native Method)
at sun.nio.ch.Net.connect(Net.java:465)
at sun.nio.ch.Net.connect(Net.java:457)
at sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:639)
at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.connect(NioClientSocketPipelineSink.java:108)
at org.jboss.netty.channel.socket.nio.NioClientSocketPipelineSink.eventSunk(NioClientSocketPipelineSink.java:70)
at org.jboss.netty.channel.Channels.connect(Channels.java:634)
at org.jboss.netty.channel.AbstractChannel.connect(AbstractChannel.java:207)
at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:165)
... 15 more
",1.0,False,1.0,0,0.0,0,0,False
"XD-1184","Story","Sprint 20","2013-12-20T17:38:47.000+0000","iperumal","iperumal","Admin & Launcher startup fails when XD_JMX_ENABLED is set to true","When exporting of MBeans are enabled via XD_JMX_ENABLED (also, jmxEnabled as in application.yml), the Admin and Lancher server application fail to start.

Since the admin applications has the same 'integrationMbeanExporter' bean name for IntegrationMBeanExporter as that its ParentConfiguration, there is a naming conflicts and the exception thrown as:

(Same is the case for launcher and its parent configuration)

Exception in thread ""main"" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'mbeanExporter' defined in class org.springframework.context.annotation.MBeanExportConfiguration: Invocation of init method failed; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.monitor.IntegrationMBeanExporter@1de40d3e] with key 'integrationMBeanExporter'; nested exception is javax.management.InstanceAlreadyExistsException: org.springframework.integration.monitor:name=integrationMBeanExporter,type=IntegrationMBeanExporter
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1553)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:124)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:609)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)
	at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:60)
	at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:42)
Caused by: org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.monitor.IntegrationMBeanExporter@1de40d3e] with key 'integrationMBeanExporter'; nested exception is javax.management.InstanceAlreadyExistsException: org.springframework.integration.monitor:name=integrationMBeanExporter,type=IntegrationMBeanExporter
	at org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:610)
	at org.springframework.jmx.export.MBeanExporter.registerBeans(MBeanExporter.java:535)
	at org.springframework.jmx.export.MBeanExporter.afterPropertiesSet(MBeanExporter.java:417)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1612)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1549)
	... 15 more
Caused by: javax.management.InstanceAlreadyExistsException: org.springframework.integration.monitor:name=integrationMBeanExporter,type=IntegrationMBeanExporter
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:513)
	at org.springframework.jmx.support.MBeanRegistrationSupport.doRegister(MBeanRegistrationSupport.java:195)
	at org.springframework.jmx.export.MBeanExporter.registerBeanInstance(MBeanExporter.java:663)
	at org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:600)
	... 19 more
",2.0,3.219572054482518e-05,0.0,2.2077065516451556e-05,True,True,True,1387561127000.0,1387561162000.0,1387564762000.0,1388647328000.0,1388648228000.0,1387561127000.0,-1.0,1387561151000.0,1387564751000.0,1387561127000.0,1387564727000.0,1387561162000.0,1387564762000.0,"iperumal",1387564751000.0,1387561127000.0,43,1387561127000.0,-1.0,"Admin & Launcher startup fails when XD_JMX_ENABLED is set to true","When exporting of MBeans are enabled via XD_JMX_ENABLED (also, jmxEnabled as in application.yml), the Admin and Lancher server application fail to start.

Since the admin applications has the same 'integrationMbeanExporter' bean name for IntegrationMBeanExporter as that its ParentConfiguration, there is a naming conflicts and the exception thrown as:

(Same is the case for launcher and its parent configuration)

Exception in thread ""main"" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'mbeanExporter' defined in class org.springframework.context.annotation.MBeanExportConfiguration: Invocation of init method failed; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.monitor.IntegrationMBeanExporter@1de40d3e] with key 'integrationMBeanExporter'; nested exception is javax.management.InstanceAlreadyExistsException: org.springframework.integration.monitor:name=integrationMBeanExporter,type=IntegrationMBeanExporter
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1553)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:539)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:475)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:304)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:228)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:300)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:195)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:700)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:760)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:482)
	at org.springframework.boot.context.embedded.EmbeddedWebApplicationContext.refresh(EmbeddedWebApplicationContext.java:124)
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:609)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:321)
	at org.springframework.boot.builder.SpringApplicationBuilder.run(SpringApplicationBuilder.java:130)
	at org.springframework.xd.dirt.server.AdminServerApplication.run(AdminServerApplication.java:60)
	at org.springframework.xd.dirt.server.AdminServerApplication.main(AdminServerApplication.java:42)
Caused by: org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [org.springframework.integration.monitor.IntegrationMBeanExporter@1de40d3e] with key 'integrationMBeanExporter'; nested exception is javax.management.InstanceAlreadyExistsException: org.springframework.integration.monitor:name=integrationMBeanExporter,type=IntegrationMBeanExporter
	at org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:610)
	at org.springframework.jmx.export.MBeanExporter.registerBeans(MBeanExporter.java:535)
	at org.springframework.jmx.export.MBeanExporter.afterPropertiesSet(MBeanExporter.java:417)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1612)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1549)
	... 15 more
Caused by: javax.management.InstanceAlreadyExistsException: org.springframework.integration.monitor:name=integrationMBeanExporter,type=IntegrationMBeanExporter
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:513)
	at org.springframework.jmx.support.MBeanRegistrationSupport.doRegister(MBeanRegistrationSupport.java:195)
	at org.springframework.jmx.export.MBeanExporter.registerBeanInstance(MBeanExporter.java:663)
	at org.springframework.jmx.export.MBeanExporter.registerBeanNameOrInstance(MBeanExporter.java:600)
	... 19 more
",2.0,False,2.0,0,0.0,0,0,False
"XD-1183","Bug","Sprint 20","2013-12-19T17:31:09.000+0000","grenfro","grenfro","topic in mqtt source was marked as topics","Changed field back to topic",1.0,1.3091601938765544e-05,1.1077509332801611e-05,0.9999808661202434,True,True,True,1387474269000.0,1387474282000.0,1387477882000.0,1388466372000.0,1388467272000.0,1387474269000.0,-1.0,1388467253000.0,1388467272000.0,1387474280000.0,1387477880000.0,1387474282000.0,1387477882000.0,"grenfro",1388467272000.0,1387474269000.0,43,1387474269000.0,-1.0,"topic in mqtt source was marked as topics","Changed field back to topic",1.0,False,1.0,0,0.0,0,0,False
"XD-1182","Story","Sprint 22","2013-12-19T11:42:39.000+0000","thomas.risberg","thomas.risberg","Update to spring-data-hadoop 2.0.0.M5","Update to spring-data-hadoop 2.0.0.M5 when it is released and remove the temporary DatasetTemplateAllowingNulls in spring-xd-hadoop

We should also review the supported hadoop distros - think we should support anything that is current/stable:
- hadoop12
- hadoop22
- phd1 (PHD 1.1)
- hdp13
- hdp20
- cdh4
",3.0,0.9659004036060748,0.0,0.965860854369518,True,True,True,1387453359000.0,1392191369000.0,1392194969000.0,1392357737000.0,1392358637000.0,1387453359000.0,-1.0,1392191175000.0,1392194775000.0,1387453359000.0,1387456959000.0,1392191369000.0,1392194969000.0,"thomas.risberg",1392194775000.0,1387453359000.0,43,1387453359000.0,-1.0,"Update to spring-data-hadoop 2.0.0.M5","Update to spring-data-hadoop 2.0.0.M5 when it is released and remove the temporary DatasetTemplateAllowingNulls in spring-xd-hadoop

We should also review the supported hadoop distros - think we should support anything that is current/stable:
- hadoop12
- hadoop22
- phd1 (PHD 1.1)
- hdp13
- hdp20
- cdh4
",3.0,False,3.0,0,0.0,-1,1,False
"XD-1180","Story","","2013-12-19T07:21:53.000+0000","eric.bottard","","JMX exposition is broken","However this is triggered (depending on whether https://github.com/spring-projects/spring-xd/pull/477/files is merged yet or not), jmx seems to be broken because of duplicate beans / mbeans names",4.0,-1.0,0.999966786866578,-1.0,False,False,True,1387437713000.0,-1.0,-1.0,1389333653000.0,1389334553000.0,1387437713000.0,-1.0,-1.0,-1.0,1389334490000.0,1389334553000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1387437713000.0,-1.0,"Enabling of JMX support is broken","However this is triggered (depending on whether https://github.com/spring-projects/spring-xd/pull/477/files is merged yet or not), jmx seems to be broken because of duplicate beans / mbeans names",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-1179","Bug","","2013-12-19T06:43:56.000+0000","grenfro","grenfro","Nodes can not connect with Admin using Redis as transport","This has happened more than once, where a node fails for whatever reason, and when it is restarted, it does not receive requests from the admin server.

This could be file handle count based.  Since this is not Rabbit as a transport I'm not chasing this down yet.  But felt it needed to be recorded.",5.0,0.9711579853637538,0.0,-1.0,True,False,True,1387435436000.0,1387437692000.0,1387437759000.0,1387436859000.0,1387437759000.0,1387435436000.0,-1.0,-1.0,-1.0,1387435436000.0,1387437759000.0,1387437692000.0,1387437759000.0,"grenfro",-1.0,-1.0,0,1387435436000.0,-1.0,"Nodes can not connect with Admin using Redis as transport","This has happened more than once, where a node fails for whatever reason, and when it is restarted, it does not receive requests from the admin server.

This could be file handle count based.  Since this is not Rabbit as a transport I'm not chasing this down yet.  But felt it needed to be recorded.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1178","Story","Sprint 20","2013-12-18T11:34:42.000+0000","thomas.risberg","thomas.risberg","Make avro sink options consistent with hdfs and add docs","",3.0,0.0022575097529494,0.0,1.5219166873816438e-05,True,True,True,1387366482000.0,1387369597000.0,1387373197000.0,1388745421000.0,1388746321000.0,1387366482000.0,-1.0,1387366503000.0,1387370103000.0,1387366482000.0,1387370082000.0,1387369597000.0,1387373197000.0,"thomas.risberg",1387370103000.0,1387366482000.0,43,1387366482000.0,-1.0,"Make avro sink options consistent with hdfs and add docs","",3.0,False,3.0,0,0.0,-1,0,False
"XD-1177","Story","","2013-12-18T11:32:06.000+0000","mark.pollack","aclement","Change module naming strategy in parser taking into account no duplicate labels and repeated modules","Labels should only be used help uniquely identify a module in a stream.

The stream definition a | x: b | x: c | d should throw an error since the label x is applied to two modules.

Also ambiguity exists with a stream definition such as 

http | transform | transform | filter | hdfs

since a module with the name 'transform' will exist twice, meaning a tap on transform will result in message from both transform modules.

To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example.

http | transform | transform | filter | hdfs
http, transform.0, transform.1, filter, hdfs

If a use tries to tap on transform, an error would be shown saying that  you could try to tap on transform.0 or transform.1 or alternatively use labels.

http | transform | filter | transform | hdfs
http, transform.0, filter, transform.1, hdfs

http | x: transform | filter | transform | hdfs
http, x, filter, transform, hdfs

",4.0,-1.0,0.0,-1.0,False,False,True,1387366326000.0,-1.0,-1.0,1391117279000.0,1391118179000.0,1387366326000.0,-1.0,-1.0,-1.0,1387366326000.0,1387369926000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1387366326000.0,-1.0,"Change module naming strategy in parser taking into account no duplicate labels and repeated modules","Labels should only be used help uniquely identify a module in a stream.

The stream definition a | x: b | x: c | d should throw an error since the label x is applied to two modules.

Also ambiguity exists with a stream definition such as 

http | transform | transform | filter | hdfs

since a module with the name 'transform' will exist twice, meaning a tap on transform will result in message from both transform modules.

To make the naming unique and the usage with taps, the following naming strategy is proposed demonstrated by example.

http | transform | transform | filter | hdfs
http, transform.0, transform.1, filter, hdfs

If a use tries to tap on transform, an error would be shown saying that  you could try to tap on transform.0 or transform.1 or alternatively use labels.

http | transform | filter | transform | hdfs
http, transform.0, filter, transform.1, hdfs

http | x: transform | filter | transform | hdfs
http, x, filter, transform, hdfs

",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1176","Story","Sprint 20","2013-12-18T10:42:19.000+0000","thomas.risberg","thomas.risberg","Update to spring-data-hadoop 2.0.0.M4","Update dependencies to spring-data-hadoop 2.0.0.M4",1.0,0.0080964036904537,0.0,0.0048954999058557,True,True,True,1387363339000.0,1387363382000.0,1387366982000.0,1387367750000.0,1387368650000.0,1387363339000.0,-1.0,1387363365000.0,1387366965000.0,1387363339000.0,1387366939000.0,1387363382000.0,1387366982000.0,"thomas.risberg",1387366965000.0,1387363339000.0,43,1387363339000.0,-1.0,"Update to spring-data-hadoop 2.0.0.M4","Update dependencies to spring-data-hadoop 2.0.0.M4",1.0,False,1.0,0,0.0,-1,0,False
"XD-1175","Story","","2013-12-18T04:49:08.000+0000","eric.bottard","eric.bottard","Support external defaults for PojoModuleOptions","The current behavior when there are ""global"" external defaults to module options is to set them in the placeholder construct: ${foo:${the_default}} where ${the_default} is sourced from some properties file.

The downside is that the module options infrastructure is unaware of them.

Provide support for such defaults (at least when using PojoModuleOptions) in the form of

* Annotate a field (or the setter?) with @Value(""${the_default}"")
* (maybe) Annotate the pojo with @PropertySource to indicate the location of the properties file
* (maybe) come up with a general naming scheme for the properties file",5.0,-1.0,0.0,-1.0,False,False,True,1387342148000.0,-1.0,-1.0,1389577485000.0,1389578385000.0,1387342148000.0,-1.0,-1.0,-1.0,1387342148000.0,1387345748000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1387342148000.0,-1.0,"Support external defaults for PojoModuleOptions","The current behavior when there are ""global"" external defaults to module options is to set them in the placeholder construct: ${foo:${the_default}} where ${the_default} is sourced from some properties file.

The downside is that the module options infrastructure is unaware of them.

Provide support for such defaults (at least when using PojoModuleOptions) in the form of

* Annotate a field (or the setter?) with @Value(""${the_default}"")
* (maybe) Annotate the pojo with @PropertySource to indicate the location of the properties file
* (maybe) come up with a general naming scheme for the properties file",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1174","Story","Sprint 20","2013-12-17T08:30:00.000+0000","mark.pollack","jvalkeal","Update HDFS sink documentation to reflect new functionality introduced in XD-990 and XD-991","",2.0,0.0774334173223072,0.0,6.042546809717289e-05,True,True,True,1387269000000.0,1387375362000.0,1387378962000.0,1388641693000.0,1388642593000.0,1387269000000.0,-1.0,1387269083000.0,1387272683000.0,1387269000000.0,1387272600000.0,1387375362000.0,1387378962000.0,"mark.pollack",1387272683000.0,1387269000000.0,43,1387269000000.0,-1.0,"Update HDFS sink documentation to reflect new functionality introduced in XD-990 and XD-991","",2.0,False,2.0,0,0.0,0,0,False
"XD-1173","Bug","Sprint 20","2013-12-17T06:19:19.000+0000","mark.pollack","","Duplicate messages on tap","
If I fiddle with the testTappingWithLabels method I can reproduce the same issue:

HttpSource source = newHttpSource();

FileSink sink = newFileSink().binary(true); FileSink tapsink1 = newFileSink().binary(true); stream().create(""myhttp"", ""%s | flibble: transform
--expression=payload.toUpperCase() | flibble2: transform
--expression=payload.toUpperCase() | %s"", source, sink); stream().create(""mytap4"", ""tap:stream:myhttp.flibble > transform
--expression=payload.replaceAll('A','.') | %s"", tapsink1); source.ensureReady().postData(""Dracarys!"");

assertThat(sink, eventually(hasContentsThat(equalTo(""DRACARYS!""))));

assertThat(tapsink1, eventually(hasContentsThat(equalTo(""DR.C.RYS!""))));


java.lang.AssertionError:
Expected: ""DR.C.RYS!"", trying at most 10 times
      but: failed after 10*100=1000ms:
""DR.C.RYS!DR.C.RYS!"",
",3.0,0.8565924365396241,0.0029449050653491,2.408591929129591e-06,True,True,True,1387261159000.0,1390461922000.0,1390465522000.0,1390996882000.0,1390997782000.0,1387261159000.0,-1.0,1387261168000.0,1387264768000.0,1387272163000.0,1387275763000.0,1390461922000.0,1390465522000.0,"mark.pollack",1387264768000.0,1387261159000.0,43,1387261159000.0,-1.0,"Duplicate messages on tap","If I fiddle with the testTappingWithLabels method I can reproduce the same issue:

HttpSource source = newHttpSource();

FileSink sink = newFileSink().binary(true); FileSink tapsink1 = newFileSink().binary(true); stream().create(""myhttp"", ""%s | flibble: transform
--expression=payload.toUpperCase() | flibble2: transform
--expression=payload.toUpperCase() | %s"", source, sink); stream().create(""mytap4"", ""tap:stream:myhttp.flibble > transform
--expression=payload.replaceAll('A','.') | %s"", tapsink1); source.ensureReady().postData(""Dracarys!"");

assertThat(sink, eventually(hasContentsThat(equalTo(""DRACARYS!""))));

assertThat(tapsink1, eventually(hasContentsThat(equalTo(""DR.C.RYS!""))));


java.lang.AssertionError:
Expected: ""DR.C.RYS!"", trying at most 10 times
      but: failed after 10*100=1000ms:
""DR.C.RYS!DR.C.RYS!"",
",3.0,False,3.0,0,0.0,0,0,True
"XD-1172","Story","Sprint 20","2013-12-17T06:06:58.000+0000","eric.bottard","eric.bottard","Restore previous CmdLine library to populate options","Expected benefits are
--key<space>value as well as --key=value on the command line (eg XD-1108)

nice usage screen 
",4.0,0.0084847958737082,0.0,1.7361676269843854e-05,True,True,True,1387260418000.0,1387272147000.0,1387275747000.0,1388641873000.0,1388642773000.0,1387260418000.0,-1.0,1387260442000.0,1387264042000.0,1387260418000.0,1387264018000.0,1387272147000.0,1387275747000.0,"eric.bottard",1387264042000.0,1387260418000.0,43,1387260418000.0,-1.0,"Restore previous CmdLine library to populate options","Expected benefits are
--key<space>value as well as --key=value on the command line (eg XD-1108)

nice usage screen 
",4.0,False,4.0,0,0.0,-1,0,False
"XD-1171","Story","Sprint 20","2013-12-16T16:24:43.000+0000","iperumal","iperumal","Container (Launcher) id is not unique","XD container's id is set to use its application context id which is derived from:

${vcap.application.name:${spring.application.name:${spring.config.name:application}}}:${vcap.application.instance_index:${spring.application.index:${server.port:${PORT:null}}}}

With the *default* values[the PORT is not set and embedded tomcat uses local port], the launcher id is set to ""application:0""

When I have multiple launchers then all the launchers have the same id as ""application:0"" which doesn't seem correct.

Do we need to use the Id that is generated at the XDContainer's constructor here?

public XDContainer() {
		this.id = UUID.randomUUID().toString();
	}

",3.0,0.1779934132106624,0.1778786300184536,0.1778433121131585,True,True,True,1387211083000.0,1387231242000.0,1387234842000.0,1387323440000.0,1387324340000.0,1387211083000.0,-1.0,1387231225000.0,1387234825000.0,1387231229000.0,1387234829000.0,1387231242000.0,1387234842000.0,"iperumal",1387234825000.0,1387211083000.0,43,1387211083000.0,-1.0,"Container (Launcher) id is not unique","XD container's id is set to use its application context id which is derived from:

${vcap.application.name:${spring.application.name:${spring.config.name:application}}}:${vcap.application.instance_index:${spring.application.index:${server.port:${PORT:null}}}}

With the *default* values[the PORT is not set and embedded tomcat uses local port], the launcher id is set to ""application:0""

When I have multiple launchers then all the launchers have the same id as ""application:0"" which doesn't seem correct.

Do we need to use the Id that is generated at the XDContainer's constructor here?

public XDContainer() {
		this.id = UUID.randomUUID().toString();
	}

",3.0,False,3.0,0,0.0,0,0,False
"XD-1170","Bug","","2013-12-16T16:23:51.000+0000","daigo","","Splunk sink module doesn't work because of incompatible onInit method.","Splunk sink module doesn't work at all. It throws java.lang.VerifyError exception like following.

nested exception is java.lang.VerifyError: class org.springframework.integration.splunk.outbound.SplunkOutboundChannelAdapter overrides final method onInit.()V

This is because SplunkOutputChannelAdapter refers old spring integration jar, but recent AbstractReplyProducingMessageHandler (which SplunkOutputChannelAdapter extends) set final to onInit method. Hence it doesn't work.

SplunkOutboundChannelAdapter should be fixed to not override onInit method and replace the jar file spring-integration-splunk-1.0.0.M1.jar.",2.0,0.9792208738112244,0.448844749512588,-1.0,True,False,True,1387211031000.0,1395734805000.0,1395738405000.0,1395914780000.0,1395915680000.0,1387211031000.0,-1.0,-1.0,-1.0,1391118067000.0,1391121667000.0,1395734805000.0,1395738405000.0,"daigo",-1.0,-1.0,0,1387211031000.0,-1.0,"Splunk module is broken","Splunk sink module doesn't work at all. It throws java.lang.VerifyError exception like following.

nested exception is java.lang.VerifyError: class org.springframework.integration.splunk.outbound.SplunkOutboundChannelAdapter overrides final method onInit.()V

This is because SplunkOutputChannelAdapter refers old spring integration jar, but recent AbstractReplyProducingMessageHandler (which SplunkOutputChannelAdapter extends) set final to onInit method. Hence it doesn't work.

SplunkOutboundChannelAdapter should be fixed to not override onInit method and replace the jar file spring-integration-splunk-1.0.0.M1.jar.",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1169","Bug","","2013-12-16T11:39:41.000+0000","daigo","","Column name of JDBC sink module should not hard code to ""payload"".","Current implementation of JDBC sink module insert data into ""payload"" column. But I can't just change the default column ""payload"" to something else using --columns option. Because JdbcMessagePayloadTransformer compare the columnName to ""payload"" and it hard coded.


",1.0,0.999997360764956,0.5753635033088677,-1.0,True,False,True,1387193981000.0,1394014121000.0,1394014139000.0,1394013239000.0,1394014139000.0,1387193981000.0,-1.0,-1.0,-1.0,1391118051000.0,1391121651000.0,1394014121000.0,1394014139000.0,"daigo",-1.0,-1.0,0,1387193981000.0,-1.0,"Column name of JDBC sink module should not hard code to ""payload"".","Current implementation of JDBC sink module insert data into ""payload"" column. But I can't just change the default column ""payload"" to something else using --columns option. Because JdbcMessagePayloadTransformer compare the columnName to ""payload"" and it hard coded.


",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1167","Story","","2013-12-16T08:58:15.000+0000","eric.bottard","","Mail Source ModuleOptions (+ profiles)","",2.0,-1.0,-1.0,-1.0,False,False,False,1387184295000.0,-1.0,-1.0,1396248227000.0,1396249127000.0,1387184295000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1387184295000.0,-1.0,"Mail Source ModuleOptions (+ profiles)","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1166","Story","Sprint 20","2013-12-16T08:40:35.000+0000","dturanski","dturanski","Create Gemfire Integration Test Scripts","",3.0,0.9437463166345474,0.0,0.9437404944103414,True,True,True,1387183235000.0,1388642079000.0,1388645679000.0,1388728136000.0,1388729036000.0,1387183235000.0,-1.0,1388642070000.0,1388645670000.0,1387183235000.0,1387186835000.0,1388642079000.0,1388645679000.0,"dturanski",1388645670000.0,1387183235000.0,43,1387183235000.0,-1.0,"Create Gemfire Integration Test Scripts","",3.0,False,3.0,0,0.0,-1,0,False
"XD-1164","Story","Sprint 20","2013-12-16T08:20:51.000+0000","mark.pollack","grussell","Update Spring Integration version to 4.0.M2","",1.0,0.9615771491996324,0.0,0.0001387419573021,True,True,True,1387182051000.0,1387292942000.0,1387296542000.0,1387296473000.0,1387297373000.0,1387182051000.0,-1.0,1387182067000.0,1387185667000.0,1387182051000.0,1387185651000.0,1387292942000.0,1387296542000.0,"mark.pollack",1387185667000.0,1387182051000.0,43,1387182051000.0,-1.0,"Update Spring Integration version to 4.0.M2","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1163","Story","Sprint 20","2013-12-16T08:15:43.000+0000","mark.pollack","","Update Spring Framework dependency to 4.0 GA","",1.0,-1.0,0.863190037692638,6.679708001335941e-05,False,True,True,1387181743000.0,-1.0,-1.0,1387285638000.0,1387286538000.0,1387181743000.0,-1.0,1387181750000.0,1387185350000.0,1387272201000.0,1387275801000.0,-1.0,-1.0,"mark.pollack",1387185350000.0,1387181743000.0,43,1387181743000.0,-1.0,"Update Spring Framework dependency to 4.0 GA","",1.0,False,1.0,0,0.0,0,0,False
"XD-1162","Bug","","2013-12-13T17:07:49.000+0000","daigo","","Column option of JDBC sink should not convert underscore to property name.","Current implementation of column option of JDBC sink convert underscore to java property name. If database column contains underscore, there is no way to store data.

So JdbcMessagePayloadTransformer should not use JdbcUtils.convertUnderscoreNameToPropertyName even if column contains ""_"".",1.0,-1.0,0.5969078867668364,-1.0,False,False,True,1386954469000.0,-1.0,-1.0,1393928734000.0,1393929634000.0,1386954469000.0,-1.0,-1.0,-1.0,1391118000000.0,1391121600000.0,-1.0,-1.0,"daigo",-1.0,-1.0,0,1386954469000.0,-1.0,"Column option of JDBC sink should not convert underscore to property name.","Current implementation of column option of JDBC sink convert underscore to java property name. If database column contains underscore, there is no way to store data.

So JdbcMessagePayloadTransformer should not use JdbcUtils.convertUnderscoreNameToPropertyName even if column contains ""_"".",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1161","Bug","Sprint 20","2013-12-13T13:33:27.000+0000","thomas.risberg","","Re-deployment of hdfs sink reuses filename of first deployment","Need to check for existing files with the same file counter",3.0,0.5731972330331259,0.5732159667289562,0.5476655473283408,True,True,True,1386941607000.0,1387186384000.0,1387189984000.0,1387367745000.0,1387368645000.0,1386941607000.0,-1.0,1387175481000.0,1387179081000.0,1387186392000.0,1387189992000.0,1387186384000.0,1387189984000.0,"thomas.risberg",1387179081000.0,1386941607000.0,43,1386941607000.0,-1.0,"Re-deployment of hdfs sink reuses filename of first deployment","Need to check for existing files with the same file counter",3.0,False,3.0,0,0.0,0,0,False
"XD-1159","Improvement","Sprint 29","2013-12-13T08:09:46.000+0000","david_geary","Mark Pollack","Add a MongoDB Sink","This should be quite straightforward, since the Spring Data Mongo jars are already included. We have this working by just adding the attached sink context file and the spring-integration-mongodb jar.

(This works for JSON string streams, but a mongo converter probably needs added to support Tuple conversion)
",5.0,0.9958558528550634,0.9958469927153664,0.9900146225613796,True,True,True,1386922186000.0,1401871028000.0,1401874628000.0,1401932336000.0,1401933236000.0,1386922186000.0,-1.0,1401783345000.0,1401786945000.0,1401870895000.0,1401874495000.0,1401871028000.0,1401874628000.0,"david_geary",1401786945000.0,1386922186000.0,43,1386922186000.0,-1.0,"Add a MongoDB Sink","This should be quite straightforward, since the Spring Data Mongo jars are already included. We have this working by just adding the attached sink context file and the spring-integration-mongodb jar.

(This works for JSON string streams, but a mongo converter probably needs added to support Tuple conversion)
",5.0,False,5.0,0,0.0,0,0,False
"XD-1158","Story","","2013-12-13T07:48:32.000+0000","grenfro","Glenn renfro","Create Exercise Script for JMS & MQTT","Create a script to sanity check JMS and MQTT",3.0,2.071789385657944e-05,1.5269458183128604,-1.0,True,False,True,1386920912000.0,1386920945000.0,1386924545000.0,1388512838000.0,1388513738000.0,1386920912000.0,-1.0,-1.0,-1.0,1389353071000.0,1388513738000.0,1386920945000.0,1386924545000.0,"grenfro",-1.0,-1.0,0,1386920912000.0,-1.0,"Create integration test script for JMS & MQTT","Create a script to sanity check JMS and MQTT",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-1157","Story","Sprint 20","2013-12-12T12:50:51.000+0000","dturanski","","Update docs for gemfire sink to include locator configuration","",1.0,-1.0,0.9970438904562836,0.2721809872080166,False,True,True,1386852651000.0,-1.0,-1.0,1388647016000.0,1388647916000.0,1386852651000.0,-1.0,1387341288000.0,1387344888000.0,1388642609000.0,1388646209000.0,-1.0,-1.0,"dturanski",1387344888000.0,1386852651000.0,43,1386852651000.0,-1.0,"Update docs for gemfire sink to include locator configuration","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1156","Story","Sprint 22","2013-12-12T11:41:44.000+0000","iperumal","iperumal","Refactor/Simplify JobPlugin","Currently, the JobPlugin extends AbstractPlugin and the AbstractPlugin has got lots of unused code (like it doesn't do anything with preProcessSharedContext()) in there. ",2.0,0.9882580896866798,0.9882562430520372,0.9882547321691476,True,True,True,1386848504000.0,1392735342000.0,1392738942000.0,1392804386000.0,1392805286000.0,1386848504000.0,-1.0,1392735322000.0,1392738922000.0,1392735331000.0,1392738931000.0,1392735342000.0,1392738942000.0,"iperumal",1392738922000.0,1386848504000.0,43,1386848504000.0,-1.0,"Refactor/Simplify JobPlugin","Currently, the JobPlugin extends AbstractPlugin and the AbstractPlugin has got lots of unused code (like it doesn't do anything with preProcessSharedContext()) in there. ",2.0,False,2.0,0,0.0,0,0,False
"XD-1155","Bug","Sprint 19","2013-12-11T12:21:10.000+0000","thomas.risberg","thomas.risberg","The lib directory for hadoop12 contains mix of hadoop versions","This causes issues depending on which version of the core/common jar gets loaded first - like:

xd:>hadoop fs ls
-ls: Fatal internal error
java.lang.UnsupportedOperationException: Not implemented by the DistributedFileSystem FileSystem implementation
at org.apache.hadoop.fs.FileSystem.getScheme(FileSystem.java:213)
at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2401)
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2411)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2428)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2467)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2449)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:367)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:166)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:351)
at org.apache.hadoop.fs.Path.getFileSystem(Path.java:287)
at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:224)
at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:207)
at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)
at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
at org.apache.hadoop.fs.FsShell.run(FsShell.java:255)
at org.springframework.xd.shell.hadoop.FsShellCommands.run(FsShellCommands.java:412)
at org.springframework.xd.shell.hadoop.FsShellCommands.runCommand(FsShellCommands.java:407)
at org.springframework.xd.shell.hadoop.FsShellCommands.ls(FsShellCommands.java:110)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:191)
at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)
at org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:48)
at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)
at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:483)
at org.springframework.shell.core.JLineShell.run(JLineShell.java:157)
at java.lang.Thread.run(Thread.java:724)
",3.0,0.0007535592216809,0.0,0.0005517130015878,True,True,True,1386764470000.0,1386764526000.0,1386768126000.0,1386837884000.0,1386838784000.0,1386764470000.0,-1.0,1386764511000.0,1386768111000.0,1386764470000.0,1386768070000.0,1386764526000.0,1386768126000.0,"thomas.risberg",1386768111000.0,1386764470000.0,43,1386764470000.0,-1.0,"The lib directory for hadoop12 contains mix of hadoop versions","This causes issues depending on which version of the core/common jar gets loaded first - like:

xd:>hadoop fs ls
-ls: Fatal internal error
java.lang.UnsupportedOperationException: Not implemented by the DistributedFileSystem FileSystem implementation
at org.apache.hadoop.fs.FileSystem.getScheme(FileSystem.java:213)
at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2401)
at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2411)
at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2428)
at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:88)
at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2467)
at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2449)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:367)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:166)
at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:351)
at org.apache.hadoop.fs.Path.getFileSystem(Path.java:287)
at org.apache.hadoop.fs.shell.PathData.expandAsGlob(PathData.java:325)
at org.apache.hadoop.fs.shell.Command.expandArgument(Command.java:224)
at org.apache.hadoop.fs.shell.Command.expandArguments(Command.java:207)
at org.apache.hadoop.fs.shell.Command.processRawArguments(Command.java:190)
at org.apache.hadoop.fs.shell.Command.run(Command.java:154)
at org.apache.hadoop.fs.FsShell.run(FsShell.java:255)
at org.springframework.xd.shell.hadoop.FsShellCommands.run(FsShellCommands.java:412)
at org.springframework.xd.shell.hadoop.FsShellCommands.runCommand(FsShellCommands.java:407)
at org.springframework.xd.shell.hadoop.FsShellCommands.ls(FsShellCommands.java:110)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:191)
at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)
at org.springframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:48)
at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)
at org.springframework.shell.core.JLineShell.promptLoop(JLineShell.java:483)
at org.springframework.shell.core.JLineShell.run(JLineShell.java:157)
at java.lang.Thread.run(Thread.java:724)
",3.0,False,3.0,0,0.0,-1,0,False
"XD-1154","Bug","Sprint 19","2013-12-11T09:28:39.000+0000","eric.bottard","","Update doc about trigger chances","Trigger has been changed to be one single modules, taking params (as opposed to 3 before)

Update doc at https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs

Also, arguably, the module could be advertised as a source itself (it is only mentioned in the context of batch)",3.0,-1.0,0.9999900876581432,4.625759533194781e-05,False,True,True,1386754119000.0,-1.0,-1.0,1387358525000.0,1387359425000.0,1386754119000.0,-1.0,1386754147000.0,1386757747000.0,1387359419000.0,1387359425000.0,-1.0,-1.0,"eric.bottard",1386757747000.0,1386754119000.0,43,1386754119000.0,-1.0,"Update doc about trigger changes","Trigger has been changed to be one single modules, taking params (as opposed to 3 before)

Update doc at https://github.com/spring-projects/spring-xd/wiki/Batch-Jobs

Also, arguably, the module could be advertised as a source itself (it is only mentioned in the context of batch)",3.0,False,3.0,0,0.0,-1,0,True
"XD-1153","Story","Sprint 19","2013-12-09T10:42:20.000+0000","grussell","grussell","Fix Tail Source to Use Native Adapter by Default","Support all available options",3.0,0.0008027522935779,0.0,0.0006651376146788,True,True,True,1386585740000.0,1386585810000.0,1386589410000.0,1386672040000.0,1386672940000.0,1386585740000.0,1386597073000.0,1386585798000.0,1386589398000.0,1386585740000.0,1386589340000.0,1386585810000.0,1386589410000.0,"grussell",1386589398000.0,1386585740000.0,43,1386597073000.0,1386597073000.0,"Fix Tail Source to Use Native Adapter by Default","Support all available options",1.0,True,1.0,1,200.0,0,0,False
"XD-1151","Story","Sprint 37","2013-12-09T09:16:48.000+0000","eric.bottard","eric.bottard","Apply Composite GoF pattern to ModuleDefinition","",8.0,0.9724845700211528,0.9724828243872328,0.9724828243872328,True,True,True,1386580608000.0,1413878281000.0,1413881881000.0,1414649740000.0,1414650640000.0,1386580608000.0,-1.0,1413878232000.0,1413881832000.0,1413878232000.0,1413881832000.0,1413878281000.0,1413881881000.0,"eric.bottard",1413881832000.0,1413878099000.0,44,1413878099000.0,-1.0,"Apply Composite GoF pattern to ModuleDefinition","",8.0,False,8.0,0,0.0,0,0,False
"XD-1150","Story","Sprint 20","2013-12-09T07:26:40.000+0000","dturanski","","Update docs for separate control and data transport","",1.0,0.9897263637921594,0.9896986720503592,0.3175244836796368,True,True,True,1386574000000.0,1388468265000.0,1388471865000.0,1388487028000.0,1388487928000.0,1386574000000.0,-1.0,1387181719000.0,1387185319000.0,1388468212000.0,1388471812000.0,1388468265000.0,1388471865000.0,"dturanski",1387185319000.0,1386574000000.0,43,1386574000000.0,-1.0,"Update docs for separate control and data transport","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1148","Story","Sprint 20","2013-12-06T09:35:19.000+0000","dturanski","dturanski","Allow local data transport for the container","Currently local is not a supported data transport for the container. It should be an option. Note that local is not valid for control transport for a standalone container. So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. ,--transport=local should fail. ",2.0,0.9988222306128044,0.4899242739324848,0.9988126474607522,True,True,True,1386322519000.0,1387364788000.0,1387366017000.0,1387365117000.0,1387366017000.0,1386322519000.0,1387364803000.0,1387364778000.0,1387366017000.0,1386833754000.0,1386837354000.0,1387364788000.0,1387366017000.0,"dturanski",1387366017000.0,1387364803000.0,43,1387364803000.0,-1.0,"Allow local data transport option for the container","Currently local is not a supported data transport for the container. It should be an option. Note that local is not valid for control transport for a standalone container. So we need to revisit the current semantics that default the control transport to be the same as the data transport, i.e. ,--transport=local should fail. ",2.0,False,2.0,0,0.0,0,0,True
"XD-1146","Story","Sprint 21","2013-12-06T09:21:42.000+0000","grussell","grussell","With Partitioned Jobs, Wire Partitioner and StepExecutionHandlers with the MessageBus","",16.0,0.9999928639965572,0.0,0.9999846301464308,True,True,True,1386321702000.0,1389965172000.0,1389965198000.0,1389964298000.0,1389965198000.0,1386321702000.0,-1.0,1389965142000.0,1389965198000.0,1386321702000.0,1386325302000.0,1389965172000.0,1389965198000.0,"grussell",1389965198000.0,1386321702000.0,43,1386321702000.0,-1.0,"With Partitioned Jobs, Wire Partitioner and StepExecutionHandlers with the MessageBus","",16.0,False,16.0,0,0.0,0,0,False
"XD-1145","Story","","2013-12-05T06:59:30.000+0000","eric.bottard","","Add --date option to the trigger module","The TriggerSourceOptionsMetadata class should be able to use an actual Date object, thanks to Spring binding conversion.

BUT, the ${date} construct will receive a toString version of it. Make sure this works properly",4.0,-1.0,-1.0,-1.0,False,False,False,1386226770000.0,-1.0,-1.0,1389700242000.0,1389701142000.0,1386226770000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1386226770000.0,-1.0,"Add --date option to the trigger module","The TriggerSourceOptionsMetadata class should be able to use an actual Date object, thanks to Spring binding conversion.

BUT, the ${date} construct will receive a toString version of it. Make sure this works properly",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1144","Story","Sprint 19","2013-12-04T20:12:57.000+0000","mark.pollack","mark.pollack","Spike for transformation of the parsed stream/job definition to a Physical Deployment Model taking into account a Deployment Manifest","The initial parsing of a stream/job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 

1) module co-location
2) partitioning
3) number of instances
4) node assignment

an potentially other data related to the runtime properties of a module (e.g. concurrency settings) 

The an external DeploymentManifest will be used to capture this information.

A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.",10.0,0.7351495915340337,0.0,0.0002536101013671,True,True,True,1386187977000.0,1386666269000.0,1386669869000.0,1386837682000.0,1386838582000.0,1386187977000.0,-1.0,1386188142000.0,1386191742000.0,1386187977000.0,1386191577000.0,1386666269000.0,1386669869000.0,"mark.pollack",1386191742000.0,1386187977000.0,43,1386187977000.0,-1.0,"Spike for transformation of the parsed stream/job definition to a Physical Deployment Model taking into account a Deployment Manifest","The initial parsing of a stream/job definitions into a list of ModuleDeploymentRequests needs to be transformed into a Physical Deployment Model that takes into account 

1) module co-location
2) partitioning
3) number of instances
4) node assignment

an potentially other data related to the runtime properties of a module (e.g. concurrency settings) 

The an external DeploymentManifest will be used to capture this information.

A deployment of a stream or job will then need to optionally provide a reference to deployment manifest.",10.0,False,10.0,0,0.0,-1,0,False
"XD-1143","Story","","2013-12-04T20:06:50.000+0000","mark.pollack","mark.pollack","Spike for matching algorithm for stream/job deployments and nodes","Base matching algorithm on the model used in http://research.cs.wisc.edu/htcondor/ 

Jobs/Stream - specify their requirements and preferences
Nodes - specify their requirements and preferences

e.g.  

A job/stream module ""requires"" a linux x85-64 platform and ""prefers"" the machine with the most free memory

A node ""requires"" that only only can run jobs when there is 25% or more free memory and it ""prefers"" to run stream modules over job modules.

The requirements and preferences are represented as SpEL expressions in a ""Advertisement"" data structure.  There is a 'require' expression and a 'preference' expression. 

Matching occurs between Node Ads and Job/Stream Ads so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression.

The Job/Streams Ads can make use of well defined attributes about the nodes, such as it's memory/cpu usage.",10.0,-1.0,0.0,-1.0,False,False,True,1386187610000.0,-1.0,-1.0,1398431745000.0,1398432645000.0,1386187610000.0,-1.0,-1.0,-1.0,1386187610000.0,1386191210000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1386187610000.0,-1.0,"Spike for matching algorithm for stream/job deployments and nodes","Base matching algorithm on the model used in http://research.cs.wisc.edu/htcondor/ 

Jobs/Stream - specify their requirements and preferences
Nodes - specify their requirements and preferences

e.g.  

A job/stream module ""requires"" a linux x85-64 platform and ""prefers"" the machine with the most free memory

A node ""requires"" that only only can run jobs when there is 25% or more free memory and it ""prefers"" to run stream modules over job modules.

The requirements and preferences are represented as SpEL expressions in a ""Advertisement"" data structure.  There is a 'require' expression and a 'preference' expression. 

Matching occurs between Node Ads and Job/Stream Ads so that the requirements of both Ads evaluate to true and the matching nodes are ranked according to the preference expression.

The Job/Streams Ads can make use of well defined attributes about the nodes, such as it's memory/cpu usage.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1142","Story","","2013-12-04T19:51:55.000+0000","mark.pollack","Mark Pollack","Spike to model the cluster nodes.","Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them.  The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group.

The project https://github.com/spring-projects/spring-data-grid is the start of this model.",8.0,-1.0,0.9999986169626078,-1.0,False,False,True,1386186715000.0,-1.0,-1.0,1397754555000.0,1397755455000.0,1386186715000.0,-1.0,-1.0,-1.0,1397755439000.0,1397755455000.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1386186715000.0,-1.0,"Spike to model the cluster nodes.","Each node in the cluster advertises itself and the admin node listens to these ads and creates groups out of them.  The deployment of jobs and stream processing can then be deployed onto a specific group and specific nodes within the group.

The project https://github.com/spring-projects/spring-data-grid is the start of this model.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1140","Story","Sprint 21","2013-12-04T10:11:00.000+0000","grussell","grussell","Add (S)FTP Gateway and Batch Partitioner to List/Process Remote Files","",16.0,0.6478636462387533,0.0,0.6093730810983977,True,True,True,1386151860000.0,1389965935000.0,1389969535000.0,1392038117000.0,1392039017000.0,1386151860000.0,-1.0,1389739335000.0,1389742935000.0,1386151860000.0,1386155460000.0,1389965935000.0,1389969535000.0,"grussell",1389742935000.0,1386321609000.0,43,1386321609000.0,-1.0,"Add (S)FTP Gateway and Batch Partitioner to List/Process Remote Files","",16.0,False,16.0,0,0.0,-1,0,False
"XD-1139","Story","Sprint 21","2013-12-04T10:10:08.000+0000","grussell","grussell","Add TaskLet to Stream from (S)FTP to HDFS","",3.0,0.6478651488432026,0.0,0.609368593093969,True,True,True,1386151808000.0,1389965921000.0,1389969521000.0,1392038110000.0,1392039010000.0,1386151808000.0,-1.0,1389739284000.0,1389742884000.0,1386151808000.0,1386155408000.0,1389965921000.0,1389969521000.0,"grussell",1389742884000.0,1386151808000.0,43,1386151808000.0,-1.0,"Add TaskLet to Stream from (S)FTP to HDFS","",3.0,False,3.0,0,0.0,-1,0,False
"XD-1137","Story","","2013-12-04T08:19:31.000+0000","eric.bottard","eric.bottard","Have the REST info about a module advertise enum properties","When a ModuleOption is backed by an enum, change the (currently String) type representation to be the possible values

ie java.lang.String -> String
but
traffic.Light -> Red | Green | Orange",3.0,-1.0,0.0,-1.0,False,False,True,1386145171000.0,-1.0,-1.0,1448380022000.0,1448380922000.0,1386145171000.0,-1.0,-1.0,-1.0,1386145171000.0,1386148771000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1386145171000.0,-1.0,"Have the REST info about a module advertise enum properties","When a ModuleOption is backed by an enum, change the (currently String) type representation to be the possible values

ie java.lang.String -> String
but
traffic.Light -> Red | Green | Orange",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1136","Bug","Sprint 19","2013-12-03T16:36:45.000+0000","thomas.risberg","thomas.risberg","Failure in writing to HDFS when undeploying and redeploying a stream with numbers in directory and/or file name","",3.0,0.1430065359477124,0.0,0.9981699346405228,True,True,True,1386088605000.0,1386090246000.0,1386093846000.0,1386099180000.0,1386100080000.0,1386088605000.0,-1.0,1386100059000.0,1386100080000.0,1386088605000.0,1386092205000.0,1386090246000.0,1386093846000.0,"thomas.risberg",1386100080000.0,1386088605000.0,43,1386088605000.0,-1.0,"Failure in writing to HDFS when undeploying and redeploying a stream with numbers in directory and/or file name","",3.0,False,3.0,0,0.0,0,0,False
"XD-1135","Story","Sprint 19","2013-12-03T08:44:52.000+0000","dturanski","dturanski","Local control channels enabled in SingleNodeApplication when alternate transport is selected.","This causes intermittent test failures when testing streams with other transports since ModuleDeployer receives duplicate requests from multiple threads. Fix is to check for local transport before invoking setUpControlChannels()",1.0,0.999271099078461,0.0,0.9986983912115376,True,True,True,1386060292000.0,1386079485000.0,1386079499000.0,1386078599000.0,1386079499000.0,1386060292000.0,-1.0,1386079474000.0,1386079499000.0,1386060292000.0,1386063892000.0,1386079485000.0,1386079499000.0,"dturanski",1386079499000.0,1386060292000.0,43,1386060292000.0,-1.0,"Local control channels enabled in SingleNodeApplication when alternate transport is selected.","This causes intermittent test failures when testing streams with other transports since ModuleDeployer receives duplicate requests from multiple threads. Fix is to check for local transport before invoking setUpControlChannels()",1.0,False,1.0,0,0.0,0,0,False
"XD-1134","Story","Sprint 19","2013-12-02T18:27:06.000+0000","grenfro","grenfro","Logging is not producing a log file","* need to add a -Dxd.home=$XD_HOME to the start scripts else all files will write to the /logs directory.
* Need to update .bat files to use % for env variables instead of $.
* Need to rename logger.config to logging.config so that boot will pick up the log config files.
* Admin needs to use xd-admin-logger configs instead of xd-container-logger
* Renamed logging file for singlenode from admin.log to singlenode.log",1.0,3.314660744472803e-05,0.0,0.9998390021924112,True,True,True,1386008826000.0,1386008833000.0,1386012433000.0,1386219109000.0,1386220009000.0,1386008826000.0,-1.0,1386219975000.0,1386220009000.0,1386008826000.0,1386012426000.0,1386008833000.0,1386012433000.0,"grenfro",1386220009000.0,1386008826000.0,43,1386008826000.0,-1.0,"Logging is not producing a log file","* need to add a -Dxd.home=$XD_HOME to the start scripts else all files will write to the /logs directory.
* Need to update .bat files to use % for env variables instead of $.
* Need to rename logger.config to logging.config so that boot will pick up the log config files.
* Admin needs to use xd-admin-logger configs instead of xd-container-logger
* Renamed logging file for singlenode from admin.log to singlenode.log",1.0,False,1.0,0,0.0,0,1,False
"XD-1133","Story","Sprint 19","2013-12-02T09:22:12.000+0000","mark.pollack","luke","Create test script that reproduces failure in writing to HDFS when undeploying and redeploying a stream","See https://github.com/spring-projects/spring-xd/pull/415#issuecomment-29024329

This test should be created so that it reproduces the failure and then turned off, so that when we switch to the new HDFS writing code

",2.0,0.4990262648819183,0.0,0.0001168482141697,True,True,True,1385976132000.0,1386053005000.0,1386056605000.0,1386129278000.0,1386130178000.0,1385976132000.0,-1.0,1385976150000.0,1385979750000.0,1385976132000.0,1385979732000.0,1386053005000.0,1386056605000.0,"mark.pollack",1385979750000.0,1385976132000.0,43,1385976132000.0,-1.0,"Create test script that reproduces failure in writing to HDFS when undeploying and redeploying a stream","See https://github.com/spring-projects/spring-xd/pull/415#issuecomment-29024329

This test should be created so that it reproduces the failure and then turned off, so that when we switch to the new HDFS writing code

",2.0,False,2.0,0,0.0,-1,0,False
"XD-1132","Improvement","Sprint 19","2013-12-01T12:08:41.000+0000","dmarley","grussell","JMS Module - add support for TOPICS","As a Spring XD user I need to listen on a JMS Topic and ingest the messages, so I can process the messages.

Currently the module only allows for Queues",2.0,0.6981391569727116,0.3045599571946651,0.6980876320326589,True,True,True,1385899721000.0,1386075865000.0,1386079465000.0,1386151126000.0,1386152026000.0,1385899721000.0,-1.0,1386075852000.0,1386079452000.0,1385976563000.0,1385980163000.0,1386075865000.0,1386079465000.0,"dmarley",1386079452000.0,1385899721000.0,43,1385899721000.0,-1.0,"JMS Module - add support for TOPICS","As a Spring XD user I need to listen on a JMS Topic and ingest the messages, so I can process the messages.

Currently the module only allows for Queues",2.0,False,2.0,0,0.0,0,0,False
"XD-1131","Improvement","Sprint 19","2013-11-27T07:52:37.000+0000","eric.bottard","eric.bottard","Poorman's support for Plugin contributed Module Options Metadata","Pending a better approach and extension point (see XD-1050), provide a way to factor out common recurring options for modules:
* inputType
* outputType
* job parameters",4.0,0.3529807190421207,0.0,0.1636054125871913,True,True,True,1385538757000.0,1385717289000.0,1385720889000.0,1386043641000.0,1386044541000.0,1385538757000.0,-1.0,1385621506000.0,1385625106000.0,1385538757000.0,1385542357000.0,1385717289000.0,1385720889000.0,"eric.bottard",1385625106000.0,1385538757000.0,43,1385538757000.0,-1.0,"Basic support for Plugin contributed Module Options Metadata","Pending a better approach and extension point (see XD-1050), provide a way to factor out common recurring options for modules:
* inputType
* outputType
* job parameters",4.0,False,4.0,0,0.0,-1,0,True
"XD-1130","Story","Sprint 19","2013-11-26T11:08:46.000+0000","grenfro","grenfro","Update the EC2 deployer ","EC2 Deployer 
Needs to change its configuration behavior to use environment variables vs. the property files
* Remove ConfigureSystem class, since we will use environment variables instead
* Allow users to set environment variables via the xd-ec2 properties.
* If properties are not present use those that are available in the application.yml
* Utilize JClouds environment variable setup features to implement this behavior.",5.0,0.3738811589458176,0.3738789763736563,0.3738666084647423,True,True,True,1385464126000.0,1385978035000.0,1385981635000.0,1386837751000.0,1386838651000.0,1385464126000.0,-1.0,1385978015000.0,1385981615000.0,1385978032000.0,1385981632000.0,1385978035000.0,1385981635000.0,"grenfro",1385981615000.0,1385464126000.0,43,1385464126000.0,-1.0,"Update the EC2 deployer ","EC2 Deployer 
Needs to change its configuration behavior to use environment variables vs. the property files
* Remove ConfigureSystem class, since we will use environment variables instead
* Allow users to set environment variables via the xd-ec2 properties.
* If properties are not present use those that are available in the application.yml
* Utilize JClouds environment variable setup features to implement this behavior.",5.0,False,5.0,0,0.0,-1,0,False
"XD-1129","Story","Sprint 19","2013-11-26T09:14:13.000+0000","mark.pollack","luke","Enable --refresh-dependencies into be present when executing gradlew","This could be inside gradlew or a .settings file.",2.0,0.9994118039427328,0.0,0.0002971299670731,True,True,True,1385457253000.0,1385622067000.0,1385622164000.0,1385621264000.0,1385622164000.0,1385457253000.0,-1.0,1385457302000.0,1385460902000.0,1385457253000.0,1385460853000.0,1385622067000.0,1385622164000.0,"mark.pollack",1385460902000.0,1385457253000.0,43,1385457253000.0,-1.0,"Enable --refresh-dependencies into be present when executing gradlew","This could be inside gradlew or a .settings file.",2.0,False,2.0,0,0.0,0,0,False
"XD-1128","Story","Sprint 19","2013-11-26T09:00:39.000+0000","david_syer","david_syer","Disable JMX by default (and extend to other contexts than modules)","",1.0,0.9999904380885356,9.56191146434937e-06,0.9999598399718498,True,True,True,1385456439000.0,1385979342000.0,1385979347000.0,1385978447000.0,1385979347000.0,1385456439000.0,-1.0,1385979326000.0,1385979347000.0,1385456444000.0,1385460044000.0,1385979342000.0,1385979347000.0,"david_syer",1385979347000.0,1385456439000.0,43,1385456439000.0,-1.0,"Disable JMX by default (and extend to other contexts than modules)","",1.0,False,1.0,0,0.0,0,0,False
"XD-1127","Story","Sprint 21","2013-11-26T08:31:42.000+0000","mark.pollack","grussell","Add support for deploying a batch job with partitioning across multiple XD nodes.","This is a very big story, needs some planning/discussion before starting work.  Should be able to be implemented as a plugin. ",20.0,0.99999358206087,0.7856512851281314,0.7856490846918582,True,True,True,1385454702000.0,1390908131000.0,1390908166000.0,1390907266000.0,1390908166000.0,1385454702000.0,-1.0,1389739211000.0,1389742811000.0,1389739223000.0,1389742823000.0,1390908131000.0,1390908166000.0,"mark.pollack",1389742811000.0,1385454702000.0,43,1385454702000.0,-1.0,"Add support for deploying a batch job with partitioning across multiple XD nodes.","This is a very big story, needs some planning/discussion before starting work.  Should be able to be implemented as a plugin. ",20.0,False,20.0,0,0.0,0,1,False
"XD-1125","Story","","2013-11-26T04:41:07.000+0000","eric.bottard","","Provide ModuleOptionsMetadata using simple approach","Following merge of XD-953, provide module options using the ""simple"" approach where applicable",5.0,-1.0,-1.0,-1.0,False,False,False,1385440867000.0,-1.0,-1.0,1391116682000.0,1391117582000.0,1385440867000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1385440867000.0,-1.0,"Provide ModuleOptionsMetadata using simple approach","Following merge of XD-953, provide module options using the ""simple"" approach where applicable",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1124","Story","Sprint 19","2013-11-26T03:35:32.000+0000","eric.bottard","eric.bottard","Turn off RestTemplate logging in shell","Example:
{noformat}
xd:>module delete --name sink:file
12:31:19,495  WARN Spring Shell client.RestTemplate:566 - DELETE request for ""http://localhost:9393/modules/sink/file"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Cannot delete non-composed module sink:file
{noformat}

The WARN log is redundant with the command result and should be silenced",1.0,-1.0,0.0,0.9881831610044312,False,False,False,1385436932000.0,-1.0,-1.0,1385436932000.0,1385437609000.0,1385436932000.0,-1.0,1385437601000.0,1385437609000.0,1385436932000.0,1385437609000.0,-1.0,-1.0,"eric.bottard",1385437609000.0,1385436932000.0,43,1385436932000.0,-1.0,"Turn off RestTemplate logging in shell","Example:
{noformat}
xd:>module delete --name sink:file
12:31:19,495  WARN Spring Shell client.RestTemplate:566 - DELETE request for ""http://localhost:9393/modules/sink/file"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: Cannot delete non-composed module sink:file
{noformat}

The WARN log is redundant with the command result and should be silenced",1.0,False,1.0,0,0.0,-1,0,False
"XD-1123","Story","Sprint 19","2013-11-26T02:30:30.000+0000","eric.bottard","","Add profile activation on top of XD-953","",5.0,0.8726331657415718,0.8726294313373772,0.7633831710319777,True,True,True,1385433030000.0,1386134052000.0,1386137652000.0,1386235471000.0,1386236371000.0,1385433030000.0,-1.0,1386046287000.0,1386049887000.0,1386134049000.0,1386137649000.0,1386134052000.0,1386137652000.0,"eric.bottard",1386049887000.0,1385433030000.0,43,1385433030000.0,-1.0,"Add profile activation on top of XD-953","",5.0,False,5.0,0,0.0,0,0,False
"XD-1122","Story","Sprint 22","2013-11-26T01:46:43.000+0000","eric.bottard","iperumal","Add jmxPort to list of coerced cmd line options","Following merge of XD-1109.

See discussion at https://github.com/spring-projects/spring-xd/commit/eaf886eab3b2ef07da55575029ccabb2c8a36af9#commitcomment-4701947",2.0,0.9670079799157176,0.956573631694227,0.956574912579575,True,True,True,1385430403000.0,1392979931000.0,1392983531000.0,1393236603000.0,1393237503000.0,1385430403000.0,-1.0,1392898479000.0,1392902079000.0,1392898469000.0,1392902069000.0,1392979931000.0,1392983531000.0,"eric.bottard",1392902079000.0,1385430403000.0,43,1385430403000.0,-1.0,"Add jmxPort to list of coerced cmd line options","Following merge of XD-1109.

See discussion at https://github.com/spring-projects/spring-xd/commit/eaf886eab3b2ef07da55575029ccabb2c8a36af9#commitcomment-4701947",2.0,False,2.0,0,0.0,0,0,False
"XD-1120","Story","Sprint 31","2013-11-25T13:56:36.000+0000","hillert","hillert","Standardize Date/Time/TimeZone handling","We should we centrally standardize on date/time formats so that we don't create inconsistencies, and follow ISO 8601 internally. Internally we should only work with UTC (or make that the default config option).

Ultimately, whatever the user sees is just a formatting concern.",8.0,0.937649670195884,0.917391281048363,0.9343812020932566,True,True,True,1385387796000.0,1404284698000.0,1404288298000.0,1405540374000.0,1405541274000.0,1385387796000.0,-1.0,1404218827000.0,1404222427000.0,1403876421000.0,1403880021000.0,1404284698000.0,1404288298000.0,"hillert",1404222427000.0,1385387796000.0,43,1385387796000.0,-1.0,"Standardize Date/Time/TimeZone handling","We should we centrally standardize on date/time formats so that we don't create inconsistencies, and follow ISO 8601 internally. Internally we should only work with UTC (or make that the default config option).

Ultimately, whatever the user sees is just a formatting concern.",8.0,False,8.0,0,0.0,0,0,False
"XD-1119","Story","","2013-11-25T11:11:25.000+0000","iperumal","","Create scripts for batch DB creation","We need to create a shell script that calls the batch DB creation sql files for the JDBC option selected. 
",3.0,0.0849863648249841,0.0849831628269373,-1.0,True,False,True,1385377885000.0,1385457510000.0,1385461110000.0,1386313900000.0,1386314800000.0,1385377885000.0,-1.0,-1.0,-1.0,1385457507000.0,1385461107000.0,1385457510000.0,1385461110000.0,"iperumal",-1.0,-1.0,0,1385377885000.0,-1.0,"Create scripts for batch DB creation","We need to create a shell script that calls the batch DB creation sql files for the JDBC option selected. 
",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1118","Story","Sprint 19","2013-11-25T09:52:59.000+0000","mark.pollack","luke","Add starting of newly build XD server, running of smoketest bash script, and killing of XD server to CI test","",5.0,0.749316794088202,0.0,3.0064066525766408e-05,True,True,True,1385373179000.0,1385622419000.0,1385626019000.0,1385704902000.0,1385705802000.0,1385373179000.0,-1.0,1385373189000.0,1385376789000.0,1385373179000.0,1385376779000.0,1385622419000.0,1385626019000.0,"mark.pollack",1385376789000.0,1385373179000.0,43,1385373179000.0,-1.0,"Add starting of newly build XD server, running of smoketest bash script, and killing of XD server to CI test","",5.0,False,5.0,0,0.0,0,0,False
"XD-1117","Story","Sprint 19","2013-11-25T09:29:27.000+0000","mark.pollack","luke","Add bash based scripts of simple module create to src/main/scripts","",1.0,0.7504296175655204,0.0,4.490775946206492e-05,True,True,True,1385371767000.0,1385622424000.0,1385626024000.0,1385704885000.0,1385705785000.0,1385371767000.0,-1.0,1385371782000.0,1385375382000.0,1385371767000.0,1385375367000.0,1385622424000.0,1385626024000.0,"mark.pollack",1385375382000.0,1385371767000.0,43,1385371767000.0,-1.0,"Add bash based scripts of simple module create to src/main/scripts","",1.0,False,1.0,0,0.0,-1,0,False
"XD-1116","Story","Sprint 19","2013-11-25T09:25:34.000+0000","eric.bottard","","Add documentation about message store to aggregator doco","",2.0,0.9933339534306886,10.435137816637212,0.9611664031252908,True,True,True,1385371534000.0,1385435610000.0,1385436040000.0,1385435140000.0,1385436040000.0,1385371534000.0,-1.0,1385433535000.0,1385436040000.0,1386044663000.0,1385436040000.0,1385435610000.0,1385436040000.0,"eric.bottard",1385436040000.0,1385371534000.0,43,1385371534000.0,-1.0,"Add documentation about message store to aggregator doco","",2.0,False,2.0,0,0.0,-1,0,False
"XD-1111","Story","","2013-11-22T13:56:39.000+0000","mark.fisher","","Clear Redis after tests","The following keys remain after running the test suite:

{code}
redis 127.0.0.1:6379> keys *
1) ""containers""
2) ""containers.application:9292""
{code}

",2.0,-1.0,0.9489450599945518,-1.0,False,False,True,1385128599000.0,-1.0,-1.0,1386768598000.0,1386769498000.0,1385128599000.0,-1.0,-1.0,-1.0,1386685722000.0,1386689322000.0,-1.0,-1.0,"mark.fisher",-1.0,-1.0,0,1385128599000.0,-1.0,"Clear Redis after tests","The following keys remain after running the test suite:

{code}
redis 127.0.0.1:6379> keys *
1) ""containers""
2) ""containers.application:9292""
{code}

",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-1110","Story","Sprint 19","2013-11-22T07:57:21.000+0000","iperumal","iperumal","Fix JobRepoTests to use different batch job repo","Currently, the JobRepoTests use the same batch job repository that the XD runtime uses. Since the batch job repo doesn't delete the job instances, there would be stale data from this test. ",2.0,7.481012864089512e-05,0.0,4.878921433101855e-05,True,True,True,1385107041000.0,1385107064000.0,1385110664000.0,1385413586000.0,1385414486000.0,1385107041000.0,-1.0,1385107056000.0,1385110656000.0,1385107041000.0,1385110641000.0,1385107064000.0,1385110664000.0,"iperumal",1385110656000.0,1385107041000.0,43,1385107041000.0,-1.0,"Fix JobRepoTests to use different batch job repo","Currently, the JobRepoTests use the same batch job repository that the XD runtime uses. Since the batch job repo doesn't delete the job instances, there would be stale data from this test. ",2.0,False,2.0,0,0.0,0,0,False
"XD-1109","Story","Sprint 19","2013-11-22T07:48:10.000+0000","eric.bottard","eric.bottard","Provide options validation","Wherever they come from (cmd line args or ENV_VARS), options such as transport, analytics, etc should be validated and issues should be reported to users",2.0,0.8695055029655041,0.8694993440785381,0.8694531524262935,True,True,True,1385106490000.0,1385388848000.0,1385392448000.0,1385430324000.0,1385431224000.0,1385106490000.0,-1.0,1385388831000.0,1385392431000.0,1385388846000.0,1385392446000.0,1385388848000.0,1385392448000.0,"eric.bottard",1385392431000.0,1385106490000.0,43,1385106490000.0,-1.0,"Provide cmdline options validation","Wherever they come from (cmd line args or ENV_VARS), options such as transport, analytics, etc should be validated and issues should be reported to users",2.0,False,2.0,0,0.0,0,0,True
"XD-1108","Story","Sprint 19","2013-11-22T07:47:00.000+0000","eric.bottard","","Restore lax command line options","Restore --foo=bar as well as --foo bar

Validation of values should be done as a separate story",2.0,0.1712785619289254,0.1712781208542425,0.0360699848788571,True,True,True,1385106420000.0,1386659704000.0,1386663304000.0,1394174279000.0,1394175179000.0,1385106420000.0,-1.0,1385433530000.0,1385437130000.0,1386659700000.0,1386663300000.0,1386659704000.0,1386663304000.0,"eric.bottard",1385437130000.0,1385106420000.0,43,1385106420000.0,-1.0,"Restore lax command line options","Restore --foo=bar as well as --foo bar

Validation of values should be done as a separate story",2.0,False,2.0,0,0.0,0,0,False
"XD-1107","Story","Sprint 19","2013-11-21T23:27:52.000+0000","iperumal","iperumal","Create Mock on DistributedJobService instead of instantiating object","Currently, the tests use instantiated DistributedJobService object instead of a simple mock. We can just use mock object for the tests.",1.0,0.00190554476162,0.0,0.0016066357794051,True,True,True,1385076472000.0,1385076523000.0,1385080123000.0,1385102336000.0,1385103236000.0,1385076472000.0,-1.0,1385076515000.0,1385080115000.0,1385076472000.0,1385080072000.0,1385076523000.0,1385080123000.0,"iperumal",1385080115000.0,1385076472000.0,43,1385076472000.0,-1.0,"Create Mock on DistributedJobService instead of instantiating object","Currently, the tests use instantiated DistributedJobService object instead of a simple mock. We can just use mock object for the tests.",1.0,False,1.0,0,0.0,-1,0,False
"XD-1106","Improvement","Sprint 19","2013-11-21T09:58:58.000+0000","mark.pollack","luke","Create OOTB batch job that moved data from JDBC to HDFS","Basic inverse of current hdfsjdbc job.",6.0,0.5393713289263816,0.0,3.992023211800421e-05,True,True,True,1385027938000.0,1385622432000.0,1385626032000.0,1386129236000.0,1386130136000.0,1385027938000.0,-1.0,1385027982000.0,1385031582000.0,1385027938000.0,1385031538000.0,1385622432000.0,1385626032000.0,"mark.pollack",1385031582000.0,1385027959000.0,43,1385027959000.0,-1.0,"Create OOTB batch job that moved data from JDBC to HDFS","Basic inverse of current hdfsjdbc job.",6.0,False,6.0,0,0.0,-1,0,False
"XD-1105","Story","Sprint 22","2013-11-21T04:03:31.000+0000","eric.bottard","luke","Add some test coverage to mqtt modules","Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is

somesource | mqtt --topic=foo

with 

mqtt --topics=foo | somesink


And asserting that what is emitted to somesource ends up in somesink.

",3.0,0.9593723788631524,0.9593705866328294,0.996061237821795,True,True,True,1385006611000.0,1393571335000.0,1393574935000.0,1393933135000.0,1393934035000.0,1385006611000.0,-1.0,1393898872000.0,1393902472000.0,1393571319000.0,1393574919000.0,1393571335000.0,1393574935000.0,"eric.bottard",1393902472000.0,1385006611000.0,43,1385006611000.0,-1.0,"Add some test coverage to mqtt modules","Even though it may be hard to come up with a mqtt broker, an easy test that should be automated is

somesource | mqtt --topic=foo

with 

mqtt --topics=foo | somesink


And asserting that what is emitted to somesource ends up in somesink.

",3.0,False,3.0,0,0.0,-1,0,False
"XD-1104","Story","Sprint 22","2013-11-21T04:01:11.000+0000","eric.bottard","eric.bottard","Create Shell Integration test fixture for jdbc related sink","Would be nice to have some kind of regression testing on the jdbc sink, as it becomes more prominent in XD.

Use of an in memory db where we expose eg a JdbcTemplate to assert state",5.0,0.9994007781255214,0.9993999936570116,0.9993978643853426,True,True,True,1385006471000.0,1393924363000.0,1393927963000.0,1393928810000.0,1393929710000.0,1385006471000.0,-1.0,1393924337000.0,1393927937000.0,1393924356000.0,1393927956000.0,1393924363000.0,1393927963000.0,"eric.bottard",1393927937000.0,1385006471000.0,43,1385006471000.0,-1.0,"Create Shell Integration test fixture for jdbc related sink","Would be nice to have some kind of regression testing on the jdbc sink, as it becomes more prominent in XD.

Use of an in memory db where we expose eg a JdbcTemplate to assert state",5.0,False,5.0,0,0.0,0,0,False
"XD-1103","Bug","Sprint 19","2013-11-20T14:03:34.000+0000","thomas.risberg","Thomas Risberg","JDBC sink is broken - looks like some config options got booted","The JDBC sink is broken. Simple ""time | jdbc"" results in:

org.springframework.jdbc.BadSqlGrammarException: PreparedStatementCallback; bad SQL grammar [insert into test (payload) values(?)]; nested exception is java.sql.SQLSyntaxErrorException: user lacks privilege or object not found: TEST

Looks like some config options got clobbered during bootification. ",5.0,0.0324452693922476,0.7267143019802964,4.292419496801922e-05,True,True,True,1384956214000.0,1385028022000.0,1385031622000.0,1387168518000.0,1387169418000.0,1384956214000.0,1384956402000.0,1384956309000.0,1384959909000.0,1386564581000.0,1386568181000.0,1385028022000.0,1385031622000.0,"thomas.risberg",1384959909000.0,1384956402000.0,43,1384956402000.0,-1.0,"JDBC sink is broken - looks like some config options got booted","The JDBC sink is broken. Simple ""time | jdbc"" results in:

org.springframework.jdbc.BadSqlGrammarException: PreparedStatementCallback; bad SQL grammar [insert into test (payload) values(?)]; nested exception is java.sql.SQLSyntaxErrorException: user lacks privilege or object not found: TEST

Looks like some config options got clobbered during bootification. ",5.0,False,5.0,0,0.0,0,0,False
"XD-1102","Story","Sprint 19","2013-11-19T08:58:30.000+0000","mark.pollack","eric.bottard","Create microbenchmark for performace of redis and jdbc based aggregators","would be good to have a general feel for the general performance of these two options.  Redis can run on the same node as the benchmark.",8.0,0.831405988023952,0.0,7.18562874251497e-05,True,True,True,1384851510000.0,1385025066000.0,1385028666000.0,1385059360000.0,1385060260000.0,1384851510000.0,-1.0,1384851525000.0,1384855125000.0,1384851510000.0,1384855110000.0,1385025066000.0,1385028666000.0,"mark.pollack",1384855125000.0,1384851510000.0,43,1384851510000.0,-1.0,"Create microbenchmark for performace of redis and jdbc based aggregators","would be good to have a general feel for the general performance of these two options.  Redis can run on the same node as the benchmark.",8.0,False,8.0,0,0.0,0,0,False
"XD-1101","Improvement","Sprint 19","2013-11-19T08:57:33.000+0000","mark.pollack","eric.bottard","Create aggregator module that uses an embedded database stored in the local filesystem","Similar to XD-1100.  SI has the jdbc based message store.

<int-jdbc:message-store id=""messageStore"" data-source=""dataSource""
    table-prefix=""MY_INT_""/>

The configuration of this aggregator would be configured so that it uses an embedded database, hsqldb or H2 depending if there is any real perf benefit to one or the other, and store the data on the local file system.",8.0,0.3649805185110266,0.0,3.852952083724649e-05,True,True,True,1384851453000.0,1384927235000.0,1384930835000.0,1385058186000.0,1385059086000.0,1384851453000.0,-1.0,1384851461000.0,1384855061000.0,1384851453000.0,1384855053000.0,1384927235000.0,1384930835000.0,"mark.pollack",1384855061000.0,1384851453000.0,43,1384851453000.0,-1.0,"Create aggregator module that uses an embedded database stored in the local filesystem","Similar to XD-1100.  SI has the jdbc based message store.

<int-jdbc:message-store id=""messageStore"" data-source=""dataSource""
    table-prefix=""MY_INT_""/>

The configuration of this aggregator would be configured so that it uses an embedded database, hsqldb or H2 depending if there is any real perf benefit to one or the other, and store the data on the local file system.",8.0,False,8.0,0,0.0,0,0,False
"XD-1100","Improvement","Sprint 19","2013-11-19T08:53:37.000+0000","mark.pollack","eric.bottard","Create aggregator module that uses redis as a message store","There is a need for a persistent store for messages so that they can survive the crash of a container node.  The redis implementation in SI is useful since users may already be using redis for other features such as analytics.

This module would sit alongside the current in-memory based aggregator.  It brings in redis as a dependency and there should be configuration options exposed so that one can use a different version of redis as compared to the one that maybe used for analytics.  

Due to the need to have redis on the CP, and the large number of different options that each message store implementation provides and the different 3rd party library dependencies, I would like to avoid going down the path of using a profile here as it would seem to go beyond what we had discussed for profile support.  We can revisit as the module contribution story based on boot unfolds.",4.0,0.3656927880382752,0.0,8.65954979962764e-05,True,True,True,1384851217000.0,1384927231000.0,1384930831000.0,1385058180000.0,1385059080000.0,1384851217000.0,-1.0,1384851235000.0,1384854835000.0,1384851217000.0,1384854817000.0,1384927231000.0,1384930831000.0,"mark.pollack",1384854835000.0,1384851217000.0,43,1384851217000.0,-1.0,"Create aggregator module that uses redis as a message store","There is a need for a persistent store for messages so that they can survive the crash of a container node.  The redis implementation in SI is useful since users may already be using redis for other features such as analytics.

This module would sit alongside the current in-memory based aggregator.  It brings in redis as a dependency and there should be configuration options exposed so that one can use a different version of redis as compared to the one that maybe used for analytics.  

Due to the need to have redis on the CP, and the large number of different options that each message store implementation provides and the different 3rd party library dependencies, I would like to avoid going down the path of using a profile here as it would seem to go beyond what we had discussed for profile support.  We can revisit as the module contribution story based on boot unfolds.",4.0,False,4.0,0,0.0,0,0,False
"XD-1096","Story","Sprint 19","2013-11-19T01:31:07.000+0000","iperumal","iperumal","Make Batch Job controllers HATEOAS compliant","Currently, the BatchJobsController and BatchJobExecutionsController are not HATEOAS compliant and we need make them so.",3.0,5.288682747788008e-05,0.0,0.0394341814617566,True,True,True,1384824667000.0,1384824697000.0,1384828297000.0,1385391016000.0,1385391916000.0,1384824667000.0,-1.0,1384847036000.0,1384850636000.0,1384824667000.0,1384828267000.0,1384824697000.0,1384828297000.0,"iperumal",1384850636000.0,1384824667000.0,43,1384824667000.0,-1.0,"Make Batch Job controllers HATEOAS compliant","Currently, the BatchJobsController and BatchJobExecutionsController are not HATEOAS compliant and we need make them so.",3.0,False,3.0,0,0.0,0,0,False
"XD-1095","Improvement","Sprint 19","2013-11-18T21:01:50.000+0000","mark.pollack","iperumal","Create a shell command for stopping all job executions","",2.0,0.8321354406272801,0.0,0.015007321357133,True,True,True,1384808510000.0,1386944724000.0,1386948324000.0,1387374757000.0,1387375657000.0,1384808510000.0,-1.0,1384847036000.0,1384850636000.0,1384808510000.0,1384812110000.0,1386944724000.0,1386948324000.0,"mark.pollack",1384850636000.0,1384808510000.0,43,1384808510000.0,-1.0,"Create a shell command for stopping all job executions","",2.0,False,2.0,0,0.0,0,0,False
"XD-1094","Improvement","Sprint 19","2013-11-18T21:01:12.000+0000","mark.pollack","iperumal","Create shell command for stopping a specific job","",2.0,0.0007151979565772,0.0,0.0205214985100042,True,True,True,1384808472000.0,1384809816000.0,1384813416000.0,1386686772000.0,1386687672000.0,1384808472000.0,-1.0,1384847036000.0,1384850636000.0,1384808472000.0,1384812072000.0,1384809816000.0,1384813416000.0,"mark.pollack",1384850636000.0,1384808472000.0,43,1384808472000.0,-1.0,"Create shell command for stopping a specific job","",2.0,False,2.0,0,0.0,0,0,False
"XD-1093","Improvement","Sprint 19","2013-11-18T21:00:40.000+0000","mark.pollack","hillert","Create shell command for getting information on a given step execution","",2.0,0.9650595535743522,0.0,0.005355131996872,True,True,True,1384808440000.0,1391763906000.0,1391767506000.0,1392014832000.0,1392015732000.0,1384808440000.0,-1.0,1384847036000.0,1384850636000.0,1384808440000.0,1384812040000.0,1391763906000.0,1391767506000.0,"mark.pollack",1384850636000.0,1384808440000.0,43,1384808440000.0,-1.0,"Create shell command for getting information on a given step execution","",2.0,False,2.0,0,0.0,0,0,False
"XD-1092","Improvement","Sprint 19","2013-11-18T21:00:08.000+0000","mark.pollack","hillert","Create shell command for getting information on all steps of a given job execution","",2.0,0.7174526757797067,0.0,0.0218007472542357,True,True,True,1384808408000.0,1386079605000.0,1386083205000.0,1386579328000.0,1386580228000.0,1384808408000.0,-1.0,1384847035000.0,1384850635000.0,1384808408000.0,1384812008000.0,1386079605000.0,1386083205000.0,"mark.pollack",1384850635000.0,1384808408000.0,43,1384808408000.0,-1.0,"Create shell command for getting information on all steps of a given job execution","",2.0,False,2.0,0,0.0,-1,0,False
"XD-1091","Improvement","Sprint 19","2013-11-18T20:59:32.000+0000","mark.pollack","iperumal","Create shell command for getting information on the progress of a given step execution","",2.0,0.0883393350933019,0.0,0.0198499611348819,True,True,True,1384808372000.0,1384980436000.0,1384984036000.0,1386755234000.0,1386756134000.0,1384808372000.0,-1.0,1384847035000.0,1384850635000.0,1384808372000.0,1384811972000.0,1384980436000.0,1384984036000.0,"mark.pollack",1384850635000.0,1384808372000.0,43,1384808372000.0,-1.0,"Create shell command for getting information on the progress of a given step execution","",2.0,False,2.0,0,0.0,0,0,False
"XD-1090","Improvement","Sprint 19","2013-11-18T20:57:31.000+0000","mark.pollack","Ilayaperumal Gopinathan","Create shell command for restarting a specific job instance","",2.0,0.8862171946045481,0.8763654754197788,0.0046087023479855,True,True,True,1384808251000.0,1392266109000.0,1392269709000.0,1393222735000.0,1393223635000.0,1384808251000.0,-1.0,1384847035000.0,1384850635000.0,1392183203000.0,1392186803000.0,1392266109000.0,1392269709000.0,"mark.pollack",1384850635000.0,1384808251000.0,43,1384808251000.0,-1.0,"Create shell command for restarting a specific job instance","",2.0,False,2.0,0,0.0,0,0,False
"XD-1089","Improvement","Sprint 19","2013-11-18T20:56:43.000+0000","mark.pollack","hillert","Create shell command for getting information on all job executions for a given name","",2.0,0.2410870111167979,0.0,0.0308612736792245,True,True,True,1384808203000.0,1385111557000.0,1385115157000.0,1386065579000.0,1386066479000.0,1384808203000.0,-1.0,1384847035000.0,1384850635000.0,1384808203000.0,1384811803000.0,1385111557000.0,1385115157000.0,"mark.pollack",1384850635000.0,1384808203000.0,43,1384808203000.0,-1.0,"Create shell command for getting information on all job executions for a given name","",2.0,False,2.0,0,0.0,-1,0,False
"XD-1088","Improvement","Sprint 19","2013-11-18T20:52:24.000+0000","mark.pollack","iperumal","Create REST API for getting information on the progress of a given step execution","There is a NPE in the current spring batch admin functionality.

Should be springmvc test framework style tests.

GET /batch/jobs/executions/{jobExecutionId}/steps/{stepExecutionId}/progress",4.0,0.0982089648768634,0.0,0.0200651574810222,True,True,True,1384807944000.0,1384999275000.0,1385002875000.0,1386755247000.0,1386756147000.0,1384807944000.0,-1.0,1384847035000.0,1384850635000.0,1384807944000.0,1384811544000.0,1384999275000.0,1385002875000.0,"mark.pollack",1384850635000.0,1384807944000.0,43,1384807944000.0,-1.0,"Create REST API for getting information on the progress of a given step execution","There is a NPE in the current spring batch admin functionality.

Should be springmvc test framework style tests.

GET /batch/jobs/executions/{jobExecutionId}/steps/{stepExecutionId}/progress",4.0,False,4.0,0,0.0,0,0,False
"XD-1087","Improvement","Sprint 19","2013-11-18T20:48:59.000+0000","mark.pollack","Ilayaperumal Gopinathan","Create REST API for restarting a specific job instance","Functionality adopted from spring-batch admin
Should include springmvc test framework style tests

POST /batch/jobs/{jobName}/{jobInstanceId}/executions - restart a specific job instance

",3.0,0.2920520795446887,0.949028092427604,0.0053609690999394,True,True,True,1384807739000.0,1386948486000.0,1386952086000.0,1392136857000.0,1392137757000.0,1384807739000.0,-1.0,1384847035000.0,1384850635000.0,1391764132000.0,1391767732000.0,1386948486000.0,1386952086000.0,"mark.pollack",1384850635000.0,1384807739000.0,43,1384807739000.0,-1.0,"Create REST API for restarting a specific job instance","Functionality adopted from spring-batch admin
Should include springmvc test framework style tests

POST /batch/jobs/{jobName}/{jobInstanceId}/executions - restart a specific job instance

",3.0,False,3.0,0,0.0,-1,0,False
"XD-1086","Improvement","Sprint 19","2013-11-18T20:47:02.000+0000","mark.pollack","iperumal","Create REST API for stopping all job executions","Adopted functionality from spring batch admin
Should include SpringMVC test framework style tests.

DELETE /batch/jobs/executions/ - stop all job executions",3.0,0.827608322332683,0.0,0.0153479515426221,True,True,True,1384807622000.0,1386932945000.0,1386936545000.0,1387374752000.0,1387375652000.0,1384807622000.0,-1.0,1384847036000.0,1384850636000.0,1384807622000.0,1384811222000.0,1386932945000.0,1386936545000.0,"mark.pollack",1384850636000.0,1384807633000.0,43,1384807633000.0,-1.0,"Create REST API for stopping all job executions","Adopted functionality from spring batch admin
Should include SpringMVC test framework style tests.

DELETE /batch/jobs/executions/ - stop all job executions",3.0,False,3.0,0,0.0,0,0,False
"XD-1085","Improvement","Sprint 19","2013-11-18T20:45:53.000+0000","mark.pollack","iperumal","Create REST API for stopping a specific job","Functionality adopted from spring batch admin
Should include springmvc test framework style tests

DELETE /batch/jobs/executions/{executionId} Stop a specific job",3.0,0.0001847447059095,0.0,0.0536343766428355,True,True,True,1384807553000.0,1384807689000.0,1384811289000.0,1385542804000.0,1385543704000.0,1384807553000.0,-1.0,1384847036000.0,1384850636000.0,1384807553000.0,1384811153000.0,1384807689000.0,1384811289000.0,"mark.pollack",1384850636000.0,1384807553000.0,43,1384807553000.0,-1.0,"Create REST API for stopping a specific job","Functionality adopted from spring batch admin
Should include springmvc test framework style tests

DELETE /batch/jobs/executions/{executionId} Stop a specific job",3.0,False,3.0,0,0.0,0,1,False
"XD-1084","Improvement","Sprint 19","2013-11-18T20:45:04.000+0000","mark.pollack","hillert","Create REST API for getting information on a given step execution","Functionality adopted from spring batch admin
Should include springmvc test framework style tests

GET /batch/jobs/executions/{executionId}/steps/{stepId} - Get information on a given step execution",4.0,0.983532381082906,0.0,0.0058069854515418,True,True,True,1384807504000.0,1391503061000.0,1391506661000.0,1391614267000.0,1391615167000.0,1384807504000.0,-1.0,1384847036000.0,1384850636000.0,1384807504000.0,1384811104000.0,1391503061000.0,1391506661000.0,"mark.pollack",1384850636000.0,1384807504000.0,43,1384807504000.0,-1.0,"Create REST API for getting information on a given step execution","Functionality adopted from spring batch admin
Should include springmvc test framework style tests

GET /batch/jobs/executions/{executionId}/steps/{stepId} - Get information on a given step execution",4.0,False,4.0,0,0.0,0,1,False
"XD-1083","Improvement","Sprint 19","2013-11-18T20:44:07.000+0000","mark.pollack","hillert","Create REST API for getting information on all steps of a given job execution","Adopted functionality from spring batch admin
Should include springmvc test framework style tests

GET /batch/jobs/executions/{executionId}/steps - Get information on all steps of a given job execution",4.0,0.7562209709644775,0.0,0.0223308517515289,True,True,True,1384807447000.0,1386148071000.0,1386151671000.0,1386579341000.0,1386580241000.0,1384807447000.0,-1.0,1384847035000.0,1384850635000.0,1384807447000.0,1384811047000.0,1386148071000.0,1386151671000.0,"mark.pollack",1384850635000.0,1384807447000.0,43,1384807447000.0,-1.0,"Create REST API for getting information on all steps of a given job execution","Adopted functionality from spring batch admin
Should include springmvc test framework style tests

GET /batch/jobs/executions/{executionId}/steps - Get information on all steps of a given job execution",4.0,False,4.0,0,0.0,-1,1,False
"XD-1082","Improvement","Sprint 19","2013-11-18T20:43:11.000+0000","mark.pollack","hillert","Create REST API for getting information on all job executions for a given name","Adopted functionality from Spring Batch admin
Should include springmvc test framework style tests

GET /batch/jobs/executions/{executionId} - Get information on all executions of a given job name.",2.0,0.2378159080690527,0.0,0.0690876264765031,True,True,True,1384807391000.0,1384943855000.0,1384947455000.0,1385380313000.0,1385381213000.0,1384807391000.0,-1.0,1384847035000.0,1384850635000.0,1384807391000.0,1384810991000.0,1384943855000.0,1384947455000.0,"mark.pollack",1384850635000.0,1384807391000.0,43,1384807391000.0,-1.0,"Create REST API for getting information on a job execution for a given execution id","Adopted functionality from Spring Batch admin
Should include springmvc test framework style tests

GET /batch/jobs/executions/{executionId} - Get information on all executions of a given job name.",2.0,False,2.0,0,0.0,0,3,True
"XD-1081","Story","Sprint 19","2013-11-18T20:24:03.000+0000","mark.pollack","hillert","Create documentation on the lifcycle of a job in XD","This should go in the first section of the Batch Job documentation.

Here is a rough suggestion

The lifecycle of a job in XD.

1. Register a job module with the MoudleRegistry by putting a XML/jar files in the $XD_HOME/modules/jobs directory.
2. Create a job definition from a job module by providing a name and properties that apply to all job instances can be configured at this point.  The job is not yet deployed
3. Deploy the job to an xd-container.  A job instance can not be created by sending a message to a job queue that contains optional runtime job parameters
4. Launch a job by sending a message to the job queue with job parametres.  A Job Instance is created, representing a specific run of the job.  A Job Instance is the Job Definition plus the runtime job parameters.  You can query for the Job Instances associated with a given job name
5. The job is executed creating a Job Exection object that captures the success or failure of the job.  You can query for the Job Executions associated with a given job name.
6. Undeploy a job.  This removes the job from xd-container.",1.0,0.9724468906188494,0.0,0.0018775760227037,True,True,True,1384806243000.0,1405932994000.0,1405936594000.0,1406530695000.0,1406531595000.0,1384806243000.0,-1.0,1384847034000.0,1384850634000.0,1384806243000.0,1384809843000.0,1405932994000.0,1405936594000.0,"mark.pollack",1384850634000.0,1384806243000.0,43,1384806243000.0,-1.0,"Create documentation on the lifcycle of a job in XD","This should go in the first section of the Batch Job documentation.

Here is a rough suggestion

The lifecycle of a job in XD.

1. Register a job module with the MoudleRegistry by putting a XML/jar files in the $XD_HOME/modules/jobs directory.
2. Create a job definition from a job module by providing a name and properties that apply to all job instances can be configured at this point.  The job is not yet deployed
3. Deploy the job to an xd-container.  A job instance can not be created by sending a message to a job queue that contains optional runtime job parameters
4. Launch a job by sending a message to the job queue with job parametres.  A Job Instance is created, representing a specific run of the job.  A Job Instance is the Job Definition plus the runtime job parameters.  You can query for the Job Instances associated with a given job name
5. The job is executed creating a Job Exection object that captures the success or failure of the job.  You can query for the Job Executions associated with a given job name.
6. Undeploy a job.  This removes the job from xd-container.",1.0,False,1.0,0,0.0,0,0,False
"XD-1079","Story","Sprint 19","2013-11-18T19:51:25.000+0000","mark.pollack","iperumal","Small method name refactorings and add Javadoc in batch controllers","BatchJobsController.listForJob should be 'executionsForJob'
BatchJobsController.jobInstances should be 'instancesForJob'

The JavaDoc for the class and each method should be more descriptive about their functionality


",1.0,0.061323178993599,0.0,0.9986684109704248,True,True,True,1384804285000.0,1384806910000.0,1384810510000.0,1384846191000.0,1384847091000.0,1384804285000.0,-1.0,1384847034000.0,1384847091000.0,1384804285000.0,1384807885000.0,1384806910000.0,1384810510000.0,"mark.pollack",1384847091000.0,1384804285000.0,43,1384804285000.0,-1.0,"Small method name refactorings and add Javadoc in batch controllers","BatchJobsController.listForJob should be 'executionsForJob'
BatchJobsController.jobInstances should be 'instancesForJob'

The JavaDoc for the class and each method should be more descriptive about their functionality


",1.0,False,1.0,0,0.0,0,0,False
"XD-1078","Bug","Sprint 19","2013-11-18T11:20:27.000+0000","hillert","hillert","Add ""spring-xd-exec"" directory to Git repo","Without the directory, Spring XD cannot be imported into STS using its own Gradle support.",1.0,-1.0,0.0,0.7402365610914481,False,True,True,1384773627000.0,-1.0,-1.0,1384871898000.0,1384872798000.0,1384773627000.0,-1.0,1384847037000.0,1384850637000.0,1384773627000.0,1384777227000.0,-1.0,-1.0,"hillert",1384850637000.0,1384773627000.0,43,1384773627000.0,-1.0,"Add ""spring-xd-exec"" directory to Git repo","Without the directory, Spring XD cannot be imported into STS using its own Gradle support.",1.0,False,1.0,0,0.0,0,1,False
"XD-1077","Story","Sprint 19","2013-11-18T09:02:50.000+0000","mark.pollack","","Proof of Concept for module contributions based off Boot","Develop out a Proof of concept for review that allows users to easily create a new Java based module to Spring XD.

An ideal flow would be

1.  In the shell, have a command to create a new custom module - for example a stream processor.  This would pick the appropriate starter pom to create an XD application.
2. Open up the project in an IDE.   There should be a shell of code that has a Processor module and also something in the test directory the shell of some code for testing the module.  
3. On the command line, issue a command equivalent to mvn assembly:assembly that would build, test and create an packaged file (single) that can be dropped into a module registry directory ($XD_HOME/modules/processors/).

The same general flow would apply for batch jobs.  

ATM XML centric stream defs/batch job defs would be sufficient.  Having the option for JavaConfig style batch job definitions would be nice as there isn't a DSL for batch jobs (and likely won't be except for a trivial case).

One shouldn't have to restart the xd-container in order to use this new processor.",40.0,0.0053389601385438,0.0053386625616037,0.0027001139620488,True,True,True,1384765370000.0,1384926843000.0,1384930443000.0,1415008749000.0,1415009649000.0,1384765370000.0,-1.0,1384847033000.0,1384850633000.0,1384926834000.0,1384930434000.0,1384926843000.0,1384930443000.0,"mark.pollack",1384850633000.0,1384765370000.0,43,1384765370000.0,-1.0,"Proof of Concept for module contributions based off Boot","Develop out a Proof of concept for review that allows users to easily create a new module to Spring XD. Key features to be enabled:

1. Allow existing modules to be included as a classpath dependency
2. Consolidate shared libraries in existing modules (e.g. a shared lib for all gemfire modules)
3. User develops a module as a normal Java project
4. User's project can then be added to a runtime container in the same way (as a dependency)

Nice to have: 

1. at development time container can run continuously and pick up changes from user's module project.
2. Java config for a module.
3. command line functionality to create project scaffolding.

",40.0,False,40.0,0,0.0,0,1,True
"XD-1073","Story","Sprint 19","2013-11-15T11:53:52.000+0000","mark.pollack","grenfro","Change default container port from 9000 to something else","Change the default port since it conflicts with the default port for the http source",1.0,0.9959051865298064,0.0,0.3866797507321774,True,True,True,1384516432000.0,1385367915000.0,1385371416000.0,1385370516000.0,1385371416000.0,1384516432000.0,-1.0,1384847037000.0,1384850637000.0,1384516432000.0,1384520032000.0,1385367915000.0,1385371416000.0,"mark.pollack",1384850637000.0,1384516432000.0,43,1384516432000.0,-1.0,"Change default container port from 9000 to something else","Change the default port since it conflicts with the default port for the http source",1.0,False,1.0,0,0.0,0,3,False
"XD-1072","Improvement","Sprint 19","2013-11-15T11:00:46.000+0000","dturanski","dturanski","Add simple bridge module","Add a bridge module per XD-956 to support definitions like topic:foo > queue:bar . Convenient for testing for XD-1066",1.0,0.9420767908866686,0.0,0.6241667087402787,True,True,True,1384513246000.0,1385017047000.0,1385020647000.0,1385047123000.0,1385048023000.0,1384513246000.0,-1.0,1384847036000.0,1384850636000.0,1384513246000.0,1384516846000.0,1385017047000.0,1385020647000.0,"dturanski",1384850636000.0,1384513246000.0,43,1384513246000.0,-1.0,"Add bridge module","Add a bridge module per XD-956 to support definitions like topic:foo > queue:bar . Convenient for testing for XD-1066",1.0,False,1.0,0,0.0,0,0,True
"XD-1071","Bug","Sprint 19","2013-11-15T08:57:20.000+0000","mark.pollack","luke","Close HDFS file when Batch job ends","The File to HDFS batch job will not close the file being written to in HDFS when the job completes.  The ItemWriter for HDFS needs to incorporate functionality that is present in the standard FlatFileWriter, perhaps inheriting from AbstractItemStreamItemWriter",6.0,0.2312092643295404,0.0,0.2311164638830145,True,True,True,1384505840000.0,1384847171000.0,1384850771000.0,1385981226000.0,1385982126000.0,1384505840000.0,-1.0,1384847034000.0,1384850634000.0,1384505840000.0,1384509440000.0,1384847171000.0,1384850771000.0,"mark.pollack",1384850634000.0,1384505840000.0,43,1384505840000.0,-1.0,"Close HDFS file when Batch job ends","The File to HDFS batch job will not close the file being written to in HDFS when the job completes.  The ItemWriter for HDFS needs to incorporate functionality that is present in the standard FlatFileWriter, perhaps inheriting from AbstractItemStreamItemWriter",6.0,False,6.0,0,0.0,0,0,False
"XD-1070","Bug","Sprint 19","2013-11-15T08:55:06.000+0000","mark.pollack","luke","File to HDFS batch job fails due to ""/data"" directory not available in HDFS","The batch job for File to HDFS will try to check for the default '/data/' directory even if the target directory in HDFS is something else.  If the /data directory isn't there, the job will fail.

This should be fixed so there isn't a check on the directory that isn't the final HDFS target directory and the target directory should be created if it doesn't exist.",4.0,0.2937788283742321,0.0,0.231187813683191,True,True,True,1384505706000.0,1384939444000.0,1384943044000.0,1385981216000.0,1385982116000.0,1384505706000.0,-1.0,1384847034000.0,1384850634000.0,1384505706000.0,1384509306000.0,1384939444000.0,1384943044000.0,"mark.pollack",1384850634000.0,1384505706000.0,43,1384505706000.0,-1.0,"File to HDFS batch job fails due to ""/data"" directory not available in HDFS","The batch job for File to HDFS will try to check for the default '/data/' directory even if the target directory in HDFS is something else.  If the /data directory isn't there, the job will fail.

This should be fixed so there isn't a check on the directory that isn't the final HDFS target directory and the target directory should be created if it doesn't exist.",4.0,False,4.0,0,0.0,0,0,False
"XD-1068","Story","Sprint 24","2013-11-15T08:15:39.000+0000","mark.pollack","iperumal","Remove existing 'purpose built' json processors and ensure all functionality is still available with #jsonPath based SpEL expression based processors","",2.0,0.9997578049111404,0.9992787809466162,0.9992787809466162,True,True,True,1384503339000.0,1395681700000.0,1395684408000.0,1395683508000.0,1395684408000.0,1384503339000.0,-1.0,1395676344000.0,1395679944000.0,1395676344000.0,1395679944000.0,1395681700000.0,1395684408000.0,"mark.pollack",1395679944000.0,1384503339000.0,43,1384503339000.0,-1.0,"Remove existing 'purpose built' json processors and ensure all functionality is still available with #jsonPath based SpEL expression based processors","",2.0,False,2.0,0,0.0,0,1,False
"XD-1067","Improvement","Sprint 19","2013-11-14T04:48:14.000+0000","mark.fisher","Eric Bottard","Container with redis as transport emits stack trace on shutdown","This is ""harmless"" (the attempt to brpop continues even after the connection itself has been closed), but ugly:

{code}
12:45:15,084 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 inbound.RedisQueueMessageDrivenEndpoint:181 - Failed to execute listening task. Will attempt to resubmit in 5000 milliseconds.
org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is com.lambdaworks.redis.RedisException: Connection closed
	at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:46)
	at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:36)
	at org.springframework.data.redis.connection.lettuce.LettuceConverters.toDataAccessException(LettuceConverters.java:159)
	at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:253)
	at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1508)
	at org.springframework.data.redis.core.DefaultListOperations$12.inRedis(DefaultListOperations.java:163)
	at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:51)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:185)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:153)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:86)
	at org.springframework.data.redis.core.DefaultListOperations.rightPop(DefaultListOperations.java:160)
	at org.springframework.data.redis.core.DefaultBoundListOperations.rightPop(DefaultBoundListOperations.java:105)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:178)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:286)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:724)
Caused by: com.lambdaworks.redis.RedisException: Connection closed
	at com.lambdaworks.redis.RedisAsyncConnection.await(RedisAsyncConnection.java:1079)
	at com.lambdaworks.redis.RedisConnection.await(RedisConnection.java:820)
	at com.lambdaworks.redis.RedisConnection.brpop(RedisConnection.java:101)
	at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1506)
	... 12 more
{code}

We should shutdown the consumer gracefully (i.e. before the connection is closed).",3.0,0.5688373116346656,0.5688363683783703,0.1043578676653864,True,True,True,1384404494000.0,1386816722000.0,1386820322000.0,1388644223000.0,1388645123000.0,1384404494000.0,-1.0,1384847037000.0,1384850637000.0,1386816718000.0,1386820318000.0,1386816722000.0,1386820322000.0,"mark.fisher",1384850637000.0,1384404494000.0,43,1384404494000.0,-1.0,"Container with redis as transport emits stack trace on shutdown","This is ""harmless"" (the attempt to brpop continues even after the connection itself has been closed), but ugly:

{code}
12:45:15,084 ERROR redisInboundAdapter-redis:queue-inbound-channel-adapter1 inbound.RedisQueueMessageDrivenEndpoint:181 - Failed to execute listening task. Will attempt to resubmit in 5000 milliseconds.
org.springframework.data.redis.RedisSystemException: Redis exception; nested exception is com.lambdaworks.redis.RedisException: Connection closed
	at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:46)
	at org.springframework.data.redis.connection.lettuce.LettuceExceptionConverter.convert(LettuceExceptionConverter.java:36)
	at org.springframework.data.redis.connection.lettuce.LettuceConverters.toDataAccessException(LettuceConverters.java:159)
	at org.springframework.data.redis.connection.lettuce.LettuceConnection.convertLettuceAccessException(LettuceConnection.java:253)
	at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1508)
	at org.springframework.data.redis.core.DefaultListOperations$12.inRedis(DefaultListOperations.java:163)
	at org.springframework.data.redis.core.AbstractOperations$ValueDeserializingRedisCallback.doInRedis(AbstractOperations.java:51)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:185)
	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:153)
	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:86)
	at org.springframework.data.redis.core.DefaultListOperations.rightPop(DefaultListOperations.java:160)
	at org.springframework.data.redis.core.DefaultBoundListOperations.rightPop(DefaultBoundListOperations.java:105)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.popMessageAndSend(RedisQueueMessageDrivenEndpoint.java:178)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint.access$300(RedisQueueMessageDrivenEndpoint.java:51)
	at org.springframework.integration.redis.inbound.RedisQueueMessageDrivenEndpoint$ListenerTask.run(RedisQueueMessageDrivenEndpoint.java:286)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at java.lang.Thread.run(Thread.java:724)
Caused by: com.lambdaworks.redis.RedisException: Connection closed
	at com.lambdaworks.redis.RedisAsyncConnection.await(RedisAsyncConnection.java:1079)
	at com.lambdaworks.redis.RedisConnection.await(RedisConnection.java:820)
	at com.lambdaworks.redis.RedisConnection.brpop(RedisConnection.java:101)
	at org.springframework.data.redis.connection.lettuce.LettuceConnection.bRPop(LettuceConnection.java:1506)
	... 12 more
{code}

We should shutdown the consumer gracefully (i.e. before the connection is closed).",3.0,False,3.0,0,0.0,0,0,False
"XD-1066","Bug","Sprint 19","2013-11-14T03:37:17.000+0000","mark.fisher","dturanski","Topic channels are not broadcasting","This needs to work for all transports (local, rabbit, and redis), and we need to ensure that we have test coverage for each of those to avoid any regressions.

The incorrect behavior was observed with all three transports:

{code}
xd:>stream create a --definition ""topic:foo > transform --expression=payload+'-a' | log""
Created new stream 'a'

xd:>stream create b --definition ""topic:foo > transform --expression=payload+'-b' | log""
Created new stream 'b'

xd:>stream create s --definition ""http > topic:foo""
Created new stream 's'

xd:>http post --data hi
> POST (text/plain;Charset=UTF-8) http://localhost:9000 hi
> 200 OK

// only one line in log!
{code}
",5.0,0.9522133456833004,0.1414982053953919,0.6897410366253715,True,True,True,1384400237000.0,1385017057000.0,1385020657000.0,1385047112000.0,1385048012000.0,1384400237000.0,-1.0,1384847034000.0,1384850634000.0,1384491896000.0,1384495496000.0,1385017057000.0,1385020657000.0,"mark.fisher",1384850634000.0,1384400237000.0,43,1384400237000.0,-1.0,"Topic channels are not broadcasting","This needs to work for all transports (local, rabbit, and redis), and we need to ensure that we have test coverage for each of those to avoid any regressions.

The incorrect behavior was observed with all three transports:

{code}
xd:>stream create a --definition ""topic:foo > transform --expression=payload+'-a' | log""
Created new stream 'a'

xd:>stream create b --definition ""topic:foo > transform --expression=payload+'-b' | log""
Created new stream 'b'

xd:>stream create s --definition ""http > topic:foo""
Created new stream 's'

xd:>http post --data hi
> POST (text/plain;Charset=UTF-8) http://localhost:9000 hi
> 200 OK

// only one line in log!
{code}
",5.0,False,5.0,0,0.0,0,0,False
"XD-1065","Bug","Sprint 19","2013-11-14T03:00:17.000+0000","mark.fisher","Mark Fisher","Extra ""job"" queues being created for all streams","This is most likely an issue for any transport, since it's probably happening within the JobPlugin, but I noticed when using Rabbit that every stream I created also triggered creation of ""job:[streamname]"" Queues.",2.0,0.966721992617772,0.963013918000314,0.1771087825477127,True,True,True,1384398017000.0,1386848926000.0,1386852526000.0,1386932395000.0,1386933295000.0,1384398017000.0,-1.0,1384847037000.0,1384850637000.0,1386839525000.0,1386843125000.0,1386848926000.0,1386852526000.0,"mark.fisher",1384850637000.0,1384398017000.0,43,1384398017000.0,-1.0,"Extra ""job"" queues being created for all streams","This is most likely an issue for any transport, since it's probably happening within the JobPlugin, but I noticed when using Rabbit that every stream I created also triggered creation of ""job:[streamname]"" Queues.",2.0,False,2.0,0,0.0,0,0,False
"XD-1064","Story","Sprint 19","2013-11-14T02:51:48.000+0000","mark.fisher","dturanski","Update router sink logic to match new channel syntax","for example, the following should work:

{code}
xd:>stream create a1 --definition ""queue:a > transform --expression=payload+'-a' | log""
Created new stream 'a1'

xd:>stream create b1 --definition ""queue:b > transform --expression=payload+'-b' | log""
Created new stream 'b1'

xd:>stream create s1 --definition ""http | router --expression=payload.contains('a')?'queue:a':'queue:b'""
Created new stream 's1'

xd:>http post --data ""ha""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 ha
> 200 OK

// log shows: ""ha-a""

xd:>http post --data ""hi""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 hi
> 200 OK

// log shows: ""ha-b""
{code}

This needs to be tested against all transports (local, redis, and rabbit)
",4.0,0.9637846695916052,0.1450611685201184,0.6910520862477671,True,True,True,1384397508000.0,1385024448000.0,1385028048000.0,1385047106000.0,1385048006000.0,1384397508000.0,-1.0,1384847036000.0,1384850636000.0,1384491870000.0,1384495470000.0,1385024448000.0,1385028048000.0,"mark.fisher",1384850636000.0,1384397508000.0,43,1384397508000.0,-1.0,"Update router sink logic to match new channel syntax","for example, the following should work:

{code}
xd:>stream create a1 --definition ""queue:a > transform --expression=payload+'-a' | log""
Created new stream 'a1'

xd:>stream create b1 --definition ""queue:b > transform --expression=payload+'-b' | log""
Created new stream 'b1'

xd:>stream create s1 --definition ""http | router --expression=payload.contains('a')?'queue:a':'queue:b'""
Created new stream 's1'

xd:>http post --data ""ha""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 ha
> 200 OK

// log shows: ""ha-a""

xd:>http post --data ""hi""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 hi
> 200 OK

// log shows: ""ha-b""
{code}

This needs to be tested against all transports (local, redis, and rabbit)
",4.0,False,4.0,0,0.0,0,0,False
"XD-1063","Story","","2013-11-14T02:21:45.000+0000","eric.bottard","eric.bottard","Fix mqtt module properties","Use of dot in property name prevents the user from specifying a value in stream definition

Also, defaults are repeated at .xml and .properties level",2.0,0.9999723357226048,0.0,-1.0,True,False,True,1384395705000.0,1384757172000.0,1384757182000.0,1384756282000.0,1384757182000.0,1384395705000.0,-1.0,-1.0,-1.0,1384395705000.0,1384399305000.0,1384757172000.0,1384757182000.0,"eric.bottard",-1.0,-1.0,0,1384395705000.0,-1.0,"Fix mqtt module properties","Use of dot in property name prevents the user from specifying a value in stream definition

Also, defaults are repeated at .xml and .properties level",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1061","Story","Sprint 19","2013-11-12T12:55:40.000+0000","hillert","eric.bottard","Upgrade asciidoctor-gradle-plugin to 0.7.0","Looks like we need to spend a cycle on Asciidoc - as we still have the author-tag-issue - I thought we can simply upgrade the asciidoctor-gradle-plugin to 0.7.0 (currently 0.4.1) but that breaks the docs being generated.",2.0,0.9999652905777084,0.5918758408058331,0.9999353686619398,True,True,True,1384260940000.0,1385096419000.0,1385096448000.0,1385095548000.0,1385096448000.0,1384260940000.0,-1.0,1385096394000.0,1385096448000.0,1384755457000.0,1384759057000.0,1385096419000.0,1385096448000.0,"hillert",1385096448000.0,1384260940000.0,43,1384260940000.0,-1.0,"Upgrade asciidoctor-gradle-plugin to 0.7.0","Looks like we need to spend a cycle on Asciidoc - as we still have the author-tag-issue - I thought we can simply upgrade the asciidoctor-gradle-plugin to 0.7.0 (currently 0.4.1) but that breaks the docs being generated.",2.0,False,2.0,0,0.0,0,0,False
"XD-1060","Story","Sprint 22","2013-11-12T08:28:43.000+0000","iojohnso","","Add support for Hortonworks Data Platform 2.0","(apologies if a ticket already exists for this, but I didn't see one)

I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn't supported by Spring XD yet.

How hard would it be to add these Distro's in?  Is it just a matter of dropping in a lib folder for hadoop22 and/or hdp20, and allowing those and options to be passed in via the --hadoopDistro option?

I'm currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox

http://hortonworks.com/hadoop-tutorial/using-spring-xd-to-stream-tweets-to-hadoop-for-sentiment-analysis/

Thanks!",1.0,0.9810348397529124,0.9810348397529124,0.9793598990352144,True,True,True,1384244923000.0,1392204765000.0,1392208365000.0,1392357743000.0,1392358643000.0,1384244923000.0,-1.0,1392191175000.0,1392194775000.0,1392204765000.0,1392208365000.0,1392204765000.0,1392208365000.0,"iojohnso",1392194775000.0,1384244923000.0,43,1384244923000.0,-1.0,"Add support for Hortonworks Data Platform 2.0","(apologies if a ticket already exists for this, but I didn't see one)

I spun up the Hortonworks Data Platform 2.0 sandbox, but see it isn't supported by Spring XD yet.

How hard would it be to add these Distro's in?  Is it just a matter of dropping in a lib folder for hadoop22 and/or hdp20, and allowing those and options to be passed in via the --hadoopDistro option?

I'm currently trying to work through the following tutorial, but using the HDP 2.0 sandbox instead of the 1.3 sandbox

http://hortonworks.com/hadoop-tutorial/using-spring-xd-to-stream-tweets-to-hadoop-for-sentiment-analysis/

Thanks!",1.0,False,1.0,0,0.0,0,1,False
"XD-1059","Bug","Sprint 19","2013-11-12T06:33:53.000+0000","iperumal","iperumal","Batch Job's step execution count is always '0'","The batch job's step execution count is retrieved from org.springframework.batch.admin.web.JobExecutionInfo in batch job repository.

But, the JobExecutionInfo always have the stepExecutionCount set to '0'.",2.0,0.781686858129824,0.7816786801374557,0.996081106057112,True,True,True,1384238033000.0,1384715954000.0,1384719554000.0,1384848530000.0,1384849430000.0,1384238033000.0,-1.0,1384847034000.0,1384849430000.0,1384715949000.0,1384719549000.0,1384715954000.0,1384719554000.0,"iperumal",1384849430000.0,1384238033000.0,43,1384238033000.0,-1.0,"Batch Job's step execution count is always '0'","The batch job's step execution count is retrieved from org.springframework.batch.admin.web.JobExecutionInfo in batch job repository.

But, the JobExecutionInfo always have the stepExecutionCount set to '0'.",2.0,False,2.0,0,0.0,0,0,False
"XD-1058","Story","Sprint 19","2013-11-12T04:37:53.000+0000","iperumal","iperumal","Remove unnecessary LESS files from XD UI styles","Currently, the bootstrap.less file has all the styles that the bootstrap supports. But we should only add/compile the LESS that are needed by XD UI.",2.0,0.8235204355780517,0.8235155655193349,0.9999220790605332,True,True,True,1384231073000.0,1384738369000.0,1384741969000.0,1384846182000.0,1384847082000.0,1384231073000.0,-1.0,1384847034000.0,1384847082000.0,1384738366000.0,1384741966000.0,1384738369000.0,1384741969000.0,"iperumal",1384847082000.0,1384231073000.0,43,1384231073000.0,-1.0,"Remove unnecessary LESS files from XD UI styles","Currently, the bootstrap.less file has all the styles that the bootstrap supports. But we should only add/compile the LESS that are needed by XD UI.",2.0,False,2.0,0,0.0,0,0,False
"XD-1053","Improvement","Sprint 19","2013-11-11T16:08:24.000+0000","mark.pollack","luke","Add ""http get"" command to shell","This will allow for some easy demonstration of how 'HATEOS' works via links in our REST API.  There are probably some quite useful commands here that could be used from the Spring Data REST shell longer term, but a good place to take a look at now.

Goal is to show how metrics such as counters are accessible w/o having to switch tabs to use wget.
The pretty printing of the returned JSON is an important feature to help understand the response, this functionality can be taken from/reused from the Spring Data REST shell",3.0,0.855528378518645,0.0,0.8572824693044576,True,True,True,1384186104000.0,1386230194000.0,1386233794000.0,1386574476000.0,1386575376000.0,1384186104000.0,-1.0,1386234385000.0,1386237985000.0,1384186104000.0,1384189704000.0,1386230194000.0,1386233794000.0,"mark.pollack",1386237985000.0,1384186104000.0,43,1384186104000.0,-1.0,"Add ""http get"" command to shell","This will allow for some easy demonstration of how 'HATEOS' works via links in our REST API.  There are probably some quite useful commands here that could be used from the Spring Data REST shell longer term, but a good place to take a look at now.

Goal is to show how metrics such as counters are accessible w/o having to switch tabs to use wget.
The pretty printing of the returned JSON is an important feature to help understand the response, this functionality can be taken from/reused from the Spring Data REST shell",3.0,False,3.0,0,0.0,-1,1,False
"XD-1052","Story","Sprint 24","2013-11-11T08:21:05.000+0000","mark.fisher","eric.bottard","Enforce consistent naming across CLI options, and command/template/operations method names","e.g. see comment on PR #390:
https://github.com/spring-projects/spring-xd/pull/390/files#r7563787

In that case, it's ""delete"" in one place and ""destroy"" in another. There are other cases as well.",4.0,-1.0,0.8698178489754403,0.869818242626541,False,True,True,1384158065000.0,-1.0,-1.0,1396858768000.0,1396859668000.0,1384158065000.0,-1.0,1395206151000.0,1395209751000.0,1395206146000.0,1395209746000.0,-1.0,-1.0,"mark.fisher",1395209751000.0,1384158065000.0,43,1384158065000.0,-1.0,"Enforce consistent naming across CLI options, and command/template/operations method names","e.g. see comment on PR #390:
https://github.com/spring-projects/spring-xd/pull/390/files#r7563787

In that case, it's ""delete"" in one place and ""destroy"" in another. There are other cases as well.",4.0,False,4.0,0,0.0,0,0,False
"XD-1051","Story","Sprint 30","2013-11-11T06:15:18.000+0000","mark.fisher","dturanski","Rename or reconsider the ""module display"" command","note from PR #365:

{quote}
We should probably create a new story to (at least) rename ""module display"" to ""module showconfig"" or something, but possibly even reconsider it altogether. In some sense, it's even violating the encapsulation of ModuleRegistry. It wont work for java-config style modules or spring-integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module.
{quote}

",1.0,0.9995480421581096,0.9995434254700452,0.999546556327468,True,True,True,1384150518000.0,1402986679000.0,1402990279000.0,1402994296000.0,1402995196000.0,1384150518000.0,-1.0,1402986651000.0,1402990251000.0,1402986592000.0,1402990192000.0,1402986679000.0,1402990279000.0,"mark.fisher",1402990251000.0,1402055307000.0,44,1402055307000.0,-1.0,"Rename or reconsider the ""module display"" command","note from PR #365:

{quote}
We should probably create a new story to (at least) rename ""module display"" to ""module showconfig"" or something, but possibly even reconsider it altogether. In some sense, it's even violating the encapsulation of ModuleRegistry. It wont work for java-config style modules or spring-integration Groovy DSL modules. Personally, if anything, I'd rather see those config files themselves exposed as part of an admin UI. What you are doing with the options here fits better with the encapsulation principle and the fact that typical usage should not require detailed knowledge of the actual underlying configuration of a Module.
{quote}

",1.0,False,1.0,0,0.0,0,1,False
"XD-1050","Story","Sprint 21","2013-11-11T05:50:18.000+0000","mark.fisher","eric.bottard","Improve Module Options support","note from PR #365 - which has been merged - providing the initial level of support...

Pending issues (to be addressed in another PR?):
- [x] complex case
- [x] default values for complex case, when option is not surfaced back to the module (eg ""suffix"" in our canonical example)
- [ ] plugin provided options and values
- [ ] descriptive defaults instead of actual defaults (e.g. \<use stream name\>)
- [ ] JSR303 Validation

",12.0,-1.0,0.0,0.999994270333405,False,True,True,1384149018000.0,-1.0,-1.0,1391129327000.0,1391130227000.0,1384149018000.0,-1.0,1391130187000.0,1391130227000.0,1384149018000.0,1384152618000.0,-1.0,-1.0,"mark.fisher",1391130227000.0,1384149018000.0,43,1384149018000.0,-1.0,"Improve Module Options support","note from PR #365 - which has been merged - providing the initial level of support...

Pending issues (to be addressed in another PR?):
- [x] complex case
- [x] default values for complex case, when option is not surfaced back to the module (eg ""suffix"" in our canonical example)
- [ ] plugin provided options and values
- [ ] descriptive defaults instead of actual defaults (e.g. \<use stream name\>)
- [ ] JSR303 Validation

",12.0,False,12.0,0,0.0,0,0,False
"XD-1049","Story","Sprint 18","2013-11-10T14:13:56.000+0000","mark.pollack","mark.fisher","Create documentation for composed modules","",3.0,0.8369477677835027,0.0,0.0002619858527639,True,True,True,1384092836000.0,1384179091000.0,1384182691000.0,1384194995000.0,1384195895000.0,1384092836000.0,-1.0,1384092863000.0,1384096463000.0,1384092836000.0,1384096436000.0,1384179091000.0,1384182691000.0,"mark.pollack",1384096463000.0,1384092836000.0,43,1384092836000.0,-1.0,"Create documentation for composed modules","",3.0,False,3.0,0,0.0,0,0,False
"XD-1047","Improvement","Sprint 19","2013-11-10T12:41:11.000+0000","david_geary","Luke Taylor","Allow Aggregate Counter to use timestamp field in data.","Currently the aggregate counter aggerates by the current time. However the data may already have a timestamp in it (eg streams from activity events on a website). 
It would be useful as an alternative approach to be able to specify this field to aggregate on. 

This would have the following benefits:

1) The aggregate counts would be more accurate as they would reflect the acutal event times and not have any lag from an intermediate messgaging system they might have passed through.
2) If for whatever reason XD is down, comes back up and starts pulling queued messages from the messaging system the aggregate counter will reflect the correct event time. Currently you would get a gap and then a spike as a backlog of messages would get allocated to the current aggregate count.
3) Old data could be rerun through XD still creating the correct aggregate counts.

Configuration would be something like 

stream create --name mytap --definition ""tap:mystream > aggregatecounter --name=mycount --timestampField=eventtime""

without the timestampfield it would behave as currently. ",5.0,0.0687074186964914,0.8442285557594896,0.0567667778321131,True,True,True,1384087271000.0,1385006848000.0,1385010448000.0,1397470326000.0,1397471226000.0,1384087271000.0,-1.0,1384847035000.0,1384850635000.0,1395386388000.0,1395389988000.0,1385006848000.0,1385010448000.0,"david_geary",1384850635000.0,1384087271000.0,43,1384087271000.0,-1.0,"Allow Aggregate Counter to use timestamp field in data.","Currently the aggregate counter aggerates by the current time. However the data may already have a timestamp in it (eg streams from activity events on a website). 
It would be useful as an alternative approach to be able to specify this field to aggregate on. 

This would have the following benefits:

1) The aggregate counts would be more accurate as they would reflect the acutal event times and not have any lag from an intermediate messgaging system they might have passed through.
2) If for whatever reason XD is down, comes back up and starts pulling queued messages from the messaging system the aggregate counter will reflect the correct event time. Currently you would get a gap and then a spike as a backlog of messages would get allocated to the current aggregate count.
3) Old data could be rerun through XD still creating the correct aggregate counts.

Configuration would be something like 

stream create --name mytap --definition ""tap:mystream > aggregatecounter --name=mycount --timestampField=eventtime""

without the timestampfield it would behave as currently. ",5.0,False,5.0,0,0.0,0,0,False
"XD-1044","Story","Sprint 19","2013-11-09T12:51:04.000+0000","mark.fisher","mark.pollack","refactor module dependency tracking to be closer to stream deployment","see comments on this PR (which is part of the code that needs to be refactored):
https://github.com/spring-projects/spring-xd/pull/390
",8.0,0.6366728936202699,0.6366650754362777,0.6366600166113415,True,True,True,1384001464000.0,1385385857000.0,1385389457000.0,1386174982000.0,1386175882000.0,1384001464000.0,-1.0,1385385829000.0,1385389429000.0,1385385840000.0,1385389440000.0,1385385857000.0,1385389457000.0,"mark.fisher",1385389429000.0,1384001464000.0,43,1384001464000.0,-1.0,"refactor module dependency tracking to be closer to stream deployment","see comments on this PR (which is part of the code that needs to be refactored):
https://github.com/spring-projects/spring-xd/pull/390
",8.0,False,8.0,0,0.0,0,0,False
"XD-1042","Story","Sprint 19","2013-11-09T11:55:51.000+0000","eric.bottard","eric.bottard","Support short names for types in ModuleOptions","The ModuleOptions PR currently uses FQN for types (eg java.lang.String)

Would be nice to have support for short names for common types, both in the properties files and the annotation",3.0,0.6653041947666787,0.0,0.6636632472816588,True,True,True,1383998151000.0,1385711131000.0,1385714731000.0,1386571983000.0,1386572883000.0,1383998151000.0,-1.0,1385706906000.0,1385710506000.0,1383998151000.0,1384001751000.0,1385711131000.0,1385714731000.0,"eric.bottard",1385710506000.0,1383998151000.0,43,1383998151000.0,-1.0,"Support short names for types in ModuleOptions","The ModuleOptions PR currently uses FQN for types (eg java.lang.String)

Would be nice to have support for short names for common types, both in the properties files and the annotation",3.0,False,3.0,0,0.0,-1,0,False
"XD-1041","Story","Sprint 18","2013-11-08T09:12:37.000+0000","thomas.risberg","thomas.risberg","Upgrade to Spring for Apache Hadoop 1.0.2.RELEASE and Pivotal HD 1.1","Make sure the sinks and jobs work against Pivotal HD 1.1",3.0,0.0428198333769519,0.0003353179795864,0.002827030445782,True,True,True,1383901957000.0,1383917664000.0,1383921264000.0,1384267873000.0,1384268773000.0,1383901957000.0,-1.0,1383902994000.0,1383906594000.0,1383902080000.0,1383905680000.0,1383917664000.0,1383921264000.0,"thomas.risberg",1383906594000.0,1383901957000.0,43,1383901957000.0,-1.0,"Upgrade to Spring for Apache Hadoop 1.0.2.RELEASE and Pivotal HD 1.1","Make sure the sinks and jobs work against Pivotal HD 1.1",3.0,False,3.0,0,0.0,-1,0,False
"XD-1040","Story","Sprint 19","2013-11-07T11:05:36.000+0000","luke","","Remove ""legacy"" application code following Spring bootification","See the discussion in https://github.com/spring-projects/spring-xd/pull/370

There are now various superseded classes and tests which we no longer need.",2.0,0.9994156867144328,-1.0,0.9995737146814808,True,True,False,1383822336000.0,1384846872000.0,1384847471000.0,1384846571000.0,1384847471000.0,1383822336000.0,-1.0,1384847034000.0,1384847471000.0,-1.0,-1.0,1384846872000.0,1384847471000.0,"luke",1384847471000.0,1383822336000.0,43,1383822336000.0,-1.0,"Remove ""legacy"" application code following Spring bootification","See the discussion in https://github.com/spring-projects/spring-xd/pull/370

There are now various superseded classes and tests which we no longer need.",2.0,False,2.0,0,0.0,0,0,False
"XD-1039","Story","Sprint 18","2013-11-07T02:10:12.000+0000","eric.bottard","eric.bottard","Composed of Composed fails at stream deployment time","Although composition of a module out of an already composed module seems to work at the 'module compose' level, trying to deploy a stream with that more complex module fails with

	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:312)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.IllegalArgumentException: each module before the last must provide 'output'
	at org.springframework.util.Assert.notNull(Assert.java:112)
	at org.springframework.xd.module.CompositeModule.initialize(CompositeModule.java:132)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:234)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:224)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleCompositeModuleDeployment(ModuleDeployer.java:180)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:129)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	... 63 more",5.0,0.0007497352625275,0.0007493154191773,0.0007466913982389,True,True,True,1383790212000.0,1383797355000.0,1383800955000.0,1393316675000.0,1393317575000.0,1383790212000.0,-1.0,1383797326000.0,1383800926000.0,1383797351000.0,1383800951000.0,1383797355000.0,1383800955000.0,"eric.bottard",1383800926000.0,1383790212000.0,43,1383790212000.0,-1.0,"Composed of Composed fails at stream deployment time","Although composition of a module out of an already composed module seems to work at the 'module compose' level, trying to deploy a stream with that more complex module fails with

	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:312)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.IllegalArgumentException: each module before the last must provide 'output'
	at org.springframework.util.Assert.notNull(Assert.java:112)
	at org.springframework.xd.module.CompositeModule.initialize(CompositeModule.java:132)
	at org.springframework.xd.dirt.module.ModuleDeployer.deploy(ModuleDeployer.java:234)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:224)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleCompositeModuleDeployment(ModuleDeployer.java:180)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:129)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	... 63 more",5.0,False,5.0,0,0.0,0,0,False
"XD-1037","Story","Sprint 19","2013-11-06T01:30:11.000+0000","eric.bottard","","Support ModuleOptions for composed modules","With the following semantics (assuming http | filter | transform composition):


* Fully qualified names always available (eg filter.expression)
   * using module type as key
   * or label if ambiguity
* Simple names available if no ambiguity (eg here ""port"" refers to http.port)

As per discussion:

VIII) Use of --expression option in composed module filter | filter
Should not be supported (require FQN always)
Should pertain to first filter module (always)
Should pertain to all modules with an expression option (here: both filters)
Should fail if not qualified and ambiguity. Use label.expression (or type.expression, eg if we had filter | transform) (see VII  nesting1.nesting2.nesting3.expression) (MF)

VIII) Use of --expression option in composed module http | filter --expression=something | transform
Should not be supported (already valued)
Should pertain to first filter module (always)
Should pertain to all modules with an expression option (here: both filters)
Should fail if not qualified and ambiguity. Use label.expression (or type.expression, eg if we had filter | transform) (see VII  nesting1.nesting2.nesting3.expression) (MF)
",15.0,0.181175909766761,0.1811755977914644,0.1801276727703593,True,True,True,1383701411000.0,1385443625000.0,1385447225000.0,1393316657000.0,1393317557000.0,1383701411000.0,-1.0,1385433545000.0,1385437145000.0,1385443622000.0,1385447222000.0,1385443625000.0,1385447225000.0,"eric.bottard",1385437145000.0,1383701411000.0,43,1383701411000.0,-1.0,"Support ModuleOptions for composed modules","With the following semantics (assuming http | filter | transform composition):


* Fully qualified names always available (eg filter.expression)
   * using module type as key
   * or label if ambiguity
* Simple names available if no ambiguity (eg here ""port"" refers to http.port)

As per discussion:

VIII) Use of --expression option in composed module filter | filter
Should not be supported (require FQN always)
Should pertain to first filter module (always)
Should pertain to all modules with an expression option (here: both filters)
Should fail if not qualified and ambiguity. Use label.expression (or type.expression, eg if we had filter | transform) (see VII  nesting1.nesting2.nesting3.expression) (MF)

VIII) Use of --expression option in composed module http | filter --expression=something | transform
Should not be supported (already valued)
Should pertain to first filter module (always)
Should pertain to all modules with an expression option (here: both filters)
Should fail if not qualified and ambiguity. Use label.expression (or type.expression, eg if we had filter | transform) (see VII  nesting1.nesting2.nesting3.expression) (MF)
",15.0,False,15.0,0,0.0,0,0,False
"XD-1036","Story","Sprint 18","2013-11-06T01:24:13.000+0000","eric.bottard","eric.bottard","Support composed module deletion","Provided it is not currently used in any stream:

V) Attempt to destroy a composed module
Should not be supported at all
Should not be supported if involved in at least one stream (EB?, MF!)
Should be supported and have no other consequences whatsoever (see IV) (EB?)
Should be supported and invalidate/destroy streams involving it
",10.0,0.2088397837987906,0.2088506285625049,0.2087595325473048,True,True,True,1383701053000.0,1383797339000.0,1383800939000.0,1384161205000.0,1384162105000.0,1383701053000.0,-1.0,1383797302000.0,1383800902000.0,1383797344000.0,1383800944000.0,1383797339000.0,1383800939000.0,"eric.bottard",1383800902000.0,1383701053000.0,43,1383701053000.0,-1.0,"Support composed module deletion","Provided it is not currently used in any stream:

V) Attempt to destroy a composed module
Should not be supported at all
Should not be supported if involved in at least one stream (EB?, MF!)
Should be supported and have no other consequences whatsoever (see IV) (EB?)
Should be supported and invalidate/destroy streams involving it
",10.0,False,10.0,0,0.0,0,0,False
"XD-1035","Story","Sprint 18","2013-11-06T01:22:37.000+0000","eric.bottard","eric.bottard","Attempt to compose a module with same name+type as existing should fail","As per discussion, any attempt to create something that would collide with existing should fail:

II) Attempt to create a composed module with same name as an already existing, not composed module, with same type
should fail (EB, MF)
should work and shadow previous module from now on

III) Attempt to create a composed module with same name as an already existing composed module, with same type
should fail (EB, MF)
should work and shadow previous module from now on
should work and retroactively change definitions of streams with that module
",5.0,0.2701109480783586,0.0,0.2699050119700363,True,True,True,1383700957000.0,1383711450000.0,1383715050000.0,1383738904000.0,1383739804000.0,1383700957000.0,-1.0,1383711442000.0,1383715042000.0,1383700957000.0,1383704557000.0,1383711450000.0,1383715050000.0,"eric.bottard",1383715042000.0,1383700957000.0,43,1383700957000.0,-1.0,"Attempt to compose a module with same name+type as existing should fail","As per discussion, any attempt to create something that would collide with existing should fail:

II) Attempt to create a composed module with same name as an already existing, not composed module, with same type
should fail (EB, MF)
should work and shadow previous module from now on

III) Attempt to create a composed module with same name as an already existing composed module, with same type
should fail (EB, MF)
should work and shadow previous module from now on
should work and retroactively change definitions of streams with that module
",5.0,False,5.0,0,0.0,0,0,False
"XD-1033","Story","Sprint 18","2013-11-05T06:11:56.000+0000","eric.bottard","eric.bottard","Add a tcp-client source module","Add a module that can act as a tcp *client* (as opposed to our current tcp module, which acts as a server, waiting for an incoming connection)

Also, the module should allow to send ""commands"" to the remote server. The typical minimal case for such a protocol is to send ""PING"" messages, but a stateful mechanism should be put in place for more complex cases.",5.0,-1.0,0.9995594228439256,0.999407974446525,False,True,True,1383631916000.0,-1.0,-1.0,1383703648000.0,1383704548000.0,1383631916000.0,-1.0,1383704505000.0,1383704548000.0,1383704516000.0,1383704548000.0,-1.0,-1.0,"eric.bottard",1383704548000.0,1383631916000.0,43,1383631916000.0,-1.0,"Add a tcp-client source module","Add a module that can act as a tcp *client* (as opposed to our current tcp module, which acts as a server, waiting for an incoming connection)

Also, the module should allow to send ""commands"" to the remote server. The typical minimal case for such a protocol is to send ""PING"" messages, but a stateful mechanism should be put in place for more complex cases.",5.0,False,5.0,0,0.0,0,0,False
"XD-1032","Story","Sprint 18","2013-11-05T03:00:06.000+0000","eric.bottard","Eric Bottard","Convert hadoop module to isolated classloader scheme","* rename spring-xd-extension-hdfs to something else, as it seems it is all spring ""data"" stuff and is not coupled to xd. But leave it in extensions/ for now
* rename and move spring-xd-hadoop inside extensions (maybe to spring-xd-extension-hadoop (or hdfs))
* make hadoop related modules depend on the latter (which itself will depend on the former)",8.0,0.6781368873403727,0.4690171558400818,1.104028409315868e-05,True,True,True,1383620406000.0,1385401698000.0,1385405298000.0,1386246250000.0,1386247150000.0,1383620406000.0,-1.0,1383620435000.0,1383624035000.0,1384852394000.0,1384855994000.0,1385401698000.0,1385405298000.0,"eric.bottard",1383624035000.0,1383620406000.0,43,1383620406000.0,-1.0,"Convert hadoop module to isolated classloader scheme","* rename spring-xd-extension-hdfs to something else, as it seems it is all spring ""data"" stuff and is not coupled to xd. But leave it in extensions/ for now
* rename and move spring-xd-hadoop inside extensions (maybe to spring-xd-extension-hadoop (or hdfs))
* make hadoop related modules depend on the latter (which itself will depend on the former)",8.0,False,8.0,0,0.0,0,0,False
"XD-1031","Story","Sprint 18","2013-11-04T11:44:58.000+0000","mark.pollack","hillert","UI - Launch a job with parameters","Use a modal dialog to specify runtime parameters.
There should be a little text are that gives hints as to the spring batch parameter key/value conventions, e.g. for type.

Might be a good idea to have a checkbox that lets you select to 'auto increment' job instance number.

4 columns
key, value, type, identifying and an 'add parameter' button that adds a new row.

This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown.

",8.0,0.2579567478544569,0.0,0.001842615292611,True,True,True,1383565498000.0,1383730272000.0,1383733872000.0,1384203364000.0,1384204264000.0,1383565498000.0,-1.0,1383566675000.0,1383570275000.0,1383565498000.0,1383569098000.0,1383730272000.0,1383733872000.0,"mark.pollack",1383570275000.0,1383565498000.0,43,1383565498000.0,-1.0,"UI - Launch a job with parameters","Use a modal dialog to specify runtime parameters.
There should be a little text are that gives hints as to the spring batch parameter key/value conventions, e.g. for type.

Might be a good idea to have a checkbox that lets you select to 'auto increment' job instance number.

4 columns
key, value, type, identifying and an 'add parameter' button that adds a new row.

This would appear as a modal dialog box, polling of the state of the deployments would be suspended while the job parameter modal dialog box is shown.

",8.0,False,8.0,0,0.0,-1,2,False
"XD-1030","Story","","2013-11-03T23:48:46.000+0000","abilan","","Refactoring to JdbcMessagePayloadTransformer to extends from AbstractPayloadTransformer not JsonToObjectTransformer","",1.0,-1.0,-1.0,-1.0,False,False,False,1383522526000.0,-1.0,-1.0,1390948337000.0,1390949237000.0,1383522526000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"abilan",-1.0,-1.0,0,1383522526000.0,-1.0,"Refactoring to JdbcMessagePayloadTransformer to extends from AbstractPayloadTransformer not JsonToObjectTransformer","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1029","Story","Sprint 18","2013-11-01T09:17:16.000+0000","grussell","grussell","Migrate to SI Redis Queue Adapters","",3.0,0.4324969024652902,0.0,0.4324737433272733,True,True,True,1383297436000.0,1383558886000.0,1383562486000.0,1383901049000.0,1383901949000.0,1383297436000.0,-1.0,1383558872000.0,1383562472000.0,1383297436000.0,1383301036000.0,1383558886000.0,1383562486000.0,"grussell",1383562472000.0,1383297436000.0,43,1383297436000.0,-1.0,"Migrate to SI Redis Queue and Topic Adapters","",3.0,False,3.0,0,0.0,-1,0,True
"XD-1028","Story","Sprint 18","2013-11-01T08:09:36.000+0000","hillert","hillert","UI - Do not hard-code server url","Specify the default URL as:

{code}
urlRoot: location.protocol+'//'+location.hostname+(location.port ? ':'+location.port: '')
{code}",1.0,0.0664787463697215,0.0001048315706098,0.0341285002096631,True,True,True,1383293376000.0,1383310498000.0,1383314098000.0,1383550032000.0,1383550932000.0,1383293376000.0,-1.0,1383302166000.0,1383305766000.0,1383293403000.0,1383297003000.0,1383310498000.0,1383314098000.0,"hillert",1383305766000.0,1383293376000.0,43,1383293376000.0,-1.0,"UI - Do not hard-code server url","Specify the default URL as:

{code}
urlRoot: location.protocol+'//'+location.hostname+(location.port ? ':'+location.port: '')
{code}",1.0,False,1.0,0,0.0,0,0,False
"XD-1025","Story","Sprint 18","2013-10-31T13:25:59.000+0000","iperumal","iperumal","UI: Implement Job Deploy/Undeploy from the Job Definitions page","From XD-1023, the job status(deployed/undeployed) is available from JobInstance Repository and a job can be deployed/undeployed correctly. 

Implement Job deploy/undeploy for a given job from JobDefinitions page and indicate status of the job definition (deployed/undeployed).",2.0,0.0009856100926473,0.0008279124778237,0.000722780734608,True,True,True,1383225959000.0,1383226034000.0,1383229634000.0,1383301154000.0,1383302054000.0,1383225959000.0,-1.0,1383226014000.0,1383229614000.0,1383226022000.0,1383229622000.0,1383226034000.0,1383229634000.0,"iperumal",1383229614000.0,1383225959000.0,43,1383225959000.0,-1.0,"UI: Implement Job Deploy/Undeploy from the Job Definitions page","From XD-1023, the job status(deployed/undeployed) is available from JobInstance Repository and a job can be deployed/undeployed correctly. 

Implement Job deploy/undeploy for a given job from JobDefinitions page and indicate status of the job definition (deployed/undeployed).",2.0,False,2.0,0,0.0,0,0,False
"XD-1024","Story","Sprint 18","2013-10-30T09:06:01.000+0000","grussell","dturanski","Kryo Redis Serializer","The current (XD) uses an inadequate {{MessageRedisSerializer}} when {{extractPayload}} is false (not currently being used). When porting this to SI, the serializer will be dropped.

Suggest creation of Kryo serializer for Messages for when Redis source/sinks are created.",5.0,0.880432472334116,0.7745874776477476,0.8804184723468433,True,True,True,1383123961000.0,1383815730000.0,1383819330000.0,1383908776000.0,1383909676000.0,1383123961000.0,-1.0,1383815719000.0,1383819319000.0,1383732566000.0,1383736166000.0,1383815730000.0,1383819330000.0,"grussell",1383819319000.0,1383123961000.0,43,1383123961000.0,-1.0,"Kryo Redis Serializer","The current (XD) uses an inadequate {{MessageRedisSerializer}} when {{extractPayload}} is false (not currently being used). When porting this to SI, the serializer will be dropped.

Suggest creation of Kryo serializer for Messages for when Redis source/sinks are created.",5.0,False,5.0,0,0.0,0,0,False
"XD-1023","Story","Sprint 18","2013-10-29T08:01:16.000+0000","eric.bottard","eric.bottard","Refactor job deploy to go through XDController.deploy(name)","Job deployment currently goes through an overloaded version of deploy() that takes 4 parameters. This prohibits job handling code from benefiting from common behavior (and eg currently breaks deployAll)

Given that the 3 additional parameters are in fine handled as module parameters, let's push them to the job definition, known at creation time, rather than at deployment time (as it does not really make sense to change those between deploys)",4.0,0.0083188227009972,0.0082969885206796,0.0082369445248063,True,True,True,1383033676000.0,1383035200000.0,1383038800000.0,1383215975000.0,1383216875000.0,1383033676000.0,-1.0,1383035185000.0,1383038785000.0,1383035196000.0,1383038796000.0,1383035200000.0,1383038800000.0,"eric.bottard",1383038785000.0,1383033676000.0,43,1383033676000.0,-1.0,"Refactor job deploy to go through XDController.deploy(name)","Job deployment currently goes through an overloaded version of deploy() that takes 4 parameters. This prohibits job handling code from benefiting from common behavior (and eg currently breaks deployAll)

Given that the 3 additional parameters are in fine handled as module parameters, let's push them to the job definition, known at creation time, rather than at deployment time (as it does not really make sense to change those between deploys)",4.0,False,4.0,0,0.0,-1,0,False
"XD-1022","Story","","2013-10-29T02:50:09.000+0000","eric.bottard","","Switch ""module list"" to horizontal display","The module list command currently has a very simplistic two column display of (module name, module type).
The is not very readable.

Switch to a 4 column display: (Sources, Processors, Sinks, Jobs)

Additionally, mark composed module [e.g. ""myhttp (c)""]",5.0,-1.0,-1.0,-1.0,False,False,False,1383015009000.0,-1.0,-1.0,1390960429000.0,1390961329000.0,1383015009000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1383015009000.0,-1.0,"Switch ""module list"" to horizontal display","The module list command currently has a very simplistic two column display of (module name, module type).
The is not very readable.

Switch to a 4 column display: (Sources, Processors, Sinks, Jobs)

Additionally, mark composed module [e.g. ""myhttp (c)""]",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-1021","Story","Sprint 19","2013-10-29T02:45:23.000+0000","eric.bottard","","Fix undeploy of stream with a composed module","Create a composed module
Create a stream that uses that module
Try to undeploy the stream.
Kaboom

The dispatch is not correctly implemented in ModuleDeployer",5.0,0.9908894247054848,0.3461352777509783,0.1778448551480161,True,True,True,1383014723000.0,1393223720000.0,1393227320000.0,1393316685000.0,1393317585000.0,1383014723000.0,-1.0,1384847034000.0,1384850634000.0,1386580907000.0,1386584507000.0,1393223720000.0,1393227320000.0,"eric.bottard",1384850634000.0,1383014723000.0,43,1383014723000.0,-1.0,"Fix undeploy of stream with a composed module","Create a composed module
Create a stream that uses that module
Try to undeploy the stream.
Kaboom

The dispatch is not correctly implemented in ModuleDeployer",5.0,False,5.0,0,0.0,0,0,False
"XD-1019","Improvement","Sprint 18","2013-10-28T15:30:38.000+0000","grussell","grussell","Add Spring Retry to Rabbit Message Bus","Log and purge bad messages",2.0,0.0002409888854264,0.0,0.0001745091928949,True,True,True,1382974238000.0,1382974296000.0,1382977896000.0,1383214013000.0,1383214913000.0,1382974238000.0,-1.0,1382974280000.0,1382977880000.0,1382974238000.0,1382977838000.0,1382974296000.0,1382977896000.0,"grussell",1382977880000.0,1382974238000.0,43,1382974238000.0,-1.0,"Add Spring Retry to Rabbit Message Bus","Log and purge bad messages",2.0,False,2.0,0,0.0,0,0,False
"XD-1017","Bug","Sprint 21","2013-10-25T13:59:23.000+0000","grenfro","hillert","Spring Batch Admin UI looks to localhost when getting status updates","Currently when you bring up a admin-ui on a browser that is not on the XD admin server, the page reports an error.

This is because the javascript is looking to localhost to get its updates.  ",5.0,0.9999747093855492,0.999974034969164,0.9999962907098806,True,True,True,1382709563000.0,1391605919000.0,1391606144000.0,1391605244000.0,1391606144000.0,1382709563000.0,-1.0,1391606111000.0,1391606144000.0,1391605913000.0,1391606144000.0,1391605919000.0,1391606144000.0,"grenfro",1391606144000.0,1382709563000.0,43,1382709563000.0,-1.0,"Spring Batch Admin UI looks to localhost when getting status updates","Currently when you bring up a admin-ui on a browser that is not on the XD admin server, the page reports an error.

This is because the javascript is looking to localhost to get its updates.  ",5.0,False,5.0,0,0.0,0,0,False
"XD-1016","Story","Sprint 38","2013-10-25T12:55:41.000+0000","dturanski","liujiong","Provide an option to pretty print JSON output","Probably the cleanest approach is to provide a properties file in the xd config directory that enables this globally, e.g., json.pretty.print=true.  This will require some refactoring of the ModuleTypeConversion plugin, i.e., use DI in streams.xml",3.0,0.9905202056968072,0.7690826762503965,0.9712420066585168,True,True,True,1382705741000.0,1415025612000.0,1415029212000.0,1415334030000.0,1415334930000.0,1382705741000.0,-1.0,1414396580000.0,1414400180000.0,1407800285000.0,1407803885000.0,1415025612000.0,1415029212000.0,"dturanski",1414400180000.0,1382705741000.0,43,1382705741000.0,-1.0,"Provide an option to pretty print JSON output","Probably the cleanest approach is to provide a properties file in the xd config directory that enables this globally, e.g., json.pretty.print=true.  This will require some refactoring of the ModuleTypeConversion plugin, i.e., use DI in streams.xml",3.0,False,3.0,0,0.0,0,0,False
"XD-1015","Story","Sprint 18","2013-10-25T09:14:40.000+0000","iperumal","iperumal","Create a reusable responsive UI layout to render the XD PagedResoures in tabular view","Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint. 

As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.",5.0,0.0003159038174482,0.0002974299099951,0.0002844981747779,True,True,True,1382692480000.0,1382692651000.0,1382696251000.0,1383232884000.0,1383233784000.0,1382692480000.0,-1.0,1382692634000.0,1382696234000.0,1382692641000.0,1382696241000.0,1382692651000.0,1382696251000.0,"iperumal",1382696234000.0,1382692480000.0,43,1382692480000.0,-1.0,"Create a reusable responsive UI layout to render the XD PagedResoures in tabular view","Create a reusable responsive UI layout to render the PagedResources returned from REST endpoint. 

As part of this, try upgrading the bootstrap to 3.0.0 and use the responsive styles offered in it.",5.0,False,5.0,0,0.0,0,0,False
"XD-1014","Story","Sprint 18","2013-10-25T09:11:17.000+0000","hillert","hillert","Command to show the XML of the job definition","",8.0,0.4986694446855919,0.0,4.745782330642128e-05,True,True,True,1382692277000.0,1383123090000.0,1383126690000.0,1383555302000.0,1383556202000.0,1382692277000.0,-1.0,1382692318000.0,1382695918000.0,1382692277000.0,1382695877000.0,1383123090000.0,1383126690000.0,"hillert",1382695918000.0,1382692277000.0,43,1382692277000.0,-1.0,"Command to show the XML of the job definition","",8.0,False,8.0,0,0.0,0,0,False
"XD-1013","Story","Sprint 18","2013-10-25T09:10:07.000+0000","hillert","hillert","Add additional REST endpoint that return the XML definition of a job","",8.0,0.0001643518518518,0.0,0.0002152777777777,True,True,True,1382692207000.0,1382692349000.0,1382695949000.0,1383555307000.0,1383556207000.0,1382692207000.0,-1.0,1382692393000.0,1382695993000.0,1382692207000.0,1382695807000.0,1382692349000.0,1382695949000.0,"hillert",1382695993000.0,1382692207000.0,43,1382692207000.0,-1.0,"Add additional REST endpoint that return the XML definition of a job","",8.0,False,8.0,0,0.0,0,0,False
"XD-1012","Story","Sprint 19","2013-10-25T09:00:42.000+0000","grenfro","grenfro","DataSource for batch infrastructure should be configurable, not hardcoded to hsqldb","Currently HSQLDB is the only option for batch jobs.  This should be configurable so that a user may select another JDBC data store option.  

Steps:
* hsqldb.properties needs to be changed to batch-jdbc.properties
* hsql prefixes should be changed to batch-jdbc
* JDBC Connection String needs to be configurable
* JDBC Driver needs to be configurable
* The Setup Scripts to be used for spring batch need to be configurable.
* HSQLServerBean should be renamed to something batch-jdbc.properties

Tests:
Should be able write tests that support HSQLDB",8.0,0.5003664211803883,0.5002628453552174,0.8004718775722174,True,True,True,1382691642000.0,1384421111000.0,1384424711000.0,1386147147000.0,1386148047000.0,1382691642000.0,-1.0,1385458397000.0,1385461997000.0,1384420753000.0,1384424353000.0,1384421111000.0,1384424711000.0,"grenfro",1385461997000.0,1382691642000.0,43,1382691642000.0,-1.0,"DataSource for batch infrastructure should be configurable, not hardcoded to hsqldb","Currently HSQLDB is the only option for batch jobs.  This should be configurable so that a user may select another JDBC data store option.  

Steps:
* hsqldb.properties needs to be changed to batch-jdbc.properties
* hsql prefixes should be changed to batch-jdbc
* JDBC Connection String needs to be configurable
* JDBC Driver needs to be configurable
* The Setup Scripts to be used for spring batch need to be configurable.
* HSQLServerBean should be renamed to something batch-jdbc.properties

Tests:
Should be able write tests that support HSQLDB",8.0,False,8.0,0,0.0,0,2,False
"XD-1010","Improvement","Sprint 19","2013-10-25T07:33:32.000+0000","eric.bottard","eric.bottard","Convert remaing FileSink assertions to eventually() constructs","See discussion at https://github.com/spring-projects/spring-xd/pull/362",5.0,-1.0,0.9999969319203348,0.99999386384067,False,True,True,1382686412000.0,-1.0,-1.0,1385618943000.0,1385619843000.0,1382686412000.0,-1.0,1385619825000.0,1385619843000.0,1385619834000.0,1385619843000.0,-1.0,-1.0,"eric.bottard",1385619843000.0,1382686412000.0,43,1382686412000.0,-1.0,"Convert remaing FileSink assertions to eventually() constructs","See discussion at https://github.com/spring-projects/spring-xd/pull/362",5.0,False,5.0,0,0.0,-1,0,False
"XD-1009","Bug","","2013-10-24T08:09:32.000+0000","hillert","","Don't perform error-level logging for normal application behavior","Assumption: The batch job *wordCountJob* does not exist. Executing the REST endpoint:

http://localhost:9393/batch/jobs/wordCountJob/executions

yields the correct response:

{code}
<errors xmlns:atom=""http://www.w3.org/2005/Atom"">
<error logref=""NoSuchBatchJobException"">
<message>
Batch Job with the name wordCountJob.job doesn't exist
</message>
</error>
</errors>
{code}

One could argue, though, that this is technically not an error but a status/application message, no??

However, the console/log logs at error level with a full stacktrace. I think the full stacktrace should only be available when debugging is allowed. In normal mode, I think we should log only at info or warn level without the full stacktrace.
",2.0,-1.0,-1.0,-1.0,False,False,False,1382602172000.0,-1.0,-1.0,1402058631000.0,1402059531000.0,1382602172000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1382602172000.0,-1.0,"Don't perform error-level logging for normal application behavior in batch admin functionality","Assumption: The batch job *wordCountJob* does not exist. Executing the REST endpoint:

http://localhost:9393/batch/jobs/wordCountJob/executions

yields the correct response:

{code}
<errors xmlns:atom=""http://www.w3.org/2005/Atom"">
<error logref=""NoSuchBatchJobException"">
<message>
Batch Job with the name wordCountJob.job doesn't exist
</message>
</error>
</errors>
{code}

One could argue, though, that this is technically not an error but a status/application message, no??

However, the console/log logs at error level with a full stacktrace. I think the full stacktrace should only be available when debugging is allowed. In normal mode, I think we should log only at info or warn level without the full stacktrace.
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-1008","Story","Sprint 18","2013-10-23T22:18:42.000+0000","iperumal","","Jobs list REST endpoint should include deployed/undeployed status","Currently, the jobs definition list REST endpoint doesn't include deployed/undeployed status on a given job.",4.0,0.5551537821472131,0.7205620657886439,0.027228137741801,True,True,True,1382566722000.0,1382927668000.0,1382931268000.0,1383215995000.0,1383216895000.0,1382566722000.0,-1.0,1382584425000.0,1382588025000.0,1383035212000.0,1383038812000.0,1382927668000.0,1382931268000.0,"iperumal",1382588025000.0,1382566722000.0,43,1382566722000.0,-1.0,"Jobs list REST endpoint should include deployed/undeployed status","Currently, the jobs definition list REST endpoint doesn't include deployed/undeployed status on a given job.",4.0,False,4.0,0,0.0,-1,0,False
"XD-1007","Story","Sprint 26","2013-10-23T22:09:48.000+0000","iperumal","","UI: User should be able to see step execution info in a table below job detail","On clicking the job detail page, we should display all the step executions associated with the specific job execution in a table view.",3.0,-1.0,0.960107254728094,0.8941786810609255,False,True,True,1382566188000.0,-1.0,-1.0,1399540204000.0,1399541104000.0,1382566188000.0,-1.0,1397744796000.0,1397748396000.0,1398863928000.0,1398867528000.0,-1.0,-1.0,"iperumal",1397748396000.0,1382566188000.0,43,1382566188000.0,-1.0,"UI: User should be able to see step execution info in a table below job detail","On clicking the job detail page, we should display all the step executions associated with the specific job execution in a table view.",3.0,False,3.0,0,0.0,0,0,False
"XD-1006","Story","Sprint 26","2013-10-23T21:18:22.000+0000","iperumal","","UI: User should be able to view job detail from a specific job execution at Job Executions page","On clicking ""details"" link on a job execution row, user should see the job details.

Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.",3.0,0.9877017974014646,0.9877017974014646,0.9258237861359484,True,True,True,1382563102000.0,1398759442000.0,1398763042000.0,1398960208000.0,1398961108000.0,1382563102000.0,-1.0,1397744766000.0,1397748366000.0,1398759442000.0,1398763042000.0,1398759442000.0,1398763042000.0,"iperumal",1397748366000.0,1382563102000.0,43,1382563102000.0,-1.0,"UI: User should be able to view job detail from a specific job execution at Job Executions page","On clicking ""details"" link on a job execution row, user should see the job details.

Job detail page will show all the information about the job, where as the table listing of jobs on the Execution tab may have omitted some columns or aggregated values to convey information more easily.",3.0,False,3.0,0,0.0,0,0,False
"XD-1004","Story","Sprint 18","2013-10-23T21:11:59.000+0000","iperumal","iperumal","UI: User should be able to schedule a job from Deployed jobs page","From the Deployed jobs page, by clicking the ""Schedule"" button on a specific deployed job row, user should be able to schedule this job with:
1) Cron trigger (with cron expression) as a source to job launching named channel
2) Fixed rate/delay trigger as a source to job launching named channel

",4.0,0.9007073231019742,0.0,0.0378676354712618,True,True,True,1382562719000.0,1398165980000.0,1398169580000.0,1399885161000.0,1399886061000.0,1382562719000.0,1398165976000.0,1383218713000.0,1383222313000.0,1382562719000.0,1382566319000.0,1398165980000.0,1398169580000.0,"iperumal",1383222313000.0,1382562719000.0,43,1398165976000.0,1398165976000.0,"UI: User should be able to schedule a job from Deployed jobs page","From the Deployed jobs page, by clicking the ""Schedule"" button on a specific deployed job row, user should be able to schedule this job with:
1) Cron trigger (with cron expression) as a source to job launching named channel
2) Fixed rate/delay trigger as a source to job launching named channel

",2.0,True,2.0,1,100.0,0,0,False
"XD-1003","Story","Sprint 18","2013-10-23T21:08:17.000+0000","iperumal","iperumal","UI: User should be able to launch a job from Deployed jobs page","From the Deployed jobs page, user should be able to click on the ""Launch"" button on a specific job and specify the Job parameters as key value pairs in the text box and we will convert that into JSON string as JobParameters into JobLaunch request.

",2.0,0.6594501383246091,0.0,0.6594189863966565,True,True,True,1382562497000.0,1383218731000.0,1383222331000.0,1383556720000.0,1383557620000.0,1382562497000.0,-1.0,1383218700000.0,1383222300000.0,1382562497000.0,1382566097000.0,1383218731000.0,1383222331000.0,"iperumal",1383222300000.0,1382562497000.0,43,1382562497000.0,-1.0,"UI: User should be able to launch a job from Deployed jobs page","From the Deployed jobs page, user should be able to click on the ""Launch"" button on a specific job and specify the Job parameters as key value pairs in the text box and we will convert that into JSON string as JobParameters into JobLaunch request.

",2.0,False,2.0,0,0.0,0,0,False
"XD-1002","Story","Sprint 18","2013-10-23T21:05:21.000+0000","iperumal","iperumal","UI: User should be able to get all the job executions on a given job at deployed jobs page","On clicking a specific job on the deployed jobs list, we need to redirect the user to show the list of all the job executions on that job.

User should be able to navigate back to the deployed jobs list.",3.0,0.0102072716725126,0.0,0.0102070861059986,True,True,True,1382562321000.0,1383112381000.0,1383115981000.0,1436450454000.0,1436451354000.0,1382562321000.0,-1.0,1383112371000.0,1383115971000.0,1382562321000.0,1382565921000.0,1383112381000.0,1383115981000.0,"iperumal",1383115971000.0,1382562321000.0,43,1382562321000.0,-1.0,"UI: User should be able to get all the job executions on a given job at deployed jobs page","On clicking a specific job name in the deployed jobs list, we need to redirect the user to show the list of all the job executions on that job.

User should be able to navigate back to the deployed jobs list.",3.0,False,3.0,0,0.0,-1,0,True
"XD-1001","Story","Sprint 18","2013-10-23T21:03:12.000+0000","iperumal","iperumal","User should be able to view the list of all Deployed Jobs","On clicking the Deployed Jobs, we can have a table view of all the deployed jobs.
This is again a responsive table layout with all the job definitions with status deployed. The deployed XD job corresponds to a single batch Job Instance. 

This story addresses the UI layout changes to display existing JobInstance information.",2.0,0.5641071382801487,0.0,0.5641153402537485,True,True,True,1382562192000.0,1383112408000.0,1383116008000.0,1383536667000.0,1383537567000.0,1382562192000.0,-1.0,1383112416000.0,1383116016000.0,1382562192000.0,1382565792000.0,1383112408000.0,1383116008000.0,"iperumal",1383116016000.0,1382562192000.0,43,1382562192000.0,-1.0,"User should be able to view the list of all Deployed Jobs","On clicking the Deployed Jobs, we can have a table view of all the deployed jobs.
This is again a responsive table layout with all the job definitions with status deployed. The deployed XD job corresponds to a single batch Job Instance. 

This story addresses the UI layout changes to display existing JobInstance information.",2.0,False,2.0,0,0.0,0,0,False
"XD-1000","Story","Sprint 18","2013-10-23T20:49:05.000+0000","iperumal","iperumal","User should be able to view the list of all the job definitions","Create a tab view with tabs Job Definitions, Runtime Jobs (Deployed Jobs?, Job Instances (Is Runtime Jobs a better name here?) and Job Executions. On clicking Job Definitions tab, we can have a table view of job definitions. Since we bootstrap.js, we can have a responsive table layout to list all the available job definitions. At the REST layer, /jobs provides the list of job definitions. We can expand the JobsController list()s QueryOptions to add more criteria (especially to list JobDefinitions status (Deployed/Undeployed).
Also, this is the UI implementation for the shell command job list
",4.0,0.5769095781543248,0.0,0.0343353766808541,True,True,True,1382561345000.0,1382949274000.0,1382952874000.0,1383232871000.0,1383233771000.0,1382561345000.0,-1.0,1382584433000.0,1382588033000.0,1382561345000.0,1382564945000.0,1382949274000.0,1382952874000.0,"iperumal",1382588033000.0,1382561345000.0,43,1382561345000.0,-1.0,"User should be able to view the list of all the job definitions","Create a tab view with tabs Job Definitions, Runtime Jobs (Deployed Jobs?, Job Instances (Is Runtime Jobs a better name here?) and Job Executions. On clicking Job Definitions tab, we can have a table view of job definitions. Since we bootstrap.js, we can have a responsive table layout to list all the available job definitions. At the REST layer, /jobs provides the list of job definitions. We can expand the JobsController list()s QueryOptions to add more criteria (especially to list JobDefinitions status (Deployed/Undeployed).
Also, this is the UI implementation for the shell command job list
",4.0,False,4.0,0,0.0,0,0,False
"XD-999","Story","Sprint 18","2013-10-23T14:45:45.000+0000","hillert","hillert","Return the step execution information in the current job execution controller","Related to https://jira.springsource.org/browse/BATCH-2109

DistributedJobService#listJobExecutionsForJob overrides 
SimpleJobService#listJobExecutionsForJob

and does not include the *StepExecution*s. This is due to serializion issues with Jackson. 

In order to fix this, we need to add a Jackson MixIn.
",4.0,0.0002133105802047,0.0,7.756748371082842e-05,True,True,True,1382539545000.0,1382539578000.0,1382543178000.0,1382693349000.0,1382694249000.0,1382539545000.0,-1.0,1382539557000.0,1382543157000.0,1382539545000.0,1382543145000.0,1382539578000.0,1382543178000.0,"hillert",1382543157000.0,1382539545000.0,43,1382539545000.0,-1.0,"Return the step execution information in the current job execution controller","Related to https://jira.springsource.org/browse/BATCH-2109

DistributedJobService#listJobExecutionsForJob overrides 
SimpleJobService#listJobExecutionsForJob

and does not include the *StepExecution*s. This is due to serializion issues with Jackson. 

In order to fix this, we need to add a Jackson MixIn.
",4.0,False,4.0,0,0.0,0,0,False
"XD-998","Story","Sprint 18","2013-10-23T08:54:37.000+0000","mark.pollack","dturanski","Add documentation for gemfire cache-listener source","Need some sample usage, docs for 

https://github.com/spring-projects/spring-xd/tree/master/modules/source/gemfire

 ",1.0,0.1322398631504096,0.0,0.0008314013663364,True,True,True,1382518477000.0,1382689940000.0,1382693540000.0,1383814183000.0,1383815083000.0,1382518477000.0,-1.0,1382519555000.0,1382523155000.0,1382518477000.0,1382522077000.0,1382689940000.0,1382693540000.0,"mark.pollack",1382523155000.0,1382518477000.0,43,1382518477000.0,-1.0,"Add documentation for gemfire cache-listener source","Need some sample usage, docs for 

https://github.com/spring-projects/spring-xd/tree/master/modules/source/gemfire

 ",1.0,False,1.0,0,0.0,0,0,False
"XD-997","Story","Sprint 18","2013-10-23T07:36:18.000+0000","mark.pollack","","The HDFS Sink should support writing POJOs to HDFS using Avro and partitioning","Support for partitioning on a field, e.g. date.",4.0,0.8472349246997134,0.4717562730953293,0.0003214375836972,True,True,True,1382513778000.0,1397743246000.0,1397746846000.0,1400488374000.0,1400489274000.0,1382513778000.0,-1.0,1382519556000.0,1382523156000.0,1390993831000.0,1390997431000.0,1397743246000.0,1397746846000.0,"mark.pollack",1382523156000.0,1382513792000.0,43,1382513792000.0,-1.0,"The HDFS Sink should support writing POJOs to HDFS using Avro/Kite SDK with support for partitioning","Support for partitioning on a field, e.g. date.",4.0,False,4.0,0,0.0,0,0,True
"XD-995","Story","Sprint 18","2013-10-23T05:52:39.000+0000","eric.bottard","","Refactor tests with FileSink|FileSource to use eventually() matcher","Some tests (esp. ModuleClasspathTests.testModuleWithClasspathAfterServerStarted) seem to fail because of a race condition.

Add a Hamcrest matcher that knows how to read the content of a FileSink|Source and refactor those to read like e.g.

assertThat(fileSink, eventually(hasContent(""foo)))

",5.0,0.1233109654986834,0.1232940593041128,0.0168991503228378,True,True,True,1382507559000.0,1382595085000.0,1382598685000.0,1383216458000.0,1383217358000.0,1382507559000.0,-1.0,1382519554000.0,1382523154000.0,1382595073000.0,1382598673000.0,1382595085000.0,1382598685000.0,"eric.bottard",1382523154000.0,1382507559000.0,43,1382507559000.0,-1.0,"Refactor tests with FileSink|FileSource to use eventually() matcher","Some tests (esp. ModuleClasspathTests.testModuleWithClasspathAfterServerStarted) seem to fail because of a race condition.

Add a Hamcrest matcher that knows how to read the content of a FileSink|Source and refactor those to read like e.g.

assertThat(fileSink, eventually(hasContent(""foo)))

",5.0,False,5.0,0,0.0,0,0,False
"XD-994","Story","","2013-10-22T15:18:16.000+0000","thomas.risberg","","The HDFS Sink should support writing POJOs to HDFS using a Parquet","Writing POJOs using CDK Data (Parquet)
",8.0,-1.0,0.1581454377341042,-1.0,False,False,True,1382455096000.0,-1.0,-1.0,1436450183000.0,1436451083000.0,1382455096000.0,-1.0,-1.0,-1.0,1390994315000.0,1390997915000.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1382455096000.0,-1.0,"The HDFS Sink should support writing POJOs to HDFS using Parquet","Writing POJOs using Kite SDK ",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-991","Improvement","Sprint 18","2013-10-22T14:45:55.000+0000","thomas.risberg","jvalkeal","The HDFS Store Library should support compression when writing text","Need to support writing text in compressed format

should initially support:
 - bzip2
 - LZO",8.0,0.0256350231224815,0.0128498373989236,0.0140292022743867,True,True,True,1382453155000.0,1382574487000.0,1382578087000.0,1387185311000.0,1387186211000.0,1382453155000.0,-1.0,1382519556000.0,1382523156000.0,1382513974000.0,1382517574000.0,1382574487000.0,1382578087000.0,"thomas.risberg",1382523156000.0,1382453155000.0,43,1382453155000.0,-1.0,"The HDFS Store Library should support compression when writing text","Need to support writing text in compressed format

should initially support:
 - bzip2
 - LZO",8.0,False,8.0,0,0.0,0,0,False
"XD-990","Improvement","Sprint 18","2013-10-22T14:41:21.000+0000","thomas.risberg","jvalkeal","The HDFS Store Library should support writing text with delimiter","Support writing lines of text separated by a delimiter

Support writing a CSV (comma-separated variables), TSV (tab-separated variables),

No compression",8.0,0.0256904190525003,0.0129046815121983,0.0140863043093844,True,True,True,1382452881000.0,1382574482000.0,1382578082000.0,1387185302000.0,1387186202000.0,1382452881000.0,-1.0,1382519556000.0,1382523156000.0,1382513963000.0,1382517563000.0,1382574482000.0,1382578082000.0,"thomas.risberg",1382523156000.0,1382452881000.0,43,1382452881000.0,-1.0,"The HDFS Store Library should support writing text with delimiter","Support writing lines of text separated by a delimiter

Support writing a CSV (comma-separated variables), TSV (tab-separated variables),

No compression",8.0,False,8.0,0,0.0,0,1,False
"XD-988","Story","Sprint 18","2013-10-22T08:40:38.000+0000","mark.pollack","luke","Create OOTB batch job for export and processing multiple files from HDFS to JDBC","Same setup as XD-987 for ItemReader and ItemProcessor, but should write to HDFS.  

One can assume that the table structure has been created already external to the batch job execution.",5.0,0.4504396874550224,0.0,0.0779834862239505,True,True,True,1382431238000.0,1382941365000.0,1382944965000.0,1383562847000.0,1383563747000.0,1382431238000.0,-1.0,1382519555000.0,1382523155000.0,1382431238000.0,1382434838000.0,1382941365000.0,1382944965000.0,"mark.pollack",1382523155000.0,1382431238000.0,43,1382431238000.0,-1.0,"Create OOTB batch job for export and processing multiple files from HDFS to JDBC","Same setup as XD-987 for ItemReader and ItemProcessor, but should write to HDFS.  

One can assume that the table structure has been created already external to the batch job execution.",5.0,False,5.0,0,0.0,0,0,False
"XD-987","Story","Sprint 18","2013-10-22T08:37:01.000+0000","mark.pollack","luke","Create OOTB batch job for export and processing multiple files from HDFS to MongoDB","The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure
The ItemProcessor will be a no-op groovy script.
The ItemWriter will write the data to a MongoDB collection
** A TupleToDBObject converter will need to be developed.

*the sample job should be documented*",8.0,0.6818682823001581,0.0,0.0781915974105116,True,True,True,1382431021000.0,1383203080000.0,1383206680000.0,1383562391000.0,1383563291000.0,1382431021000.0,-1.0,1382519555000.0,1382523155000.0,1382431021000.0,1382434621000.0,1383203080000.0,1383206680000.0,"mark.pollack",1382523155000.0,1382431021000.0,43,1382431021000.0,-1.0,"Create OOTB batch job for export and processing multiple files from HDFS to MongoDB","The ItemReader will read multiple files from HDFS and the data will be converted to a tuple data structure
The ItemProcessor will be a no-op groovy script.
The ItemWriter will write the data to a MongoDB collection
** A TupleToDBObject converter will need to be developed.

*the sample job should be documented*",8.0,False,8.0,0,0.0,0,1,False
"XD-985","Story","Sprint 18","2013-10-22T08:32:24.000+0000","mark.pollack","luke","Create OOTB stream that polls for files in a directory and launches a batch job.","Same processing as XD-984, but the job instacne is launched via an event from the input file source.  Supporting a single file per job launch is OK.

> job create blah --definition ""filehdfs""

> stream create csvStream --definition ""file --ref=true --dir=/Users/luke --pattern=*.csv > job:blah""




*the job should be documented*",5.0,0.7706730645019034,0.0,0.0791215389727768,True,True,True,1382430744000.0,1383295796000.0,1383299396000.0,1383552307000.0,1383553207000.0,1382430744000.0,-1.0,1382519555000.0,1382523155000.0,1382430744000.0,1382434344000.0,1383295796000.0,1383299396000.0,"mark.pollack",1382523155000.0,1382430744000.0,43,1382430744000.0,-1.0,"Create OOTB 'file to HDFS' batch import job that is launched by a stream.","Same processing as XD-984, but the job instacne is launched via an event from the input file source.  Supporting a single file per job launch is OK.

> job create blah --definition ""filehdfs""

> stream create csvStream --definition ""file --ref=true --dir=/Users/luke --pattern=*.csv > job:blah""




*the job should be documented*",5.0,False,5.0,0,0.0,0,4,True
"XD-984","Story","Sprint 18","2013-10-22T08:25:34.000+0000","mark.pollack","luke","Create OOTB batch job for import and processing multiple files to JDBC","Create a sample batch job for inclusion in the distribution that will perform the following tasks.

ItemReader
* Read a from a directory with multiple files (configurable)
* Support for CSV (assume first line has header values)
* Convert to tuple data structure
ItemProcessor
* Provide groovy based no-op ItemProcessor.  (configurable)
ItemWriter
* Write to JDBC.
** Provide 
** Assume there is an DB instance running somewhere, specify connection info (configurable)

**The sample job should be documented**
",10.0,0.1495601852058005,0.0003776075179519,0.0794587744320524,True,True,True,1382430334000.0,1382598269000.0,1382601869000.0,1383552293000.0,1383553193000.0,1382430334000.0,-1.0,1382519555000.0,1382523155000.0,1382430758000.0,1382434358000.0,1382598269000.0,1382601869000.0,"mark.pollack",1382523155000.0,1382430334000.0,43,1382430334000.0,-1.0,"Create OOTB batch job for import and processing multiple files to JDBC","Create a sample batch job for inclusion in the distribution that will perform the following tasks.

ItemReader
* Read a from a directory with multiple files (configurable)
* Support for CSV (assume first line has header values)
* Convert to tuple data structure
ItemProcessor
* Provide groovy based no-op ItemProcessor.  (configurable)
ItemWriter
* Write to JDBC.
** Provide 
** Assume there is an DB instance running somewhere, specify connection info (configurable)

**The sample job should be documented**
",10.0,False,10.0,0,0.0,0,5,False
"XD-981","Story","Sprint 19","2013-10-21T20:11:57.000+0000","thomas.risberg","thomas.risberg","Missing guava-11.0.2.jar dependency for hadoop distros","We used to have a shared guava-11.0.2.jar dependency in the lib dir. That's no longer there so hadoop distros that require this now fail (at least any hadoop 2.0.x based ones)",3.0,0.9904894392739532,0.8066644146880039,0.8189392514888851,True,True,True,1382386317000.0,1385362505000.0,1385366105000.0,1385390182000.0,1385391082000.0,1382386317000.0,-1.0,1384847037000.0,1384850637000.0,1384810154000.0,1384813754000.0,1385362505000.0,1385366105000.0,"thomas.risberg",1384850637000.0,1382386317000.0,43,1382386317000.0,-1.0,"Missing guava-11.0.2.jar dependency for hadoop distros","We used to have a shared guava-11.0.2.jar dependency in the lib dir. That's no longer there so hadoop distros that require this now fail (at least any hadoop 2.0.x based ones)

We should also upgrade to current Hadoop versions (Hadoop 2.2 stable)",3.0,False,3.0,0,0.0,0,0,True
"XD-980","Story","","2013-10-21T15:05:05.000+0000","grenfro","","Install XD Container on EC2","*Update the Deployer class to add the following methods *
** RunningInstance  deployContainer

*For each of the containers:
** Using XD-977 install distribution
** Setup XD_HOME variable
** Create configurator directory
** Copy the configurator to containers
** Run Configurators
** Verify that configuration files are setup correctly
** start container server
** Report if container started. (using jmx/Jolokia, if available) If it didn't start report failure but continue.
** Report public DNS name of container


* Integration Testing
** Verify that config files have been setup properly
** For each container:
*** Verify container has been started
*** Verify that container is working by creating a stream in admin (trigger|log).  Verify that the log on the container is being updated.

",10.0,0.9998452048671146,-1.0,-1.0,True,False,False,1382367905000.0,1384841760000.0,1384842143000.0,1384841243000.0,1384842143000.0,1382367905000.0,-1.0,-1.0,-1.0,-1.0,-1.0,1384841760000.0,1384842143000.0,"grenfro",-1.0,-1.0,0,1382367905000.0,-1.0,"Install XD Container on EC2","*Update the Deployer class to add the following methods*
* RunningInstance  deployContainer

* For each of the containers:
** Using XD-977 install distribution
** Setup XD_HOME variable
** Create configurator directory
** Copy the configurator to containers
** Run Configurators
** Verify that configuration files are setup correctly
** start container server
** Report if container started. (using jmx/Jolokia, if available) If it didn't start report failure but continue.
** Report public DNS name of container


* Integration Testing
** Verify that config files have been setup properly
** For each container:
*** Verify container has been started
*** Verify that container is working by creating a stream in admin (trigger|log).  Verify that the log on the container is being updated.

",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-978","Story","Sprint 18","2013-10-21T14:21:31.000+0000","grenfro","","Installer needs to launch a single node XD instance","* Create a Deployer class has methods
** RunningInstance  deploySingleNode
*** takes into account machine size as specified in properties file
** void  destroyAllInstances()  
*** or whatever JClouds returns from the destroy call
** ctor gets passed in the root boostrapping credentials.
* Install Script Steps
** Setup XD_HOME variable
** Make sure privileges are set to ubuntu not root.
** start up redis and rabbit using ports as specified in xd-ec2.properties
** Use port watch to make sure they started
** Start singlenode after configuration.
** Display hostname of singlenode server
** Report successful and failed startup
** Hit root of xd-admin to see if there is a response on 9393
* Integration Testing
** Verify that config files have been setup
** Verify XD has been started
** Verify XD can process a basic http post

",10.0,0.4252639989777131,0.1754991910567523,0.1132917123525597,True,True,True,1382365291000.0,1382944353000.0,1382947953000.0,1383726044000.0,1383726944000.0,1382365291000.0,-1.0,1382519555000.0,1382523155000.0,1382604260000.0,1382607860000.0,1382944353000.0,1382947953000.0,"grenfro",1382523155000.0,1382365291000.0,43,1382365291000.0,-1.0,"Installer needs to launch a single node XD instance","* Create a Deployer class has methods
** RunningInstance  deploySingleNode
*** takes into account machine size as specified in properties file
** void  destroyAllInstances()  
*** or whatever JClouds returns from the destroy call
** ctor gets passed in the root boostrapping credentials.
* Install Script Steps
** Setup XD_HOME variable
** Make sure privileges are set to ubuntu not root.
** start up redis and rabbit using ports as specified in xd-ec2.properties
** Use port watch to make sure they started
** Start singlenode after configuration.
** Display hostname of singlenode server
** Report successful and failed startup
** Hit root of xd-admin to see if there is a response on 9393
* Integration Testing
** Verify that config files have been setup
** Verify XD has been started
** Verify XD can process a basic http post

",10.0,False,10.0,0,0.0,-1,0,False
"XD-977","Story","Sprint 18","2013-10-21T13:47:33.000+0000","grenfro","","Add functionality to download XD distribution zip to each EC2 instance as specified by user ","* User specifies download distribution zip file from properties file.  See XD-969 for key-value pairs
* Copy the distribution from the shared EBS/S3  the ebs volume assigned to the each node/admin instance  .  
** If not on shared ebs/s3 pull from http site specified by user and put on the shared EBS volume. 
** Unzip the distribution from on the ebs volume for the instance to the /home/ubuntu directory
** make sure privileges are set to ubuntu not root.


How to verify it works
* Create a JUNit style integration test that
** Deletes a known .zip distribution from EBS/S3.
** Invoke the application functionality (should be a 1 liner) that will start up the instance and download the .zip distribution from the URI provided in xd-ec2.properties.  Verify the file is now in EBS/S3 and also on the instance
** Tear down created instance
** Create new instance passing in the same URI of the .zip distribution.  Verify as before
",5.0,0.4260913625927694,0.1767196023262652,0.1146135124888999,True,True,True,1382363253000.0,1382944327000.0,1382947927000.0,1383726084000.0,1383726984000.0,1382363253000.0,-1.0,1382519555000.0,1382523155000.0,1382604251000.0,1382607851000.0,1382944327000.0,1382947927000.0,"grenfro",1382523155000.0,1382363253000.0,43,1382363253000.0,-1.0,"Add functionality to download XD distribution zip to each EC2 instance as specified by user ","* User specifies download distribution zip file from properties file.  See XD-969 for key-value pairs
* Copy the distribution from the shared EBS/S3  the ebs volume assigned to the each node/admin instance  .  
** If not on shared ebs/s3 pull from http site specified by user and put on the shared EBS volume. 
** Unzip the distribution from on the ebs volume for the instance to the /home/ubuntu directory
** make sure privileges are set to ubuntu not root.


How to verify it works
* Create a JUNit style integration test that
** Deletes a known .zip distribution from EBS/S3.
** Invoke the application functionality (should be a 1 liner) that will start up the instance and download the .zip distribution from the URI provided in xd-ec2.properties.  Verify the file is now in EBS/S3 and also on the instance
** Tear down created instance
** Create new instance passing in the same URI of the .zip distribution.  Verify as before
",5.0,False,5.0,0,0.0,-1,5,False
"XD-976","Story","Sprint 18","2013-10-21T13:37:05.000+0000","grenfro","","Bootstrap Project for XD AWS Installer","* Create a Spring application context.  (XML or Java, dealers choice)
* Use XD eclipse code format policy
* Create Source Package structure
* Create Test Package Structure
* Gradle build
* JClouds and Spring deps

",5.0,0.4160136188494888,0.3149124316320199,0.2045551714608979,True,True,True,1382362625000.0,1382681779000.0,1382685379000.0,1383128897000.0,1383129797000.0,1382362625000.0,-1.0,1382519554000.0,1382523154000.0,1382604217000.0,1382607817000.0,1382681779000.0,1382685379000.0,"grenfro",1382523154000.0,1382362625000.0,43,1382362625000.0,-1.0,"Bootstrap Project for XD AWS Installer","* Create a Spring application context.  (XML or Java, dealers choice)
* Use XD eclipse code format policy
* Create Source Package structure
* Create Test Package Structure
* Gradle build
* JClouds and Spring deps

",5.0,False,5.0,0,0.0,-1,0,False
"XD-972","Story","Sprint 19","2013-10-21T13:11:43.000+0000","grenfro","grenfro","Create CI for XD-EC2 project","Automated test will use directly use the Deployer class
* asserts on basic info of RunningInstance
** check that EBS was mounted
** that application was unzipped
** redis and rabbit are running via port checks

* http requests on admin port for 
** root path
** list of modules
* @AfterClass that will look for the cluster name and terminate all instances
Look at live tag in JClouds tests for some additional tactics
",5.0,0.8268061877818429,0.8268068525057433,0.828117688037097,True,True,True,1382361103000.0,1384848771000.0,1384852371000.0,1385368971000.0,1385369871000.0,1382361103000.0,-1.0,1384852717000.0,1384856317000.0,1384848773000.0,1384852373000.0,1384848771000.0,1384852371000.0,"grenfro",1384856317000.0,1382361103000.0,43,1382361103000.0,-1.0,"Create CI for XD-EC2 project","Automated test will use directly use the Deployer class
* asserts on basic info of RunningInstance
** check that EBS was mounted
** that application was unzipped
** redis and rabbit are running via port checks

* http requests on admin port for 
** root path
** list of modules
* @AfterClass that will look for the cluster name and terminate all instances
Look at live tag in JClouds tests for some additional tactics
",5.0,False,5.0,0,0.0,0,2,False
"XD-970","Story","","2013-10-21T13:04:33.000+0000","grenfro","","User needs the ability to install XD on ec2","* Create a Spring application context.  (XML or Java, dealers choice)
* Use XD eclipse code format policy
* Setup a volume on EBS/S3 to copy builds to and check there first before doing a wget .  
** If not there pull from http site and put on the shared EBS volume. 
** User specifies download zip file from properties file
* Report successful and failed nodes
* Report public DNS name of admin-server and all xd-containers to user
* The install script steps:
** Make sure privileges are set to ubuntu not root.
** Unzip the distribution from EBS to local drive
** Setup XD_HOME variable
** start up redis and rabbit using ports as specified in xd-ec2.properties
** Use port watch to make sure they started
** Hit root of xd-admin to see if there is a response on 9393
* Integration Testing
** Verify that config files have been setup
** Verify XD has been started
** Verify XD can process a basic http post
*Launching of run single node server.*
* Create a Deployer class has methods
** RunningInstance  deploySingleNode
*** takes into account machine size
** RunningInstance  deployAdminServer
** RunningInstance  deployContainer
** void  destroyAllInstances()  
*** or whatever JClouds returns from the destroy call
** ctor gets passed in the root boostrapping credentials.
*Launching of run multi node*
* Re-use scripts as much as possible (not cut-n-paste) from single node case
* configure xd-containers to point back to DNS/port locations of rabbit/redis/batch
** Create configurator directory
** Copy the configurator to containers
** Validate that configuration completed successfully
* Start containers after configuration.
* Display hostname of admin server
* Display hostname for each of the container started
* Validate containers are running via JMX (if available) via Jolokia
",10.0,0.7455590264763332,0.7456387630310957,-1.0,True,False,True,1382360673000.0,1383735164000.0,1383738764000.0,1384203344000.0,1384204244000.0,1382360673000.0,-1.0,-1.0,-1.0,1383735311000.0,1383738911000.0,1383735164000.0,1383738764000.0,"grenfro",-1.0,-1.0,0,1382360673000.0,-1.0,"Install XD admin instance on EC2","*Update the Deployer class to add the following methods *
** RunningInstance  deployAdminServer

* The install script steps:
** Using XD-977 install distribution
** Setup XD_HOME variable
** start up redis and rabbit using ports as specified in xd-ec2.properties on admin server
** Create configurator directory
** Copy the configurator to containers
** Use port watch to make sure they started
** start admin server
** use port watch to make sure the admin started on 9393
** Report if admin server started.  If it didn't start abort install.
** Report public DNS name of admin-server 


* Integration Testing
** Verify XD admin has been started
*** Create a basic stream (trigger>log)and make sure we get a success code from xd admin was received.
*** Query the redis to see if the stream was created.

",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-969","Story","Sprint 18","2013-10-21T12:57:21.000+0000","grenfro","","Add functionality to provision EC2 instance and mount EBS","* The Application should 
**  Create a Spring Application Context.
**  Should take a command line parameter --config that will point to a property file with key/value pairs specified below.  By default the file xd-ec2.properties will be looked for on the classpath
** Associate EBS shared volume for each machine instance

* Packaging 
** Use gradle application plugin to generate a bin directory with a script to start the application.  Seehttp://www.gradle.org/docs/current/userguide/application_plugin.html
** The config file xd-ec2.properties should be in a directory (ideally a 'config' directory parallel to 'bin' and will be loaded by the application by default - loading via the CP is probably the easiest way.
** Create a POJO to easily reference these properties, vs. using a raw java Properties object.

How to verify it works

* Integration testing
** Create JUnit based tests.  JClouds itself has extensive testing, can look at those for structure.
** Verify what you created has been installed.
** Verify ports are open

Instance Information
** EBS of 50GB Base for each instance.
* Report successful and failed Instances.

Key-Value pairs in configuration file
properties may include: 
* cluster-name= a name describing the cluster you are creating
* aws-access-keys= the access key assigned to you by admin
* aws-secret-key= the secret key assigned to you by admin
* private-key-file= the private key file assigned to you by admin.  Used for ssh-ing into 
* multi-node=true/false if true then installer will run the number of nodes as enumerated in the number-nodes property value.  if false the installer will start a single node server
* number-nodes=The number of nodes(containers) to deploy for this cluster.  Value is an integer > 0
* machine-size=The size of machines to be assigned for admin and nodes.  small, medium, large
* redis-port=6379
* rabbit-port=5279
* xd-dist-url=The url to download the XD to install.  for example: http://blahblahblah.zip
* ami = The ami image to use for your cluster. for example: ami-dfadsfdadf

",10.0,0.8693066322073064,0.1785423163773183,0.1165732741904562,True,True,True,1382360241000.0,1383548265000.0,1383551865000.0,1383725975000.0,1383726875000.0,1382360241000.0,-1.0,1382519554000.0,1382523154000.0,1382604243000.0,1382607843000.0,1383548265000.0,1383551865000.0,"grenfro",1382523154000.0,1382360241000.0,43,1382360241000.0,-1.0,"Add functionality to provision EC2 instance and mount EBS","* The Application should 
**  Create a Spring Application Context.
**  Should take a command line parameter --config that will point to a property file with key/value pairs specified below.  By default the file xd-ec2.properties will be looked for on the classpath
** Associate EBS shared volume for each machine instance

* Packaging 
** Use gradle application plugin to generate a bin directory with a script to start the application.  Seehttp://www.gradle.org/docs/current/userguide/application_plugin.html
** The config file xd-ec2.properties should be in a directory (ideally a 'config' directory parallel to 'bin' and will be loaded by the application by default - loading via the CP is probably the easiest way.
** Create a POJO to easily reference these properties, vs. using a raw java Properties object.

How to verify it works

* Integration testing
** Create JUnit based tests.  JClouds itself has extensive testing, can look at those for structure.
** Verify what you created has been installed.
** Verify ports are open

Instance Information
** EBS of 50GB Base for each instance.
* Report successful and failed Instances.

Key-Value pairs in configuration file
properties may include: 
* cluster-name= a name describing the cluster you are creating
* aws-access-keys= the access key assigned to you by admin
* aws-secret-key= the secret key assigned to you by admin
* private-key-file= the private key file assigned to you by admin.  Used for ssh-ing into 
* multi-node=true/false if true then installer will run the number of nodes as enumerated in the number-nodes property value.  if false the installer will start a single node server
* number-nodes=The number of nodes(containers) to deploy for this cluster.  Value is an integer > 0
* machine-size=The size of machines to be assigned for admin and nodes.  small, medium, large
* redis-port=6379
* rabbit-port=5279
* xd-dist-url=The url to download the XD to install.  for example: http://blahblahblah.zip
* ami = The ami image to use for your cluster. for example: ami-dfadsfdadf

",10.0,False,10.0,0,0.0,-1,7,False
"XD-968","Story","","2013-10-21T12:53:47.000+0000","grenfro","","Users need instructions on instance management.","* Logging into your XD Account For Example: https://946513944028.signin.aws.amazon.com/console
* Discuss how to terminate running instances.
** Users can terminate all instances using the UI on the EC2 admin page
* Usage monitoring via CloudWatch
* Investigate what metadata in each instance is required so that cloudwatch can track
",3.0,-1.0,-1.0,-1.0,False,False,False,1382360027000.0,-1.0,-1.0,1384840988000.0,1384841888000.0,1382360027000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1382360027000.0,-1.0,"Create google doc with instructions on managing EC2 Instances","* Logging into your XD Account For Example: https://946513944028.signin.aws.amazon.com/console
* Discuss how to terminate running instances.
** Users can terminate all instances using the UI on the EC2 admin page
* Usage monitoring via CloudWatch
* Investigate what metadata in each instance is required so that cloudwatch can track
",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-967","Story","","2013-10-21T12:51:03.000+0000","grenfro","","User needs instructions on usage","Add instructions to github wiki on the usage of the installer
",5.0,0.999984286903416,-1.0,-1.0,True,False,False,1382359863000.0,1384841830000.0,1384841869000.0,1384840969000.0,1384841869000.0,1382359863000.0,-1.0,-1.0,-1.0,-1.0,-1.0,1384841830000.0,1384841869000.0,"grenfro",-1.0,-1.0,0,1382359863000.0,-1.0,"User requires wiki page on how to use the XD EC2 Installer","Add instructions to github wiki on the usage of the installer
",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-966","Story","Sprint 18","2013-10-21T12:49:44.000+0000","grenfro","","User requests that XD team members are issued EC2 accounts","* Setup groups for xd user
* Setup privileges so users can only see their instances
* Setup user accounts
** Send created access key to users
** Send username and passwords to user
",5.0,0.4181657045677741,0.418187946332333,0.273351286429444,True,True,True,1382359784000.0,1382604196000.0,1382607796000.0,1382943370000.0,1382944270000.0,1382359784000.0,-1.0,1382519554000.0,1382523154000.0,1382604209000.0,1382607809000.0,1382604196000.0,1382607796000.0,"grenfro",1382523154000.0,1382359784000.0,43,1382359784000.0,-1.0,"User requests that XD team members are issued EC2 accounts","* Setup groups for xd user
* Setup privileges so users can only see their instances
* Setup user accounts
** Send created access key to users
** Send username and passwords to user
",5.0,False,5.0,0,0.0,-1,0,False
"XD-962","Story","Sprint 17","2013-10-21T06:58:22.000+0000","dturanski","dturanski","fix gradle clean test","Currently ./gradlew clean test fails since the module dependencies are not packaged before the test task.  ",1.0,0.9941625155666252,0.9820205479452054,0.9931506849315068,True,True,True,1382338702000.0,1382351475000.0,1382351550000.0,1382350650000.0,1382351550000.0,1382338702000.0,-1.0,1382351462000.0,1382351550000.0,1382351319000.0,1382351550000.0,1382351475000.0,1382351550000.0,"dturanski",1382351550000.0,1382338702000.0,43,1382338702000.0,-1.0,"fix gradle clean test","Currently ./gradlew clean test fails since the module dependencies are not packaged before the test task.  ",1.0,False,1.0,0,0.0,-1,0,False
"XD-961","Story","Sprint 17","2013-10-18T11:35:32.000+0000","mark.pollack","eric.bottard","Accessing xd-admin URLs in the browser return XML and not JSON","Here is an example:
the following request for streams:
http://ec2-23-20-25-30.compute-1.amazonaws.com:9393/streams

Returns:
This XML file does not appear to have any style information associated with it. The document tree is shown below.
<errors xmlns:atom=""http://www.w3.org/2005/Atom"">
<error logref=""HttpMessageNotWritableException"">
<message>
Could not marshal [PagedResource { content: [links: [<http://ec2-23-20-25-30.compute-1.amazonaws.com:9393/streams/ticktock>;rel=""self""]], metadata: Metadata { number: 0, total pages: 1, total elements: 1, size: 20 }, links: [] }]: null; nested exception is javax.xml.bind.MarshalException - with linked exception: [com.sun.istack.SAXException2: unable to marshal type ""org.springframework.xd.rest.client.domain.StreamDefinitionResource"" as an element because it is not known to this context.]
</message>
</error>
</errors>",1.0,-1.0,0.0,0.9999644210413962,False,True,True,1382096132000.0,-1.0,-1.0,1382320084000.0,1382320984000.0,1382096132000.0,-1.0,1382320976000.0,1382320984000.0,1382096132000.0,1382099732000.0,-1.0,-1.0,"mark.pollack",1382320984000.0,1382096132000.0,43,1382096132000.0,-1.0,"Accessing xd-admin URLs in the browser return XML and not JSON","Here is an example:
the following request for streams:
http://ec2-23-20-25-30.compute-1.amazonaws.com:9393/streams

Returns:
This XML file does not appear to have any style information associated with it. The document tree is shown below.
<errors xmlns:atom=""http://www.w3.org/2005/Atom"">
<error logref=""HttpMessageNotWritableException"">
<message>
Could not marshal [PagedResource { content: [links: [<http://ec2-23-20-25-30.compute-1.amazonaws.com:9393/streams/ticktock>;rel=""self""]], metadata: Metadata { number: 0, total pages: 1, total elements: 1, size: 20 }, links: [] }]: null; nested exception is javax.xml.bind.MarshalException - with linked exception: [com.sun.istack.SAXException2: unable to marshal type ""org.springframework.xd.rest.client.domain.StreamDefinitionResource"" as an element because it is not known to this context.]
</message>
</error>
</errors>",1.0,False,1.0,0,0.0,0,0,False
"XD-959","Bug","","2013-10-16T08:02:45.000+0000","thomas.risberg","","Running gradle idea creates project configured with source 1.6","The gradle idea task creates a project configured with source 1.6

This results in compile failures on Java 7 specific code",3.0,-1.0,0.9996970404386726,-1.0,False,False,True,1381910565000.0,-1.0,-1.0,1384398446000.0,1384399346000.0,1381910565000.0,-1.0,-1.0,-1.0,1384398592000.0,1384399346000.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1381910565000.0,-1.0,"Running gradle idea creates project configured with source 1.6","The gradle idea task creates a project configured with source 1.6

This results in compile failures on Java 7 specific code",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-958","Bug","","2013-10-16T07:45:50.000+0000","thomas.risberg","","Build failure on Ubuntu","Build fails:

gradle clean build

...

:spring-xd-dirt:test

org.springframework.xd.dirt.stream.FileSourceModuleTests > classMethod FAILED
    java.io.IOException at FileSourceModuleTests.java:53

org.springframework.xd.dirt.stream.FileSourceModuleTests > classMethod FAILED
    java.lang.IllegalArgumentException at FileSourceModuleTests.java:130

328 tests completed, 2 failed, 11 skipped
:spring-xd-dirt:test FAILED

FAILURE: Build failed with an exception.


Looks like it is trying to create a directory under the local filesystem /

java.io.IOException: Unable to create directory /tmpfilesourcetests
	at org.apache.commons.io.FileUtils.forceMkdir(FileUtils.java:2024)
	at org.springframework.xd.dirt.stream.FileSourceModuleTests.createTempDir(FileSourceModuleTests.java:53)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:80)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:47)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:69)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:49)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)
	at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:103)
	at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:355)
	at org.gradle.internal.concurrent.DefaultExecutorFactory$StoppableExecutorImpl$1.run(DefaultExecutorFactory.java:66)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
",3.0,-1.0,-1.0,-1.0,False,False,False,1381909550000.0,-1.0,-1.0,1382370752000.0,1382371652000.0,1381909550000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1381909550000.0,-1.0,"Build failure on Ubuntu","Build fails:

gradle clean build

...

:spring-xd-dirt:test

org.springframework.xd.dirt.stream.FileSourceModuleTests > classMethod FAILED
    java.io.IOException at FileSourceModuleTests.java:53

org.springframework.xd.dirt.stream.FileSourceModuleTests > classMethod FAILED
    java.lang.IllegalArgumentException at FileSourceModuleTests.java:130

328 tests completed, 2 failed, 11 skipped
:spring-xd-dirt:test FAILED

FAILURE: Build failed with an exception.


Looks like it is trying to create a directory under the local filesystem /

java.io.IOException: Unable to create directory /tmpfilesourcetests
	at org.apache.commons.io.FileUtils.forceMkdir(FileUtils.java:2024)
	at org.springframework.xd.dirt.stream.FileSourceModuleTests.createTempDir(FileSourceModuleTests.java:53)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.runTestClass(JUnitTestClassExecuter.java:80)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecuter.execute(JUnitTestClassExecuter.java:47)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassProcessor.processTestClass(JUnitTestClassProcessor.java:69)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:49)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.messaging.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32)
	at org.gradle.messaging.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.processTestClass(TestWorker.java:103)
	at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35)
	at org.gradle.messaging.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.messaging.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:355)
	at org.gradle.internal.concurrent.DefaultExecutorFactory$StoppableExecutorImpl$1.run(DefaultExecutorFactory.java:66)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-957","Story","Sprint 17","2013-10-15T09:55:50.000+0000","grussell","grussell","Update SI Dependency to 4.0.0.BUILD-SNAPSHOT","",3.0,0.0001944534109072,0.0,0.000136117387635,True,True,True,1381830950000.0,1381831000000.0,1381834600000.0,1382087181000.0,1382088081000.0,1381830950000.0,-1.0,1381830985000.0,1381834585000.0,1381830950000.0,1381834550000.0,1381831000000.0,1381834600000.0,"grussell",1381834585000.0,1381830950000.0,43,1381830950000.0,-1.0,"Update SI Dependency to 4.0.0.BUILD-SNAPSHOT","",3.0,False,3.0,0,0.0,-1,0,False
"XD-956","Improvement","Sprint 18","2013-10-15T09:40:44.000+0000","aclement","aclement","Refactor DSL parser according to latest syntax proposals","We have recently revised the syntax for stream definitions, this issue covers that refactoring.",12.0,-1.0,0.0,0.9999815559766402,False,True,True,1381830044000.0,-1.0,-1.0,1383889432000.0,1383890332000.0,1381830044000.0,-1.0,1383890294000.0,1383890332000.0,1381830044000.0,1381833644000.0,-1.0,-1.0,"aclement",1383890332000.0,1383890270000.0,43,1383890270000.0,-1.0,"Refactor DSL parser according to latest syntax proposals","We have recently revised the syntax for stream definitions, this issue covers that refactoring.",12.0,False,12.0,0,0.0,0,0,False
"XD-955","Story","Sprint 18","2013-10-14T09:08:27.000+0000","luke","grenfro","Update Jobs documentation to include ""job launch"" command","This is currently missing and probably supersedes some of the stuff that's in there now.",1.0,-1.0,0.0032626011055411,0.8271852940488369,False,True,True,1381741707000.0,-1.0,-1.0,1382681161000.0,1382682061000.0,1381741707000.0,-1.0,1382519554000.0,1382523154000.0,1381744775000.0,1381748375000.0,-1.0,-1.0,"luke",1382523154000.0,1381741707000.0,43,1381741707000.0,-1.0,"Update Jobs documentation to include ""job launch"" command","This is currently missing and probably supersedes some of the stuff that's in there now.",1.0,False,1.0,0,0.0,-1,0,False
"XD-954","Story","Sprint 21","2013-10-14T07:57:18.000+0000","eric.bottard","eric.bottard","Stream definition completion REST layer + Shell adapter","Provide the infrastructure for HTTP GET /completions?start='http | file --d"" that would return a list of possible completions (in this case returning the file option names that start with ""d"")

This story is about (and only about):
- Having that REST controller, delegating to some ""CompletionsEngine""
- Implementing the Spring Shell Converter that talks to that

It's an empty shell, useless (but easy to do) without the actual ""CompletionsEngine""",3.0,-1.0,0.810206257858019,0.9999967971259268,False,True,True,1381737438000.0,-1.0,-1.0,1391415345000.0,1391416245000.0,1381737438000.0,-1.0,1391416214000.0,1391416245000.0,1389579268000.0,1389582868000.0,-1.0,-1.0,"eric.bottard",1391416245000.0,1381737438000.0,43,1381737438000.0,-1.0,"Stream definition completion REST layer + Shell adapter","Provide the infrastructure for HTTP GET /completions?start='http | file --d"" that would return a list of possible completions (in this case returning the file option names that start with ""d"")

This story is about (and only about):
- Having that REST controller, delegating to some ""CompletionsEngine""
- Implementing the Spring Shell Converter that talks to that

It's an empty shell, useless (but easy to do) without the actual ""CompletionsEngine""",3.0,False,3.0,0,0.0,0,0,False
"XD-953","Story","Sprint 18","2013-10-14T07:25:02.000+0000","eric.bottard","eric.bottard","Richer module options metadata","Module options are currently implemented using PropertySourcesPlaceholderConfigurer, doing simple ""text"" replacements.

This story proposes to introduce a richer model for module options, which could take the following xml representation (keeping in mind that there would be an underlying set of classes that could also apply to e.g. @Configuration approach):

{code:xml}
<xd:params>
  <xd:param name=""port"" help=""the http port to listen on"" />
</xd:params>
{code}

So, the very first obvious benefit is providing help metadata.
The second one is an easy way to list available option names, without resorting to brittle PropertySourcesPlaceholderConfigurer hacks (we wouldn't for example detect xd.stream.name or XD_TRANSPORT as a valid option)

One could also specify defaults/computations, this time benefiting from SpEL everywhere:
{code:xml}
<xd:params>
  <xd:param name=""directory"" default=""headers.directory"" />
</xd:params>
{code}

Lastly, this opens the door to better type checking / combinations / optionality support:
{code:xml}
<xd:params valid=""fixedDelay || cron"">
  <xd:param name=""fixedDelay"" type=""int"" />
  <xd:param name=""cron"" type=""string"" />
</xd:params>
{code}

The 'valid' attribute can e.g. be evaluated by SpEL.

Some final remarks:
- That construct can be compatible with our current approach by behaving as a PropertySource itself (instead of creating a PropertySource, the StreamPlugin would give this new bean the java.util.Properties)
- Plugins could benefit from a hook in that construct and advertise the fact that they expect/provide new options (e.g. --inputType)

There is a slight problem though, which is that if this construct lives in the same AppContext as the module definition itself, then the AppContext needs to be refreshed for the logic to kick in. One could circumvent that using profiles, or we could rely on a different filename convention (e.g. <module>-params.xml)


",10.0,0.4924215560783228,0.4924753403272677,0.4924177722618141,True,True,True,1381735502000.0,1383557446000.0,1383561046000.0,1385434570000.0,1385435470000.0,1381735502000.0,-1.0,1383557432000.0,1383561032000.0,1383557645000.0,1383561245000.0,1383557446000.0,1383561046000.0,"eric.bottard",1383561032000.0,1381735502000.0,43,1381735502000.0,-1.0,"Richer module options metadata","Module options are currently implemented using PropertySourcesPlaceholderConfigurer, doing simple ""text"" replacements.

This story proposes to introduce a richer model for module options, which could take the following xml representation (keeping in mind that there would be an underlying set of classes that could also apply to e.g. @Configuration approach):

{code:xml}
<xd:params>
  <xd:param name=""port"" help=""the http port to listen on"" />
</xd:params>
{code}

So, the very first obvious benefit is providing help metadata.
The second one is an easy way to list available option names, without resorting to brittle PropertySourcesPlaceholderConfigurer hacks (we wouldn't for example detect xd.stream.name or XD_TRANSPORT as a valid option)

One could also specify defaults/computations, this time benefiting from SpEL everywhere:
{code:xml}
<xd:params>
  <xd:param name=""directory"" default=""headers.directory"" />
</xd:params>
{code}

Lastly, this opens the door to better type checking / combinations / optionality support:
{code:xml}
<xd:params valid=""fixedDelay || cron"">
  <xd:param name=""fixedDelay"" type=""int"" />
  <xd:param name=""cron"" type=""string"" />
</xd:params>
{code}

The 'valid' attribute can e.g. be evaluated by SpEL.

Some final remarks:
- That construct can be compatible with our current approach by behaving as a PropertySource itself (instead of creating a PropertySource, the StreamPlugin would give this new bean the java.util.Properties)
- Plugins could benefit from a hook in that construct and advertise the fact that they expect/provide new options (e.g. --inputType)

There is a slight problem though, which is that if this construct lives in the same AppContext as the module definition itself, then the AppContext needs to be refreshed for the logic to kick in. One could circumvent that using profiles, or we could rely on a different filename convention (e.g. <module>-params.xml)


",10.0,False,10.0,0,0.0,0,0,False
"XD-950","Story","Sprint 17","2013-10-10T15:58:51.000+0000","mark.pollack","iperumal","Spike for advanced Job Orchestration features","",20.0,0.9999535300364047,9.887226296858036e-06,0.0608954267623486,True,True,True,1381420731000.0,1382432090000.0,1382432137000.0,1382431237000.0,1382432137000.0,1381420731000.0,-1.0,1381482321000.0,1381485921000.0,1381420741000.0,1381424341000.0,1382432090000.0,1382432137000.0,"mark.pollack",1381485921000.0,1381420731000.0,43,1381420731000.0,-1.0,"Spike for advanced Job Orchestration features","",20.0,False,20.0,0,0.0,-1,0,False
"XD-949","Story","Sprint 17","2013-10-10T15:58:00.000+0000","mark.pollack","","Provide job that exports HDFS CSV data to JDBC","",10.0,0.999967373217762,-1.0,0.0609438631494336,True,True,False,1381420680000.0,1382432086000.0,1382432119000.0,1382431219000.0,1382432119000.0,1381420680000.0,-1.0,1381482321000.0,1381485921000.0,-1.0,-1.0,1382432086000.0,1382432119000.0,"mark.pollack",1381485921000.0,1381420680000.0,43,1381420680000.0,-1.0,"Spike for job that exports HDFS CSV data to JDBC","",10.0,False,10.0,0,0.0,-1,2,True
"XD-948","Story","Sprint 17","2013-10-10T15:57:23.000+0000","mark.pollack","luke","Provide job that imports data from CSV file to HDFS","TBD",10.0,0.6804384835776274,1.3841022021066037e-05,0.0609776111582364,True,True,True,1381420643000.0,1382108897000.0,1382112497000.0,1382431229000.0,1382432129000.0,1381420643000.0,-1.0,1381482321000.0,1381485921000.0,1381420657000.0,1381424257000.0,1382108897000.0,1382112497000.0,"mark.pollack",1381485921000.0,1381420643000.0,43,1381420643000.0,-1.0,"Spike for job that imports data from CSV file to HDFS","TBD",10.0,False,10.0,0,0.0,-1,2,True
"XD-947","Story","Sprint 17","2013-10-10T15:56:23.000+0000","mark.pollack","","Spike for Deployment SPI","SPI for deployment on to YARN + Local 'dirt' cluster.
Dirt cluster improvements.",20.0,0.0359603782630757,0.2577374274314348,0.0046952819937479,True,True,True,1381420583000.0,1381893424000.0,1381897024000.0,1394568627000.0,1394569527000.0,1381420583000.0,-1.0,1381482321000.0,1381485921000.0,1384809558000.0,1384813158000.0,1381893424000.0,1381897024000.0,"mark.pollack",1381485921000.0,1381420583000.0,43,1381420583000.0,-1.0,"Spike for Deployment SPI","SPI for deployment on to YARN + Local 'dirt' cluster.",20.0,False,20.0,0,0.0,0,0,True
"XD-946","Story","Sprint 17","2013-10-10T15:53:53.000+0000","mark.pollack","grenfro","Spike for EC2 deployments","",20.0,0.750049298329522,0.0001013203236585,0.0674248622642804,True,True,True,1381420433000.0,1382108889000.0,1382112489000.0,1382337414000.0,1382338314000.0,1381420433000.0,-1.0,1381482321000.0,1381485921000.0,1381420526000.0,1381424126000.0,1382108889000.0,1382112489000.0,"mark.pollack",1381485921000.0,1381420433000.0,43,1381420433000.0,-1.0,"Spike for EC2 deployments","",20.0,False,20.0,0,0.0,-1,0,False
"XD-945","Story","Sprint 17","2013-10-10T15:47:38.000+0000","mark.pollack","thomas.risberg","Spike for writing to HDFS","See Epic https://jira.springsource.org/browse/XD-234",10.0,0.4878405697719687,0.0003942477377689,0.0615214207937521,True,True,True,1381420058000.0,1381913779000.0,1381917379000.0,1382431212000.0,1382432112000.0,1381420058000.0,-1.0,1381482321000.0,1381485921000.0,1381420457000.0,1381424057000.0,1381913779000.0,1381917379000.0,"mark.pollack",1381485921000.0,1381420058000.0,43,1381420058000.0,-1.0,"Spike for writing to HDFS","See Epic https://jira.springsource.org/browse/XD-234",10.0,False,10.0,0,0.0,0,0,False
"XD-944","Improvement","Sprint 24","2013-10-10T12:08:51.000+0000","dturanski","iperumal","Clean up unused JSON mapping classes","There are a few obsolete classes lurking around, TypedJsonMapper comes to mind but there are likely some others",1.0,0.9831747655231672,0.9828057324785928,0.9828057324785928,True,True,True,1381406931000.0,1395681670000.0,1395685270000.0,1395925056000.0,1395925956000.0,1381406931000.0,1395743317000.0,1395676312000.0,1395679912000.0,1395676312000.0,1395679912000.0,1395681670000.0,1395685270000.0,"dturanski",1395679912000.0,1395676312000.0,44,1395743317000.0,1395743317000.0,"Clean up unused JSON mapping classes","There are a few obsolete classes lurking around, TypedJsonMapper comes to mind but there are likely some others",2.0,True,2.0,1,-50.0,0,0,False
"XD-943","Story","","2013-10-10T08:39:15.000+0000","eric.bottard","","JobPlugin should not add properties that are not needed","If you create a simple ""http | log"" stream and list the modules, you'll see that JobPlugin adds its numberFormat, makeUnique, etc properties.

Even though they do no harm, it's really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties",3.0,-1.0,-1.0,-1.0,False,False,False,1381394355000.0,-1.0,-1.0,1390957328000.0,1390958228000.0,1381394355000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1381394355000.0,-1.0,"JobPlugin should not add properties that are not needed","If you create a simple ""http | log"" stream and list the modules, you'll see that JobPlugin adds its numberFormat, makeUnique, etc properties.

Even though they do no harm, it's really strange for users. Plus, it could conflict with e.g. activation of profiles given present properties",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-942","Story","Sprint 17","2013-10-10T07:00:00.000+0000","dturanski","","Change repo links in build.gradle repo.spring.io","",1.0,-1.0,0.9999729634562488,0.0604601491001015,False,True,True,1381388400000.0,-1.0,-1.0,1382940953000.0,1382941853000.0,1381388400000.0,-1.0,1381482322000.0,1381485922000.0,1382941811000.0,1382941853000.0,-1.0,-1.0,"dturanski",1381485922000.0,1381388400000.0,43,1381388400000.0,-1.0,"Change repo links in build.gradle repo.spring.io","",1.0,False,1.0,0,0.0,-1,0,False
"XD-941","Story","Sprint 17","2013-10-10T01:28:33.000+0000","eric.bottard","eric.bottard","Convert remaining modules to be CP-aware","Now that XD-924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects.

The hadoop module will get its own story, as classpath handling is a bit more tricky for that one.

From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP-aware module and project.",3.0,0.3646425313048235,0.0,0.1194858943252887,True,True,True,1381368513000.0,1381715831000.0,1381719431000.0,1382320102000.0,1382321002000.0,1381368513000.0,-1.0,1381482322000.0,1381485922000.0,1381368513000.0,1381372113000.0,1381715831000.0,1381719431000.0,"eric.bottard",1381485922000.0,1381368513000.0,43,1381368513000.0,-1.0,"Convert remaining modules to be CP-aware","Now that XD-924 is merged, we can convert splunk, twitter and gemfire modules to rely on the newly created projects.

The hadoop module will get its own story, as classpath handling is a bit more tricky for that one.

From there on, no new dependency should be added to DIRT for the sole purpose of a module. Rather, it should directly be created as a CP-aware module and project.",3.0,False,3.0,0,0.0,-1,0,False
"XD-940","Story","Sprint 16","2013-10-09T20:37:31.000+0000","grussell","grussell","Splunk Pulls in an Old SI Jar into STS","",1.0,-1.0,0.0,0.9291217257318952,False,True,True,1381351051000.0,-1.0,-1.0,1381351449000.0,1381352349000.0,1381351051000.0,-1.0,1381352257000.0,1381352349000.0,1381351051000.0,1381352349000.0,-1.0,-1.0,"grussell",1381352349000.0,1381351051000.0,43,1381351051000.0,-1.0,"Splunk Pulls in an Old SI Jar into STS","",1.0,False,1.0,0,0.0,-1,0,False
"XD-937","Story","Sprint 18","2013-10-09T10:20:39.000+0000","iperumal","mark.fisher","Inject ModuleDefinitionRepository into ModulesController","Currently, ModulesController creates the ModuleDefinitionRepository instance with ModuleRegistry. Instead, we should inject the moduleDefinitionRepository into ModulesController directly.",1.0,0.7060320330588372,0.2712569761994852,0.7050121028983211,True,True,True,1381314039000.0,1382521298000.0,1382524898000.0,1383023060000.0,1383023960000.0,1381314039000.0,-1.0,1382519554000.0,1382523154000.0,1381777867000.0,1381781467000.0,1382521298000.0,1382524898000.0,"iperumal",1382523154000.0,1381314039000.0,43,1381314039000.0,-1.0,"Inject ModuleDefinitionRepository into ModulesController","Currently, ModulesController creates the ModuleDefinitionRepository instance with ModuleRegistry. Instead, we should inject the moduleDefinitionRepository into ModulesController directly.",1.0,False,1.0,0,0.0,0,0,False
"XD-935","Story","Sprint 17","2013-10-09T09:52:48.000+0000","dturanski","dturanski","Implement Kryo serialization for Tuple","Currently TupleCodec uses JSON for serialization/deserialization. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple's conversionService field. ",3.0,0.1810783407697586,0.0,0.0762810721917489,True,True,True,1381312368000.0,1381715810000.0,1381719410000.0,1383539465000.0,1383540365000.0,1381312368000.0,-1.0,1381482322000.0,1381485922000.0,1381312368000.0,1381315968000.0,1381715810000.0,1381719410000.0,"dturanski",1381485922000.0,1381312368000.0,43,1381312368000.0,-1.0,"Implement Kryo serialization for Tuple","Currently TupleCodec uses JSON for serialization/deserialization. It should use Kryo. This will require some customization and potentially changes to Tuple to address the Tuple's conversionService field. ",3.0,False,3.0,0,0.0,-1,0,False
"XD-933","Story","Sprint 17","2013-10-09T08:56:46.000+0000","eric.bottard","","Remove work around Spring HATEOAS#89","See https://github.com/spring-projects/spring-hateoas/issues/89

Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https://github.com/spring-projects/spring-xd/blob/4919ea2498a13ef47aaa9437937308fb26a7a24f/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/rest/XDController.java#L219",2.0,0.9995709906174488,0.9990295112075802,0.0502393176663744,True,True,True,1381309006000.0,1384757334000.0,1384758814000.0,1384757914000.0,1384758814000.0,1381309006000.0,-1.0,1381482322000.0,1381485922000.0,1384755466000.0,1384758814000.0,1384757334000.0,1384758814000.0,"eric.bottard",1381485922000.0,1381309006000.0,43,1381309006000.0,-1.0,"Remove work around Spring HATEOAS#89","See https://github.com/spring-projects/spring-hateoas/issues/89

Updating HATEOAS version and removing in a lot of controllers should be possible now. See eg https://github.com/spring-projects/spring-xd/blob/4919ea2498a13ef47aaa9437937308fb26a7a24f/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/rest/XDController.java#L219",2.0,False,2.0,0,0.0,0,0,False
"XD-931","Story","Sprint 25","2013-10-08T13:40:24.000+0000","iperumal","mark.fisher","Format option to display runtime module properties in shell","The runtime module properties requires a format option when displayed in the Shell 

Based on the PR (https://github.com/spring-projects/spring-xd/pull/340), the module properties are stored as String and displayed as is.
",2.0,-1.0,0.8999304514064254,0.9999890916251106,False,True,True,1381239624000.0,-1.0,-1.0,1397464789000.0,1397465689000.0,1381239624000.0,-1.0,1397465512000.0,1397465689000.0,1395841954000.0,1395845554000.0,-1.0,-1.0,"iperumal",1397465689000.0,1381239624000.0,43,1381239624000.0,-1.0,"Format option to display runtime module properties in shell","The runtime module properties requires a format option when displayed in the Shell 

Based on the PR (https://github.com/spring-projects/spring-xd/pull/340), the module properties are stored as String and displayed as is.
",2.0,False,2.0,0,0.0,0,0,False
"XD-928","Story","Sprint 17","2013-10-08T08:33:15.000+0000","grenfro","","Refactor src/test/resources in Dirt","* In the testmodules.source
** Rename source-config to packaged-source
** Rename source-config to packaged-source-no-lib
* All xml files should be prefixed with test.  i.e. testsource, testsink
* Make sure all tests pass with new configuration",1.0,-1.0,0.9999984752203056,0.0137296947322699,False,True,True,1381221195000.0,-1.0,-1.0,1400239436000.0,1400240336000.0,1381221195000.0,-1.0,1381482322000.0,1381485922000.0,1400240307000.0,1400240336000.0,-1.0,-1.0,"grenfro",1381485922000.0,1381221195000.0,43,1381221195000.0,-1.0,"Refactor src/test/resources in Dirt","* In the testmodules.source
** Rename source-config to packaged-source
** Rename source-config to packaged-source-no-lib
* All xml files should be prefixed with test.  i.e. testsource, testsink
* Make sure all tests pass with new configuration",1.0,False,1.0,0,0.0,0,1,False
"XD-927","Story","Sprint 17","2013-10-08T08:24:12.000+0000","grenfro","eric.bottard","ModuleType  Refactor","Remove ModuleType.getModuleTypeByTypeName.  All code should use the enum.
",3.0,0.6114115222148709,0.2310332041616076,0.2378074493067622,True,True,True,1381220652000.0,1381893415000.0,1381897015000.0,1382320096000.0,1382320996000.0,1381220652000.0,-1.0,1381482322000.0,1381485922000.0,1381474868000.0,1381478468000.0,1381893415000.0,1381897015000.0,"grenfro",1381485922000.0,1381220652000.0,43,1381220652000.0,-1.0,"ModuleType  Refactor","Remove ModuleType.getModuleTypeByTypeName.  All code should use the enum.
",3.0,False,3.0,0,0.0,-1,0,False
"XD-926","Story","Sprint 17","2013-10-07T13:14:16.000+0000","grussell","grussell","Update Core Spring Dependency to 4.0.0.M3","",8.0,-1.0,0.0,0.486815430752855,False,True,True,1381151656000.0,-1.0,-1.0,1381829997000.0,1381830897000.0,1381151656000.0,-1.0,1381482321000.0,1381485921000.0,1381151656000.0,1381155256000.0,-1.0,-1.0,"grussell",1381485921000.0,1381151656000.0,43,1381151656000.0,-1.0,"Update Core Spring Dependency to 4.0.0.M3","",8.0,False,8.0,0,0.0,-1,0,False
"XD-924","Story","Sprint 16","2013-10-07T01:43:57.000+0000","eric.bottard","eric.bottard","Split integration.x in dedicated XD projects where appropriate","Similar to what is done for e.g. hadoop, reactor, and http, some of the classes in the .x package (namely gemfire, splunk, twitter) should go in dedicated (albeit small) projects. This would enable further modularization (see XD-915)",3.0,0.0001186324056279,0.0,7.592473960186964e-05,True,True,True,1381110237000.0,1381110262000.0,1381113862000.0,1381320072000.0,1381320972000.0,1381110237000.0,-1.0,1381110253000.0,1381113853000.0,1381110237000.0,1381113837000.0,1381110262000.0,1381113862000.0,"eric.bottard",1381113853000.0,1381110237000.0,43,1381110237000.0,-1.0,"Split integration.x in dedicated XD projects where appropriate","Similar to what is done for e.g. hadoop, reactor, and http, some of the classes in the .x package (namely gemfire, splunk, twitter) should go in dedicated (albeit small) projects. This would enable further modularization (see XD-915)",3.0,False,3.0,0,0.0,0,0,False
"XD-923","Story","Sprint 37","2013-10-04T08:00:00.000+0000","grenfro","David Turanski","Error Channel for streams modules that fail to process a message","As a user, I'd like to be notified when a exception is thrown in a module so that I can tap into an error channel to receive the failures for each stream/module.

",8.0,0.9896756875487788,0.989674949568652,0.97199149929548,True,True,True,1380873600000.0,1414400102000.0,1414403702000.0,1414748951000.0,1414749851000.0,1380873600000.0,-1.0,1413801028000.0,1413804628000.0,1414400077000.0,1414403677000.0,1414400102000.0,1414403702000.0,"grenfro",1413804628000.0,1380873600000.0,43,1380873600000.0,-1.0,"Error Channel for streams modules that fail to process a message","As a user, I'd like to be notified when a exception is thrown in a module so that I can tap into an error channel to receive the failures for each stream/module.

",8.0,False,8.0,0,0.0,0,2,False
"XD-922","Story","Sprint 16","2013-10-02T22:36:20.000+0000","iperumal","iperumal","Handle SingleNodeServer's stop()  method cleanly","SingleNode server needs to stop cleanly with stopping both the admin server & container server. 

Also, all the tests that require SingleNode main server needs to handle the server shutdown appropriately.",2.0,-1.0,0.2475943914956011,0.2474569281524926,False,True,True,1380753380000.0,-1.0,-1.0,1380796128000.0,1380797028000.0,1380753380000.0,-1.0,1380764181000.0,1380767781000.0,1380764187000.0,1380767787000.0,-1.0,-1.0,"iperumal",1380767781000.0,1380753380000.0,43,1380753380000.0,-1.0,"Handle SingleNodeServer's stop()  method cleanly","SingleNode server needs to stop cleanly with stopping both the admin server & container server. 

Also, all the tests that require SingleNode main server needs to handle the server shutdown appropriately.",2.0,False,2.0,0,0.0,0,0,False
"XD-921","Story","Sprint 32","2013-10-02T09:23:25.000+0000","eric.bottard","grenfro","Add more ""hands on"" example to MQTT doco","Not everyone may be familiar with MQTT, or esp. with MQTT inside Rabbit",3.0,0.9828585188016824,0.9828585188016824,0.9960724094085748,True,True,True,1380705805000.0,1405596639000.0,1405600239000.0,1406029846000.0,1406030746000.0,1380705805000.0,-1.0,1405931280000.0,1405934880000.0,1405596639000.0,1405600239000.0,1405596639000.0,1405600239000.0,"eric.bottard",1405934880000.0,1380705805000.0,43,1380705805000.0,-1.0,"Add more ""hands on"" example to MQTT doco","Not everyone may be familiar with MQTT, or esp. with MQTT inside Rabbit",3.0,False,3.0,0,0.0,0,0,False
"XD-920","Bug","Sprint 16","2013-10-02T08:20:28.000+0000","eric.bottard","grussell","MQTT source module does not cleanly undeploy","Was attempting to test mqtt, turns out I don't have the proper rabbitmq thing installed. So far so good, I get these kinds of exceptions:
{noformat}
Unable to connect to server (32103) - java.net.ConnectException: Connection refused
	at org.eclipse.paho.client.mqttv3.internal.TCPNetworkModule.start(TCPNetworkModule.java:75)
	at org.eclipse.paho.client.mqttv3.internal.ClientComms$ConnectBG.run(ClientComms.java:521)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.eclipse.paho.client.mqttv3.internal.TCPNetworkModule.start(TCPNetworkModule.java:66)
	... 2 more
{noformat}

Problem is, I still get them after undeploying my ""mqtt | log"" stream",2.0,0.2198427477757086,0.1138268156424581,0.2197134285123111,True,True,True,1380702028000.0,1380719028000.0,1380722628000.0,1380778456000.0,1380779356000.0,1380702028000.0,1380719052000.0,1380719018000.0,1380722618000.0,1380710830000.0,1380714430000.0,1380719028000.0,1380722628000.0,"eric.bottard",1380722618000.0,1380719052000.0,43,1380719052000.0,-1.0,"MQTT source module does not cleanly undeploy","Was attempting to test mqtt, turns out I don't have the proper rabbitmq thing installed. So far so good, I get these kinds of exceptions:
{noformat}
Unable to connect to server (32103) - java.net.ConnectException: Connection refused
	at org.eclipse.paho.client.mqttv3.internal.TCPNetworkModule.start(TCPNetworkModule.java:75)
	at org.eclipse.paho.client.mqttv3.internal.ClientComms$ConnectBG.run(ClientComms.java:521)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at org.eclipse.paho.client.mqttv3.internal.TCPNetworkModule.start(TCPNetworkModule.java:66)
	... 2 more
{noformat}

Problem is, I still get them after undeploying my ""mqtt | log"" stream",2.0,False,2.0,0,0.0,-1,0,False
"XD-919","Story","Sprint 17","2013-10-02T06:04:05.000+0000","dturanski","dturanski","Remove json parameter from twittersearch source","json parameter is no longer required. Use --outputType=application/json instead",2.0,0.9535589948203936,0.9465040532549144,0.9535474896971622,True,True,True,1380693845000.0,1382351470000.0,1382355070000.0,1382431301000.0,1382432201000.0,1380693845000.0,-1.0,1382351450000.0,1382355050000.0,1382339206000.0,1382342806000.0,1382351470000.0,1382355070000.0,"dturanski",1382355050000.0,1380693845000.0,43,1380693845000.0,-1.0,"Remove json parameter from twittersearch source","json parameter is no longer required. Use --outputType=application/json instead",2.0,False,2.0,0,0.0,-1,0,False
"XD-916","Story","Sprint 16","2013-10-01T10:23:54.000+0000","dturanski","dturanski","file source should be able to produce file contents or file reference","File source should output either the File itself (serialized File object) or the contents as a byte[]. This option is configured by a parameter --contents=true.  The byte[] may be converted to a String using XD Message Conversion, e.g., --output = text/plain;charset=UTF-8",4.0,0.0415750632071232,0.0415550395202795,0.0415668181595993,True,True,True,1380623034000.0,1380693628000.0,1380697228000.0,1382320123000.0,1382321023000.0,1380623034000.0,-1.0,1380693614000.0,1380697214000.0,1380693594000.0,1380697194000.0,1380693628000.0,1380697228000.0,"dturanski",1380697214000.0,1380623034000.0,43,1380623034000.0,-1.0,"File source should be able to produce file contents or file reference","File source should output either the File itself (serialized File object) or the contents as a byte[]. This option is configured by a parameter --contents=true.  The byte[] may be converted to a String using XD Message Conversion, e.g., --output = text/plain;charset=UTF-8",4.0,False,4.0,0,0.0,0,0,True
"XD-915","Story","Sprint 16","2013-10-01T09:45:36.000+0000","eric.bottard","eric.bottard","Convert modules to be CP-aware","Once XD-887 is merged, gradually convert more modules.

Recipe:
1) Move the <module>.xml file to <module>/config/<module>.xml
2) Declare a :module.<type>.<module> gradle project
3) Move dependencies from dirt project to newly created module project
4) gradle build picks it up.

gradle clean build + manual test
Also have a look at gradle cleanEclipse eclipse",8.0,0.2980343923144972,0.0,0.2979994723200844,True,True,True,1380620736000.0,1380851175000.0,1380854775000.0,1381393032000.0,1381393932000.0,1380620736000.0,-1.0,1380851148000.0,1380854748000.0,1380620736000.0,1380624336000.0,1380851175000.0,1380854775000.0,"eric.bottard",1380854748000.0,1380620736000.0,43,1380620736000.0,-1.0,"Convert modules to be CP-aware","Once XD-887 is merged, gradually convert more modules.

Recipe:
1) Move the <module>.xml file to <module>/config/<module>.xml
2) Declare a :module.<type>.<module> gradle project
3) Move dependencies from dirt project to newly created module project
4) gradle build picks it up.

gradle clean build + manual test
Also have a look at gradle cleanEclipse eclipse",8.0,False,8.0,0,0.0,-1,0,False
"XD-914","Story","Sprint 18","2013-09-30T18:34:24.000+0000","mark.pollack","","Remove existing 'purpose built' json processors and replace with documentation using #jsonPath functionality","See issue https://jira.springsource.org/browse/XD-862

The docs should be updated to include examples that show how to use the standard 'SpEL' based splitter, transformer, filters with #jsonPath expressions.",2.0,-1.0,-1.0,0.92010257040667,False,True,False,1380566064000.0,-1.0,-1.0,1384502362000.0,1384503262000.0,1380566064000.0,-1.0,1384188690000.0,1384192290000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1384192290000.0,1380566064000.0,43,1380566064000.0,-1.0,"Add documentation for #jsonPath functionality with SpEL based processors","See issue https://jira.springsource.org/browse/XD-862

The docs should be updated to include examples that show how to use the standard 'SpEL' based splitter, transformer, filters with #jsonPath expressions.",2.0,False,2.0,0,0.0,-1,1,True
"XD-913","Story","","2013-09-30T11:37:30.000+0000","thomas.risberg","","The XD build breaks with Gradle 1.8","The XD build breaks with Gradle 1.8 due to some changes in dependency resolution.",1.0,-1.0,-1.0,-1.0,False,False,False,1380541050000.0,-1.0,-1.0,1380612762000.0,1380613662000.0,1380541050000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1380541050000.0,-1.0,"The XD build breaks with Gradle 1.8","The XD build breaks with Gradle 1.8 due to some changes in dependency resolution.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-912","Story","","2013-09-30T10:50:06.000+0000","dturanski","","Support for registering custom message converters","Users need to register custom message converters used by modules.",5.0,-1.0,-1.0,-1.0,False,False,False,1380538206000.0,-1.0,-1.0,1402058553000.0,1402059453000.0,1380538206000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"dturanski",-1.0,-1.0,0,1380538206000.0,-1.0,"Support for registering custom message converters","Users need to register custom message converters used by modules.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-911","Story","","2013-09-30T09:14:58.000+0000","grenfro","grenfro","Reduce Sonar Critical Errors ","Should keep the critical error count as close to zero as possible.",1.0,0.0001002208570739,0.0,-1.0,True,False,True,1380532498000.0,1380532525000.0,1380536125000.0,1380801003000.0,1380801903000.0,1380532498000.0,-1.0,-1.0,-1.0,1380532498000.0,1380536098000.0,1380532525000.0,1380536125000.0,"grenfro",-1.0,-1.0,0,1380532498000.0,-1.0,"Reduce Sonar Critical Errors ","Should keep the critical error count as close to zero as possible.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-910","Story","Sprint 16","2013-09-30T07:26:41.000+0000","grenfro","grenfro","Add a Processor for Restful webservices","Offers the functionality to make http request to a web service. i.e. outbound http gateway.
Example implementations:
stream create --name foo --definition ""trigger |rest --reply-timeout=1 --url=http://earthquake.usgs.gov/earthquakes/feed/geojson/all/day|log""

stream create --name foos --definition ""trigger --payload=lat=34.0567006&lon=-84.34368810000001&site=all&smap=1&searchresult=Roswell%2C%20GA%2030076%2C%20USA#.UktzaWSG1Dd | rest --url=http://forecast.weather.gov/MapClick.php? |log""

",1.0,0.0093808495320374,0.009377969762419,0.8744103671706264,True,True,True,1380526001000.0,1380532516000.0,1380536116000.0,1381219601000.0,1381220501000.0,1380526001000.0,-1.0,1381133279000.0,1381136879000.0,1380532514000.0,1380536114000.0,1380532516000.0,1380536116000.0,"grenfro",1381136879000.0,1380526001000.0,43,1380526001000.0,-1.0,"Add a Processor for Restful webservices","Offers the functionality to make http request to a web service. i.e. outbound http gateway.
Example implementations:
stream create --name foo --definition ""trigger |rest --reply-timeout=1 --url=http://earthquake.usgs.gov/earthquakes/feed/geojson/all/day|log""

stream create --name foos --definition ""trigger --payload=lat=34.0567006&lon=-84.34368810000001&site=all&smap=1&searchresult=Roswell%2C%20GA%2030076%2C%20USA#.UktzaWSG1Dd | rest --url=http://forecast.weather.gov/MapClick.php? |log""

",1.0,False,1.0,0,0.0,-1,2,False
"XD-909","Story","","2013-09-30T03:35:00.000+0000","luke","","Support additional aggregate counter query options","",5.0,-1.0,-1.0,-1.0,False,False,False,1380512100000.0,-1.0,-1.0,1382338771000.0,1382339671000.0,1380512100000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"luke",-1.0,-1.0,0,1380512100000.0,-1.0,"Support additional aggregate counter query options","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-908","Story","Sprint 16","2013-09-30T03:31:48.000+0000","luke","luke","Add aggregate counter query by number of points","It should be possible to supply a start or end date (or none for the present), plus a ""count"" value for the number of points required (i.e. after or prior to the given time).",3.0,-1.0,0.6953454895528449,0.999945249105165,False,True,True,1380511908000.0,-1.0,-1.0,1381387706000.0,1381388606000.0,1380511908000.0,-1.0,1381388558000.0,1381388606000.0,1381121516000.0,1381125116000.0,-1.0,-1.0,"luke",1381388606000.0,1381121509000.0,43,1381121509000.0,-1.0,"Add aggregate counter query by number of points","It should be possible to supply a start or end date (or none for the present), plus a ""count"" value for the number of points required (i.e. after or prior to the given time).",3.0,False,3.0,0,0.0,0,0,False
"XD-907","Story","Sprint 17","2013-09-30T03:28:52.000+0000","luke","luke","Add aggregate counter year resolution query support","",3.0,-1.0,0.6158050557959462,0.6191611693641246,False,True,True,1380511732000.0,-1.0,-1.0,1382078419000.0,1382079319000.0,1380511732000.0,-1.0,1381482321000.0,1381485921000.0,1381477060000.0,1381480660000.0,-1.0,-1.0,"luke",1381485921000.0,1381121566000.0,43,1381121566000.0,-1.0,"Add aggregate counter year resolution query support","",3.0,False,3.0,0,0.0,0,0,False
"XD-906","Story","Sprint 16","2013-09-30T03:27:58.000+0000","luke","luke","Add aggregate counter monthly resolution query support","",5.0,-1.0,0.856968613696659,0.999977516974684,False,True,True,1380511678000.0,-1.0,-1.0,1381222426000.0,1381223326000.0,1380511678000.0,-1.0,1381223310000.0,1381223326000.0,1381121538000.0,1381125138000.0,-1.0,-1.0,"luke",1381223326000.0,1380511678000.0,43,1380511678000.0,-1.0,"Add aggregate counter monthly resolution query support","",5.0,False,5.0,0,0.0,0,0,False
"XD-905","Story","Sprint 16","2013-09-27T17:50:35.000+0000","iperumal","iperumal","Container start/stop publish events are not getting processed","
It looks like Container's ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher's context whereas the ContainerEventListeners are running in XDContainer's context. This makes the container start/stop events not getting processed.
",2.0,0.0001542694058056,8.570522544759554e-05,4.713787399617755e-05,True,True,True,1380304235000.0,1380304271000.0,1380307871000.0,1380536693000.0,1380537593000.0,1380304235000.0,-1.0,1380304246000.0,1380307846000.0,1380304255000.0,1380307855000.0,1380304271000.0,1380307871000.0,"iperumal",1380307846000.0,1380304235000.0,43,1380304235000.0,-1.0,"Container start/stop publish events are not getting processed","
It looks like Container's ContainerStartedEvent and ContainerStoppedEvent are published from ContainerLauncher's context whereas the ContainerEventListeners are running in XDContainer's context. This makes the container start/stop events not getting processed.
",2.0,False,2.0,0,0.0,0,0,False
"XD-902","Story","","2013-09-25T03:19:48.000+0000","eric.bottard","eric.bottard","Properly close Redis/Rabbit connection factories in tests","Tests that leverage [Redis|Rabbit]AvailableRule often create another connection factory in the test body but fail to close it.

Tests should properly close the resource. As an added benefit, the rule itself can expose the resource that it created for deciding whether to skip the test or not",2.0,-1.0,0.0,-1.0,False,False,True,1380079188000.0,-1.0,-1.0,1381132434000.0,1381133334000.0,1380079188000.0,-1.0,-1.0,-1.0,1380079188000.0,1380082788000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1380079188000.0,-1.0,"Properly close Redis/Rabbit connection factories in tests","Tests that leverage [Redis|Rabbit]AvailableRule often create another connection factory in the test body but fail to close it.

Tests should properly close the resource. As an added benefit, the rule itself can expose the resource that it created for deciding whether to skip the test or not",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-901","Story","Sprint 16","2013-09-24T14:54:33.000+0000","thomas.risberg","","Wrong Jetty Util on classpath for WebHdfs","We currently include jetty-util-6.1.26.jar but we need to add correct jar for different distributions - PHD uses jetty-util-7.6.10.v20130312.jar

Need to check hadoop-hdfs dependencies for the distros and add jetty-util-* to the jar copy for each distro
",3.0,0.419223483362069,0.4822707456770459,0.1983358351432558,True,True,True,1380034473000.0,1380541925000.0,1380545525000.0,1381244030000.0,1381244930000.0,1380034473000.0,-1.0,1380274550000.0,1380278150000.0,1380618241000.0,1380621841000.0,1380541925000.0,1380545525000.0,"thomas.risberg",1380278150000.0,1380034473000.0,43,1380034473000.0,-1.0,"Wrong Jetty Util on classpath for WebHdfs","We currently include jetty-util-6.1.26.jar but we need to add correct jar for different distributions - PHD uses jetty-util-7.6.10.v20130312.jar

Need to check hadoop-hdfs dependencies for the distros and add jetty-util-* to the jar copy for each distro
",3.0,False,3.0,0,0.0,0,0,False
"XD-900","Story","Sprint 16","2013-09-24T13:27:35.000+0000","grussell","grussell","Move JsonPropertyAccessor to Module Parent Context","When INT-3133 is resolved, SpEL {{PropertyAccessor}} s are inherited from parent contexts.

Instead of adding the {{JsonPropertyAccessor}} to each module's context, add it to the parent instead.",1.0,0.0001251455064817,0.0,8.938964748695905e-05,True,True,True,1380029255000.0,1380029318000.0,1380032918000.0,1380531769000.0,1380532669000.0,1380029255000.0,-1.0,1380029300000.0,1380032900000.0,1380029255000.0,1380032855000.0,1380029318000.0,1380032918000.0,"grussell",1380032900000.0,1380029255000.0,43,1380029255000.0,-1.0,"Move SpEL PropertyAccessors to Module Parent Context","When INT-3133 is resolved, SpEL {{PropertyAccessor}} s are inherited from parent contexts.

Instead of adding the {{JsonPropertyAccessor}} to each module's context, add it to the parent instead.",1.0,False,1.0,0,0.0,-1,1,True
"XD-899","Story","Sprint 16","2013-09-24T12:48:43.000+0000","mark.fisher","iperumal","xd-container should start even if xd-admin is not running","currently xd-container will not start due to a DB connection failure if the xd-admin is not already running

In fact, if someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd-admin or xd-container to run

so... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection",2.0,0.1014805979295703,0.1014149394848,0.1013766387253507,True,True,True,1380026923000.0,1380045470000.0,1380049070000.0,1380208787000.0,1380209687000.0,1380026923000.0,-1.0,1380045451000.0,1380049051000.0,1380045458000.0,1380049058000.0,1380045470000.0,1380049070000.0,"mark.fisher",1380049051000.0,1380026923000.0,43,1380026923000.0,-1.0,"xd-container should start even if xd-admin is not running","currently xd-container will not start due to a DB connection failure if the xd-admin is not already running

In fact, if someone is not using Batch jobs at all with XD, they should not even need a DB connection for either xd-admin or xd-container to run

so... consider using LazyConnectionDataSourceProxy so a connection failure would only occur when the DataSource is actually invoked to retrieve a connection",2.0,False,2.0,0,0.0,0,0,False
"XD-896","Story","Sprint 16","2013-09-24T06:49:44.000+0000","dturanski","dturanski","Add --inputType and --outputType module parameters","The ability to configure message conversion via parameters. Consider programatic configuration of data type channels. Values can be media type, e.g., application/json or a java class name.",8.0,0.4431448722703302,0.0,0.4431357255587393,True,True,True,1380005384000.0,1380538318000.0,1380541918000.0,1381207102000.0,1381208002000.0,1380005384000.0,-1.0,1380538307000.0,1380541907000.0,1380005384000.0,1380008984000.0,1380538318000.0,1380541918000.0,"dturanski",1380541907000.0,1380005384000.0,43,1380005384000.0,-1.0,"Add --inputType and --outputType module parameters","The ability to configure message conversion via parameters. Consider programatic configuration of data type channels. Values can be media type, e.g., application/json or a java class name.",8.0,False,8.0,0,0.0,0,1,False
"XD-892","Bug","","2013-09-23T11:16:21.000+0000","williamsj","","Spring Batch Behavior change from M2 to M3","In M3, the batch job behavior has changed. In M2, it was much easier to create an invoke a batch job. In M3, a trigger is required. Figuring that change out isn't a big deal but the behavior of this batch job in M3 throws a stack trace, yet it executes. 

In M2, this same batch job runs fine with no stack trace. 

Logs are attached. I can't see a difference in the container log property files from M2 to M3. Turning the log settings down will suppress the traces, but I was not expecting the traces since they did not show up in M2.

Stream Definitions:

job create --name pdfLoadBatchJob --definition ""batch-pdfload --inputPath='LOCAL_PDF_PATH' --hdfsPath='REMOTE_HDFS_PATH'""


stream create --name pdfloadtrigger --definition ""trigger > job:pdfLoadBatchJob""",1.0,0.0036770426455136,0.0036759398134409,-1.0,True,False,True,1379934981000.0,1379971657000.0,1379975257000.0,1389908401000.0,1389909301000.0,1379934981000.0,-1.0,-1.0,-1.0,1379971646000.0,1379975246000.0,1379971657000.0,1379975257000.0,"williamsj",-1.0,-1.0,0,1379934981000.0,-1.0,"Spring Batch Behavior change from M2 to M3","In M3, the batch job behavior has changed. In M2, it was much easier to create an invoke a batch job. In M3, a trigger is required. Figuring that change out isn't a big deal but the behavior of this batch job in M3 throws a stack trace, yet it executes. 

In M2, this same batch job runs fine with no stack trace. 

Logs are attached. I can't see a difference in the container log property files from M2 to M3. Turning the log settings down will suppress the traces, but I was not expecting the traces since they did not show up in M2.

Stream Definitions:

job create --name pdfLoadBatchJob --definition ""batch-pdfload --inputPath='LOCAL_PDF_PATH' --hdfsPath='REMOTE_HDFS_PATH'""


stream create --name pdfloadtrigger --definition ""trigger > job:pdfLoadBatchJob""",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-891","Bug","Sprint 16","2013-09-23T09:43:17.000+0000","dturanski","iperumal","Provide a way to access currently deployed modules","For testing, it would be useful to access the deployed Module instances to connect sources and or sinks to a module's input and output channels, etc. This could be a simple as exposing the deployedModuleMap on ModuleDeployer or possibly something more elaborate if this level of granularity is generally useful for runtime administration.",3.0,0.1995731653761046,0.1995710404111665,0.2002751121273165,True,True,True,1379929397000.0,1380211152000.0,1380214752000.0,1381340285000.0,1381341185000.0,1379929397000.0,-1.0,1380212143000.0,1380215743000.0,1380211149000.0,1380214749000.0,1380211152000.0,1380214752000.0,"dturanski",1380215743000.0,1379929397000.0,43,1379929397000.0,-1.0,"Provide a way to access currently deployed modules","For testing, it would be useful to access the deployed Module instances to connect sources and or sinks to a module's input and output channels, etc. This could be a simple as exposing the deployedModuleMap on ModuleDeployer or possibly something more elaborate if this level of granularity is generally useful for runtime administration.",3.0,False,3.0,0,0.0,0,0,False
"XD-890","Story","Sprint 16","2013-09-23T08:18:08.000+0000","hillert","iperumal","Run JavaScript tests (Jasmine) as part of the build process","We probably need to look into some options to run our JavaScript tests (Jasmine) as part of the build process - some possibilities:

* Jasmine Gradle Plugin https://github.com/dzhaughnroth/jasmine-gradle-plugin
* Saga - http://timurstrekalov.github.io/saga/

Looks like Maven has slightly better support: http://searls.github.io/jasmine-maven-plugin/index.html

See also: XD-865",8.0,0.0819090576014734,0.0818933790157809,0.0818768752413677,True,True,True,1379924288000.0,1380023549000.0,1380027149000.0,1381135232000.0,1381136132000.0,1379924288000.0,-1.0,1380023510000.0,1380027110000.0,1380023530000.0,1380027130000.0,1380023549000.0,1380027149000.0,"hillert",1380027110000.0,1379924288000.0,43,1379924288000.0,-1.0,"Run JavaScript tests (Jasmine) as part of the build process","We probably need to look into some options to run our JavaScript tests (Jasmine) as part of the build process - some possibilities:

* Jasmine Gradle Plugin https://github.com/dzhaughnroth/jasmine-gradle-plugin
* Saga - http://timurstrekalov.github.io/saga/

Looks like Maven has slightly better support: http://searls.github.io/jasmine-maven-plugin/index.html

See also: XD-865",8.0,False,8.0,0,0.0,0,0,False
"XD-889","Story","Sprint 16","2013-09-23T07:22:06.000+0000","mark.pollack","iperumal","Change default admin port from 8080","This conflicts with and 'out of the box' hadoop installation that uses 8080 as the 'map reduce shuffle port'.

8088 sound ok?

",1.0,0.6627167869886317,0.6626614665449617,0.6625895499681907,True,True,True,1379920926000.0,1380040722000.0,1380044322000.0,1380100791000.0,1380101691000.0,1379920926000.0,-1.0,1380040699000.0,1380044299000.0,1380040712000.0,1380044312000.0,1380040722000.0,1380044322000.0,"mark.pollack",1380044299000.0,1379920926000.0,43,1379920926000.0,-1.0,"Change default admin port from 8080","This conflicts with and 'out of the box' hadoop installation that uses 8080 as the 'map reduce shuffle port'.

8088 sound ok?

",1.0,False,1.0,0,0.0,0,0,False
"XD-887","Story","Sprint 16","2013-09-23T02:06:16.000+0000","eric.bottard","eric.bottard","Package modules with their support jars","Currently, a lot of jars that are on the classpath of xd-dirt are there to support modules.
We should move those to the lib/ construct of a module and remove them from the CP of dirt.

But this should not be done by simple ""mv"", as we'd lose version tracking and dependency management offered by gradle.

Pending a ""dependency-aware"" ModuleRegistry, we should be able to alter the build.gradle file so that it
- knows about individual modules (maybe handles them as project)
- copies its libs into the appropriate directory
- does not copy dependencies that are already legitimate dependencies of xd-dirt (this can be achieved by runtime introspection of the dependeny tree of both projects)

",6.0,0.4789818849778643,0.4789763354574123,0.478955524755717,True,True,True,1379901976000.0,1380247218000.0,1380250818000.0,1380621859000.0,1380622759000.0,1379901976000.0,-1.0,1380247199000.0,1380250799000.0,1380247214000.0,1380250814000.0,1380247218000.0,1380250818000.0,"eric.bottard",1380250799000.0,1379901976000.0,43,1379901976000.0,-1.0,"Package modules with their support jars","Currently, a lot of jars that are on the classpath of xd-dirt are there to support modules.
We should move those to the lib/ construct of a module and remove them from the CP of dirt.

But this should not be done by simple ""mv"", as we'd lose version tracking and dependency management offered by gradle.

Pending a ""dependency-aware"" ModuleRegistry, we should be able to alter the build.gradle file so that it
- knows about individual modules (maybe handles them as project)
- copies its libs into the appropriate directory
- does not copy dependencies that are already legitimate dependencies of xd-dirt (this can be achieved by runtime introspection of the dependeny tree of both projects)

",6.0,False,6.0,0,0.0,0,0,False
"XD-886","Story","Sprint 16","2013-09-22T18:45:24.000+0000","iperumal","","Fix package tangle between org.springframework.xd.dirt.plugins.job and org.springframework.xd.dirt.job","",1.0,-1.0,1.000027678021575,0.9234149143019256,False,True,True,1379875524000.0,-1.0,-1.0,1380019143000.0,1380020043000.0,1379875524000.0,-1.0,1380008975000.0,1380012575000.0,1380020047000.0,1380020043000.0,-1.0,-1.0,"iperumal",1380012575000.0,1379875524000.0,43,1379875524000.0,-1.0,"Fix package tangle between org.springframework.xd.dirt.plugins.job and org.springframework.xd.dirt.job","",1.0,False,1.0,0,0.0,-1,0,False
"XD-885","Story","Sprint 22","2013-09-20T19:24:05.000+0000","hillert","iperumal","Add Batch Job Listeners Automatically","Add Batch Job Listeners Automatically

* Each major listener category should send notifications to own channel (StepExecution, Chunk, Item etc.)
* Add attribute to disallow automatic adding of listeners",8.0,0.8885097962154228,0.8880933689319419,0.8885091440192074,True,True,True,1379705045000.0,1393328396000.0,1393331996000.0,1395036954000.0,1395037854000.0,1379705045000.0,-1.0,1393328386000.0,1393331986000.0,1393322011000.0,1393325611000.0,1393328396000.0,1393331996000.0,"hillert",1393331986000.0,1379705045000.0,43,1379705045000.0,-1.0,"Add Batch Job Listeners Automatically","Add Batch Job Listeners Automatically

* Each major listener category should send notifications to own channel (StepExecution, Chunk, Item etc.)
* Add attribute to disallow automatic adding of listeners",8.0,False,8.0,0,0.0,0,0,False
"XD-882","Story","","2013-09-20T18:01:50.000+0000","grenfro","grenfro","Disable the JMX setting in SingleNodeMainIntegrationTests.testConfiguration","Set the enableJmx to false because contexts are not getting destroyed properly, and in some cases prevents testSystemPropertiesOverridesDefault from running successfully.",1.0,0.9915155061439438,0.0,-1.0,True,False,True,1379700110000.0,1379703499000.0,1379703528000.0,1379702628000.0,1379703528000.0,1379700110000.0,-1.0,-1.0,-1.0,1379700110000.0,1379703528000.0,1379703499000.0,1379703528000.0,"grenfro",-1.0,-1.0,0,1379700110000.0,-1.0,"Disable the JMX setting in SingleNodeMainIntegrationTests.testConfiguration","Set the enableJmx to false because contexts are not getting destroyed properly, and in some cases prevents testSystemPropertiesOverridesDefault from running successfully.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-881","Story","","2013-09-20T14:44:26.000+0000","aeisenberg","","Introduce wire.js into the XD admin UI","We should consider moving to wire.js to encourage dependency injection in the UI Javascript code.

See here: https://github.com/cujojs/wire/blob/master/docs/get.md",5.0,-1.0,0.963438650845154,-1.0,False,False,True,1379688266000.0,-1.0,-1.0,1380308377000.0,1380309277000.0,1379688266000.0,-1.0,-1.0,-1.0,1380286572000.0,1380290172000.0,-1.0,-1.0,"aeisenberg",-1.0,-1.0,0,1379688266000.0,-1.0,"Introduce wire.js into the XD admin UI","We should consider moving to wire.js to encourage dependency injection in the UI Javascript code.

See here: https://github.com/cujojs/wire/blob/master/docs/get.md",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-880","Story","Sprint 16","2013-09-20T14:14:44.000+0000","grussell","grussell","Update Java Version to 7","",1.0,0.0001307503725424,0.0,0.0001153679757727,True,True,True,1379686484000.0,1379686552000.0,1379690152000.0,1380205659000.0,1380206559000.0,1379686484000.0,-1.0,1379686544000.0,1379690144000.0,1379686484000.0,1379690084000.0,1379686552000.0,1379690152000.0,"grussell",1379690144000.0,1379686484000.0,43,1379686484000.0,-1.0,"Update Java Version to 7","",1.0,False,1.0,0,0.0,0,0,False
"XD-879","Story","Sprint 21","2013-09-20T12:35:04.000+0000","iperumal","","Cleanup hsqldb data directory used by tests after each test completion","Currently, the ""data"" directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues. We should delete the ""data"" directory after each test completion.",2.0,0.9462468239359124,0.94624781151105,0.9028587876186448,True,True,True,1379680504000.0,1390220173000.0,1390223773000.0,1390817997000.0,1390818897000.0,1379680504000.0,-1.0,1389736900000.0,1389740500000.0,1390220184000.0,1390223784000.0,1390220173000.0,1390223773000.0,"iperumal",1389740500000.0,1379680504000.0,43,1379680504000.0,-1.0,"Cleanup hsqldb data directory used by tests after each test completion","Currently, the ""data"" directory created by the HSQLDB process during the tests run is not cleaned up and may cause issues. We should delete the ""data"" directory after each test completion.",2.0,False,2.0,0,0.0,0,0,False
"XD-878","Improvement","","2013-09-20T10:27:38.000+0000","jencompgeek","","Remove code in StreamPlugin to extract target output channel from proxy","The SI JMX MBeanExporter proxies the output channel created in a module context when JMX is enabled. We have to unwrap the proxy in StreamPlugin in order to add the tap. XD-877 temporarily introduced this code, but a more elegant solution is called for. Perhaps this will involve SI making addInterceptor an interface method.",3.0,-1.0,0.5846434040671444,-1.0,False,False,True,1379672858000.0,-1.0,-1.0,1398938793000.0,1398939693000.0,1379672858000.0,-1.0,-1.0,-1.0,1390937086000.0,1390940686000.0,-1.0,-1.0,"jencompgeek",-1.0,-1.0,0,1379672858000.0,-1.0,"Remove code in StreamPlugin to extract target output channel from proxy","The SI JMX MBeanExporter proxies the output channel created in a module context when JMX is enabled. We have to unwrap the proxy in StreamPlugin in order to add the tap. XD-877 temporarily introduced this code, but a more elegant solution is called for. Perhaps this will involve SI making addInterceptor an interface method.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-877","Bug","Sprint 16","2013-09-20T07:29:27.000+0000","jencompgeek","jencompgeek","Taps do not work when JMX is enabled","When JMX is enabled, the Module's output channel processed by StreamPlugin is a Proxy. Thus it fails the ""instanceof"" test used to apply the WireTap to the output channel. ",3.0,2.4545137690040728e-05,0.0,1.3090740101355055e-05,True,True,True,1379662167000.0,1379662182000.0,1379665782000.0,1380272386000.0,1380273286000.0,1379662167000.0,-1.0,1379662175000.0,1379665775000.0,1379662167000.0,1379665767000.0,1379662182000.0,1379665782000.0,"jencompgeek",1379665775000.0,1379662167000.0,43,1379662167000.0,-1.0,"Taps do not work when JMX is enabled","When JMX is enabled, the Module's output channel processed by StreamPlugin is a Proxy. Thus it fails the ""instanceof"" test used to apply the WireTap to the output channel. ",3.0,False,3.0,0,0.0,-1,1,False
"XD-875","Story","Sprint 16","2013-09-19T23:17:51.000+0000","iperumal","iperumal","Handle AdminServer shutdown cleanly","The admin Server's tomcat is not shutdown properly. There is an existing method shutdownCleanly() on AdminServer but the spring-xd-shell tests hang when we use this method to shutdown the admin server. 
",2.0,0.0078030934518847,0.0,0.00743262017227,True,True,True,1379632671000.0,1379633008000.0,1379636608000.0,1379674959000.0,1379675859000.0,1379632671000.0,-1.0,1379632992000.0,1379636592000.0,1379632671000.0,1379636271000.0,1379633008000.0,1379636608000.0,"iperumal",1379636592000.0,1379632671000.0,43,1379632671000.0,-1.0,"Handle AdminServer shutdown cleanly","The admin Server's tomcat is not shutdown properly. There is an existing method shutdownCleanly() on AdminServer but the spring-xd-shell tests hang when we use this method to shutdown the admin server. 
",2.0,False,2.0,0,0.0,-1,0,False
"XD-874","Story","Sprint 21","2013-09-19T20:33:03.000+0000","hillert","luke","For file based item reader jobs, step/job completion message should have name of file sent on named channel","It looks like we don't handle deletion of source files currently. We should provide some support for that - Maybe there is a way to into Spring Integration's PseudoTransactionManager support:

http://docs.spring.io/spring-integration/api/org/springframework/integration/transaction/PseudoTransactionManager.html

The *File Source* should possibly also support File archival functionality (But that might also be a dedicated processor?). Not sure where we want to set the semantic boundaries for the File Source.
",8.0,0.95809402743222,0.912076484441743,0.9580963635975496,True,True,True,1379622783000.0,1391516088000.0,1391519688000.0,1392035388000.0,1392036288000.0,1379622783000.0,-1.0,1391516117000.0,1391519717000.0,1390944849000.0,1390948449000.0,1391516088000.0,1391519688000.0,"hillert",1391519717000.0,1379622783000.0,43,1379622783000.0,-1.0,"For file based item reader jobs, step/job completion message should have name of file sent on named channel","It looks like we don't handle deletion of source files currently. We should provide some support for that - Maybe there is a way to into Spring Integration's PseudoTransactionManager support:

http://docs.spring.io/spring-integration/api/org/springframework/integration/transaction/PseudoTransactionManager.html

The *File Source* should possibly also support File archival functionality (But that might also be a dedicated processor?). Not sure where we want to set the semantic boundaries for the File Source.
",8.0,False,8.0,0,0.0,0,1,False
"XD-872","Story","","2013-09-19T20:02:09.000+0000","hillert","","Consider making in-memory meta data stores persistent","Just wanted to create story for this - so we can consider whether this should be addressed.

In at least 2 modules we use non-persisted states. We may want to consider making them persistent:
 
*Twitter Search* uses an in-memory *MetadataStore* that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration:

""Create a Redis-backed MetadataStore""
See: https://jira.springsource.org/browse/INT-3085

*File Soure*'s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in-memory Queue to keep track of duplicate files.

",8.0,-1.0,-1.0,-1.0,False,False,False,1379620929000.0,-1.0,-1.0,1448380808000.0,1448381708000.0,1379620929000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1379620929000.0,-1.0,"Make in-memory meta data stores persistent","Just wanted to create story for this - so we can consider whether this should be addressed.

In at least 2 modules we use non-persisted states. We may want to consider making them persistent:
 
*Twitter Search* uses an in-memory *MetadataStore* that keeps track of the twitter ids. There exists a corresponding issue for Spring Integration:

""Create a Redis-backed MetadataStore""
See: https://jira.springsource.org/browse/INT-3085

*File Soure*'s File Inbound Channel Adapter uses a AcceptOnceFileListFilter, which uses an in-memory Queue to keep track of duplicate files.

",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-871","Bug","Sprint 18","2013-09-18T13:10:11.000+0000","hillert","dturanski","No errors in Shell when creating stream with HTTP Source + already used port ","2 issues exist:

1) 

Current this does not create an error in the shell

{code}
stream create --name s1 --definition ""http | log""
stream create --name s2 --definition ""http | log""
{code}

On the server-side I see:

{code}
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:344)
	at sun.nio.ch.Net.bind(Net.java:336)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	... 3 more
{code}

2)
The Stream should not be saved to the StreamDefinitionRepo in case of an error.

",4.0,0.9995233761349802,0.9999844921569084,0.9999745574449278,True,True,True,1379509811000.0,1383634788000.0,1383636755000.0,1383635855000.0,1383636755000.0,1379509811000.0,-1.0,1383636650000.0,1383636755000.0,1383636691000.0,1383636755000.0,1383634788000.0,1383636755000.0,"hillert",1383636755000.0,1379509811000.0,43,1379509811000.0,-1.0,"No errors in Shell when creating stream with HTTP Source + already used port ","2 issues exist:

1) 

Current this does not create an error in the shell

{code}
stream create --name s1 --definition ""http | log""
stream create --name s2 --definition ""http | log""
{code}

On the server-side I see:

{code}
Caused by: java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:344)
	at sun.nio.ch.Net.bind(Net.java:336)
	at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:199)
	at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)
	at org.jboss.netty.channel.socket.nio.NioServerBoss$RegisterTask.run(NioServerBoss.java:193)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.processTaskQueue(AbstractNioSelector.java:366)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:290)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	... 3 more
{code}

2)
The Stream should not be saved to the StreamDefinitionRepo in case of an error.

",4.0,False,4.0,0,0.0,0,0,False
"XD-870","Story","Sprint 16","2013-09-18T07:57:50.000+0000","mark.pollack","","Support for listing of modules in the Shell","Commands that pair up with the functionality described in XD-859

module list  (would list all modules in a table format)
module list --type=source  (would list only source modules)

and so on.",4.0,0.0107148931842372,0.0108332558861333,0.0052044518404085,True,True,True,1379491070000.0,1379503291000.0,1379506891000.0,1380630732000.0,1380631632000.0,1379491070000.0,-1.0,1379497006000.0,1379500606000.0,1379503426000.0,1379507026000.0,1379503291000.0,1379506891000.0,"mark.pollack",1379500606000.0,1379491070000.0,43,1379491070000.0,-1.0,"Support for listing of modules in the Shell","Commands that pair up with the functionality described in XD-859

module list  (would list all modules in a table format)
module list --type=source  (would list only source modules)

and so on.",4.0,False,4.0,0,0.0,0,0,False
"XD-868","Story","Sprint 16","2013-09-18T07:25:30.000+0000","mark.pollack","jbrisbin","Create microbenchmark performance test of reactor syslog adapter vs standard syslog adapter","We need to verify that we are seeing improved throughput when using the reactor based syslog adapter.  

A suggestion on a basic stream to perform a microbenchmark this would be using in-memory counters, singlenode with the stream definition ""syslog | counter"".

Based on the results of this microbenchmark, other stories may need to be created.",4.0,0.6166292007647642,0.0,0.0069649435889091,True,True,True,1379489130000.0,1380186418000.0,1380190018000.0,1380619036000.0,1380619936000.0,1379489130000.0,-1.0,1379497006000.0,1379500606000.0,1379489130000.0,1379492730000.0,1380186418000.0,1380190018000.0,"mark.pollack",1379500606000.0,1379489130000.0,43,1379489130000.0,-1.0,"Create microbenchmark performance test of reactor syslog adapter vs standard syslog adapter","We need to verify that we are seeing improved throughput when using the reactor based syslog adapter.  

A suggestion on a basic stream to perform a microbenchmark this would be using in-memory counters, singlenode with the stream definition ""syslog | counter"".

Based on the results of this microbenchmark, other stories may need to be created.",4.0,False,4.0,0,0.0,0,0,False
"XD-866","Story","Sprint 16","2013-09-18T06:00:17.000+0000","grenfro","grenfro","Remove remaining Thread.sleeps from the job tests","Get rid of all the thread.sleeps and code that supported them.",1.0,8.899815885058877e-05,8.528990223181425e-05,0.0240869808672499,True,True,True,1379484017000.0,1379484065000.0,1379487665000.0,1380022454000.0,1380023354000.0,1379484017000.0,-1.0,1379497008000.0,1379500608000.0,1379484063000.0,1379487663000.0,1379484065000.0,1379487665000.0,"grenfro",1379500608000.0,1379484017000.0,43,1379484017000.0,-1.0,"Remove remaining Thread.sleeps from the job tests","Get rid of all the thread.sleeps and code that supported them.",1.0,False,1.0,0,0.0,-1,0,False
"XD-865","Story","","2013-09-17T14:56:42.000+0000","aeisenberg","","Add a test suite to the admin-ui","The admin-ui currently has no unit tests.  Need to add a test suite and hook it up to the build so that tests are run on every build.",5.0,-1.0,-1.0,-1.0,False,False,False,1379429802000.0,-1.0,-1.0,1379924631000.0,1379925531000.0,1379429802000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"aeisenberg",-1.0,-1.0,0,1379429802000.0,-1.0,"Add a test suite to the admin-ui","The admin-ui currently has no unit tests.  Need to add a test suite and hook it up to the build so that tests are run on every build.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-864","Story","Sprint 16","2013-09-17T13:11:59.000+0000","iperumal","iperumal","Move BatchJobExecutionsByJobName to BatchJobsController from BatchJobExecutionsController","We need to move the BatchJobExecutionsByJobName method to BatchJobsController as that seems appropriate",1.0,-1.0,0.000171647953861,0.140156275925993,False,True,True,1379423519000.0,-1.0,-1.0,1379946948000.0,1379947848000.0,1379423519000.0,-1.0,1379497007000.0,1379500607000.0,1379423609000.0,1379427209000.0,-1.0,-1.0,"iperumal",1379500607000.0,1379423519000.0,43,1379423519000.0,-1.0,"Move BatchJobExecutionsByJobName to BatchJobsController from BatchJobExecutionsController","We need to move the BatchJobExecutionsByJobName method to BatchJobsController as that seems appropriate",1.0,False,1.0,0,0.0,0,0,False
"XD-863","Story","Sprint 16","2013-09-17T09:26:42.000+0000","mark.pollack","","Serialization of Spring Batch Context Objects","Needs some investigation on how the update information from Spring Batch listeners can be sent as a message across modules in a single node configuration as well as across JVMs in a distributed node configuration.",4.0,0.2789701519891916,0.2789831272696604,0.1411142846484591,True,True,True,1379410002000.0,1379582003000.0,1379585603000.0,1380025659000.0,1380026559000.0,1379410002000.0,-1.0,1379497007000.0,1379500607000.0,1379582011000.0,1379585611000.0,1379582003000.0,1379585603000.0,"mark.pollack",1379500607000.0,1379410002000.0,43,1379410002000.0,-1.0,"Serialization of Spring Batch Context Objects","Needs some investigation on how the update information from Spring Batch listeners can be sent as a message across modules in a single node configuration as well as across JVMs in a distributed node configuration.",4.0,False,4.0,0,0.0,0,0,False
"XD-862","Story","Sprint 16","2013-09-17T09:25:20.000+0000","mark.pollack","","First class JSON Path support","Similar to xpath with XML, there are now some initial support in SI that enable the use of filter/routers based on JSON Path expressions.  That support needs to be reviewed and then brought up to the level of exposure in Spring XD so that router/filter modules could use JSON Path.  json-path-filter/router are components that need to be created, perhaps others.

This story needs to be broken down further.",10.0,0.7490826623178065,0.7490717416311138,0.3170166140046886,True,True,True,1379409920000.0,1379615699000.0,1379619299000.0,1379683728000.0,1379684628000.0,1379409920000.0,-1.0,1379497007000.0,1379500607000.0,1379615696000.0,1379619296000.0,1379615699000.0,1379619299000.0,"mark.pollack",1379500607000.0,1379409920000.0,43,1379409920000.0,-1.0,"First class JSON Path support","Similar to xpath with XML, there are now some initial support in SI that enable the use of filter/routers based on JSON Path expressions.  That support needs to be reviewed and then brought up to the level of exposure in Spring XD so that router/filter modules could use JSON Path.  json-path-filter/router are components that need to be created, perhaps others.

This story needs to be broken down further.",10.0,False,10.0,0,0.0,0,0,False
"XD-859","Story","Sprint 16","2013-09-17T09:01:30.000+0000","mark.pollack","","Support for listing of modules in the REST API","From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors.  A brief description of them would also be nice - this might come from adding some metadata into the definition.

Finer grained description/implementation suggestion TBD",8.0,0.118948399493517,0.1189456691274091,0.0604202716031685,True,True,True,1379408490000.0,1379582750000.0,1379586350000.0,1380872595000.0,1380873495000.0,1379408490000.0,-1.0,1379497006000.0,1379500606000.0,1379582746000.0,1379586346000.0,1379582750000.0,1379586350000.0,"mark.pollack",1379500606000.0,1379408490000.0,43,1379408490000.0,-1.0,"Support for listing of modules in the REST API","From the CLI, one should be able to get a listing of modules and be able to specifically ask for jobs, sources, sinks, and processors.  A brief description of them would also be nice - this might come from adding some metadata into the definition.

Finer grained description/implementation suggestion TBD",8.0,False,8.0,0,0.0,0,1,False
"XD-858","Story","Sprint 16","2013-09-17T07:30:57.000+0000","jencompgeek","jencompgeek","Upgrade to Spring Data Redis 1.1.0.RELEASE","",1.0,-1.0,0.0698612604310653,0.9619515691394052,False,True,True,1379403057000.0,-1.0,-1.0,1379499822000.0,1379500722000.0,1379403057000.0,-1.0,1379497006000.0,1379500606000.0,1379409880000.0,1379413480000.0,-1.0,-1.0,"jencompgeek",1379500606000.0,1379403057000.0,43,1379403057000.0,-1.0,"Upgrade to Spring Data Redis 1.1.0.RELEASE","",1.0,False,1.0,0,0.0,-1,0,False
"XD-857","Story","Sprint 18","2013-09-17T07:14:02.000+0000","eric.bottard","eric.bottard","Refactor FileModuleRegistry as ""ResourceModuleRegistry""","Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems/locations. (HDFS /HTTP)",5.0,0.9999823995108512,0.99999107920413,0.999976371946074,True,True,True,1379402042000.0,1383549581000.0,1383549654000.0,1383548754000.0,1383549654000.0,1379402042000.0,1383549609000.0,1383549556000.0,1383549654000.0,1383549617000.0,1383549654000.0,1383549581000.0,1383549654000.0,"eric.bottard",1383549654000.0,1383549609000.0,43,1383549609000.0,-1.0,"Refactor FileModuleRegistry as ""ResourceModuleRegistry""","Apart from sanity checks, there is not much that ties FileModuleRegistry to actual java.io.Files. Using the Resource abstraction would work just the same, and would allow loading modules from the classpath in constrained environments or other file systems/locations. (HDFS /HTTP)",5.0,False,5.0,0,0.0,0,2,False
"XD-855","Story","","2013-09-17T06:50:08.000+0000","eric.bottard","eric.bottard","Change metrics assertions in integration tests to use smart Thread.sleep","Similar to what has been done for e.g. FileSink, refactor metrics related sinks to use smart Thread.sleep() timings",5.0,-1.0,0.0,-1.0,False,False,True,1379400608000.0,-1.0,-1.0,1379406914000.0,1379407814000.0,1379400608000.0,-1.0,-1.0,-1.0,1379400608000.0,1379404208000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1379400608000.0,-1.0,"Change metrics assertions in integration tests to use smart Thread.sleep","Similar to what has been done for e.g. FileSink, refactor metrics related sinks to use smart Thread.sleep() timings",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-854","Story","Sprint 16","2013-09-17T06:46:02.000+0000","eric.bottard","","Update doc about modules and spring","The doc at http://docs.spring.io/spring-xd/docs/1.0.0.M3/reference/html/#_modules_and_spring refers to an old version of the counter sink, when it was still hardwired to use redis.
The text next to it that explains placeholders is out of date (with respect to the redis placeholders)",2.0,0.5996112565593372,0.5985458955266171,0.5476635725862206,True,True,True,1379400362000.0,1379506173000.0,1379509773000.0,1379575928000.0,1379576828000.0,1379400362000.0,-1.0,1379497006000.0,1379500606000.0,1379505985000.0,1379509585000.0,1379506173000.0,1379509773000.0,"eric.bottard",1379500606000.0,1379400362000.0,43,1379400362000.0,-1.0,"Update doc about modules and spring","The doc at http://docs.spring.io/spring-xd/docs/1.0.0.M3/reference/html/#_modules_and_spring refers to an old version of the counter sink, when it was still hardwired to use redis.
The text next to it that explains placeholders is out of date (with respect to the redis placeholders)",2.0,False,2.0,0,0.0,-1,0,False
"XD-851","Story","","2013-09-16T07:45:05.000+0000","grenfro","","Refactor DSL for Taps and Jobs Usage","The syntax for both taps and job channels will be prefixed with the word tap.
* The parser will search both the all (stream and job for now) registries to find the ""name"".  
* If the name is present in only in one registry then that definition will be used.  
* If the name is  present both registries a syntax exception will be issued.
   - The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants.
	- i.e. 	tap:{stream}:streamName > mySink
* If the user attempts to place a job on the greater (left hand) side without specifying a notification a syntax error will be thrown.
Format: 
-- tap:[type]:name > $
-- $ > tap:[type]:name  
-- Example:
   tap:streamName >mySink
   tap:stream:streamName > mySink
   mySource|myProcessor > tap:stream:mySink
   myEmailSource > tap:job:jobName.step1
   myEmailSource > tap:jobName.step1
   tap:job:jobName-notifications > myEmailSink",4.0,-1.0,-1.0,-1.0,False,False,False,1379317505000.0,-1.0,-1.0,1382338520000.0,1382339420000.0,1379317505000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1379317505000.0,-1.0,"Refactor DSL for Taps and Jobs Usage","The syntax for both taps and job channels will be prefixed with the word tap.
* The parser will search both the all (stream and job for now) registries to find the ""name"".  
* If the name is present in only in one registry then that definition will be used.  
* If the name is  present both registries a syntax exception will be issued.
   - The user then can optionally specify which registry to use by adding a second token to the definition that specifies the type of channel the user wants.
	- i.e. 	tap:{stream}:streamName > mySink
* If the user attempts to place a job on the greater (left hand) side without specifying a notification a syntax error will be thrown.
Format: 
-- tap:[type]:name > $
-- $ > tap:[type]:name  
-- Example:
   tap:streamName >mySink
   tap:stream:streamName > mySink
   mySource|myProcessor > tap:stream:mySink
   myEmailSource > tap:job:jobName.step1
   myEmailSource > tap:jobName.step1
   tap:job:jobName-notifications > myEmailSink",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-850","Story","","2013-09-16T07:29:32.000+0000","thomas.risberg","","JAR version mismatch","Looks like there are some version mismatch issues with the build/packaging of the XD components. Looking in xd/lib I see the following which looks suspicious:

mqtt-client-0.2.1.jar
mqtt-client-1.0.jar

jackson-core-asl-1.9.13.jar
jackson-mapper-asl-1.9.12.jar

spring-integration-core-3.0.0.M3.jar
spring-integration-http-2.2.5.RELEASE.jar

spring-data-commons-1.6.0.M1.jar
spring-data-commons-core-1.4.0.RELEASE.jar
",3.0,-1.0,-1.0,-1.0,False,False,False,1379316572000.0,-1.0,-1.0,1398341745000.0,1398342645000.0,1379316572000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1379316572000.0,-1.0,"JAR version mismatches","Looks like there are some version mismatch issues with the build/packaging of the XD components. Looking in xd/lib I see the following which looks suspicious:

mqtt-client-0.2.1.jar
mqtt-client-1.0.jar

jackson-core-asl-1.9.13.jar
jackson-mapper-asl-1.9.12.jar

spring-integration-core-3.0.0.M3.jar
spring-integration-http-2.2.5.RELEASE.jar

spring-data-commons-1.6.0.M1.jar
spring-data-commons-core-1.4.0.RELEASE.jar
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-849","Improvement","","2013-09-16T04:22:16.000+0000","dturanski","dturanski","gemfire modules should support connection via locator","The gemfire modules currently accept server host and port. Provide an option to specify a locator host and port",2.0,-1.0,0.0,-1.0,False,False,True,1379305336000.0,-1.0,-1.0,1387170522000.0,1387171422000.0,1379305336000.0,-1.0,-1.0,-1.0,1379305336000.0,1379308936000.0,-1.0,-1.0,"dturanski",-1.0,-1.0,0,1379305336000.0,-1.0,"Gemfire modules should support connection via locator","The gemfire modules currently accept server host and port. Provide an option to specify a locator host and port",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-848","Bug","","2013-09-14T13:25:27.000+0000","willmcqueen","","ICLA website is missing Spring XD project option","The README.md at:

  https://github.com/spring-projects/spring-xd/blob/master/README.md

...says:

""Before we accept a non-trivial patch or pull request we will need you to sign the contributor's agreement""

Navigating to the <contributor's agreement> link takes the user to the ICA web page at:

https://support.springsource.com/spring_committer_signup

//exp: The Project field's dropdown should have an option for ""Spring XD""
//act: There's no ""Spring XD"" option",1.0,-1.0,-1.0,-1.0,False,False,False,1379165127000.0,-1.0,-1.0,1379631895000.0,1379632795000.0,1379165127000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"willmcqueen",-1.0,-1.0,0,1379165127000.0,-1.0,"ICLA website is missing Spring XD project option","The README.md at:

  https://github.com/spring-projects/spring-xd/blob/master/README.md

...says:

""Before we accept a non-trivial patch or pull request we will need you to sign the contributor's agreement""

Navigating to the <contributor's agreement> link takes the user to the ICLA web page at:

https://support.springsource.com/spring_committer_signup

//exp: The Project field's dropdown should have an option for ""Spring XD""
//act: There's no ""Spring XD"" option",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-847","Story","Sprint 16","2013-09-14T09:45:13.000+0000","thomas.risberg","thomas.risberg","Revise the available hadoopDistro options","We should adjust our --hadoopDistro options to the ones supported in the new spring-data-hadoop 1.0.1.RELEASE - hadoop12 (default), cdh4, hdp13, phd1, hadoop20

This includes updating the wiki pages",5.0,0.3127938561810901,0.0,0.3125656095769377,True,True,True,1379151913000.0,1379497259000.0,1379500859000.0,1380255082000.0,1380255982000.0,1379151913000.0,-1.0,1379497007000.0,1379500607000.0,1379151913000.0,1379155513000.0,1379497259000.0,1379500859000.0,"thomas.risberg",1379500607000.0,1379151913000.0,43,1379151913000.0,-1.0,"Revise the available hadoopDistro options","We should adjust our --hadoopDistro options to the ones supported in the new spring-data-hadoop 1.0.1.RELEASE - hadoop12 (default), cdh4, hdp13, phd1, hadoop20

This includes updating the wiki pages",5.0,False,5.0,0,0.0,0,2,False
"XD-846","Story","","2013-09-13T11:42:54.000+0000","grenfro","","Remove deprecated tap syntax from the parser.","",2.0,-1.0,-1.0,-1.0,False,False,False,1379072574000.0,-1.0,-1.0,1382338302000.0,1382339202000.0,1379072574000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1379072574000.0,-1.0,"Remove deprecated tap syntax from the parser.","Tap '@' and using numbers instead of module names.",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-845","Story","","2013-09-13T11:40:38.000+0000","grenfro","","Remove support for the leading : on items that have a declared namespace","When using jobs, taps we no longer need to have the leading :.  i.e. :tap:foo.  We should only support tap:foo.",3.0,-1.0,-1.0,-1.0,False,False,False,1379072438000.0,-1.0,-1.0,1380008726000.0,1380009626000.0,1379072438000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1379072438000.0,-1.0,"Remove support for the leading : on items that have a declared namespace","When using jobs, taps we no longer need to have the leading :.  i.e. :tap:foo.  We should only support tap:foo.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-844","Story","","2013-09-12T14:32:35.000+0000","eric.bottard","eric.bottard","Enable Spring Boot Loader support","As part of running in Cloud Foundry, one quick workaround for the lack of ""classpath"" support would be to use the Spring Boot Loader special ClassLoader and jar-inside-a-jar support: https://github.com/spring-projects/spring-boot/tree/master/spring-boot-tools/spring-boot-loader",3.0,-1.0,0.0,-1.0,False,False,True,1378996355000.0,-1.0,-1.0,1384815349000.0,1384816249000.0,1378996355000.0,-1.0,-1.0,-1.0,1378996355000.0,1378999955000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1378996355000.0,-1.0,"Enable Spring Boot Loader support","As part of running in Cloud Foundry, one quick workaround for the lack of ""classpath"" support would be to use the Spring Boot Loader special ClassLoader and jar-inside-a-jar support: https://github.com/spring-projects/spring-boot/tree/master/spring-boot-tools/spring-boot-loader",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-843","Story","","2013-09-12T14:27:20.000+0000","eric.bottard","Eric Bottard","Initial XD on CloudFoundry support","First take on this involves
- being able to deploy the two separate applications: xd-admin & xd-container
- being able to CF-service provided redis & rabbit for internal needs of XD
- to some extent, make modules smart and CF aware (e.g. http source uses correct port)",20.0,-1.0,0.99999363502621,-1.0,False,False,True,1378996040000.0,-1.0,-1.0,1384808204000.0,1384809104000.0,1378996040000.0,-1.0,-1.0,-1.0,1384809067000.0,1384809104000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1378996040000.0,-1.0,"Initial XD on CloudFoundry support","First take on this involves
- being able to deploy the two separate applications: xd-admin & xd-container
- being able to CF-service provided redis & rabbit for internal needs of XD
- to some extent, make modules smart and CF aware (e.g. http source uses correct port)",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-842","Story","","2013-09-12T07:35:19.000+0000","thomas.risberg","","Add back classifier = 'dist' to distZip build target","Add back ""classifier = 'dist'"" to distZip build target - it was was accidentally removed.",1.0,-1.0,0.5919119500060879,-1.0,False,False,True,1378971319000.0,-1.0,-1.0,1384267864000.0,1384268764000.0,1378971319000.0,-1.0,-1.0,-1.0,1382106940000.0,1382110540000.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1378971319000.0,-1.0,"Add back classifier = 'dist' to distZip build target","Add back ""classifier = 'dist'"" to distZip build target - it was was accidentally removed.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-840","Story","","2013-09-11T16:46:13.000+0000","grussell","","Fix Javadoc Warnings","As of M3...


javadoc: warning - Error fetching URL: http://static.springsource.org/spring-shell/docs/current/api/package-list
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/CompositeModuleRegistry.java:40: warning - @param argument ""cp"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/CompositeModuleRegistry.java:40: warning - @param argument ""file"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/JobFactoryBean.java:62: warning - Tag @link: can't find jobName in org.springframework.xd.dirt.plugins.job.JobFactoryBean
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/rest/XDController.java:56: warning - @param argument ""<V>"" is not a type parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/rest/XDController.java:56: warning - @param argument ""<T>"" is not a type parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobAlreadyExistsException.java:32: warning - @param argument ""message"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/NoSuchBatchJobException.java:31: warning - @param argument ""message"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/ContainerOptions.java:30: warning - @param argument ""defaultTransport"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/ContainerOptions.java:30: warning - @param argument ""defaultAnalytics"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/OptionUtils.java:35: warning - @return tag has no arguments.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/Plugin.java:32: warning - Tag @see: reference not found: ModuleDeployer
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/MessageBus.java:53: warning - @param argument ""moduleInputChannel"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/MessageBus.java:72: warning - @param argument ""moduleOutputChannel"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/MessageBus.java:102: warning - @param argument ""moduleOutputChannel"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-reactor/src/main/java/org/springframework/xd/integration/reactor/config/ReactorNamespaceUtils.java:46: warning - @return tag has no arguments.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag.
",1.0,-1.0,0.9957897087569872,-1.0,False,False,True,1378917973000.0,-1.0,-1.0,1390908880000.0,1390909780000.0,1378917973000.0,-1.0,-1.0,-1.0,1390859291000.0,1390862891000.0,-1.0,-1.0,"grussell",-1.0,-1.0,0,1378917973000.0,-1.0,"Fix Compiler Warnings","As of M3...

{code}
javadoc: warning - Error fetching URL: http://static.springsource.org/spring-shell/docs/current/api/package-list
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/CompositeModuleRegistry.java:40: warning - @param argument ""cp"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/module/CompositeModuleRegistry.java:40: warning - @param argument ""file"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/JobFactoryBean.java:62: warning - Tag @link: can't find jobName in org.springframework.xd.dirt.plugins.job.JobFactoryBean
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/rest/XDController.java:56: warning - @param argument ""<V>"" is not a type parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/rest/XDController.java:56: warning - @param argument ""<T>"" is not a type parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobAlreadyExistsException.java:32: warning - @param argument ""message"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/NoSuchBatchJobException.java:31: warning - @param argument ""message"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/ContainerOptions.java:30: warning - @param argument ""defaultTransport"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/ContainerOptions.java:30: warning - @param argument ""defaultAnalytics"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/OptionUtils.java:35: warning - @return tag has no arguments.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/Plugin.java:32: warning - Tag @see: reference not found: ModuleDeployer
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/MessageBus.java:53: warning - @param argument ""moduleInputChannel"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/MessageBus.java:72: warning - @param argument ""moduleOutputChannel"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/MessageBus.java:102: warning - @param argument ""moduleOutputChannel"" is not a parameter name.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-reactor/src/main/java/org/springframework/xd/integration/reactor/config/ReactorNamespaceUtils.java:46: warning - @return tag has no arguments.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag.
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/plugins/job/batch/BatchJobLocator.java:32: warning - @DistributedJobService is an unknown tag.
{code}

and

{code}
warning: [options] bootstrap class path not set in conjunction with -source 1.6
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/LocalMessageBus.java:132: warning: [rawtypes] found raw type: LocalMessageBus.SharedChannelProvider
		SharedChannelProvider channelProvider = aliasHint ? queueChannelProvider
		^
  missing type arguments for generic class LocalMessageBus.SharedChannelProvider<T>
  where T is a type-variable:
    T extends AbstractMessageChannel declared in class LocalMessageBus.SharedChannelProvider
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/bus/LocalMessageBus.java:159: warning: [rawtypes] found raw type: LocalMessageBus.SharedChannelProvider
		SharedChannelProvider channelProvider = aliasHint ? queueChannelProvider
		^
  missing type arguments for generic class LocalMessageBus.SharedChannelProvider<T>
  where T is a type-variable:
    T extends AbstractMessageChannel declared in class LocalMessageBus.SharedChannelProvider
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:93: warning: [rawtypes] found raw type: List
	public List getUrls() {
	       ^
  missing type arguments for generic class List<E>
  where E is a type-variable:
    E extends Object declared in interface List
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:101: warning: [rawtypes] found raw type: List
	public List getHashTags() {
	       ^
  missing type arguments for generic class List<E>
  where E is a type-variable:
    E extends Object declared in interface List
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:109: warning: [rawtypes] found raw type: List
	public List getMentions() {
	       ^
  missing type arguments for generic class List<E>
  where E is a type-variable:
    E extends Object declared in interface List
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:117: warning: [rawtypes] found raw type: List
	public List getMedia() {
	       ^
  missing type arguments for generic class List<E>
  where E is a type-variable:
    E extends Object declared in interface List
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:126: warning: [rawtypes] found raw type: List
	public List getTickerSymbols() {
	       ^
  missing type arguments for generic class List<E>
  where E is a type-variable:
    E extends Object declared in interface List
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDEntities.java:37: warning: [serial] serializable class XDEntities has no definition of serialVersionUID
public class XDEntities implements Serializable {
       ^
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDUrlEntity.java:31: warning: [serial] serializable class XDUrlEntity has no definition of serialVersionUID
public class XDUrlEntity implements Serializable {
       ^
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDHashTagEntity.java:32: warning: [serial] serializable class XDHashTagEntity has no definition of serialVersionUID
public class XDHashTagEntity implements Serializable {
       ^
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDMentionEntity.java:32: warning: [serial] serializable class XDMentionEntity has no definition of serialVersionUID
public class XDMentionEntity implements Serializable {
       ^
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDMediaEntity.java:32: warning: [serial] serializable class XDMediaEntity has no definition of serialVersionUID
public class XDMediaEntity implements Serializable {
       ^
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDTickerSymbolEntity.java:31: warning: [serial] serializable class XDTickerSymbolEntity has no definition of serialVersionUID
public class XDTickerSymbolEntity implements Serializable {
       ^
/Users/gpr/Documents/workspace-si/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/twitter/XDTweet.java:34: warning: [serial] serializable class XDTweet has no definition of serialVersionUID
public class XDTweet implements Serializable {
       ^
{code}",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-839","Story","","2013-09-11T15:27:11.000+0000","eric.bottard","","Close parent contexts when shutting down","",1.0,-1.0,0.9998835861292872,-1.0,False,False,True,1378913231000.0,-1.0,-1.0,1379324653000.0,1379325553000.0,1378913231000.0,-1.0,-1.0,-1.0,1379325505000.0,1379325553000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1378913231000.0,-1.0,"Close parent contexts when shutting down","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-837","Story","","2013-09-11T10:07:53.000+0000","aeisenberg","","Tipsy tooltip hovers are not responsive after jobs list is refreshed from server","In the XD UI, the list of jobs is refreshed from the server every 5 seconds.  There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive (ie- they remain on the page and don't disappear) after the list of jobs is refreshed.",5.0,-1.0,0.9999926391702872,-1.0,False,False,True,1378894073000.0,-1.0,-1.0,1379436590000.0,1379437490000.0,1378894073000.0,-1.0,-1.0,-1.0,1379437486000.0,1379437490000.0,-1.0,-1.0,"aeisenberg",-1.0,-1.0,0,1378894073000.0,-1.0,"Tipsy tooltip hovers are not responsive after jobs list is refreshed from server","In the XD UI, the list of jobs is refreshed from the server every 5 seconds.  There are also tooltips that are activated when hovering over a job execution. These tooltips are no longer responsive (ie- they remain on the page and don't disappear) after the list of jobs is refreshed.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-832","Story","Sprint 15","2013-09-08T13:56:39.000+0000","mark.fisher","jencompgeek","Update docs for new Tap syntax","Now that taps are just channels, we need to update the docs.

The preceding colon is no longer needed (and will be removed altogether), so all examples should be like this:

{code}
tap:foo > bar
{code}

",2.0,0.9457618349573166,0.0,0.0001897042338535,True,True,True,1378648599000.0,1378703439000.0,1378706584000.0,1378705684000.0,1378706584000.0,1378648599000.0,-1.0,1378648610000.0,1378652210000.0,1378648599000.0,1378652199000.0,1378703439000.0,1378706584000.0,"mark.fisher",1378652210000.0,1378648599000.0,43,1378648599000.0,-1.0,"Update docs for new Tap syntax","Now that taps are just channels, we need to update the docs.

The preceding colon is no longer needed (and will be removed altogether), so all examples should be like this:

{code}
tap:foo > bar
{code}

",2.0,False,2.0,0,0.0,0,0,False
"XD-831","Story","Sprint 15","2013-09-08T13:55:05.000+0000","mark.fisher","eric.bottard","Update docs to cover Module config and lib directory structure","",2.0,-1.0,0.0,0.0001120580709825,False,True,True,1378648505000.0,-1.0,-1.0,1378808236000.0,1378809136000.0,1378648505000.0,-1.0,1378648523000.0,1378652123000.0,1378648505000.0,1378652105000.0,-1.0,-1.0,"mark.fisher",1378652123000.0,1378648505000.0,43,1378648505000.0,-1.0,"Update docs to cover Module config and lib directory structure","",2.0,False,2.0,0,0.0,0,0,False
"XD-830","Story","","2013-09-08T09:24:15.000+0000","aeisenberg","","Expose Job parameters for JobExecutions","Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in.  We are using a {{org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao}} to grab the job executions from the database. The database queries that it uses does not include the JobParameters (which is stored in a different table).

I think that this will require a change to batch admin in order to properly expose the parameters.",5.0,-1.0,0.9999915371088708,-1.0,False,False,True,1378632255000.0,-1.0,-1.0,1380285636000.0,1380286536000.0,1378632255000.0,-1.0,-1.0,-1.0,1380286522000.0,1380286536000.0,-1.0,-1.0,"aeisenberg",-1.0,-1.0,0,1378632255000.0,-1.0,"Expose Job parameters for JobExecutions","Currently, the way that we are accessing JobExecutions from batch admin, the JobParameters are not being filled in.  We are using a {{org.springframework.batch.admin.service.JdbcSearchableJobExecutionDao}} to grab the job executions from the database. The database queries that it uses does not include the JobParameters (which is stored in a different table).

I think that this will require a change to batch admin in order to properly expose the parameters.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-828","Story","Sprint 16","2013-09-07T20:22:09.000+0000","mark.fisher","mark.fisher","rename Bridge to Binding and add direction","org.springframework.integration.x.bus.Bridge should now be called Binding

we can also move the INBOUND/OUTBOUND direction (or possibly the CONSUMER/PRODUCER role) into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of ""in"" and ""out"" as Strings in that same code",2.0,0.8212309408127008,0.6704003464107642,0.8293453946380926,True,True,True,1378585329000.0,1379488086000.0,1379491686000.0,1379683702000.0,1379684602000.0,1378585329000.0,-1.0,1379497006000.0,1379500606000.0,1379322282000.0,1379325882000.0,1379488086000.0,1379491686000.0,"mark.fisher",1379500606000.0,1378585329000.0,43,1378585329000.0,-1.0,"rename Bridge to Binding and add direction","org.springframework.integration.x.bus.Bridge should now be called Binding

we can also move the INBOUND/OUTBOUND direction (or possibly the CONSUMER/PRODUCER role) into this class; that should simplify its usage in conditional code within the MessageBus code and also reduce the use of ""in"" and ""out"" as Strings in that same code",2.0,False,2.0,0,0.0,0,0,False
"XD-827","Story","Sprint 15","2013-09-07T14:18:42.000+0000","dturanski","dturanski","provide a property on twittersearch to enable the object-to-json transformer","Twitter search source should produce JSON or Pojo. The Pojo requires a custom wrapper class that is JSON friendly (e.g., zero arg constructor). The twittersearch module should have a parameter --json true/false (default true) to control the output type.",2.0,0.9997938356870426,0.0,0.999677868261004,True,True,True,1378563522000.0,1378641114000.0,1378641130000.0,1378640230000.0,1378641130000.0,1378563522000.0,-1.0,1378641105000.0,1378641130000.0,1378563522000.0,1378567122000.0,1378641114000.0,1378641130000.0,"dturanski",1378641130000.0,1378563522000.0,43,1378563522000.0,-1.0,"provide a property on twittersearch to enable the object-to-json transformer","Twitter search source should produce JSON or Pojo. The Pojo requires a custom wrapper class that is JSON friendly (e.g., zero arg constructor). The twittersearch module should have a parameter --json true/false (default true) to control the output type.",2.0,False,2.0,0,0.0,0,2,False
"XD-824","Story","Sprint 15","2013-09-07T07:37:47.000+0000","mark.fisher","mark.fisher","Rename ChannelRegistry","My current thinking is...

   ChannelRegistry -> MessageBus
   RabbitChannelRegistry -> RabbitMessageBus
   ...

Then, method names change like so:

   createInbound -> registerConsumer
   createOutbound -> registerProducer


",3.0,0.3083640654963859,0.30841323695727,0.3083148940355018,True,True,True,1378539467000.0,1378570823000.0,1378574423000.0,1378640252000.0,1378641152000.0,1378539467000.0,-1.0,1378570818000.0,1378574418000.0,1378570828000.0,1378574428000.0,1378570823000.0,1378574423000.0,"mark.fisher",1378574418000.0,1378539467000.0,43,1378539467000.0,-1.0,"Rename ChannelRegistry","My current thinking is...

   ChannelRegistry -> MessageBus
   RabbitChannelRegistry -> RabbitMessageBus
   ...

Then, method names change like so:

   createInbound -> registerConsumer
   createOutbound -> registerProducer


",3.0,False,3.0,0,0.0,0,0,False
"XD-823","Story","Sprint 15","2013-09-07T07:34:16.000+0000","dturanski","dturanski","add discardDeletes property to twitterstream source","If true(default): filter for delete messages in the twitter stream and route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.",1.0,0.7022484843833935,0.0,0.0037602639858798,True,True,True,1378539256000.0,1378548407000.0,1378552007000.0,1378551387000.0,1378552287000.0,1378539256000.0,-1.0,1378539305000.0,1378542905000.0,1378539256000.0,1378542856000.0,1378548407000.0,1378552007000.0,"dturanski",1378542905000.0,1378539256000.0,43,1378539256000.0,-1.0,"add discardDeletes property to twitterstream source","If true(default): filter for delete messages in the twitter stream and route to a discard channel. This creates a twitter stream including only new tweets and no references to deleted ones.",1.0,False,1.0,0,0.0,0,0,False
"XD-822","Story","Sprint 21","2013-09-07T06:04:40.000+0000","dturanski","luke","TupleBuilder.fromString() should not overwrite original id and timestamp fields","When converting a JSON string to a tuple the JSON may contain id. This method should handle this. Same with timestamp",2.0,0.9224036492552176,0.9223749550123208,0.9224462828202452,True,True,True,1378533880000.0,1390974362000.0,1390977962000.0,1392020006000.0,1392020906000.0,1378533880000.0,-1.0,1390974937000.0,1390978537000.0,1390973975000.0,1390977575000.0,1390974362000.0,1390977962000.0,"dturanski",1390978537000.0,1378533880000.0,43,1378533880000.0,-1.0,"TupleBuilder.fromString() should not overwrite original id and timestamp fields","When converting a JSON string to a tuple the JSON may contain id. This method should handle this. Same with timestamp",2.0,False,2.0,0,0.0,-1,0,False
"XD-821","Story","","2013-09-06T22:25:33.000+0000","aeisenberg","","Change url to access UI from browser","Currently, the url for accessing the XD UI is {{http://localhost:8080/admin-ui/index.html}}.  This feels messy and dated.  We should be able to access the ui without explicitly including the index.html, like this:

{code}
http://localhost:8080/admin-ui
{code}

",5.0,-1.0,1.0000308959083144,-1.0,False,False,True,1378506333000.0,-1.0,-1.0,1380285604000.0,1380286504000.0,1378506333000.0,-1.0,-1.0,-1.0,1380286559000.0,1380286504000.0,-1.0,-1.0,"aeisenberg",-1.0,-1.0,0,1378506333000.0,-1.0,"Change url to access UI from browser","Currently, the url for accessing the XD UI is {{http://localhost:8080/admin-ui/index.html}}.  This feels messy and dated.  We should be able to access the ui without explicitly including the index.html, like this:

{code}
http://localhost:8080/admin-ui
{code}

",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-820","Story","Sprint 15","2013-09-06T20:22:26.000+0000","grenfro","grenfro","Jobs and taps should not require a leading : since they have name spaces.","",3.0,0.0001706484641638,0.0001564277588168,0.999089874857793,True,True,True,1378498946000.0,1378498970000.0,1378502570000.0,1378638686000.0,1378639586000.0,1378498946000.0,-1.0,1378639458000.0,1378639586000.0,1378498968000.0,1378502568000.0,1378498970000.0,1378502570000.0,"grenfro",1378639586000.0,1378498946000.0,43,1378498946000.0,-1.0,"Jobs and taps should not require a leading : since they have name spaces.","",3.0,False,3.0,0,0.0,0,0,False
"XD-818","Story","","2013-09-06T12:08:18.000+0000","aeisenberg","aeisenberg","UI should poll server for latest on job info","The admin UI should be polling the server to automatically pick up any new jobs, executions, and instances.",5.0,-1.0,0.0,-1.0,False,False,True,1378469298000.0,-1.0,-1.0,1378560702000.0,1378561602000.0,1378469298000.0,-1.0,-1.0,-1.0,1378469298000.0,1378472898000.0,-1.0,-1.0,"aeisenberg",-1.0,-1.0,0,1378469298000.0,-1.0,"UI should poll server for latest on job info","The admin UI should be polling the server to automatically pick up any new jobs, executions, and instances.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-817","Story","","2013-09-06T12:04:38.000+0000","aeisenberg","","REST API for job listing should provide details on last execution ","A user should be able to view some important details of the last execution of a job from a job list.  The {{/batch/jobs}} REST endpoint should provide extra fields not currently available in the {{JobInfo}} class.

At a minimum, I would like to see:

* startTime
* startDate
* last job parameters
* duration
* last job status",5.0,-1.0,0.0025840072654932,-1.0,False,False,True,1378469078000.0,-1.0,-1.0,1378560670000.0,1378561570000.0,1378469078000.0,-1.0,-1.0,-1.0,1378469317000.0,1378472917000.0,-1.0,-1.0,"aeisenberg",-1.0,-1.0,0,1378469078000.0,-1.0,"REST API for job listing should provide details on last execution ","A user should be able to view some important details of the last execution of a job from a job list.  The {{/batch/jobs}} REST endpoint should provide extra fields not currently available in the {{JobInfo}} class.

At a minimum, I would like to see:

* startTime
* startDate
* last job parameters
* duration
* last job status",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-816","Story","Sprint 16","2013-09-06T11:43:40.000+0000","aclement","aclement","Support for composed streams","Some recent changes caused this to be turned off. Basically the change was to police whether a stream is well formed at create time, rather than deploy time.  By deferring that check we can create composed streams that are not deployable by themselves but that are when used as building blocks in proper streams.",2.0,0.7592123563274794,0.0,0.3607361472911473,True,True,True,1378467820000.0,1380633865000.0,1380637465000.0,1381319936000.0,1381320836000.0,1378467820000.0,-1.0,1379497006000.0,1379500606000.0,1378467820000.0,1378471420000.0,1380633865000.0,1380637465000.0,"aclement",1379500606000.0,1378467820000.0,43,1378467820000.0,-1.0,"Support for composed streams","Some recent changes caused this to be turned off. Basically the change was to police whether a stream is well formed at create time, rather than deploy time.  By deferring that check we can create composed streams that are not deployable by themselves but that are when used as building blocks in proper streams.",2.0,False,2.0,0,0.0,0,1,False
"XD-815","Story","","2013-09-05T16:08:01.000+0000","mark.pollack","","Updgrade to use spring-batch-admin 1.3.0 M1","Once spring-batch-admin 1.3.0.M1 is available, update the build to use it.",1.0,-1.0,-1.0,-1.0,False,False,False,1378397281000.0,-1.0,-1.0,1380715990000.0,1380716890000.0,1378397281000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1378397281000.0,-1.0,"Updgrade to use spring-batch-admin 1.3.0 M1 when available","Once spring-batch-admin 1.3.0.M1 is available, update the build to use it.  Likely to be Sept 7 or 9",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-814","Story","","2013-09-05T15:26:38.000+0000","aeisenberg","","Rebase UI on top of new batch admin API","Now that the new batch admin api is taking shape, we need to rebase the XD web UI to use this.  It's more than just changing the http urls sent to the xd server since the new API is not identical to the old one.",5.0,-1.0,-1.0,-1.0,False,False,False,1378394798000.0,-1.0,-1.0,1379005931000.0,1379006831000.0,1378394798000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"aeisenberg",-1.0,-1.0,0,1378394798000.0,-1.0,"Rebase UI on top of new batch admin API","Now that the new batch admin api is taking shape, we need to rebase the XD web UI to use this.  It's more than just changing the http urls sent to the xd server since the new API is not identical to the old one.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-813","Story","Sprint 15","2013-09-05T12:28:13.000+0000","iperumal","iperumal","Destroying XD job should remove job's entries at batch job repositories/batch job locator","When an XD job is destroyed/deleted, the batch jobRepository entries for the job (associated JobInstances, JobExecutions etc.,) and the BatchJobLocator entries.",4.0,0.0072408352853359,0.007234185453826,0.0072233794776224,True,True,True,1378384093000.0,1378392804000.0,1378396404000.0,1379586231000.0,1379587131000.0,1378384093000.0,-1.0,1378392783000.0,1378396383000.0,1378392796000.0,1378396396000.0,1378392804000.0,1378396404000.0,"iperumal",1378396383000.0,1378384093000.0,43,1378384093000.0,-1.0,"Destroying XD job should remove job's entries at batch job repositories/batch job locator","When an XD job is destroyed/deleted, the batch jobRepository entries for the job (associated JobInstances, JobExecutions etc.,) and the BatchJobLocator entries.",4.0,False,4.0,0,0.0,0,0,False
"XD-811","Story","Sprint 15","2013-09-05T10:45:56.000+0000","iperumal","iperumal","Add REST endpoint for launching Job","We need a REST endpoint to launch a job.

Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller/service to launch the job.

One possible way is to use the trigger source to launch the job at XD.",2.0,1.81405237532018e-05,1.5963660902817587e-05,0.0041520030766328,True,True,True,1378377956000.0,1378377981000.0,1378381581000.0,1379755186000.0,1379756086000.0,1378377956000.0,-1.0,1378383678000.0,1378387278000.0,1378377978000.0,1378381578000.0,1378377981000.0,1378381581000.0,"iperumal",1378387278000.0,1378377956000.0,43,1378377956000.0,-1.0,"Add REST endpoint for launching Job","We need a REST endpoint to launch a job.

Given the constraint on JobRegistry being not persistent and not available outside the container JVM, we can not use the batch job admin controller/service to launch the job.

One possible way is to use the trigger source to launch the job at XD.",2.0,False,2.0,0,0.0,0,0,False
"XD-810","Story","Sprint 15","2013-09-05T07:51:30.000+0000","jencompgeek","","Deleting a stream with reference to named channel disconnects channel from all streams","The following sequence results in ""Dispatcher has no subscribers"" error  (stack trace below), because deleting stream2 disconnects stream1 from the foo channel. Current work on XD-685 has infrastructure for disconnecting just the channels involved in a stream, so should make it easier to fix this issue once merged. 

stream create stream1 --definition ""time > :foo""
stream create stream2 --definition ""http > :foo""
stream create stream3 --definition "":foo > file""
stream destroy stream2
// expect file sink to still get time, but instead blows up b/c
// deleteOutbound(""foo"") killed links b/w foo and both local output channels

Server stack trace:
10:47:11,921 ERROR task-scheduler-6 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 24 more

10:47:12,924 ERROR task-scheduler-6 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 24 more

10:47:13,926 ERROR task-scheduler-4 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 24 more

10:47:14,928 ERROR task-scheduler-4 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 24 more

10:47:15,930 ERROR task-scheduler-1 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
",3.0,0.5781358394033992,0.1151686611168921,0.0167631373569198,True,True,True,1378367490000.0,1378474163000.0,1378477763000.0,1378551102000.0,1378552002000.0,1378367490000.0,-1.0,1378370583000.0,1378374183000.0,1378388740000.0,1378392340000.0,1378474163000.0,1378477763000.0,"jencompgeek",1378374183000.0,1378367490000.0,43,1378367490000.0,-1.0,"Deleting a stream with reference to named channel disconnects channel from all streams","The following sequence results in ""Dispatcher has no subscribers"" error  (stack trace below), because deleting stream2 disconnects stream1 from the foo channel. Current work on XD-685 has infrastructure for disconnecting just the channels involved in a stream, so should make it easier to fix this issue once merged. 

stream create stream1 --definition ""time > :foo""
stream create stream2 --definition ""http > :foo""
stream create stream3 --definition "":foo > file""
stream destroy stream2
// expect file sink to still get time, but instead blows up b/c
// deleteOutbound(""foo"") killed links b/w foo and both local output channels

Server stack trace:
10:47:11,921 ERROR task-scheduler-6 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 24 more

10:47:12,924 ERROR task-scheduler-6 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 24 more

10:47:13,926 ERROR task-scheduler-4 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 24 more

10:47:14,928 ERROR task-scheduler-4 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:81)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.springframework.integration.MessageDispatchingException: Dispatcher has no subscribers
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:109)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	... 24 more

10:47:15,930 ERROR task-scheduler-1 handler.LoggingHandler:140 - org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'SimpleModule [name=time, type=source, group=stream1, index=0].output'.
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:81)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.SourcePollingChannelAdapter.handleMessage(SourcePollingChannelAdapter.java:97)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.doPoll(AbstractPollingEndpoint.java:199)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint.access$000(AbstractPollingEndpoint.java:51)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:143)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$1.call(AbstractPollingEndpoint.java:141)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller$1.run(AbstractPollingEndpoint.java:273)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor$1.run(ErrorHandlingTaskExecutor.java:52)
	at org.springframework.core.task.SyncTaskExecutor.execute(SyncTaskExecutor.java:49)
	at org.springframework.integration.util.ErrorHandlingTaskExecutor.execute(ErrorHandlingTaskExecutor.java:49)
	at org.springframework.integration.endpoint.AbstractPollingEndpoint$Poller.run(AbstractPollingEndpoint.java:268)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
",3.0,False,3.0,0,0.0,0,0,False
"XD-808","Story","","2013-09-04T19:37:58.000+0000","thomas.risberg","thomas.risberg","Update to spring-data-hadoop 1.0.1.RELEASE","This might mean we should adjust our hadoopDistro options to the ones supported in the new release - hadoop12 (default), cdh4, hdp13, phd1 and hadoop21",3.0,-1.0,0.0,-1.0,False,False,True,1378323478000.0,-1.0,-1.0,1379494241000.0,1379495141000.0,1378323478000.0,-1.0,-1.0,-1.0,1378323478000.0,1378327078000.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1378323478000.0,-1.0,"Update to spring-data-hadoop 1.0.1.RELEASE","This might mean we should adjust our hadoopDistro options to the ones supported in the new release - hadoop12 (default), cdh4, hdp13, phd1 and hadoop21",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-807","Story","","2013-09-04T10:56:50.000+0000","hillert","","Shell: Standardize counter name parameter","The parameters are not optimal for the counter name between ""Aggregate Counter"" ""Field Value Counter""

--counterName versus --name",2.0,-1.0,-1.0,-1.0,False,False,False,1378292210000.0,-1.0,-1.0,1398341950000.0,1398342850000.0,1378292210000.0,1390858586000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1390858586000.0,-1.0,"Shell: Standardize counter name parameter","The parameters are not optimal for the counter name between ""Aggregate Counter"" ""Field Value Counter""

--counterName versus --name",-1.0,False,-1.0,1,-1.0,0,0,False
"XD-806","Story","Sprint 19","2013-09-04T07:54:52.000+0000","dturanski","","Validate module properties","Currently it's possible to do something like 
{code}
  http --prot=8888
{code}

It is possible to validate property names by parsing the module definition file(s) directly and matching property placeholders (or profile declarations that may be mapped to properties, etc). This must account recursively for imports as well. (I have some code in a branch that does this). 
",3.0,0.9578493286784214,0.9578488134315584,0.9454938377119226,True,True,True,1378281292000.0,1385717334000.0,1385720934000.0,1386043661000.0,1386044561000.0,1378281292000.0,-1.0,1385621415000.0,1385625015000.0,1385717330000.0,1385720930000.0,1385717334000.0,1385720934000.0,"dturanski",1385625015000.0,1378281292000.0,43,1378281292000.0,-1.0,"Validate module properties","Currently it's possible to do something like 
{code}
  http --prot=8888
{code}

It is possible to validate property names by parsing the module definition file(s) directly and matching property placeholders (or profile declarations that may be mapped to properties, etc). This must account recursively for imports as well. (I have some code in a branch that does this). 
",3.0,False,3.0,0,0.0,-1,0,False
"XD-805","Story","","2013-09-04T07:18:41.000+0000","hillert","","Get notified when created named channel ""is ready""","For testing purposes it would be super-helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to ""Thread.sleep"".",8.0,-1.0,-1.0,-1.0,False,False,False,1378279121000.0,-1.0,-1.0,1398418068000.0,1398418968000.0,1378279121000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1378279121000.0,-1.0,"Get notified when created named channel ""is ready""","For testing purposes it would be super-helpful if there be a hook to get notified when a named channel is up and running. In current tests one may have to resort to ""Thread.sleep"".",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-804","Story","Sprint 21","2013-09-04T07:15:33.000+0000","hillert","","Add Named Channel API","We need an abstraction in place to retrieve messages from a ""named channel"" programmatically.

Right now there is no implementation agnostic way of doing this (such as receiveMessage(), queueSize()).

This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to ""temp-files"" and non-essential sinks or sources etc. - e.g. 

{code}
:routeit > router --expression=payload.contains('a')?':foo':':bar'
{code}",8.0,0.8262729534792861,0.7962337867456559,0.7912849514953185,True,True,True,1378278933000.0,1390206778000.0,1390210378000.0,1392713753000.0,1392714653000.0,1378278933000.0,-1.0,1389701701000.0,1389705301000.0,1389773141000.0,1389776741000.0,1390206778000.0,1390210378000.0,"hillert",1389705301000.0,1378278933000.0,43,1378278933000.0,-1.0,"Add Named Channel API","We need an abstraction in place to retrieve messages from a ""named channel"" programmatically.

Right now there is no implementation agnostic way of doing this (such as receiveMessage(), queueSize()).

This could be quite useful for integration tests of streams. E.g. to do more focussed tests without resorting to ""temp-files"" and non-essential sinks or sources etc. - e.g. 

{code}
:routeit > router --expression=payload.contains('a')?':foo':':bar'
{code}",8.0,False,8.0,0,0.0,-1,1,False
"XD-802","Story","Sprint 15","2013-09-04T02:49:41.000+0000","eric.bottard","","Document splitter & aggregator processors","",3.0,-1.0,-1.0,0.0003384612025198,False,True,False,1378262981000.0,-1.0,-1.0,1378353672000.0,1378354572000.0,1378262981000.0,-1.0,1378263012000.0,1378266612000.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",1378266612000.0,1378262981000.0,43,1378262981000.0,-1.0,"Document splitter & aggregator processors","",3.0,False,3.0,0,0.0,-1,0,False
"XD-800","Story","Sprint 15","2013-09-03T12:31:39.000+0000","grenfro","grenfro","Job channels need to denote a namespace","Job channels need to have a namespace.  
i.e. job-somejobname.  Where the - is the delimiter for the namespace.  
The preference is to use the : instead of the -.  But XD-766 needs to be completed in order to support this.",2.0,0.8922323806781369,0.8922035850550716,0.9993377006694982,True,True,True,1378211499000.0,1378273469000.0,1378277069000.0,1378280054000.0,1378280954000.0,1378211499000.0,-1.0,1378280908000.0,1378280954000.0,1378273467000.0,1378277067000.0,1378273469000.0,1378277069000.0,"grenfro",1378280954000.0,1378211499000.0,43,1378211499000.0,-1.0,"Job channels need to denote a namespace","Job channels need to have a namespace.  
i.e. job-somejobname.  Where the - is the delimiter for the namespace.  
The preference is to use the : instead of the -.  But XD-766 needs to be completed in order to support this.",2.0,False,2.0,0,0.0,0,0,False
"XD-799","Story","Sprint 15","2013-09-02T22:31:16.000+0000","mark.pollack","luke","Change rabbitmq sink to use routing-key-expression instead of routing-key","The current rabbitmq sink uses the attribute routing-key, which defaults to the name of the stream.  This should be change to use the attribute routing-key-expression so that the routing-key can be determined using SpEL.  This will enable a dynamic evaluation of the routing-key based on message payload/header.

*Implementation Suggestions*

This hopefully should be changing the XML description of the sink to 		

routing-key-expression=""${routingKey:'${xd.stream.name}'}

This way, the ${xd.stream.name} is surrounded by a single quote to indicate a string literal to SpEL in the default case.  

*How to verify it works.*

One of the simple uses of this is to create a routing key based on payload.  In a distributed word-count example, the hashcode of a word would be sent to a certain number of processing modules that would perform the count. The idea is that the same word is sent to the same node over and over again, in particular if in-memory counters or state is computed - using centralized redis counters this wouldn't be necessary in the case of only counter state.

The stream 

http | rabbit --routingKey=""'word-' + payload.hashCode() % 3""

is an example of a stream that can be used to verify that messages published to a direct exchange will have routing keys of the value word-0, word-1, and word-2.  Binding a queue to each of these routing keys, one can observe the contents of messages in the queue to make sure that words are being routed to the appropriate queue, e.g. publishing ""hello"" as the payload of an http request should always appear in the same queue.  The rabbitmq admin console can be used for this purpose.
",3.0,0.9998295769247156,0.0,0.9997784500021304,True,True,True,1378161076000.0,1378278411000.0,1378278431000.0,1378277531000.0,1378278431000.0,1378161076000.0,-1.0,1378278405000.0,1378278431000.0,1378161076000.0,1378164676000.0,1378278411000.0,1378278431000.0,"mark.pollack",1378278431000.0,1378161076000.0,43,1378161076000.0,-1.0,"Change rabbitmq sink to use routing-key-expression instead of routing-key","The current rabbitmq sink uses the attribute routing-key, which defaults to the name of the stream.  This should be change to use the attribute routing-key-expression so that the routing-key can be determined using SpEL.  This will enable a dynamic evaluation of the routing-key based on message payload/header.

*Implementation Suggestions*

This hopefully should be changing the XML description of the sink to 		

routing-key-expression=""${routingKey:'${xd.stream.name}'}

This way, the ${xd.stream.name} is surrounded by a single quote to indicate a string literal to SpEL in the default case.  

*How to verify it works.*

One of the simple uses of this is to create a routing key based on payload.  In a distributed word-count example, the hashcode of a word would be sent to a certain number of processing modules that would perform the count. The idea is that the same word is sent to the same node over and over again, in particular if in-memory counters or state is computed - using centralized redis counters this wouldn't be necessary in the case of only counter state.

The stream 

http | rabbit --routingKey=""'word-' + payload.hashCode() % 3""

is an example of a stream that can be used to verify that messages published to a direct exchange will have routing keys of the value word-0, word-1, and word-2.  Binding a queue to each of these routing keys, one can observe the contents of messages in the queue to make sure that words are being routed to the appropriate queue, e.g. publishing ""hello"" as the payload of an http request should always appear in the same queue.  The rabbitmq admin console can be used for this purpose.
",3.0,False,3.0,0,0.0,0,0,False
"XD-797","Story","","2013-09-02T06:47:37.000+0000","grussell","","Package Tangle Introduced by XD-790","https://sonar.springsource.org/drilldown/measures/7173?metric=package_tangle_index&rids%5B%5D=7717",1.0,0.9937284831546788,0.9930078774810492,-1.0,True,False,True,1378104457000.0,1388671875000.0,1388675475000.0,1388737667000.0,1388738567000.0,1378104457000.0,-1.0,-1.0,-1.0,1388664212000.0,1388667812000.0,1388671875000.0,1388675475000.0,"grussell",-1.0,-1.0,0,1378104457000.0,-1.0,"Package Tangle Introduced by XD-790","https://sonar.springsource.org/drilldown/measures/7173?metric=package_tangle_index&rids%5B%5D=7717",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-796","Story","Sprint 16","2013-09-02T06:16:52.000+0000","eric.bottard","eric.bottard","Create separate commands for ""--all"" shell commands","Commands like ""stream deploy"" have changed over time to allow passing a ""--all"" option.

So it's either {{stream deploy foo}} or {{stream deploy --all}}. This has a number of drawbacks, given that these are the only 2 alternatives:
- Implementation code is cumbersome
- None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself
- TAB completion is less powerful as the shell doesn't know if we want the first or the second form.


Consider splitting those commands into two distinct commands, one as before and one literally named {{stream deploy all}}.",5.0,0.9927448631268784,0.992746669055308,0.9927383617845318,True,True,True,1378102612000.0,1380851184000.0,1380854784000.0,1380870371000.0,1380871271000.0,1378102612000.0,-1.0,1380851166000.0,1380854766000.0,1380851189000.0,1380854789000.0,1380851184000.0,1380854784000.0,"eric.bottard",1380854766000.0,1378102612000.0,43,1378102612000.0,-1.0,"Create separate commands for ""--all"" shell commands","Commands like ""stream deploy"" have changed over time to allow passing a ""--all"" option.

So it's either {{stream deploy foo}} or {{stream deploy --all}}. This has a number of drawbacks, given that these are the only 2 alternatives:
- Implementation code is cumbersome
- None of the options can be marked mandatory, yet one of them is required. This has to be checked in the command code itself
- TAB completion is less powerful as the shell doesn't know if we want the first or the second form.


Consider splitting those commands into two distinct commands, one as before and one literally named {{stream deploy all}}.",5.0,False,5.0,0,0.0,0,1,False
"XD-795","Story","Sprint 16","2013-09-02T05:46:50.000+0000","eric.bottard","eric.bottard","Refactor mail and imap source into one ""mail"" module, leveraging Profiles","Once XD-785 is merged",3.0,0.1594037987873472,0.1594040616144533,0.1593527577633476,True,True,True,1378100810000.0,1381133294000.0,1381136894000.0,1397123823000.0,1397124723000.0,1378100810000.0,-1.0,1381132323000.0,1381135923000.0,1381133299000.0,1381136899000.0,1381133294000.0,1381136894000.0,"eric.bottard",1381135923000.0,1378100810000.0,43,1378100810000.0,-1.0,"Refactor mail and imap source into one ""mail"" module, leveraging Profiles","Once XD-785 is merged",3.0,False,3.0,0,0.0,0,0,False
"XD-794","Story","Sprint 15","2013-08-30T15:29:06.000+0000","hillert","hillert","Add integration tests for SpEL and Groovy based routing","",4.0,0.0001775463099958,0.0,0.0001380915744412,True,True,True,1377876546000.0,1377876618000.0,1377880218000.0,1378281174000.0,1378282074000.0,1377876546000.0,-1.0,1377876602000.0,1377880202000.0,1377876546000.0,1377880146000.0,1377876618000.0,1377880218000.0,"hillert",1377880202000.0,1377876546000.0,43,1377876546000.0,-1.0,"Add integration tests for SpEL and Groovy based routing","",4.0,False,4.0,0,0.0,0,0,False
"XD-793","Story","Sprint 15","2013-08-30T12:19:43.000+0000","jencompgeek","jencompgeek","Upgrade Spring Data Redis to 1.1 RC1","",1.0,0.0878633237186598,0.0,0.0144220102063456,True,True,True,1377865183000.0,1377865579000.0,1377869179000.0,1377868790000.0,1377869690000.0,1377865183000.0,-1.0,1377865248000.0,1377868848000.0,1377865183000.0,1377868783000.0,1377865579000.0,1377869179000.0,"jencompgeek",1377868848000.0,1377865183000.0,43,1377865183000.0,-1.0,"Upgrade Spring Data Redis to 1.1 RC1","",1.0,False,1.0,0,0.0,-1,0,False
"XD-792","Story","Sprint 15","2013-08-30T09:24:55.000+0000","luke","luke","Update twittersearch to use Spring Integration support","",2.0,3.106550179768199e-05,0.0,2.769695340998153e-05,True,True,True,1377854695000.0,1377854778000.0,1377858378000.0,1380525569000.0,1380526469000.0,1377854695000.0,-1.0,1377854769000.0,1377858369000.0,1377854695000.0,1377858295000.0,1377854778000.0,1377858378000.0,"luke",1377858369000.0,1377854695000.0,43,1377854695000.0,-1.0,"Update twittersearch to use Spring Integration support","",2.0,False,2.0,0,0.0,0,0,False
"XD-791","Story","Sprint 15","2013-08-30T08:47:41.000+0000","eric.bottard","","Document mail related sources & sinks","",5.0,-1.0,0.9999895468015826,4.703939287823592e-05,False,True,True,1377852461000.0,-1.0,-1.0,1378808206000.0,1378809106000.0,1377852461000.0,-1.0,1377852506000.0,1377856106000.0,1378809096000.0,1378809106000.0,-1.0,-1.0,"eric.bottard",1377856106000.0,1377852461000.0,43,1377852461000.0,-1.0,"Document mail related sources & sinks","",5.0,False,5.0,0,0.0,0,0,False
"XD-790","Story","Sprint 15","2013-08-30T08:41:53.000+0000","dturanski","dturanski","Cryptic gradle error running tests when XD SingleNode is running","SingleNodeMain.launchSingleNodeServer(options) calls System.exit() causing a gradle buffer underflow. This is called from SingleNodeMainIntegrationTests. System.exit() should be called from the main method instead. ",1.0,0.998925139099646,0.0,0.9982928679817906,True,True,True,1377852113000.0,1377867912000.0,1377867929000.0,1377867029000.0,1377867929000.0,1377852113000.0,-1.0,1377867902000.0,1377867929000.0,1377852113000.0,1377855713000.0,1377867912000.0,1377867929000.0,"dturanski",1377867929000.0,1377852113000.0,43,1377852113000.0,-1.0,"Cryptic gradle error running tests when XD SingleNode is running","SingleNodeMain.launchSingleNodeServer(options) calls System.exit() causing a gradle buffer underflow. This is called from SingleNodeMainIntegrationTests. System.exit() should be called from the main method instead. ",1.0,False,1.0,0,0.0,0,0,False
"XD-789","Story","Sprint 15","2013-08-29T10:37:56.000+0000","mark.fisher","","Add index-based access to TuplePropertyAccessor","",2.0,0.0254277293662843,0.0254564360351624,0.0001020681560111,True,True,True,1377772676000.0,1377780648000.0,1377784248000.0,1378085292000.0,1378086192000.0,1377772676000.0,-1.0,1377772708000.0,1377776308000.0,1377780657000.0,1377784257000.0,1377780648000.0,1377784248000.0,"mark.fisher",1377776308000.0,1377772676000.0,43,1377772676000.0,-1.0,"Add index-based access to TuplePropertyAccessor","",2.0,False,2.0,0,0.0,0,0,False
"XD-788","Story","Sprint 32","2013-08-29T10:22:21.000+0000","hillert","","Integration Tests to run JobCommands Tests against all transports","similar to ChannelRegistry:

- AbstractChannelRegistryTests that has the real tests
- subclasses for each impl provide the registry to be tested

Thus one test can run against multiple transports.",8.0,0.8076187735573389,0.8072116322325804,0.8071039783912716,True,True,True,1377771741000.0,1405949241000.0,1405952841000.0,1412660446000.0,1412661346000.0,1377771741000.0,-1.0,1405931280000.0,1405934880000.0,1405935036000.0,1405938636000.0,1405949241000.0,1405952841000.0,"hillert",1405934880000.0,1377771741000.0,43,1377771741000.0,-1.0,"Add Integration Tests to run JobCommands Tests against all transports","similar to ChannelRegistry:

- AbstractChannelRegistryTests that has the real tests
- subclasses for each impl provide the registry to be tested

Thus one test can run against multiple transports.",8.0,False,8.0,0,0.0,0,2,True
"XD-786","Story","","2013-08-29T08:41:23.000+0000","hillert","","Add Warning-level log to postProcessAfterInitialization if Job name is not ""job""","",1.0,-1.0,-1.0,-1.0,False,False,False,1377765683000.0,-1.0,-1.0,1398342190000.0,1398343090000.0,1377765683000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1377765683000.0,-1.0,"Add Warning-level log to postProcessAfterInitialization if Job name is not ""job""","Should it be fatal vs. warning?",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-784","Story","Sprint 15","2013-08-29T07:01:01.000+0000","dturanski","dturanski","Build needs to override $XD_* environment variables","export XD_HOME=foo
gradle clean test 

build fails. Need to detected environment variables and override for the build",2.0,0.6598851308232291,0.0,0.333018506700702,True,True,True,1377759661000.0,1377785512000.0,1377789112000.0,1377797936000.0,1377798836000.0,1377759661000.0,-1.0,1377772707000.0,1377776307000.0,1377759661000.0,1377763261000.0,1377785512000.0,1377789112000.0,"dturanski",1377776307000.0,1377759661000.0,43,1377759661000.0,-1.0,"Build needs to override $XD_* environment variables","export XD_HOME=foo
gradle clean test 

build fails. Need to detected environment variables and override for the build",2.0,False,2.0,0,0.0,0,0,False
"XD-782","Story","Sprint 15","2013-08-29T02:35:48.000+0000","eric.bottard","eric.bottard","Add CompositeModuleRegistry","",3.0,0.0203504197992272,0.0203447684746509,0.207117278171382,True,True,True,1377743748000.0,1377754551000.0,1377758151000.0,1378273697000.0,1378274597000.0,1377743748000.0,-1.0,1377853696000.0,1377857296000.0,1377754548000.0,1377758148000.0,1377754551000.0,1377758151000.0,"eric.bottard",1377857296000.0,1377743748000.0,43,1377743748000.0,-1.0,"Add CompositeModuleRegistry","",3.0,False,3.0,0,0.0,-1,0,False
"XD-779","Story","Sprint 14","2013-08-29T01:38:36.000+0000","iperumal","hillert","Infinite recursion (StackOverflowError) when trying to process JobLaunchingMessageHandler's ""notifications"" channel output","Transport used: Redis

It looks like when the job is launched, the RedisChannelRegistry's composite handler tries to transform the JobLaunchingMessageHandler's output-channel (notifications) payload which is of type ""org.springframework.batch.core.JobExecution"".
and, This results in Infinite recursion (StackOverflowError).

Please see the stack trace here:

01:09:44,827 ERROR task-scheduler-1 redis.RedisQueueInboundChannelAdapter:148 - Error sending message
org.springframework.integration.MessageHandlingException: org.springframework.context.ApplicationContextException: Failed to start bean 'jobTriggerBean'; nested exception is org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:76)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)
	at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter.access$3(RedisQueueInboundChannelAdapter.java:1)
	at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter$ListenerTask.run(RedisQueueInboundChannelAdapter.java:145)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'jobTriggerBean'; nested exception is org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:170)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:339)
	at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:143)
	at org.springframework.context.support.DefaultLifecycleProcessor.start(DefaultLifecycleProcessor.java:89)
	at org.springframework.context.support.AbstractApplicationContext.start(AbstractApplicationContext.java:1278)
	at org.springframework.xd.module.SimpleModule.start(SimpleModule.java:152)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:162)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:149)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:120)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:97)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:82)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:103)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:144)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:231)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:130)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)
	... 22 more
Caused by: org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.xd.dirt.plugins.job.JobTriggerBean.executeBatchJob(JobTriggerBean.java:74)
	at org.springframework.xd.dirt.plugins.job.JobTriggerBean.start(JobTriggerBean.java:63)
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:167)
	... 45 more
Caused by: org.springframework.integration.x.json.TypedJsonMapper$SmartJsonConversionException: Infinite recursion (StackOverflowError) (through reference chain: org.springframework.batch.core.JobExecution[""stepExecutions""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""jobExecution""]->org.springframework.batch.core.JobExecution[""stepExecutions""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""jobExecution""]->org.springframework.batch.core.JobExecution[""stepExecutions""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""j
...
...
>java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""jobExecution""]->org.springframework.batch.core.JobExecution[""stepExecutions""])
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:611)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:571)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:597)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)
	at com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:119)
	at com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:23)
	at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serializeWithType(AsArraySerializerBase.java:197)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:571)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:597)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)",2.0,0.5444185526861812,0.4530915823411115,0.4526446834998563,True,True,True,1377740316000.0,1377757371000.0,1377760971000.0,1377770743000.0,1377771643000.0,1377740316000.0,-1.0,1377754496000.0,1377758096000.0,1377754510000.0,1377758110000.0,1377757371000.0,1377760971000.0,"iperumal",1377758096000.0,1377740316000.0,43,1377740316000.0,-1.0,"Infinite recursion (StackOverflowError) when trying to process JobLaunchingMessageHandler's ""notifications"" channel output","Transport used: Redis

It looks like when the job is launched, the RedisChannelRegistry's composite handler tries to transform the JobLaunchingMessageHandler's output-channel (notifications) payload which is of type ""org.springframework.batch.core.JobExecution"".
and, This results in Infinite recursion (StackOverflowError).

Please see the stack trace here:

01:09:44,827 ERROR task-scheduler-1 redis.RedisQueueInboundChannelAdapter:148 - Error sending message
org.springframework.integration.MessageHandlingException: org.springframework.context.ApplicationContextException: Failed to start bean 'jobTriggerBean'; nested exception is org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:76)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)
	at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter.access$3(RedisQueueInboundChannelAdapter.java:1)
	at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter$ListenerTask.run(RedisQueueInboundChannelAdapter.java:145)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: org.springframework.context.ApplicationContextException: Failed to start bean 'jobTriggerBean'; nested exception is org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:170)
	at org.springframework.context.support.DefaultLifecycleProcessor.access$200(DefaultLifecycleProcessor.java:51)
	at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:339)
	at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:143)
	at org.springframework.context.support.DefaultLifecycleProcessor.start(DefaultLifecycleProcessor.java:89)
	at org.springframework.context.support.AbstractApplicationContext.start(AbstractApplicationContext.java:1278)
	at org.springframework.xd.module.SimpleModule.start(SimpleModule.java:152)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:162)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleDeploy(ModuleDeployer.java:149)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:120)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)
	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:97)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:82)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:103)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:144)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:231)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:130)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)
	... 22 more
Caused by: org.springframework.integration.MessageHandlingException: error occurred in message handler [org.springframework.integration.x.redis.RedisChannelRegistry$CompositeHandler@26f8f92e]
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:79)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.xd.dirt.plugins.job.JobTriggerBean.executeBatchJob(JobTriggerBean.java:74)
	at org.springframework.xd.dirt.plugins.job.JobTriggerBean.start(JobTriggerBean.java:63)
	at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:167)
	... 45 more
Caused by: org.springframework.integration.x.json.TypedJsonMapper$SmartJsonConversionException: Infinite recursion (StackOverflowError) (through reference chain: org.springframework.batch.core.JobExecution[""stepExecutions""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""jobExecution""]->org.springframework.batch.core.JobExecution[""stepExecutions""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""jobExecution""]->org.springframework.batch.core.JobExecution[""stepExecutions""]->java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""j
...
...
>java.util.UnmodifiableRandomAccessList[0]->org.springframework.batch.core.StepExecution[""jobExecution""]->org.springframework.batch.core.JobExecution[""stepExecutions""])
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:611)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:571)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:597)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)
	at com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:119)
	at com.fasterxml.jackson.databind.ser.std.CollectionSerializer.serializeContents(CollectionSerializer.java:23)
	at com.fasterxml.jackson.databind.ser.std.AsArraySerializerBase.serializeWithType(AsArraySerializerBase.java:197)
	at com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:571)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:597)
	at com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeWithType(BeanSerializerBase.java:492)",2.0,False,2.0,0,0.0,0,0,False
"XD-778","Bug","Sprint 14","2013-08-28T15:17:15.000+0000","grenfro","grenfro","${xd.home}/data shows up after gradlew build","When the XD starts up and does not see its job repo it will create one in the${xd.home}/data directory.  When xd.home is not set in system properties the job repo creates a literal ${xd.home}/data directory.  ",2.0,0.0002943817247825,0.0001913481211086,0.7996143599405349,True,True,True,1377703035000.0,1377703055000.0,1377706655000.0,1377770074000.0,1377770974000.0,1377703035000.0,-1.0,1377757360000.0,1377760960000.0,1377703048000.0,1377706648000.0,1377703055000.0,1377706655000.0,"grenfro",1377760960000.0,1377703035000.0,43,1377703035000.0,-1.0,"${xd.home}/data shows up after gradlew build","When the XD starts up and does not see its job repo it will create one in the${xd.home}/data directory.  When xd.home is not set in system properties the job repo creates a literal ${xd.home}/data directory.  ",2.0,False,2.0,0,0.0,0,0,False
"XD-777","Bug","Sprint 30","2013-08-28T14:59:22.000+0000","luke","","Add validation on tap definitions that checks for module names that are part of the stream definition","Try:
{code}
stream create --name aa --definition ""time | log""
tap create --name t1 --definition ""tap aa.log | log""
{code}

Results in:

{code}
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.NullPointerException
{code}",2.0,0.9957264547859348,0.9923905021090728,0.9762697213442,True,True,True,1377701962000.0,1403062603000.0,1403066203000.0,1403170548000.0,1403171448000.0,1377701962000.0,-1.0,1402567050000.0,1402570650000.0,1402977638000.0,1402981238000.0,1403062603000.0,1403066203000.0,"luke",1402570650000.0,1377701962000.0,43,1377701962000.0,-1.0,"Add validation on tap definitions that checks for module names that are part of the stream definition","Try:
{code}
stream create --name aa --definition ""time | log""
tap create --name t1 --definition ""tap aa.log | log""
{code}

Results in:

{code}
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.NullPointerException
{code}",2.0,False,2.0,0,0.0,0,1,False
"XD-776","Bug","","2013-08-28T14:46:41.000+0000","hillert","","Shell: Remove ""taps list"" command","We should only allow ""tap list"" - currently ""tap list"" AND ""taps list"" are allowed but ""tap list"" does not show up under help.",1.0,-1.0,-1.0,-1.0,False,False,False,1377701201000.0,-1.0,-1.0,1378101355000.0,1378102255000.0,1377701201000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1377701201000.0,-1.0,"Shell: Remove ""taps list"" command","We should only allow ""tap list"" - currently ""tap list"" AND ""taps list"" are allowed but ""tap list"" does not show up under help.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-775","Story","Sprint 15","2013-08-28T14:14:07.000+0000","mark.fisher","","Document router processor module","for an example, see comments here:
https://jira.springsource.org/browse/XD-671
",2.0,0.5307986171917057,0.5223344294618593,0.4764656665304613,True,True,True,1377699247000.0,1377781085000.0,1377784685000.0,1377852526000.0,1377853426000.0,1377699247000.0,-1.0,1377772708000.0,1377776308000.0,1377779780000.0,1377783380000.0,1377781085000.0,1377784685000.0,"mark.fisher",1377776308000.0,1377699247000.0,43,1377699247000.0,-1.0,"Document router processor module","for an example, see comments here:
https://jira.springsource.org/browse/XD-671
",2.0,False,2.0,0,0.0,-1,0,False
"XD-774","Story","Sprint 15","2013-08-28T12:53:52.000+0000","grenfro","iperumal","Need to create a Persistent-Job-Registry ","In order to hook up the to get access to all the jobs available the job registry has to be shared.  currently the only implmentation is is the MapJobRegistry.  
====
Testability.
====
The admin will need to be see all jobs created by its containers.",2.0,0.1241874152313595,0.0117854645936678,0.1133037369852544,True,True,True,1377694432000.0,1377780227000.0,1377783827000.0,1378384383000.0,1378385283000.0,1377694432000.0,1378385279000.0,1377772708000.0,1377776308000.0,1377702574000.0,1377706174000.0,1377780227000.0,1377783827000.0,"grenfro",1377776308000.0,1377694432000.0,43,1378385279000.0,1378385279000.0,"Need to create a Persistent-Job-Registry ","In order to hook up the to get access to all the jobs available the job registry has to be shared.  currently the only implmentation is is the MapJobRegistry.  
====
Testability.
====
The admin will need to be see all jobs created by its containers.",5.0,True,5.0,1,-60.0,0,0,False
"XD-770","Story","Sprint 15","2013-08-28T08:06:32.000+0000","grenfro","grenfro","Update Batch Job docs to cover triggers as a source","",2.0,0.1344574316277998,0.1344516012424551,0.1392237716471276,True,True,True,1377677192000.0,1377769438000.0,1377773038000.0,1378362353000.0,1378363253000.0,1377677192000.0,-1.0,1377772708000.0,1377776308000.0,1377769434000.0,1377773034000.0,1377769438000.0,1377773038000.0,"grenfro",1377776308000.0,1377677192000.0,43,1377677192000.0,-1.0,"Update Batch Job docs to cover triggers as a source","",2.0,False,2.0,0,0.0,-1,0,False
"XD-768","Story","Sprint 15","2013-08-28T07:38:24.000+0000","hillert","eric.bottard","Add Email Sink","",8.0,0.2908507587505446,0.1350260484044622,0.1622562708967294,True,True,True,1377675504000.0,1377849746000.0,1377853346000.0,1378273681000.0,1378274581000.0,1377675504000.0,-1.0,1377772708000.0,1377776308000.0,1377756395000.0,1377759995000.0,1377849746000.0,1377853346000.0,"hillert",1377776308000.0,1377675504000.0,43,1377675504000.0,-1.0,"Add Email Sink","",8.0,False,8.0,0,0.0,-1,0,False
"XD-767","Story","Sprint 15","2013-08-28T07:33:49.000+0000","hillert","eric.bottard","Add Email Source","",8.0,0.2911755872931126,0.1354027629471436,0.1626384810464495,True,True,True,1377675229000.0,1377849748000.0,1377853348000.0,1378273689000.0,1378274589000.0,1377675229000.0,-1.0,1377772708000.0,1377776308000.0,1377756384000.0,1377759984000.0,1377849748000.0,1377853348000.0,"hillert",1377776308000.0,1377675229000.0,43,1377675229000.0,-1.0,"Add Email Source","",8.0,False,8.0,0,0.0,-1,0,False
"XD-766","Story","Sprint 15","2013-08-28T07:25:41.000+0000","grenfro","","Parse needs to handle a  ':' embedded in a name.","Also drop the enhanced portion of the EnhancedStreamParser.",8.0,0.9999957090217428,0.9276524161978448,0.0788355429270765,True,True,True,1377674741000.0,1385365262000.0,1385365295000.0,1385364395000.0,1385365295000.0,1377674741000.0,1378281066000.0,1378281030000.0,1378284630000.0,1384808902000.0,1384812502000.0,1385365262000.0,1385365295000.0,"grenfro",1378284630000.0,1378281081000.0,43,1378281081000.0,-1.0,"Parser needs to handle a  ':' embedded in a name.","Also drop the enhanced portion of the EnhancedStreamParser.",8.0,False,8.0,0,0.0,0,1,True
"XD-764","Story","","2013-08-28T06:56:05.000+0000","grenfro","","Consolidate Trigger Sources into a single Source","Currently we have 2 trigger sources: trigger & cron-trigger.  The preference is to have a user to just use a single trigger source.  for example:
* trigger > :myjob
* trigger --cron='...' >:myjob
* trigger --fixedDelay='...' > :myjob

One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger='cronTriggerBean'.  Each trigger bean would setup the channel with the correct poller.

",5.0,-1.0,-1.0,-1.0,False,False,False,1377672965000.0,-1.0,-1.0,1390896779000.0,1390897679000.0,1377672965000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1377672965000.0,-1.0,"Consolidate Trigger Sources into a single Source","Currently we have 2 trigger sources: trigger & cron-trigger.  The preference is to have a user to just use a single trigger source.  for example:
* trigger > :myjob
* trigger --cron='...' >:myjob
* trigger --fixedDelay='...' > :myjob

One option to handle this is to use spel to reference a bean and then have different trigger beans defined. i.e. trigger='cronTriggerBean'.  Each trigger bean would setup the channel with the correct poller.

",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-763","Story","Sprint 15","2013-08-28T06:46:35.000+0000","grenfro","grenfro","Remove Trigger Module Code ","Triggers will be a source and no longer as a unique module.
* The following have to be removed:
** spring-xd-dirt:  
-- package: org.springframework.xd.dirt.plugins.trigger
-- META-INF: spring-xd/plugins/triggers.xml
-- org.springframework.xd.dirt.stream.TriggerPlugin

*The following beans will require updates to remove the trigger code
** spring-xd-dirt:  
--META-INF: spring-xd/internal/deployers.xml - Remove Triggerdeployer
--org.springframework.xd.dirt.plugins.job.JobPlugin - Remove the registrars for fixedDelay, fixedRate, Cron.  As well as the component selection, only need the job-modules-bean
--Update the tests to use the trigger as a source, instead of the trigger module.
** spring-xd-shell:
Remove trigger commands and associated tests

** xd controllers:
Remove trigger controllers and their associated tests 
This list cover most but not all the components affected.
--Success criteria--
Successful unit and integration tests.

",5.0,0.1585607430275894,0.1585558478177154,0.1636843960290057,True,True,True,1377672395000.0,1377769568000.0,1377773168000.0,1378284339000.0,1378285239000.0,1377672395000.0,-1.0,1377772708000.0,1377776308000.0,1377769565000.0,1377773165000.0,1377769568000.0,1377773168000.0,"grenfro",1377776308000.0,1377672395000.0,43,1377672395000.0,-1.0,"Remove Trigger Module Code ","Triggers will be a source and no longer as a unique module.
* The following have to be removed:
** spring-xd-dirt:  
-- package: org.springframework.xd.dirt.plugins.trigger
-- META-INF: spring-xd/plugins/triggers.xml
-- org.springframework.xd.dirt.stream.TriggerPlugin

*The following beans will require updates to remove the trigger code
** spring-xd-dirt:  
--META-INF: spring-xd/internal/deployers.xml - Remove Triggerdeployer
--org.springframework.xd.dirt.plugins.job.JobPlugin - Remove the registrars for fixedDelay, fixedRate, Cron.  As well as the component selection, only need the job-modules-bean
--Update the tests to use the trigger as a source, instead of the trigger module.
** spring-xd-shell:
Remove trigger commands and associated tests

** xd controllers:
Remove trigger controllers and their associated tests 
This list cover most but not all the components affected.
--Success criteria--
Successful unit and integration tests.

",5.0,False,5.0,0,0.0,0,0,False
"XD-762","Story","","2013-08-26T20:18:36.000+0000","hillert","","Add Spring XD Build Plan for Java 8 to Bamboo  ","This is issue depends on XD-761",2.0,-1.0,-1.0,-1.0,False,False,False,1377548316000.0,-1.0,-1.0,1390856124000.0,1390857024000.0,1377548316000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1377548316000.0,-1.0,"Add Spring XD Build Plan for Java 8 to Bamboo  ","This is issue depends on XD-761

https://build.springsource.org/browse/XD-JDK8",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-761","Story","","2013-08-26T20:17:24.000+0000","hillert","","Make Spring XD buildable with Java 8","JavaDoc issues are causing the build to fail with Java 8",8.0,-1.0,-1.0,-1.0,False,False,False,1377548244000.0,-1.0,-1.0,1390856090000.0,1390856990000.0,1377548244000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1377548244000.0,-1.0,"Make Spring XD buildable with Java 8","JavaDoc issues are causing the build to fail with Java 8",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-759","Story","Sprint 14","2013-08-26T13:21:33.000+0000","mark.fisher","iperumal","The xd-singlenode script should have execute permissions","The xd-singlenode script currently has '644' permissions unlike xd-admin and xd-container (which have '755'): 

{code}
-rwxr-xr-x  1 mark  staff  5899 Aug 26 16:19 xd-admin
-rwxr-xr-x  1 mark  staff  5955 Aug 26 16:19 xd-container
-rw-r--r--  1 mark  staff  5919 Aug 26 16:19 xd-singlenode
{code}
",1.0,0.3555555555555555,0.3437908496732026,0.338562091503268,False,False,False,1377523293000.0,1377523565000.0,1377524058000.0,1377523293000.0,1377524058000.0,1377523293000.0,-1.0,1377523552000.0,1377524058000.0,1377523556000.0,1377524058000.0,1377523565000.0,1377524058000.0,"mark.fisher",1377524058000.0,1377523293000.0,43,1377523293000.0,-1.0,"The xd-singlenode script should have execute permissions","The xd-singlenode script currently has '644' permissions unlike xd-admin and xd-container (which have '755'): 

{code}
-rwxr-xr-x  1 mark  staff  5899 Aug 26 16:19 xd-admin
-rwxr-xr-x  1 mark  staff  5955 Aug 26 16:19 xd-container
-rw-r--r--  1 mark  staff  5919 Aug 26 16:19 xd-singlenode
{code}
",1.0,False,1.0,0,0.0,-1,0,False
"XD-757","Story","Sprint 15","2013-08-26T09:02:47.000+0000","mark.pollack","eric.bottard","Create aggregator module","Create a processing module based on SI's aggregator component.  The completion criteria for the aggregator should be a simple count of messages (e.g. received 50 messages) and a timeout so that messages don't stay in the aggregator module for more than 30 seconds. 

*Implementation suggestions*

Create an XML based processing module definition using the SI aggregator namespace.  Only the options to support the features in the description should be exposed as property placeholders.

*How to know it works*
A shell style integration test that has a source that sends a known amount of messages.  A ticktock like module would perhaps be a good example.  10 messages sent every 100ms with an aggregator set to a 'aggregate count' of 10, should have 1 message output (perhaps to file sink whose name is based on time as well, is that possible.).  A ticktock example with a 1 second delay and with the aggregator module set to have a timeout of 0.5 seconds will have only one message in the file (again assumign the file name has a timestamp/counter in the filename). ",3.0,0.865516072243594,0.8655108560996283,0.8654939036317403,True,True,True,1377507767000.0,1378171488000.0,1378175088000.0,1378273717000.0,1378274617000.0,1377507767000.0,-1.0,1378171471000.0,1378175071000.0,1378171484000.0,1378175084000.0,1378171488000.0,1378175088000.0,"mark.pollack",1378175071000.0,1377507767000.0,43,1377507767000.0,-1.0,"Create aggregator module","Create a processing module based on SI's aggregator component.  The completion criteria for the aggregator should be a simple count of messages (e.g. received 50 messages) and a timeout so that messages don't stay in the aggregator module for more than 30 seconds. 

*Implementation suggestions*

Create an XML based processing module definition using the SI aggregator namespace.  Only the options to support the features in the description should be exposed as property placeholders.

*How to know it works*
A shell style integration test that has a source that sends a known amount of messages.  A ticktock like module would perhaps be a good example.  10 messages sent every 100ms with an aggregator set to a 'aggregate count' of 10, should have 1 message output (perhaps to file sink whose name is based on time as well, is that possible.).  A ticktock example with a 1 second delay and with the aggregator module set to have a timeout of 0.5 seconds will have only one message in the file (again assumign the file name has a timestamp/counter in the filename). ",3.0,False,3.0,0,0.0,-1,0,False
"XD-756","Story","Sprint 15","2013-08-26T08:53:01.000+0000","mark.pollack","eric.bottard","Create a splitter module","The splitter functionality in Spring Integration should be exposed to XD as a processing module.  The splitter should use a SpEL expression to specify how to split the message up.  

*Implementation Suggestions*

This should be a simple XML based module definition that has input/output channels and has the SpEL expression parameterized.  The default value of the SpEL expression should result in the message not being split.

*How to check it works*

The current file or tail input source can be used to split up the text in a file into words.  The tail module should be checked to see how many lines of text it will read into memory at once.  The file module node with the file-to-string transformer will only work for small files as it keep the whole file in memory.  

If there is a big memory inefficiency in using the tail file input source, create a new story to investigate how to have a file based input source that creates a message per line of text or something that is will not result in excessive memory usage.  ",2.0,0.7855094257013407,0.7847067393080478,0.7855003042650534,True,True,True,1377507181000.0,1378109999000.0,1378113599000.0,1378273704000.0,1378274604000.0,1377507181000.0,-1.0,1378109992000.0,1378113592000.0,1378109383000.0,1378112983000.0,1378109999000.0,1378113599000.0,"mark.pollack",1378113592000.0,1377507181000.0,43,1377507181000.0,-1.0,"Create a splitter module","The splitter functionality in Spring Integration should be exposed to XD as a processing module.  The splitter should use a SpEL expression to specify how to split the message up.  

*Implementation Suggestions*

This should be a simple XML based module definition that has input/output channels and has the SpEL expression parameterized.  The default value of the SpEL expression should result in the message not being split.

*How to check it works*

The current file or tail input source can be used to split up the text in a file into words.  The tail module should be checked to see how many lines of text it will read into memory at once.  The file module node with the file-to-string transformer will only work for small files as it keep the whole file in memory.  

If there is a big memory inefficiency in using the tail file input source, create a new story to investigate how to have a file based input source that creates a message per line of text or something that is will not result in excessive memory usage.  ",2.0,False,2.0,0,0.0,-1,0,False
"XD-754","Story","Sprint 14","2013-08-25T06:57:35.000+0000","grussell","dturanski","Fix Class/Package Tangle Introduced by XD-353","{{container}} and {{event}}. {{XDContainer}} references and is referenced by {{ContainerStartedEvent}} (and stopped).

https://sonar.springsource.org/drilldown/measures/7173?metric=package_cycles&rids%5B%5D=7717

",1.0,0.9978474496958352,0.9924286382779598,0.9924847917641554,True,True,True,1377413855000.0,1377520475000.0,1377520705000.0,1377519805000.0,1377520705000.0,1377413855000.0,-1.0,1377519902000.0,1377520705000.0,1377519896000.0,1377520705000.0,1377520475000.0,1377520705000.0,"grussell",1377520705000.0,1377413855000.0,43,1377413855000.0,-1.0,"Fix Class/Package Tangle Introduced by XD-353","{{container}} and {{event}}. {{XDContainer}} references and is referenced by {{ContainerStartedEvent}} (and stopped).

https://sonar.springsource.org/drilldown/measures/7173?metric=package_cycles&rids%5B%5D=7717

",1.0,False,1.0,0,0.0,-1,1,False
"XD-753","Story","Sprint 18","2013-08-24T08:13:06.000+0000","grenfro","grenfro","JDBC property settings need to be made externally configurable","We need to have a properties section (documented as well) so that users can setup their jdbc connections for the various components.",2.0,0.8677127355146037,0.8548132972513635,0.8691385228642007,True,True,True,1377331986000.0,1382511044000.0,1382514644000.0,1383299718000.0,1383300618000.0,1377331986000.0,-1.0,1382519554000.0,1382523154000.0,1382434052000.0,1382437652000.0,1382511044000.0,1382514644000.0,"grenfro",1382523154000.0,1377331986000.0,43,1377331986000.0,-1.0,"JDBC property settings need to be made externally configurable","We need to have a properties section (documented as well) so that users can setup their jdbc connections for the various components.",2.0,False,2.0,0,0.0,0,0,False
"XD-752","Story","Sprint 14","2013-08-23T17:50:20.000+0000","iperumal","iperumal","Restrict Job launcher with more than one batch job configured in job module","Currently the Job launcher launches all the batch jobs configured in the job module.

Please refer, ModuleJobLauncher's executeBatchJob().

This makes the JobRegistry registers with multiple batch jobs under the same Spring XD job name (group name).

Also, it is understood that having multiple jobs configuration under the same config xml is uncommon.",2.0,0.7845129904045605,0.7844682216352539,0.7844383757890495,True,True,True,1377280220000.0,1377595646000.0,1377599246000.0,1377681386000.0,1377682286000.0,1377280220000.0,-1.0,1377595616000.0,1377599216000.0,1377595628000.0,1377599228000.0,1377595646000.0,1377599246000.0,"iperumal",1377599216000.0,1377280220000.0,43,1377280220000.0,-1.0,"Restrict Job launcher with more than one batch job configured in job module","Currently the Job launcher launches all the batch jobs configured in the job module.

Please refer, ModuleJobLauncher's executeBatchJob().

This makes the JobRegistry registers with multiple batch jobs under the same Spring XD job name (group name).

Also, it is understood that having multiple jobs configuration under the same config xml is uncommon.",2.0,False,2.0,0,0.0,0,0,False
"XD-746","Story","","2013-08-22T16:32:44.000+0000","grenfro","","Gradle Launch needs to use singlenodemain  vs. admin main","",1.0,0.0006585136406396,0.0005644402634054,-1.0,True,False,True,1377189164000.0,1377189178000.0,1377192778000.0,1377209524000.0,1377210424000.0,1377189164000.0,-1.0,-1.0,-1.0,1377189176000.0,1377192776000.0,1377189178000.0,1377192778000.0,"grenfro",-1.0,-1.0,0,1377189164000.0,-1.0,"Gradle Launch needs to use singlenodemain  vs. admin main","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-745","Story","Sprint 15","2013-08-22T15:19:45.000+0000","mark.pollack","iperumal","Return the list of Jobs from spring-batch-admin","The current XD JobController that returns a list of jobs has quite a different API signature than what is in spring-batch-admin.  To simplify the UI development, a new controller JobAdminController, will be created that lives under the request path /jobs/admin.  The goal is to return a the current JSON structure of spring-batch-admin /jobs/ request and make only minimal changes to implementations of controllers as found .  See #1 in the Doc (link to json output doc for spring batch).

*Implementation Suggestions*

There will need to be some SpringMVC setup that will enable the current style of spring-batch-admin controller requests to co-exist with the existing XD Controllers, e.g. the use of .json for json marshalling etc.  This may in fact be the bulk of time spend in this first story to integration spring-batch-admin style controllers into XD.

A new controller named JobAdminController that in the spring-xd-dirt project in the package org.springframework.xd.dirt.rest.  The JobAdminController will not need to follow the same HATEOAS style as the other controllers at this time.

The current controller in Spring Batch Admin looks like this
{code}
    @RequestMapping(value = ""/jobs"", method = RequestMethod.GET)
    public void jobs(ModelMap model, @RequestParam(defaultValue = ""0"") int startJob,
            @RequestParam(defaultValue = ""20"") int pageSize) {
        int total = jobService.countJobs();
        TableUtils.addPagination(model, total, startJob, pageSize, ""Job"");
        Collection<String> names = jobService.listJobs(startJob, pageSize);
        List<JobInfo> jobs = new ArrayList<JobInfo>();
        for (String name : names) {
            int count = 0;
            try {
                count = jobService.countJobExecutionsForJob(name);
            }
            catch (NoSuchJobException e) {
                // shouldn't happen
            }
            boolean launchable = jobService.isLaunchable(name);
            boolean incrementable = jobService.isIncrementable(name);
            jobs.add(new JobInfo(name, count, null, launchable, incrementable));
        }
        model.addAttribute(""jobs"", jobs);
    }
{code}

Something like
{code}
@RequestMapping(value = ""/jobs/admin/jobs"", method = RequestMethod.GET)
 public void jobs(ModelMap model, @RequestParam(defaultValue = ""0"") int startJob,
            @RequestParam(defaultValue = ""20"") int pageSize) {

       // We do *not* have to query the Spring Batch Admin JobService at this time, but
      //  instead use the  JobDeployer to get information about jobs launched by Spring XD

      Iterable<JobDefinition> jobDefinitions =  dobDeployer.findAll()

     // copy these over to a List<JobInfo> as best as possible, copy name over.  
     // not sure how description is getting added to the JSON
      // pari
    }
{code}
*How to verify it works*
A sample job needs to be in the modules/job directory. JobCommandTests/AbstractJobIntegrationTest seems to have what is need to stage a job for 
",5.0,0.351321995045538,0.3513178302322133,0.4897179088638889,True,True,True,1377184785000.0,1377606559000.0,1377610159000.0,1378384419000.0,1378385319000.0,1377184785000.0,-1.0,1377772708000.0,1377776308000.0,1377606554000.0,1377610154000.0,1377606559000.0,1377610159000.0,"mark.pollack",1377776308000.0,1377184785000.0,43,1377184785000.0,-1.0,"Return the list of Jobs from spring-batch-admin","The current XD JobController that returns a list of jobs has quite a different API signature than what is in spring-batch-admin.  To simplify the UI development, a new controller JobAdminController, will be created that lives under the request path /jobs/admin.  The goal is to return a the current JSON structure of spring-batch-admin /jobs/ request and make only minimal changes to implementations of controllers as found .  See #1 in the Doc (link to json output doc for spring batch).

*Implementation Suggestions*

There will need to be some SpringMVC setup that will enable the current style of spring-batch-admin controller requests to co-exist with the existing XD Controllers, e.g. the use of .json for json marshalling etc.  This may in fact be the bulk of time spend in this first story to integration spring-batch-admin style controllers into XD.

A new controller named JobAdminController that in the spring-xd-dirt project in the package org.springframework.xd.dirt.rest.  The JobAdminController will not need to follow the same HATEOAS style as the other controllers at this time.

The current controller in Spring Batch Admin looks like this
{code}
    @RequestMapping(value = ""/jobs"", method = RequestMethod.GET)
    public void jobs(ModelMap model, @RequestParam(defaultValue = ""0"") int startJob,
            @RequestParam(defaultValue = ""20"") int pageSize) {
        int total = jobService.countJobs();
        TableUtils.addPagination(model, total, startJob, pageSize, ""Job"");
        Collection<String> names = jobService.listJobs(startJob, pageSize);
        List<JobInfo> jobs = new ArrayList<JobInfo>();
        for (String name : names) {
            int count = 0;
            try {
                count = jobService.countJobExecutionsForJob(name);
            }
            catch (NoSuchJobException e) {
                // shouldn't happen
            }
            boolean launchable = jobService.isLaunchable(name);
            boolean incrementable = jobService.isIncrementable(name);
            jobs.add(new JobInfo(name, count, null, launchable, incrementable));
        }
        model.addAttribute(""jobs"", jobs);
    }
{code}

Something like
{code}
@RequestMapping(value = ""/jobs/admin/jobs"", method = RequestMethod.GET)
 public void jobs(ModelMap model, @RequestParam(defaultValue = ""0"") int startJob,
            @RequestParam(defaultValue = ""20"") int pageSize) {

       // We do *not* have to query the Spring Batch Admin JobService at this time, but
      //  instead use the  JobDeployer to get information about jobs launched by Spring XD

      Iterable<JobDefinition> jobDefinitions =  dobDeployer.findAll()

     // copy these over to a List<JobInfo> as best as possible, copy name over.  
     // not sure how description is getting added to the JSON
      // pari
    }
{code}
*How to verify it works*
A sample job needs to be in the modules/job directory. JobCommandTests/AbstractJobIntegrationTest seems to have what is need to stage a job for 
",5.0,False,5.0,0,0.0,0,0,False
"XD-744","Story","","2013-08-22T13:55:21.000+0000","mark.pollack","","Add dependency to spring batch admin in spring-xd-dirt","We should depend on

		<dependency>
			<groupId>org.springframework.batch</groupId>
			<artifactId>spring-batch-admin-manager</artifactId>
			<version>${project.parent.version}</version>
		</dependency>
		<dependency>
			<groupId>org.springframework.batch</groupId>
			<artifactId>spring-batch-admin-resources</artifactId>
			<version>${project.parent.version}</version>
		</dependency>

we are using spring batch 2.2.0.RELEASE. 

What version of spring batch admin to use?

 ",2.0,0.0067331716332482,0.0059351783954529,-1.0,True,False,True,1377179721000.0,1377187838000.0,1377191438000.0,1378384345000.0,1378385245000.0,1377179721000.0,-1.0,-1.0,-1.0,1377186876000.0,1377190476000.0,1377187838000.0,1377191438000.0,"mark.pollack",-1.0,-1.0,0,1377179721000.0,-1.0,"Add dependency to spring batch admin in spring-xd-dirt","We should depend on

		<dependency>
			<groupId>org.springframework.batch</groupId>
			<artifactId>spring-batch-admin-manager</artifactId>
			<version>${project.parent.version}</version>
		</dependency>
		<dependency>
			<groupId>org.springframework.batch</groupId>
			<artifactId>spring-batch-admin-resources</artifactId>
			<version>${project.parent.version}</version>
		</dependency>

we are using spring batch 2.2.0.RELEASE. 

We need to depend on spring-batch-admin version 1.3.0.BUILD-SNAPSHOT

 ",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-743","Story","Sprint 14","2013-08-22T13:26:32.000+0000","mark.pollack","iperumal","Ensure that when batch jobs are created, they are created with the job bean definition id equal to the stream name","Unlike in spring-batch-admin, in SpringXD all the jobs the /modules/jobs directory is not visible to query when the server starts.  Jobs only become visible to XDs jobs list command once they have been created.  

Creating a Job in XD is an opportunity to specify additional values to any property placeholders in the job bean definition.  This isnt part of spring-batch-admin.

We will not worry about the creation of job definition in this story.  Assume that they have been created already and that the GET for /jobs works as it does now for Spring XD. 

We should however, make sure that there is always a replacement of the job name in the job bean definition to match the --name specified in the command line.  That is job create --name myjob --description thisfunkyjob

will use myjob to replace <job id=""${xd.stream.name}"" in the file thisfunkyjob.xml

*Implementation Suggestions*

This should hopefully just be a matter of changing job definition files to follow the naming pattern.

<job id=""${xd.stream.name}""  />

*How to verify it works*

1. Create a JUnit integration style test that has job create --name myjob --defintion testJob and then deploy the job.  The name myjob should appear in the job execution table
",1.0,0.0071276321645234,0.0070011477291359,0.0071018668165741,True,True,True,1377177992000.0,1377181035000.0,1377184635000.0,1377604022000.0,1377604922000.0,1377177992000.0,-1.0,1377181024000.0,1377184624000.0,1377180981000.0,1377184581000.0,1377181035000.0,1377184635000.0,"mark.pollack",1377184624000.0,1377177992000.0,43,1377177992000.0,-1.0,"Ensure that when batch jobs are created, they are created with the job bean definition id equal to the stream name","Unlike in spring-batch-admin, in SpringXD all the jobs the /modules/jobs directory is not visible to query when the server starts.  Jobs only become visible to XDs jobs list command once they have been created.  

Creating a Job in XD is an opportunity to specify additional values to any property placeholders in the job bean definition.  This isnt part of spring-batch-admin.

We will not worry about the creation of job definition in this story.  Assume that they have been created already and that the GET for /jobs works as it does now for Spring XD. 

We should however, make sure that there is always a replacement of the job name in the job bean definition to match the --name specified in the command line.  That is job create --name myjob --description thisfunkyjob

will use myjob to replace <job id=""${xd.stream.name}"" in the file thisfunkyjob.xml

*Implementation Suggestions*

This should hopefully just be a matter of changing job definition files to follow the naming pattern.

<job id=""${xd.stream.name}""  />

*How to verify it works*

1. Create a JUnit integration style test that has job create --name myjob --defintion testJob and then deploy the job.  The name myjob should appear in the job execution table
",1.0,False,1.0,0,0.0,0,0,False
"XD-741","Improvement","Sprint 26","2013-08-22T11:04:01.000+0000","dturanski","mark.fisher","Rename xd-global-beans.xml","The above config contains beans that must be in a common parent context for the AdminServer and Modules. Hence not really global since the (Node) doesn't need them itself. So the name is a bit misleading. Come up with something better.",2.0,0.996607369503158,0.0,0.9685987137219516,True,True,True,1377169441000.0,1398339593000.0,1398343193000.0,1398410760000.0,1398411660000.0,1377169441000.0,-1.0,1397744627000.0,1397748227000.0,1377169441000.0,1377173041000.0,1398339593000.0,1398343193000.0,"dturanski",1397748227000.0,1390856658000.0,44,1390856658000.0,-1.0,"Rename xd-global-beans.xml","The above config contains beans that must be in a common parent context for the AdminServer and Modules. Hence not really global since the (Node) doesn't need them itself. So the name is a bit misleading. Come up with something better.",2.0,False,2.0,0,0.0,0,0,False
"XD-739","Story","Sprint 14","2013-08-22T10:32:10.000+0000","dturanski","dturanski","Eliminate internal dependencies on System properties","Remove System.setProperty() or System.getProperty() for internal xd properties. Use spring Environment abstraction instead. Also, replace ""."" in property names with '_' (XDPropertyKeys). This is compatible with environment variable names. 

As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. ",4.0,0.1784141247600456,0.0382139264759792,0.1783924543702855,True,True,True,1377167530000.0,1377266327000.0,1377269927000.0,1377720381000.0,1377721281000.0,1377167530000.0,-1.0,1377266315000.0,1377269915000.0,1377188691000.0,1377192291000.0,1377266327000.0,1377269927000.0,"dturanski",1377269915000.0,1377167530000.0,43,1377167530000.0,-1.0,"Eliminate internal dependencies on System properties","Remove System.setProperty() or System.getProperty() for internal xd properties. Use spring Environment abstraction instead. Also, replace ""."" in property names with '_' (XDPropertyKeys). This is compatible with environment variable names. 

As a result, XD should accept System properties or environment variables or command line options. Command line options should have highest precedence. Retain StandardEnvironment order wrt to System properties and environment variable. ",4.0,False,4.0,0,0.0,0,0,False
"XD-737","Story","","2013-08-21T14:10:33.000+0000","grenfro","","Trigger can send a message to a named channel","Trigger can send a message to a named channel.  For example:
trigger create --name mytrigger --definition ""trigger --cron='*/10 * * * * *' --message='Good Luck, we are all counting on you'""

mytrigger >:foo
Where the --message contains the message that will be sent to foo job/component.",3.0,0.8969526071039874,0.8969474894491015,-1.0,True,False,True,1377094233000.0,1377620032000.0,1377623632000.0,1377679539000.0,1377680439000.0,1377094233000.0,-1.0,-1.0,-1.0,1377620029000.0,1377623629000.0,1377620032000.0,1377623632000.0,"grenfro",-1.0,-1.0,0,1377094233000.0,-1.0,"Trigger can send a message to a named channel","Trigger can send a message to a named channel.  For example:
trigger create --name mytrigger --definition ""trigger --cron='*/10 * * * * *' --message='Good Luck, we are all counting on you'""  --channel foo 

Where the --message contains the message that will be sent to foo job/component.",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-736","Story","Sprint 15","2013-08-21T13:37:41.000+0000","grenfro","","Expose restful services that allow users to view job statuses","This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD.  

*Steps
  - Create a Branch in the BatchAdmin (we don't want to lose history)
  - Update the restful API's to XD standards.
  - Create bamboo task to push jars to artifactory
  - Update gradle.build to pull in the Batchadmin jars.
  - Expose the restful calls.",5.0,0.8706375704188449,0.8706362093394545,0.4630711939796736,True,True,True,1377092261000.0,1378371595000.0,1378375195000.0,1378560783000.0,1378561683000.0,1377092261000.0,-1.0,1377772708000.0,1377776308000.0,1378371593000.0,1378375193000.0,1378371595000.0,1378375195000.0,"grenfro",1377776308000.0,1377092261000.0,43,1377092261000.0,-1.0,"Expose restful services that allow users to view job statuses","This story utilizes BatchAdmin and its restful interfaces to show the state of jobs in Spring XD.  

*Steps
  - Create a Branch in the BatchAdmin (we don't want to lose history)
  - Update the restful API's to XD standards.
  - Create bamboo task to push jars to artifactory
  - Update gradle.build to pull in the Batchadmin jars.
  - Expose the restful calls.",5.0,False,5.0,0,0.0,0,0,False
"XD-735","Story","","2013-08-21T13:18:43.000+0000","grenfro","","Job Repo in container needs access to Admin's HSQLDB","The HSQLDB that stores the JobRepository needs to have its content exposed via TCP (network service) so that container has access to update the status of a job run.  
-Needs to have property that enumerates the host and port for the admin that is accessible by the user.
-If one is not specified it should default to localhost:9500.

*Testing:
- Unit Tests
- Bring up module and admin.  
    ^Verify that default host and port work
    ^Verify that container on different machine has access to admin


",5.0,-1.0,-1.0,-1.0,False,False,False,1377091123000.0,-1.0,-1.0,1382343381000.0,1382344281000.0,1377091123000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1377091123000.0,-1.0,"Job Repo in container needs access to Admin's HSQLDB","The HSQLDB that stores the JobRepository needs to have its content exposed via TCP (network service) so that container has access to update the status of a job run.  
-Needs to have property that enumerates the host and port for the admin that is accessible by the user.
-If one is not specified it should default to localhost:9500.

*Testing:
- Unit Tests
- Bring up module and admin.  
    ^Verify that default host and port work
    ^Verify that container on different machine has access to admin


",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-733","Story","Sprint 14","2013-08-21T12:59:46.000+0000","hillert","hillert","Create JobLaunchRequest Transformer","The JobLaunchRequest Transformer shall accept the following payloads:

* File
* JSON String
* Properties
* Map
* Tuple

Use/Migrate some of the logic from *JobParametersBean*, e.g. using the *DefaultJobParametersConverter*.

*Special Case File*

When handling a *File*, add special JobParameter *absoluteFilePath* populating it with *message.getPayload().getAbsolutePath()*

* Add unit tests


",6.0,0.0530401034928848,0.0530606377954372,0.052994471709435,True,True,True,1377089986000.0,1377113233000.0,1377116833000.0,1377527377000.0,1377528277000.0,1377089986000.0,-1.0,1377113213000.0,1377116813000.0,1377113242000.0,1377116842000.0,1377113233000.0,1377116833000.0,"hillert",1377116813000.0,1377089986000.0,43,1377089986000.0,-1.0,"Create JobLaunchRequest Transformer","The JobLaunchRequest Transformer shall accept the following payloads:

* File
* JSON String
* Properties
* Map
* Tuple

Use/Migrate some of the logic from *JobParametersBean*, e.g. using the *DefaultJobParametersConverter*.

*Special Case File*

When handling a *File*, add special JobParameter *absoluteFilePath* populating it with *message.getPayload().getAbsolutePath()*

* Add unit tests


",6.0,False,6.0,0,0.0,0,0,False
"XD-732","Story","Sprint 14","2013-08-21T09:21:17.000+0000","mark.pollack","","Add PropertyAccessor for JSON fields in SpEL","{code}
filter --expression=""payload.myfield.startsWith('foo')""
{code}

Example using index:

{code}
filter --expression=""payload.2.startsWith('foo')""
{code}

This should support nested keys as well:

{code}
filter --expression=""payload.myfield.subfield.startsWith('foo')""
{code}

This is related to https://jira.springsource.org/browse/XD-676 and that in turn depends on SI being able to configure SpEL",4.0,0.1035093146970004,0.1035064434460466,1.7227505722762058e-05,True,True,True,1377076877000.0,1377221078000.0,1377224678000.0,1378469098000.0,1378469998000.0,1377076877000.0,-1.0,1377076901000.0,1377080501000.0,1377221074000.0,1377224674000.0,1377221078000.0,1377224678000.0,"mark.pollack",1377080501000.0,1377076877000.0,43,1377076877000.0,-1.0,"Add PropertyAccessor for JSON fields in SpEL","{code}
filter --expression=""payload.myfield.startsWith('foo')""
{code}

Example using index:

{code}
filter --expression=""payload.2.startsWith('foo')""
{code}

This should support nested keys as well:

{code}
filter --expression=""payload.myfield.subfield.startsWith('foo')""
{code}

This is related to https://jira.springsource.org/browse/XD-676 and that in turn depends on SI being able to configure SpEL",4.0,False,4.0,0,0.0,0,0,False
"XD-730","Story","Sprint 14","2013-08-20T13:57:48.000+0000","mark.pollack","aeisenberg","Add additional embedded servlet container config to load static UI resources","Configure embedded servlet container needs to know where to load the UI code.",2.0,0.8221632789814608,0.0029902836721018,0.0034230511503238,True,True,True,1377007068000.0,1377596000000.0,1377599600000.0,1377722488000.0,1377723388000.0,1377007068000.0,-1.0,1377009520000.0,1377013120000.0,1377009210000.0,1377012810000.0,1377596000000.0,1377599600000.0,"mark.pollack",1377013120000.0,1377007068000.0,43,1377007068000.0,-1.0,"Add additional embedded servlet container config to load static UI resources","Configure embedded servlet container needs to know where to load the UI code.",2.0,False,2.0,0,0.0,0,0,False
"XD-729","Story","Sprint 14","2013-08-20T13:56:47.000+0000","mark.pollack","iperumal","Package up the UI code when building the distribution so that it can be shown by xd-admin","The UI code will be sitting in one or more top level directories in the repository

This story will address the need to 

1) copy over the UI code into a location so that it can be picked up by the embedded servlet container when the distribution zip is 'unzipped'.  

After running ./xd-admin or ./xd-singlenode one should be able to hit the UI at http://localhost:8999/xd

(just an example)",2.0,0.0056395201940888,0.0,0.003507949071224,True,True,True,1377007007000.0,1377011047000.0,1377014647000.0,1377722480000.0,1377723380000.0,1377007007000.0,-1.0,1377009520000.0,1377013120000.0,1377007007000.0,1377010607000.0,1377011047000.0,1377014647000.0,"mark.pollack",1377013120000.0,1377007007000.0,43,1377007007000.0,-1.0,"Package up the UI code when building the distribution so that it can be shown by xd-admin","The UI code will be sitting in one or more top level directories in the repository

This story will address the need to 

1) copy over the UI code into a location so that it can be picked up by the embedded servlet container when the distribution zip is 'unzipped'.  

After running ./xd-admin or ./xd-singlenode one should be able to hit the UI at http://localhost:8999/xd

(just an example)",2.0,False,2.0,0,0.0,0,0,False
"XD-728","Story","Sprint 14","2013-08-20T13:55:08.000+0000","mark.pollack","iperumal","Display the UI from xd-admin container when doing development in eclipse","The UI code will be sitting in one or more top level directories in the repository

This story will address the need to 

1) copy over the UI code into a location so that it can be picked up by the embedded servlet container when running inside eclipse",2.0,0.8222066518624181,0.0,0.003644301643774,True,True,True,1377006908000.0,1377595987000.0,1377599587000.0,1377722469000.0,1377723369000.0,1377006908000.0,-1.0,1377009519000.0,1377013119000.0,1377006908000.0,1377010508000.0,1377595987000.0,1377599587000.0,"mark.pollack",1377013119000.0,1377006908000.0,43,1377006908000.0,-1.0,"Display the UI from xd-admin container when doing development in eclipse","The UI code will be sitting in one or more top level directories in the repository

This story will address the need to 

1) copy over the UI code into a location so that it can be picked up by the embedded servlet container when running inside eclipse",2.0,False,2.0,0,0.0,0,0,False
"XD-727","Story","Sprint 14","2013-08-20T13:48:51.000+0000","mark.pollack","","Create directory structures and move existing UI code into Spring XD repository","Create a directory structure that best benefits UI development.  

The copying of the UI files and other gradle build tasks so that the UI can be run inside the embedded servlet container of XD will be a seperate story",4.0,0.0979484561106879,0.0979345057238437,0.004168375589055,True,True,True,1377006531000.0,1377076743000.0,1377080343000.0,1377722457000.0,1377723357000.0,1377006531000.0,-1.0,1377009519000.0,1377013119000.0,1377076733000.0,1377080333000.0,1377076743000.0,1377080343000.0,"mark.pollack",1377013119000.0,1377006531000.0,43,1377006531000.0,-1.0,"Create directory structures and move existing UI code into Spring XD repository","Create a directory structure that best benefits UI development.  

The copying of the UI files and other gradle build tasks so that the UI can be run inside the embedded servlet container of XD will be a seperate story",4.0,False,4.0,0,0.0,0,0,False
"XD-725","Story","Sprint 14","2013-08-20T10:54:20.000+0000","iperumal","","Test processor module in isolation","Register the module under test  and have access to a source channel that drives messages into the processor and a output channel where output messages are sent.  
Examples
Built-in Message conversion: send JSON to a processor module that accepts Tuples.
",5.0,-1.0,-1.0,0.0005392676737777,False,True,False,1376996060000.0,-1.0,-1.0,1401953081000.0,1401953981000.0,1376996060000.0,-1.0,1377009519000.0,1377013119000.0,-1.0,-1.0,-1.0,-1.0,"iperumal",1377013119000.0,1376996060000.0,43,1376996060000.0,-1.0,"Test processor module in isolation","Register the module under test  and have access to a source channel that drives messages into the processor and a output channel where output messages are sent.  
Examples
Built-in Message conversion: send JSON to a processor module that accepts Tuples.
",5.0,False,5.0,0,0.0,0,0,False
"XD-724","Story","Sprint 14","2013-08-20T10:52:52.000+0000","iperumal","Ilayaperumal Gopinathan","Test source module in isolation","Register the module under test and deploy the module
Verify output across all transports
Examples
Be able to start the rabbitmq source just by pointing to modules/source/rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in-memory queue backed channel for use with assertions to verify functionality.  Test that sending json, results in media-type header is set to json
Test that sending POJO -> POJO
Test that sending Tuple ->  Tuple
Test that sending a (JSON) String -> String
Test that sending raw bytes ->  raw bytes
",5.0,-1.0,0.5657709368143379,0.0005427900852801,False,True,True,1376995972000.0,-1.0,-1.0,1401953155000.0,1401954055000.0,1376995972000.0,-1.0,1377009519000.0,1377013119000.0,1391116530000.0,1391120130000.0,-1.0,-1.0,"iperumal",1377013119000.0,1376995972000.0,43,1376995972000.0,-1.0,"Test source module in isolation","Register the module under test and deploy the module
Verify output across all transports
Examples
Be able to start the rabbitmq source just by pointing to modules/source/rabbit.xml, pass in some property file for parameters to be replaced, and outgoing message is placed in a in-memory queue backed channel for use with assertions to verify functionality.  Test that sending json, results in media-type header is set to json
Test that sending POJO -> POJO
Test that sending Tuple ->  Tuple
Test that sending a (JSON) String -> String
Test that sending raw bytes ->  raw bytes
",5.0,False,5.0,0,0.0,0,0,False
"XD-723","Story","","2013-08-20T09:39:14.000+0000","eric.bottard","eric.bottard","Change inconditionnal Thread.sleep() calls in tests to smarter incremental pauses","There are a lot of Thread.sleep() calls with delays chosen in the 1-2 seconds range.

Change to a while loop with smaller pauses until a timeout is reached and give up.

This applies to verification code (e.g. verifying that a counter has expected value) as well as File setup, or http being ready to accept requests etc",8.0,-1.0,0.0,-1.0,False,False,True,1376991554000.0,-1.0,-1.0,1377681495000.0,1377682395000.0,1376991554000.0,-1.0,-1.0,-1.0,1376991554000.0,1376995154000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1376991554000.0,-1.0,"Change inconditionnal Thread.sleep() calls in tests to smarter incremental pauses","There are a lot of Thread.sleep() calls with delays chosen in the 1-2 seconds range.

Change to a while loop with smaller pauses until a timeout is reached and give up.

This applies to verification code (e.g. verifying that a counter has expected value) as well as File setup, or http being ready to accept requests etc",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-722","Story","Sprint 14","2013-08-20T09:04:23.000+0000","mark.pollack","","Batch jobs send job and step events on channels ","This is the other side of launching a job by sending a message.  The Job plugin should add listeners at the job/step level so that the job/step context information can be sent out on a channel.

:myjob.notifications is a suggested channel name that would be created automatically.",8.0,0.7526371010340012,0.752648678374252,0.0290243920087408,True,True,True,1376989463000.0,1377509539000.0,1377513139000.0,1377679568000.0,1377680468000.0,1376989463000.0,-1.0,1377009519000.0,1377013119000.0,1377509547000.0,1377513147000.0,1377509539000.0,1377513139000.0,"mark.pollack",1377013119000.0,1376989463000.0,43,1376989463000.0,-1.0,"Batch jobs send job and step events on channels ","This is the other side of launching a job by sending a message.  The Job plugin should add listeners at the job/step level so that the job/step context information can be sent out on a channel.

:myjob.notifications is a suggested channel name that would be created automatically.",8.0,False,8.0,0,0.0,-1,0,False
"XD-721","Story","Sprint 14","2013-08-20T08:52:47.000+0000","mark.pollack","","Deploy a new job module *after* XD-singlenode container has started.","After the xd-singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module.

*Implementation Suggestions*
Develop in the test tree, we can put in the jar and config from https://github.com/SpringSource/spring-xd-samples/tree/master/batch-simple 

*How to verify it works.*
In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up.

Deploy a new job and check in the job repository that the job ran and was successful.

",5.0,0.8950964784477745,0.1575788845780932,0.0006107527327299,True,True,True,1376988767000.0,1407402126000.0,1407405726000.0,1410965611000.0,1410966511000.0,1376988767000.0,-1.0,1377009519000.0,1377013119000.0,1382342942000.0,1382346542000.0,1407402126000.0,1407405726000.0,"mark.pollack",1377013119000.0,1376988767000.0,43,1376988767000.0,-1.0,"Deploy a new job module *after* XD-singlenode container has started.","After the xd-singlenode process has started, create a new job that has dependencies not already in the parent application context, and then create and run a new job that uses the new module.

*Implementation Suggestions*
Develop in the test tree, we can put in the jar and config from https://github.com/SpringSource/spring-xd-samples/tree/master/batch-simple 

*How to verify it works.*
In a JUnit test case copy in a new job that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up.

Deploy a new job and check in the job repository that the job ran and was successful.

",5.0,False,5.0,0,0.0,0,0,False
"XD-720","Story","Sprint 14","2013-08-20T08:51:19.000+0000","mark.pollack","","Deploy a new source module *after* XD-singlenode container has started.","After the xd-singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module.

*Implementation Suggestions*
Develop in the test tree, maybe of the xd-shell project, a new module.  the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot.

Could try http://www.date4j.net/..The config file  would be similar to time.xml but use the date4j class.

*How to verify it works*
In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up.

Deploy a new stream, date4j | file and see if there are contents in the file.
",5.0,0.1072378454132601,0.1006378156429131,0.0292648128540494,True,True,True,1376988679000.0,1377065045000.0,1377068645000.0,1377699897000.0,1377700797000.0,1376988679000.0,-1.0,1377009519000.0,1377013119000.0,1377060345000.0,1377063945000.0,1377065045000.0,1377068645000.0,"mark.pollack",1377013119000.0,1376988679000.0,43,1376988679000.0,-1.0,"Deploy a new source module *after* XD-singlenode container has started.","After the xd-singlenode process has started, create a new module that has dependencies not already in the parent application context, and then create a stream that uses the new module.

*Implementation Suggestions*
Develop in the test tree, maybe of the xd-shell project, a new module.  the lib and config should be sititng around in a directory waiting to be copied into the appropriate spot.

Could try http://www.date4j.net/..The config file  would be similar to time.xml but use the date4j class.

*How to verify it works*
In a JUnit test case copy in a new module that has new dependencies, ..copy the lib and config directories into the location where the ModuleRegisty will pick it up.

Deploy a new stream, date4j | file and see if there are contents in the file.
",5.0,False,5.0,0,0.0,0,0,False
"XD-719","Story","Sprint 14","2013-08-20T08:50:31.000+0000","mark.pollack","","Refactor the file source module to have lib/config directories","Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext. works as expected.

*Implementation Suggestions*

Remove from build.gradle the dependency on spring-integration-file and place that jar inside a directory

./modules/source/file/lib 

place the current file.xml inside ./modules/source/file/config

*How to verify it works.*

1. Running tests that currently use the file source, e.g. in spring shell, should work as before
2. When deploying a stream file | log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not-null URL[] property.",5.0,0.1205270075187171,0.1205286965216432,0.0050399847314135,True,True,True,1376988631000.0,1377488150000.0,1377491750000.0,1381132188000.0,1381133088000.0,1376988631000.0,-1.0,1377009519000.0,1377013119000.0,1377488157000.0,1377491757000.0,1377488150000.0,1377491750000.0,"mark.pollack",1377013119000.0,1376988631000.0,43,1376988631000.0,-1.0,"Refactor the file source module to have lib/config directories","Convert a simple module, such as file, to further test that what was done in the previous story, Use ParentLastClassLoader to create the Modules ApplicationContext. works as expected.

*Implementation Suggestions*

Remove from build.gradle the dependency on spring-integration-file and place that jar inside a directory

./modules/source/file/lib 

place the current file.xml inside ./modules/source/file/config

*How to verify it works.*

1. Running tests that currently use the file source, e.g. in spring shell, should work as before
2. When deploying a stream file | log, we should be able to interrogate the channel registry and make sure it found the dependencies for the module, ModuleDescriptor should have a not-null URL[] property.",5.0,False,5.0,0,0.0,0,0,False
"XD-718","Story","Sprint 14","2013-08-20T08:49:10.000+0000","mark.pollack","","Use ParentLastClassLoader to create the Modules ApplicationContext.","The ParentLastClassloader is located in the spring-hadoop project.  It will resolve classes first looking at the child context and then the parent.  This works well for XD since the we want and dependencies of the module to be considered first and if not found, resolve against the parent. It would be possible to even include other versions of .jars already in the parent classloaders (e.g. Spring Integration jars), but for now, we will not immediately test that case.

SimpleModule needs to change to that we can pass in the classloader to use for creating the application context. The current implementation creates a GenericApplicationContext as a field initializer...that should change to be in the ctor.

ModuleDeployer should implement .BeanClassLoaderAware.  The classloader passed in to the BeanClassLoaderAware callback will be used as the parent when creating the ParentLastClassloader.  

The URL[] to pass into ParentLastClassloader should be null or an empty array in this case of an older style module.  (Hopefully ParentLastClassloader  allows that type of fallback).

*Implementation Suggestions:*
The ModuleDeployer code is where the application context for the module is defined and instantiated.   Here is a possible impl path.

1. Assuming we can always use ParentLastClassloader (even for older style modules), then the ModuleDescriptor needs to return an array of URL[] locations for the module, getURL().  This is passed into the cto for SimpleModule.  The ctor then creates a new application context, creates the parentclassloader, sets the classloader on the application context and then proceeds as normal. 
2.AbstractModuleRegistry should try and load the resource from two possible locations, e.g. 

./modules/source/file/config/file.xml

or

./modules/source/file.xml

The module registry needs to be a bit smarter to know, ah, i see a config directory, let me try ./config/file.xml otherwise just ./file.xml


*How to verify it works.*

1. JUnit test in which one of the ModuleRegistry implementations points to a test directory that contains both old and new style modules.  FileModuleRegistry is probably a good choice here.  Need to test that the new getting for URL[] works as expected.

2. Existing tests should run as they did before, in particular the shell integration tests.",5.0,0.1019317899418876,0.1008577152958057,0.0294408774558049,True,True,True,1376988550000.0,1377061150000.0,1377064750000.0,1377699891000.0,1377700791000.0,1376988550000.0,-1.0,1377009519000.0,1377013119000.0,1377060385000.0,1377063985000.0,1377061150000.0,1377064750000.0,"mark.pollack",1377013119000.0,1376988550000.0,43,1376988550000.0,-1.0,"Use ParentLastClassLoader to create the Modules ApplicationContext.","The ParentLastClassloader is located in the spring-hadoop project.  It will resolve classes first looking at the child context and then the parent.  This works well for XD since the we want and dependencies of the module to be considered first and if not found, resolve against the parent. It would be possible to even include other versions of .jars already in the parent classloaders (e.g. Spring Integration jars), but for now, we will not immediately test that case.

SimpleModule needs to change to that we can pass in the classloader to use for creating the application context. The current implementation creates a GenericApplicationContext as a field initializer...that should change to be in the ctor.

ModuleDeployer should implement .BeanClassLoaderAware.  The classloader passed in to the BeanClassLoaderAware callback will be used as the parent when creating the ParentLastClassloader.  

The URL[] to pass into ParentLastClassloader should be null or an empty array in this case of an older style module.  (Hopefully ParentLastClassloader  allows that type of fallback).

*Implementation Suggestions:*
The ModuleDeployer code is where the application context for the module is defined and instantiated.   Here is a possible impl path.

1. Assuming we can always use ParentLastClassloader (even for older style modules), then the ModuleDescriptor needs to return an array of URL[] locations for the module, getURL().  This is passed into the cto for SimpleModule.  The ctor then creates a new application context, creates the parentclassloader, sets the classloader on the application context and then proceeds as normal. 
2.AbstractModuleRegistry should try and load the resource from two possible locations, e.g. 

./modules/source/file/config/file.xml

or

./modules/source/file.xml

The module registry needs to be a bit smarter to know, ah, i see a config directory, let me try ./config/file.xml otherwise just ./file.xml


*How to verify it works.*

1. JUnit test in which one of the ModuleRegistry implementations points to a test directory that contains both old and new style modules.  FileModuleRegistry is probably a good choice here.  Need to test that the new getting for URL[] works as expected.

2. Existing tests should run as they did before, in particular the shell integration tests.",5.0,False,5.0,0,0.0,0,0,False
"XD-717","Story","Sprint 14","2013-08-19T21:03:52.000+0000","mark.pollack","iperumal","cat command doesn't work when same data is listed in file multiple times","$ ./xd-admin --transport redis
$ ./xd-container --transport redis
$ ./xd-container --transport redis

xd:>stream create --name httpStream --definition ""http | file""
Created new stream 'httpStream'
xd:>tap create --name httpTap --definition ""tap httpStream | counter""
Created and deployed new tap 'httpTap'
xd:>http post --target http://localhost:9000 --data ""helloworld""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
xd:>http post --target http://localhost:9000 --data ""helloworld""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
xd:>http post --target http://localhost:9000 --data ""helloworld2""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld2
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
xd:>http post --target http://localhost:9000 --data ""helloworld4""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld4
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
helloworld4
xd:>counter display --name httpTap
9
xd:>

however in the regular shell.

$ cat /tmp/xd/output/httpStream.out 
helloworld
helloworld
helloworld2
helloworld3
helloworld3
helloworld3
helloworld3
helloworld3
helloworld4


",2.0,0.7497987657633486,0.7493366723310377,0.9433711951823032,True,True,True,1376946232000.0,1376996533000.0,1377000133000.0,1377012418000.0,1377013318000.0,1376946232000.0,-1.0,1377009519000.0,1377013119000.0,1376996502000.0,1377000102000.0,1376996533000.0,1377000133000.0,"mark.pollack",1377013119000.0,1376946232000.0,43,1376946232000.0,-1.0,"cat command doesn't work when same data is listed in file multiple times","$ ./xd-admin --transport redis
$ ./xd-container --transport redis
$ ./xd-container --transport redis

xd:>stream create --name httpStream --definition ""http | file""
Created new stream 'httpStream'
xd:>tap create --name httpTap --definition ""tap httpStream | counter""
Created and deployed new tap 'httpTap'
xd:>http post --target http://localhost:9000 --data ""helloworld""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
xd:>http post --target http://localhost:9000 --data ""helloworld""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
xd:>http post --target http://localhost:9000 --data ""helloworld2""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld2
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>http post --target http://localhost:9000 --data ""helloworld3""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld3
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
xd:>http post --target http://localhost:9000 --data ""helloworld4""
> POST (text/plain;Charset=UTF-8) http://localhost:9000 helloworld4
> 200 OK

xd:>! cat /tmp/xd/output/httpStream.out
command is:cat /tmp/xd/output/httpStream.out
helloworld
helloworld2
helloworld3
helloworld4
xd:>counter display --name httpTap
9
xd:>

however in the regular shell.

$ cat /tmp/xd/output/httpStream.out 
helloworld
helloworld
helloworld2
helloworld3
helloworld3
helloworld3
helloworld3
helloworld3
helloworld4


",2.0,False,2.0,0,0.0,0,0,False
"XD-716","Bug","Sprint 14","2013-08-19T18:54:18.000+0000","jencompgeek","jencompgeek","TapCommandTests hangs when using a lazily instantiated Lettuce connection","A change was made in spring-data-redis to instantiate the shared Lettuce connection lazily instead of when the context is initialized. This caused TapCommandTests to hang due to a Netty worker thread trying to initialize the Lettuce connection (Lettuce uses Netty). The change was temporarily backed out of SDR, but we need to consider using a NettyExecutionHandler in NettyHttpInboundChannelAdapter or making the HTTP module's ""input"" channel an ExecutorChannel to avoid potentially long operations like from happening in an I/O thread.

Also, we need to address why this failure simply hangs the shell. Shell was hung waiting on IO here:
	at org.springframework.http.client.SimpleClientHttpResponse.getRawStatusCode(SimpleClientHttpResponse.java:47)
	at org.springframework.http.client.AbstractClientHttpResponse.getStatusCode(AbstractClientHttpResponse.java:32)
	at org.springframework.xd.shell.command.HttpCommands$1.hasError(HttpCommands.java:93)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:484)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:460)
	at org.springframework.web.client.RestTemplate.postForEntity(RestTemplate.java:335)
	at org.springframework.xd.shell.command.HttpCommands.postHttp(HttpCommands.java:103)
	at sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:191)
	at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)
	atringframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:57)
	- locked <7fd3c7d40> (a java.lang.Class for org.springframework.shell.core.SimpleExecutionStrategy)
	at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)
	at org.springframework.xd.shell.AbstractShellIntegrationTest.executeCommand(AbstractShellIntegrationTest.java:99)
	at org.springframework.xd.shell.AbstractShellIntegrationTest.httpPostData(AbstractShellIntegrationTest.java:112)
	at org.springframework.xd.shell.command.TapCommandTests.testCreateAndDeployTap(TapCommandTests.java:56)

Full stack trace of server exception:

 Aug 19, 2013 9:59:00 AM org.jboss.netty.channel.SimpleChannelUpstreamHandler
    WARNING: EXCEPTION, please implement org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.exceptionCaught() for proper handling.
    org.springframework.integration.MessageHandlingException: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis on localhost:6379; nested exception is com.lambdaworks.redis.RedisException: Unable to connect
    	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:76)
    	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.BroadcastingDispatcher.invokeHandler(BroadcastingDispatcher.java:121)
    	at org.springframework.integration.dispatcher.BroadcastingDispatcher.dispatch(BroadcastingDispatcher.java:112)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.interceptor.WireTap.preSend(WireTap.java:121)
    	at org.springframework.integration.channel.AbstractMessageChannel$ChannelInterceptorList.preSend(AbstractMessageChannel.java:248)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:173)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)
    	at org.springframework.integration.x.http.NettyHttpInboundChannelAdapter.access$200(NettyHttpInboundChannelAdapter.java:59)
    	at org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.messageReceived(NettyHttpInboundChannelAdapter.java:122)
    	at org.jboss.netty.handler.codec.http.HttpContentEncoder.messageReceived(HttpContentEncoder.java:81)
    	at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:148)
    	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:485)
    	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    	at java.lang.Thread.run(Thread.java:680)
    Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis on localhost:6379; nested exception is com.lambdaworks.redis.RedisException: Unable to connect
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:345)
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.initConnection(LettuceConnectionFactory.java:116)
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:325)
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:106)
    	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:81)
    	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:53)
    	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:173)
    	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:153)
    	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:86)
    	at org.springframework.data.redis.core.DefaultZSetOperations.add(DefaultZSetOperations.java:41)
    	at org.springframework.data.redis.core.DefaultBoundZSetOperations.add(DefaultBoundZSetOperations.java:47)
    	at org.springframework.xd.store.AbstractRedisRepository.trackMembership(AbstractRedisRepository.java:202)
    	at org.springframework.xd.analytics.metrics.redis.RedisCounterRepository.increment(RedisCounterRepository.java:88)
    	at org.springframework.xd.analytics.metrics.redis.RedisCounterRepository.increment(RedisCounterRepository.java:82)
    	at org.springframework.xd.analytics.metrics.integration.MessageCounterHandler.process(MessageCounterHandler.java:28)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    	at java.lang.reflect.Method.invoke(Method.java:597)
    	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)
    	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:97)
    	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:82)
    	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)
    	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:103)
    	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:126)
    	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:230)
    	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:129)
    	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)
    	... 84 more
    Caused by: com.lambdaworks.redis.RedisException: Unable to connect
    	at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)
    	at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:339)
    	... 111 more
    Caused by: java.lang.IllegalStateException: await*() in I/O thread causes a dead lock or sudden performance drop. Use addListener() instead or call await*() from a different thread.
    	at org.jboss.netty.channel.DefaultChannelFuture.checkDeadLock(DefaultChannelFuture.java:342)
    	at org.jboss.netty.channel.DefaultChannelFuture.await(DefaultChannelFuture.java:231)
    	at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:166)
    	... 113 more",5.0,0.5310818740323368,0.3823074040820366,0.5023789493032824,True,True,True,1376938458000.0,1377013579000.0,1377017179000.0,1377079007000.0,1377079907000.0,1376938458000.0,-1.0,1377009519000.0,1377013119000.0,1376992535000.0,1376996135000.0,1377013579000.0,1377017179000.0,"jencompgeek",1377013119000.0,1376938458000.0,43,1376938458000.0,-1.0,"TapCommandTests hangs when using a lazily instantiated Lettuce connection","A change was made in spring-data-redis to instantiate the shared Lettuce connection lazily instead of when the context is initialized. This caused TapCommandTests to hang due to a Netty worker thread trying to initialize the Lettuce connection (Lettuce uses Netty). The change was temporarily backed out of SDR, but we need to consider using a NettyExecutionHandler in NettyHttpInboundChannelAdapter or making the HTTP module's ""input"" channel an ExecutorChannel to avoid potentially long operations like from happening in an I/O thread.

Also, we need to address why this failure simply hangs the shell. Shell was hung waiting on IO here:
	at org.springframework.http.client.SimpleClientHttpResponse.getRawStatusCode(SimpleClientHttpResponse.java:47)
	at org.springframework.http.client.AbstractClientHttpResponse.getStatusCode(AbstractClientHttpResponse.java:32)
	at org.springframework.xd.shell.command.HttpCommands$1.hasError(HttpCommands.java:93)
	at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:484)
	at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:460)
	at org.springframework.web.client.RestTemplate.postForEntity(RestTemplate.java:335)
	at org.springframework.xd.shell.command.HttpCommands.postHttp(HttpCommands.java:103)
	at sun.reflect.GeneratedMethodAccessor135.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.springframework.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:191)
	at org.springframework.shell.core.SimpleExecutionStrategy.invoke(SimpleExecutionStrategy.java:64)
	atringframework.shell.core.SimpleExecutionStrategy.execute(SimpleExecutionStrategy.java:57)
	- locked <7fd3c7d40> (a java.lang.Class for org.springframework.shell.core.SimpleExecutionStrategy)
	at org.springframework.shell.core.AbstractShell.executeCommand(AbstractShell.java:127)
	at org.springframework.xd.shell.AbstractShellIntegrationTest.executeCommand(AbstractShellIntegrationTest.java:99)
	at org.springframework.xd.shell.AbstractShellIntegrationTest.httpPostData(AbstractShellIntegrationTest.java:112)
	at org.springframework.xd.shell.command.TapCommandTests.testCreateAndDeployTap(TapCommandTests.java:56)

Full stack trace of server exception:

 Aug 19, 2013 9:59:00 AM org.jboss.netty.channel.SimpleChannelUpstreamHandler
    WARNING: EXCEPTION, please implement org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.exceptionCaught() for proper handling.
    org.springframework.integration.MessageHandlingException: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis on localhost:6379; nested exception is com.lambdaworks.redis.RedisException: Unable to connect
    	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:76)
    	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.BroadcastingDispatcher.invokeHandler(BroadcastingDispatcher.java:121)
    	at org.springframework.integration.dispatcher.BroadcastingDispatcher.dispatch(BroadcastingDispatcher.java:112)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.interceptor.WireTap.preSend(WireTap.java:121)
    	at org.springframework.integration.channel.AbstractMessageChannel$ChannelInterceptorList.preSend(AbstractMessageChannel.java:248)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:173)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendMessage(AbstractReplyProducingMessageHandler.java:223)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.sendReplyMessage(AbstractReplyProducingMessageHandler.java:207)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.produceReply(AbstractReplyProducingMessageHandler.java:172)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleResult(AbstractReplyProducingMessageHandler.java:166)
    	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:144)
    	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
    	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
    	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
    	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
    	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
    	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
    	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)
    	at org.springframework.integration.x.http.NettyHttpInboundChannelAdapter.access$200(NettyHttpInboundChannelAdapter.java:59)
    	at org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.messageReceived(NettyHttpInboundChannelAdapter.java:122)
    	at org.jboss.netty.handler.codec.http.HttpContentEncoder.messageReceived(HttpContentEncoder.java:81)
    	at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:148)
    	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:296)
    	at org.jboss.netty.handler.codec.frame.FrameDecoder.unfoldAndFireMessageReceived(FrameDecoder.java:459)
    	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:536)
    	at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:485)
    	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
    	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
    	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:88)
    	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.process(AbstractNioWorker.java:107)
    	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
    	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:88)
    	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    	at java.lang.Thread.run(Thread.java:680)
    Caused by: org.springframework.data.redis.RedisConnectionFailureException: Unable to connect to Redis on localhost:6379; nested exception is com.lambdaworks.redis.RedisException: Unable to connect
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:345)
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.initConnection(LettuceConnectionFactory.java:116)
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getSharedConnection(LettuceConnectionFactory.java:325)
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.getConnection(LettuceConnectionFactory.java:106)
    	at org.springframework.data.redis.core.RedisConnectionUtils.doGetConnection(RedisConnectionUtils.java:81)
    	at org.springframework.data.redis.core.RedisConnectionUtils.getConnection(RedisConnectionUtils.java:53)
    	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:173)
    	at org.springframework.data.redis.core.RedisTemplate.execute(RedisTemplate.java:153)
    	at org.springframework.data.redis.core.AbstractOperations.execute(AbstractOperations.java:86)
    	at org.springframework.data.redis.core.DefaultZSetOperations.add(DefaultZSetOperations.java:41)
    	at org.springframework.data.redis.core.DefaultBoundZSetOperations.add(DefaultBoundZSetOperations.java:47)
    	at org.springframework.xd.store.AbstractRedisRepository.trackMembership(AbstractRedisRepository.java:202)
    	at org.springframework.xd.analytics.metrics.redis.RedisCounterRepository.increment(RedisCounterRepository.java:88)
    	at org.springframework.xd.analytics.metrics.redis.RedisCounterRepository.increment(RedisCounterRepository.java:82)
    	at org.springframework.xd.analytics.metrics.integration.MessageCounterHandler.process(MessageCounterHandler.java:28)
    	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    	at java.lang.reflect.Method.invoke(Method.java:597)
    	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)
    	at org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:97)
    	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:82)
    	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)
    	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:103)
    	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:126)
    	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:230)
    	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:129)
    	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)
    	... 84 more
    Caused by: com.lambdaworks.redis.RedisException: Unable to connect
    	at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)
    	at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)
    	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.createLettuceConnector(LettuceConnectionFactory.java:339)
    	... 111 more
    Caused by: java.lang.IllegalStateException: await*() in I/O thread causes a dead lock or sudden performance drop. Use addListener() instead or call await*() from a different thread.
    	at org.jboss.netty.channel.DefaultChannelFuture.checkDeadLock(DefaultChannelFuture.java:342)
    	at org.jboss.netty.channel.DefaultChannelFuture.await(DefaultChannelFuture.java:231)
    	at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:166)
    	... 113 more",5.0,False,5.0,0,0.0,0,0,False
"XD-715","Improvement","Sprint 14","2013-08-19T18:30:50.000+0000","jencompgeek","","Upgrade Lettuce and Netty","Upgrade Lettuce to 2.3.3 and subsequently Netty to 3.6.6",2.0,0.9867019247883242,0.9866510717282412,0.9213176027867476,True,True,True,1376937050000.0,1377014662000.0,1377015708000.0,1377014808000.0,1377015708000.0,1376937050000.0,-1.0,1377009519000.0,1377013119000.0,1377014658000.0,1377015708000.0,1377014662000.0,1377015708000.0,"jencompgeek",1377013119000.0,1376937050000.0,43,1376937050000.0,-1.0,"Upgrade Lettuce and Netty","Upgrade Lettuce to 2.3.3 and subsequently Netty to 3.6.6",2.0,False,2.0,0,0.0,-1,0,False
"XD-713","Story","Sprint 38","2013-08-19T11:40:02.000+0000","dturanski","dturanski","Support for @Configuration based module definitions","",3.0,0.9716138119173876,0.9716127362396422,0.9716173975098726,True,True,True,1376912402000.0,1414849210000.0,1414852810000.0,1415956653000.0,1415957553000.0,1376912402000.0,1415004109000.0,1414849350000.0,1414852950000.0,1414849168000.0,1414852768000.0,1414849210000.0,1414852810000.0,"dturanski",1414852950000.0,1376912402000.0,43,1415004109000.0,-1.0,"Support for @Configuration based module definitions","",0.0,False,0.0,1,-1.0,-1,2,False
"XD-708","Story","Sprint 14","2013-08-19T11:33:06.000+0000","dturanski","dturanski","Add SingleNodeMain class ","SingleNodeMain(){
  	parent = new AC(..)
   		AdminMain.launch(parent);
   		ContainerMain.launch(parent);
}

This should make startup processing more consistent and symmetrical
",3.0,0.5654280573200078,0.2897254680917178,0.3524038704735476,True,True,True,1376911986000.0,1377068475000.0,1377072075000.0,1377187848000.0,1377188748000.0,1376911986000.0,1377068525000.0,1377009518000.0,1377013118000.0,1376992171000.0,1376995771000.0,1377068475000.0,1377072075000.0,"dturanski",1377013118000.0,1376911986000.0,43,1377068525000.0,-1.0,"Add SingleNodeMain class ","SingleNodeMain(){
  	parent = new AC(..)
   		AdminMain.launch(parent);
   		ContainerMain.launch(parent);
}

This should make startup processing more consistent and symmetrical
",0.0,False,0.0,1,-1.0,-1,0,False
"XD-707","Improvement","Sprint 19","2013-08-19T11:31:01.000+0000","dturanski","dturanski","Support use of separate control and message transport in XD","Control transport - Deploy/Undeploy requests
Message transport - Inter module communication

Currently complicated because starting a Job for example currently uses message transport vs control transport. Testing scenarios require local control and ability to switch to various message transports.

One option is to change the interpretation of transport command line arg depending on SingleNode, Admin, or Container. e.g.
SingleNode --transport rabbit (always use local for control messages)
Admin (requires --transport, message transport does not apply)
Container (enforces the same transport for message and control. Local optimization done via composite module)

The other option is use a separate transport for control vs messages. 
Either way need to rationalize the design with respect to control and module messages
",2.0,0.8757661915090588,0.8451147857317669,0.8491579076571156,True,True,True,1376911861000.0,1385095685000.0,1385099285000.0,1386255720000.0,1386256620000.0,1376911861000.0,1386256608000.0,1384847037000.0,1384850637000.0,1384809255000.0,1384812855000.0,1385095685000.0,1385099285000.0,"dturanski",1384850637000.0,1376911861000.0,43,1386256608000.0,-1.0,"Support use of separate control and message transports","Control transport - Deploy/Undeploy requests
Message transport - Inter module communication

Currently complicated because starting a Job for example currently uses message transport vs control transport. Testing scenarios require local control and ability to switch to various message transports.

One option is to change the interpretation of transport command line arg depending on SingleNode, Admin, or Container. e.g.
SingleNode --transport rabbit (always use local for control messages)
Admin (requires --transport, message transport does not apply)
Container (enforces the same transport for message and control. Local optimization done via composite module)

The other option is use a separate transport for control vs messages. 
Either way need to rationalize the design with respect to control and module messages
",0.0,False,0.0,1,-1.0,0,0,True
"XD-705","Story","Sprint 14","2013-08-19T09:27:10.000+0000","mark.pollack","","A batch job can be launched by sending a message on a channel","*Description*  When a job is created in SpringXD, a control-channel for that job is also created.  The listener for that channel will receive a message, be able to take the jobParameters and other launch information from the message, and be able to launch/run the job.

NOTE: I can see a few other stories that should probably be made to break this up after writing it. I put estimate of 10 for now, but we should break this up.  Here are some suggestions

1. create data only JobParametersBean equivalent with primitive types
2. create jobLauncher source
3. create jobParameterTransformer processor
4. Refactoring of ChannelRegistrys aliasHing to use a callback strategy.


*Implementation Suggestions*

job create --name helloWorldJob --definition ""myjob --somePropertyToOverride=someValue

* This would not execute the batch job immediately, but instead register the job definition and deploys a jobLauncher and the job definition to an XD-Container.

* The XD-Container that receives the deploy request message will create a module application context, will also create a channel with the Channel registry named after the job, e.g. :myJob.  This should be a pub/sub channel from the point of view of the middleware.  From the point of view of the spring integration channel, it should ideally be of the executor channel.  There is a limitation in the current implementation of ChannelRegistry now as createInbound only creates direct channels.  The boolean aliasHint should probably be extended to some type of callback that creates a channel.  The aliasHint was added to address the case of LocalChannelRegistry creating or looking up a queue backed channel or a direct channel.
There will be a consumer on the SI channel in the module application context that will be responsible for getting the job launch information and launching the job.  The launching of the job may need to be explicitly done in a separate thread if direct channels are created by the ChannelRegistry.  The contents of the message should be something similar to the current JobParametersBean, it needs to be easily serializable with simple types via JSON over the wire.  The current impl of JobParametersBean has ObjectMapper, so that may require a bit of reworking.  The handler of the message will use the jobLauncher to launch the job, using the information in the JobParametersBean.

* The myjob can then be launched by sending a message, perhaps this is handled by having a jobLauncher source

jobLauncher [--jobParameters <jobParameters>] [--dateFormat <dateFormat>] [--numberFormat <numberFormat>] [--makeUnique <makeUnique>]  > :myJob

e.g. with no-args

jobLauncher  > :myJob

*How to verify it works*

With the test HelloSpringXDTasklet, we should be able to create the job

job create --name helloSpringXD --definition ""myjob""

This will not launch the job (as mentioned in the implementation section.

It would then be launched by 

jobLauncher  > :myJob

where jobLauncher is a new source.

Ideally would like to be able to test a data driven triggering.  This would require a new file source that doesnt use the file-to-string-transformer, but lets a File object be the payload.

file | jobParameterTransformer > :myJob
",16.0,0.8603280017896091,0.860322409261227,0.1469269056540462,True,True,True,1376904430000.0,1377519771000.0,1377523371000.0,1377618770000.0,1377619670000.0,1376904430000.0,-1.0,1377009518000.0,1377013118000.0,1377519767000.0,1377523367000.0,1377519771000.0,1377523371000.0,"mark.pollack",1377013118000.0,1376989142000.0,43,1376989142000.0,-1.0,"A batch job can be launched by sending a message on a channel","*Description*  When a job is created in SpringXD, a control-channel for that job is also created.  The listener for that channel will receive a message, be able to take the jobParameters and other launch information from the message, and be able to launch/run the job.

NOTE: I can see a few other stories that should probably be made to break this up after writing it. I put estimate of 10 for now, but we should break this up.  Here are some suggestions

1. create data only JobParametersBean equivalent with primitive types
2. create jobLauncher source
3. create jobParameterTransformer processor
4. Refactoring of ChannelRegistrys aliasHing to use a callback strategy.


*Implementation Suggestions*

job create --name helloWorldJob --definition ""myjob --somePropertyToOverride=someValue

* This would not execute the batch job immediately, but instead register the job definition and deploys a jobLauncher and the job definition to an XD-Container.

* The XD-Container that receives the deploy request message will create a module application context, will also create a channel with the Channel registry named after the job, e.g. :myJob.  This should be a pub/sub channel from the point of view of the middleware.  From the point of view of the spring integration channel, it should ideally be of the executor channel.  There is a limitation in the current implementation of ChannelRegistry now as createInbound only creates direct channels.  The boolean aliasHint should probably be extended to some type of callback that creates a channel.  The aliasHint was added to address the case of LocalChannelRegistry creating or looking up a queue backed channel or a direct channel.
There will be a consumer on the SI channel in the module application context that will be responsible for getting the job launch information and launching the job.  The launching of the job may need to be explicitly done in a separate thread if direct channels are created by the ChannelRegistry.  The contents of the message should be something similar to the current JobParametersBean, it needs to be easily serializable with simple types via JSON over the wire.  The current impl of JobParametersBean has ObjectMapper, so that may require a bit of reworking.  The handler of the message will use the jobLauncher to launch the job, using the information in the JobParametersBean.

* The myjob can then be launched by sending a message, perhaps this is handled by having a jobLauncher source

jobLauncher [--jobParameters <jobParameters>] [--dateFormat <dateFormat>] [--numberFormat <numberFormat>] [--makeUnique <makeUnique>]  > :myJob

e.g. with no-args

jobLauncher  > :myJob

*How to verify it works*

With the test HelloSpringXDTasklet, we should be able to create the job

job create --name helloSpringXD --definition ""myjob""

This will not launch the job (as mentioned in the implementation section.

It would then be launched by 

jobLauncher  > :myJob

where jobLauncher is a new source.

Ideally would like to be able to test a data driven triggering.  This would require a new file source that doesnt use the file-to-string-transformer, but lets a File object be the payload.

file | jobParameterTransformer > :myJob
",16.0,False,16.0,0,0.0,-1,1,False
"XD-704","Story","Sprint 13","2013-08-19T09:20:11.000+0000","jencompgeek","jencompgeek","User should be able to specify Rabbit virtual host","Need to support Rabbit virtual host property in properties file and as args to Rabbit source and sink",2.0,0.681675848071905,0.681588866338069,0.666801971585967,True,True,True,1376904011000.0,1376927522000.0,1376931122000.0,1376937601000.0,1376938501000.0,1376904011000.0,1376927027000.0,1376927009000.0,1376930609000.0,1376927519000.0,1376931119000.0,1376927522000.0,1376931122000.0,"jencompgeek",1376930609000.0,1376927027000.0,43,1376927027000.0,-1.0,"User should be able to specify Rabbit virtual host","Need to support Rabbit virtual host property in properties file and as args to Rabbit source and sink",2.0,False,2.0,0,0.0,0,1,False
"XD-703","Story","Sprint 14","2013-08-16T14:58:54.000+0000","mark.pollack","hillert","JobRepository should be persistent and shared across xd-admin/xd-container","The current code is creating an in-memory job repository for each batch job that is launched.  This makes it impossible to query the tables in the job repository across the cluster.  A single job repository that is backed by a file need to be shared across all jobs that are a launched.

*Implementation Suggestions*
* The XDAdmin server should create the job repository schema, if not found, in a HSQLDB database, when it starts up
* The bean definitions should be added to the same context that the analytics are being loaded in as it is already shared across xd-admin/xd-container.  The analytics context should be renamed to something more generic, shared parent context or something.
* There is some clean up (removal) of the code in the current JobPlugin, META-INF/spring-xd/plugins/job/common.xml wouldnt be needed anymore.  That might be all, not sure.

*How to verify it works*
* A JUnit test that verifies the spring batch tables were created in the job repository when xd-admin is launched.  This would require deleting the backing db file before instantiating singlenode/xd-admin/xd-container.

* If you start the xd-container it should be able to find the necessary DataSource/JobRepository beans information to be able to contact the database.  We dont have DI style JUnit tests so this will required getting a reference to the xd-container and its application context, and performing getBean(JobRepository.class)



",8.0,0.4113324232424317,0.4113443666925839,0.4113133137221881,True,True,True,1376665134000.0,1377009534000.0,1377013134000.0,1377501513000.0,1377502413000.0,1376665134000.0,-1.0,1377009518000.0,1377013118000.0,1377009544000.0,1377013144000.0,1377009534000.0,1377013134000.0,"mark.pollack",1377013118000.0,1376989153000.0,43,1376989153000.0,-1.0,"JobRepository should be persistent and shared across xd-admin/xd-container","The current code is creating an in-memory job repository for each batch job that is launched.  This makes it impossible to query the tables in the job repository across the cluster.  A single job repository that is backed by a file need to be shared across all jobs that are a launched.

*Implementation Suggestions*
* The XDAdmin server should create the job repository schema, if not found, in a HSQLDB database, when it starts up
* The bean definitions should be added to the same context that the analytics are being loaded in as it is already shared across xd-admin/xd-container.  The analytics context should be renamed to something more generic, shared parent context or something.
* There is some clean up (removal) of the code in the current JobPlugin, META-INF/spring-xd/plugins/job/common.xml wouldnt be needed anymore.  That might be all, not sure.

*How to verify it works*
* A JUnit test that verifies the spring batch tables were created in the job repository when xd-admin is launched.  This would require deleting the backing db file before instantiating singlenode/xd-admin/xd-container.

* If you start the xd-container it should be able to find the necessary DataSource/JobRepository beans information to be able to contact the database.  We dont have DI style JUnit tests so this will required getting a reference to the xd-container and its application context, and performing getBean(JobRepository.class)



",8.0,False,8.0,0,0.0,0,0,False
"XD-702","Story","Sprint 13","2013-08-16T05:56:27.000+0000","mark.fisher","iperumal","stack overflow when trying to create a stream with the same name as a module","(NOTE: even if we do want to prevent the use of module names for stream names, we obviously need to avoid a StackOverflowError)

to reproduce:

start the xd-singlenode container

start the xd-shell, and type the following:

{code}
xd:>stream create time --definition ""time | log""
{code}

that should produce an Internal Server Error output message

check the xd-singlenode console, and find:

{code}
SEVERE: Servlet.service() for servlet [xd] in context with path [] threw exception [Handler processing failed; nested exception is java.lang.StackOverflowError] with root cause
java.lang.StackOverflowError
	at java.lang.StringValue.from(StringValue.java:24)
	at java.lang.String.<init>(String.java:178)
	at org.springframework.xd.dirt.stream.dsl.Token.<init>(Token.java:46)
	at org.springframework.xd.dirt.stream.dsl.Tokenizer.lexIdentifier(Tokenizer.java:195)
	at org.springframework.xd.dirt.stream.dsl.Tokenizer.process(Tokenizer.java:62)
	at org.springframework.xd.dirt.stream.dsl.Tokenizer.<init>(Tokenizer.java:41)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:65)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:55)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:545)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolveModuleNodes(StreamNode.java:135)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:122)
	at org.springframework.xd.dirt.stream.dsl.StreamsNode.resolve(StreamsNode.java:53)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:74)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:55)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:545)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolveModuleNodes(StreamNode.java:135)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:122)
	at org.springframework.xd.dirt.stream.dsl.StreamsNode.resolve(StreamsNode.java:53)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:74)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:55)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:545)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolveModuleNodes(StreamNode.java:135)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:122)
	at org.springframework.xd.dirt.stream.dsl.StreamsNode.resolve(StreamsNode.java:53)
	...ad nauseum
	
{code}
",2.0,0.8464910263632914,0.8044827943019419,0.8464680317785162,True,True,True,1376632587000.0,1376927088000.0,1376930688000.0,1376979595000.0,1376980495000.0,1376632587000.0,-1.0,1376927080000.0,1376930680000.0,1376912473000.0,1376916073000.0,1376927088000.0,1376930688000.0,"mark.fisher",1376930680000.0,1376632587000.0,43,1376632587000.0,-1.0,"stack overflow when trying to create a stream with the same name as a module","(NOTE: even if we do want to prevent the use of module names for stream names, we obviously need to avoid a StackOverflowError)

to reproduce:

start the xd-singlenode container

start the xd-shell, and type the following:

{code}
xd:>stream create time --definition ""time | log""
{code}

that should produce an Internal Server Error output message

check the xd-singlenode console, and find:

{code}
SEVERE: Servlet.service() for servlet [xd] in context with path [] threw exception [Handler processing failed; nested exception is java.lang.StackOverflowError] with root cause
java.lang.StackOverflowError
	at java.lang.StringValue.from(StringValue.java:24)
	at java.lang.String.<init>(String.java:178)
	at org.springframework.xd.dirt.stream.dsl.Token.<init>(Token.java:46)
	at org.springframework.xd.dirt.stream.dsl.Tokenizer.lexIdentifier(Tokenizer.java:195)
	at org.springframework.xd.dirt.stream.dsl.Tokenizer.process(Tokenizer.java:62)
	at org.springframework.xd.dirt.stream.dsl.Tokenizer.<init>(Tokenizer.java:41)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:65)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:55)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:545)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolveModuleNodes(StreamNode.java:135)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:122)
	at org.springframework.xd.dirt.stream.dsl.StreamsNode.resolve(StreamsNode.java:53)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:74)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:55)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:545)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolveModuleNodes(StreamNode.java:135)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:122)
	at org.springframework.xd.dirt.stream.dsl.StreamsNode.resolve(StreamsNode.java:53)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:74)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.parse(StreamConfigParser.java:55)
	at org.springframework.xd.dirt.stream.dsl.StreamConfigParser.lookupStream(StreamConfigParser.java:545)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolveModuleNodes(StreamNode.java:135)
	at org.springframework.xd.dirt.stream.dsl.StreamNode.resolve(StreamNode.java:122)
	at org.springframework.xd.dirt.stream.dsl.StreamsNode.resolve(StreamsNode.java:53)
	...ad nauseum
	
{code}
",2.0,False,2.0,0,0.0,0,1,False
"XD-700","Story","Sprint 14","2013-08-15T06:57:28.000+0000","dturanski","dturanski","Remove XD UUIDGenerator in favor of the new SI provided one","Remove 
<bean id=""idGenerator"" class=""org.springframework.xd.dirt.container.UUIDGenerator"" />  (container.xml)

Delete org.springframework.xd.dirt.container.UUIDGenerator
remove compile dependency on eaio from build.gradle",1.0,0.9604278677164172,0.9603965816886008,0.960412224702509,True,True,True,1376549848000.0,1377163814000.0,1377167414000.0,1377188211000.0,1377189111000.0,1376549848000.0,-1.0,1377163804000.0,1377167404000.0,1377163794000.0,1377167394000.0,1377163814000.0,1377167414000.0,"dturanski",1377167404000.0,1376549848000.0,43,1376549848000.0,-1.0,"Remove XD UUIDGenerator in favor of the new SI provided one","Remove 
<bean id=""idGenerator"" class=""org.springframework.xd.dirt.container.UUIDGenerator"" />  (container.xml)

Delete org.springframework.xd.dirt.container.UUIDGenerator
remove compile dependency on eaio from build.gradle",1.0,False,1.0,0,0.0,-1,0,False
"XD-695","Improvement","Sprint 13","2013-08-14T09:10:11.000+0000","jencompgeek","jencompgeek","Upgrade SDR to get rid of temporary no-op serializer","Spring Data Redis 1.1 M2 added the ability to use RedisTemplate with binary data. We should switch to that instead of the no-op serializer we were forced to implement previously.",3.0,0.0360876271324032,9.741502278815712e-05,7.654037504783774e-05,True,True,True,1376471411000.0,1376486970000.0,1376490570000.0,1376901656000.0,1376902556000.0,1376471411000.0,-1.0,1376471444000.0,1376475044000.0,1376471453000.0,1376475053000.0,1376486970000.0,1376490570000.0,"jencompgeek",1376475044000.0,1376471411000.0,43,1376471411000.0,-1.0,"Upgrade SDR to get rid of temporary no-op serializer","Spring Data Redis 1.1 M2 added the ability to use RedisTemplate with binary data. We should switch to that instead of the no-op serializer we were forced to implement previously.",3.0,False,3.0,0,0.0,-1,0,False
"XD-693","Story","Sprint 13","2013-08-14T07:33:56.000+0000","eric.bottard","","Add deploy/undeploy commands for taps","",3.0,0.0532750055106227,0.0393373688047883,0.000228902792614,True,True,True,1376465636000.0,1376471920000.0,1376475520000.0,1376582690000.0,1376583590000.0,1376465636000.0,-1.0,1376465663000.0,1376469263000.0,1376470276000.0,1376473876000.0,1376471920000.0,1376475520000.0,"eric.bottard",1376469263000.0,1376465636000.0,43,1376465636000.0,-1.0,"Add deploy/undeploy commands for taps","",3.0,False,3.0,0,0.0,0,0,False
"XD-692","Story","Sprint 13","2013-08-14T07:31:15.000+0000","dturanski","mark.fisher","http source module should copy Content-Type header to SI MessageHeaders.CONTENT_TYPE","",1.0,0.0008946961295072,0.0,0.0008807165024837,True,True,True,1376465475000.0,1376465859000.0,1376469459000.0,1376893771000.0,1376894671000.0,1376465475000.0,-1.0,1376465853000.0,1376469453000.0,1376465475000.0,1376469075000.0,1376465859000.0,1376469459000.0,"dturanski",1376469453000.0,1376465475000.0,43,1376465475000.0,-1.0,"http source module should copy Content-Type header to SI MessageHeaders.CONTENT_TYPE","",1.0,False,1.0,0,0.0,0,0,False
"XD-691","Story","Sprint 14","2013-08-14T06:19:35.000+0000","mark.pollack","dturanski","Change JMX option to reference 'enableJmx' instead of 'disableJmx'","Make the default value of enableJmx false until we have tested/documented JMX functionality",2.0,0.8346949209848512,0.7300634155325448,0.7536538836859625,True,True,True,1376461175000.0,1377068484000.0,1377072084000.0,1377187857000.0,1377188757000.0,1376461175000.0,-1.0,1377009520000.0,1377013120000.0,1376992356000.0,1376995956000.0,1377068484000.0,1377072084000.0,"mark.pollack",1377013120000.0,1376461175000.0,43,1376461175000.0,-1.0,"Change JMX option to reference 'enableJmx' instead of 'disableJmx'","Make the default value of enableJmx false until we have tested/documented JMX functionality",2.0,False,2.0,0,0.0,-1,0,False
"XD-690","Story","Sprint 13","2013-08-13T09:18:35.000+0000","eric.bottard","eric.bottard","Simplify ""instance"" deployment code ","AbstractDeployer has 4 subclasses, 3 of which override e.g. deploy() making the boilerplate factorization ineffective.

Introduce an intermediate class for those deployers that support the concept of an instance (Stream, Tap, Job to some extent)",8.0,7.864154593551e-05,0.0,6.487927539679575e-05,True,True,True,1376385515000.0,1376385555000.0,1376389155000.0,1376893252000.0,1376894152000.0,1376385515000.0,-1.0,1376385548000.0,1376389148000.0,1376385515000.0,1376389115000.0,1376385555000.0,1376389155000.0,"eric.bottard",1376389148000.0,1376385515000.0,43,1376385515000.0,-1.0,"Simplify ""instance"" deployment code ","AbstractDeployer has 4 subclasses, 3 of which override e.g. deploy() making the boilerplate factorization ineffective.

Introduce an intermediate class for those deployers that support the concept of an instance (Stream, Tap, Job to some extent)",8.0,False,8.0,0,0.0,-1,0,False
"XD-689","Story","Sprint 13","2013-08-13T02:56:00.000+0000","dturanski","dturanski","Support conversion of JSON string to accepted Java type across all transports","Example:
xd> create stream test --definition ""http | myTupleProcessor | file""

$ curl -H ""content-type :application/json"" -X POST -d ""{'symbol':'VMW','price'
:75}"" http://localhost:9000

where myTupleProcesser accepts content type application/x-java-object;type=org.springframework.xd.tuple.DefaultTuple (or without the type parameter) should convert the content to Tuple. If a different Java type is specified, the conversion should be attempted using TypedJsonMapper

This should work identically across all known transports",1.0,2.528572112317969e-05,0.0,2.2877557206686388e-05,True,True,True,1376362560000.0,1376362644000.0,1376366244000.0,1379683693000.0,1379684593000.0,1376362560000.0,-1.0,1376362636000.0,1376366236000.0,1376362560000.0,1376366160000.0,1376362644000.0,1376366244000.0,"dturanski",1376366236000.0,1376362560000.0,43,1376362560000.0,-1.0,"Support serialization/deserialization of Message payloads across JVMs across all transports.","
String -> byte[] (string.getBytes()) 
byte[] -> byte[] (no serialization)
Pojo -> configured serialization

",1.0,False,1.0,0,0.0,0,0,True
"XD-686","Story","Sprint 19","2013-08-12T15:24:33.000+0000","grussell","mark.fisher","Support Named Taps (or Similar)","Provide some syntax allowing multiple tap points to be directed to a named channel.

e.g. 
tap foo.4 > namedTap
tap bar.2 > namedTap

or

:tap.foo > counter",8.0,0.9991404162202888,0.7766223563154115,0.7841620630893658,True,True,True,1376321073000.0,1387184431000.0,1387188031000.0,1387192877000.0,1387193777000.0,1376321073000.0,-1.0,1384847035000.0,1384850635000.0,1384765058000.0,1384768658000.0,1387184431000.0,1387188031000.0,"grussell",1384850635000.0,1376321073000.0,43,1376321073000.0,-1.0,"Support Named Taps (or Similar)","Provide some syntax allowing multiple tap points to be directed to a named channel.

e.g. 
tap foo.4 > namedTap
tap bar.2 > namedTap

or

:tap.foo > counter",8.0,False,8.0,0,0.0,0,0,False
"XD-685","Story","Sprint 15","2013-08-12T15:22:59.000+0000","grussell","","Refactor Taps to Avoid Transport Hop","Taps are currently source modules.

They could be refactored to simply bridge the tapped module's tap pub/sub topic directly (with conversion) to the first tap module's input channel.

Note - ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it's no longer a module we'll need special handling to stop/remove the tap adapter.",16.0,0.8299039684939735,0.8289177793231017,0.6743661888085414,True,True,True,1376320979000.0,1378107539000.0,1378111139000.0,1378472810000.0,1378473710000.0,1376320979000.0,-1.0,1377772708000.0,1377776308000.0,1378105416000.0,1378109016000.0,1378107539000.0,1378111139000.0,"grussell",1377776308000.0,1377755468000.0,43,1377755468000.0,-1.0,"Refactor Taps to Avoid Transport Hop","Taps are currently source modules.

They could be refactored to simply bridge the tapped module's tap pub/sub topic directly (with conversion) to the first tap module's input channel.

Note - ensure destroy works. Currently the tap is destroyed by the simple fact it is a module; if it's no longer a module we'll need special handling to stop/remove the tap adapter.",16.0,False,16.0,0,0.0,0,1,False
"XD-683","Story","Sprint 13","2013-08-12T14:37:13.000+0000","mark.fisher","mark.pollack","Change jmxDisabled option to jmxEnabled and do not enable by default","also, the current behavior is broken; it checks if the property is set but does not actually check whether it's true or false",2.0,4.529717209754595e-05,3.321792620486703e-05,1.9628774575603245e-05,True,True,True,1376318233000.0,1376318263000.0,1376321863000.0,1376979626000.0,1376980526000.0,1376318233000.0,-1.0,1376318246000.0,1376321846000.0,1376318255000.0,1376321855000.0,1376318263000.0,1376321863000.0,"mark.fisher",1376321846000.0,1376318233000.0,43,1376318233000.0,-1.0,"Change jmxDisabled option to jmxEnabled and do not enable by default","also, the current behavior is broken; it checks if the property is set but does not actually check whether it's true or false",2.0,False,2.0,0,0.0,0,0,False
"XD-682","Story","Sprint 13","2013-08-12T12:58:42.000+0000","mark.fisher","grenfro","Modify file sink to avoid dot with empty suffix","The expression currently appends ""."" + ${suffix} (where the default suffix is 'out').

If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.",1.0,0.4073449282496189,0.4073380916244505,0.5409616396961804,True,True,True,1376312322000.0,1376550653000.0,1376554253000.0,1376896506000.0,1376897406000.0,1376312322000.0,-1.0,1376628830000.0,1376632430000.0,1376550649000.0,1376554249000.0,1376550653000.0,1376554253000.0,"mark.fisher",1376632430000.0,1376312329000.0,43,1376312329000.0,-1.0,"Modify file sink to avoid dot with empty suffix","The expression currently appends ""."" + ${suffix} (where the default suffix is 'out').

If the suffix value were an empty String, this would lead to the file name ending with a dot. We should update the expression so that it only appends the dot if the suffix is not empty. This might be possible with a ternary expression.",1.0,False,1.0,0,0.0,-1,0,False
"XD-680","Story","","2013-08-12T10:30:01.000+0000","grenfro","","Add Simple Batch  Sample to Spring XD Samples repo","This example should require no code. Just the basic XML.",2.0,-1.0,-1.0,-1.0,False,False,False,1376303401000.0,-1.0,-1.0,1390854643000.0,1390855543000.0,1376303401000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1376303401000.0,-1.0,"Add Simple Batch  Sample to Spring XD Samples repo","This example should require no code. Just the basic XML.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-679","Story","","2013-08-12T09:56:59.000+0000","mark.pollack","","Update settings file and reformat existing codebase.","Please put in suggestions for the current .settings file.

Maybe one suggestion is to not format on save?",1.0,-1.0,-1.0,-1.0,False,False,False,1376301419000.0,-1.0,-1.0,1390854588000.0,1390855488000.0,1376301419000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1376301419000.0,-1.0,"Update settings file and reformat existing codebase.","Please put in suggestions for the current .settings file.

Maybe one suggestion is to not format on save?",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-678","Story","Sprint 13","2013-08-12T09:48:57.000+0000","dturanski","dturanski","change accepted-media-types to accpted-content-types","",1.0,0.5652687536916716,0.0,0.0124040165386887,True,True,True,1376300937000.0,1376301894000.0,1376302630000.0,1376301730000.0,1376302630000.0,1376300937000.0,-1.0,1376300958000.0,1376302630000.0,1376300937000.0,1376302630000.0,1376301894000.0,1376302630000.0,"dturanski",1376302630000.0,1376300937000.0,43,1376300937000.0,-1.0,"change accepted-media-types to accpted-content-types","",1.0,False,1.0,0,0.0,-1,0,False
"XD-677","Bug","Sprint 19","2013-08-12T09:20:19.000+0000","jencompgeek","dturanski","Cannot destroy tap if tapped stream is already destroyed","xd:>tap destroy mytap
16:44:41,850  WARN Spring Shell client.RestTemplate:524 - DELETE request for ""http://localhost:8080/taps/mytap"" resulted in 400 (Bad Request); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD116E:(pos 4): unrecognized stream reference 'foo'
tap foo.http | log

Tap is then still listed when I do a ""tap list""",2.0,0.9738854478702283,0.9665109983211156,0.9665125541942208,True,True,True,1376299219000.0,1386314282000.0,1386317882000.0,1386581934000.0,1386582834000.0,1376299219000.0,-1.0,1386238462000.0,1386242062000.0,1386238446000.0,1386242046000.0,1386314282000.0,1386317882000.0,"jencompgeek",1386242062000.0,1376299219000.0,43,1376299219000.0,-1.0,"Cannot destroy tap if tapped stream is already destroyed","xd:>tap destroy mytap
16:44:41,850  WARN Spring Shell client.RestTemplate:524 - DELETE request for ""http://localhost:8080/taps/mytap"" resulted in 400 (Bad Request); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: XD116E:(pos 4): unrecognized stream reference 'foo'
tap foo.http | log

Tap is then still listed when I do a ""tap list""",2.0,False,2.0,0,0.0,0,0,False
"XD-676","Story","Sprint 13","2013-08-12T08:45:01.000+0000","mark.fisher","","add PropertyAccessor for Tuple fields in SpEL","Example using name:

{code}
filter --expression=""payload.myfield.startsWith('foo')""
{code}

Example using index:

{code}
filter --expression=""payload.2.startsWith('foo')""
{code}

This should support nested keys as well:

{code}
filter --expression=""payload.myfield.subfield.startsWith('foo')""
{code}
",5.0,0.0140491040167767,0.0140527322136717,0.0018039394961885,True,True,True,1376297101000.0,1376316462000.0,1376320062000.0,1377674296000.0,1377675196000.0,1376297101000.0,-1.0,1376299587000.0,1376303187000.0,1376316467000.0,1376320067000.0,1376316462000.0,1376320062000.0,"mark.fisher",1376303187000.0,1376297101000.0,43,1376297101000.0,-1.0,"add PropertyAccessor for Tuple fields in SpEL","Example using name:

{code}
filter --expression=""payload.myfield.startsWith('foo')""
{code}

Example using index:

{code}
filter --expression=""payload.2.startsWith('foo')""
{code}

This should support nested keys as well:

{code}
filter --expression=""payload.myfield.subfield.startsWith('foo')""
{code}
",5.0,False,5.0,0,0.0,0,0,False
"XD-675","Story","Sprint 13","2013-08-12T08:41:56.000+0000","eric.bottard","eric.bottard","Cannot chain json-field-value-filter & json-field-extractor","Because StringToJsonNodeTransformer expects a String as input, one cannot chain json related processors.

A simple solution would be to also accept Jackson IN and forward it directly in that case.",1.0,-1.0,0.0,0.9983456507622376,False,True,True,1376296916000.0,-1.0,-1.0,1376360694000.0,1376361594000.0,1376296916000.0,-1.0,1376361487000.0,1376361594000.0,1376296916000.0,1376300516000.0,-1.0,-1.0,"eric.bottard",1376361594000.0,1376296916000.0,43,1376296916000.0,-1.0,"Cannot chain json-field-value-filter & json-field-extractor","Because StringToJsonNodeTransformer expects a String as input, one cannot chain json related processors.

A simple solution would be to also accept Jackson IN and forward it directly in that case.",1.0,False,1.0,0,0.0,-1,0,False
"XD-674","Story","Sprint 12","2013-08-12T08:12:40.000+0000","hillert","hillert","Add Spring Batch word-count Sample to Spring XD Samples repo ","",2.0,0.0023685457129322,0.0,0.0151586925627664,True,True,True,1376295160000.0,1376295170000.0,1376298770000.0,1376298482000.0,1376299382000.0,1376295160000.0,-1.0,1376295224000.0,1376298824000.0,1376295160000.0,1376298760000.0,1376295170000.0,1376298770000.0,"hillert",1376298824000.0,1376295160000.0,43,1376295160000.0,-1.0,"Add Spring Batch word-count Sample to Spring XD Samples repo ","",2.0,False,2.0,0,0.0,0,0,False
"XD-673","Story","","2013-08-12T08:01:15.000+0000","mark.fisher","","http post in shell incorrectly mentions default of --target option","the value for --target is required (there is no default), but the hint for that option states otherwise:

{code}
xd:>http post --target
http post --target
required --target: the location to post to; default if option not present: 'http://localhost:9000'
{code}
",1.0,-1.0,-1.0,-1.0,False,False,False,1376294475000.0,-1.0,-1.0,1377162794000.0,1377163694000.0,1376294475000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.fisher",-1.0,-1.0,0,1376294475000.0,-1.0,"http post in shell incorrectly mentions default of --target option","the value for --target is required (there is no default), but the hint for that option states otherwise:

{code}
xd:>http post --target
http post --target
required --target: the location to post to; default if option not present: 'http://localhost:9000'
{code}
",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-672","Story","","2013-08-12T07:52:51.000+0000","mark.fisher","","ugly error messages in shell when not connected","should go thru the list of all commands available and make sure that a simple ""not connected"" message is returned instead of something like this:

{code}
org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://localhost:8080"":Unexpected end of file from server; nested exception is java.net.SocketException: Unexpected end of file from server
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:498)
....
{code}
",5.0,-1.0,0.0557825113564714,-1.0,False,False,True,1376293971000.0,-1.0,-1.0,1382340504000.0,1382341404000.0,1376293971000.0,-1.0,-1.0,-1.0,1376631312000.0,1376634912000.0,-1.0,-1.0,"mark.fisher",-1.0,-1.0,0,1376293971000.0,-1.0,"Ugly error messages in shell when not connected","should go thru the list of all commands available and make sure that a simple ""not connected"" message is returned instead of something like this:

{code}
org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://localhost:8080"":Unexpected end of file from server; nested exception is java.net.SocketException: Unexpected end of file from server
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:498)
....
{code}
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-671","Story","Sprint 14","2013-08-12T06:41:18.000+0000","mark.fisher","","Add support for dynamic routing","It should be possible to create streams like the following which rely upon named channel support and dynamic routing capabilities:

{code}
http | somerouter
:x > xtransformer | hdfs
:y > ytransformer | hdfs
{code}

The 'somerouter' processor could return ""x"" or ""y"" which determines the downstream path for each message.

This should be implemented in such a way that any developer adding a router module would only need to deal with existing Spring Integration semantics (in this case, only considering the return of ""x"" or ""y"" - whether it be SpEL or a POJO method invocation). Perhaps in the plugin that modifies a module context, we could simply add a new ChannelResolver implementation (by adding that ChannelResolver as a bean and/or a BeanPostProcessor that configures that as the resolver for any router, if necessary). That ChannelResolver would have a reference to the ChannelRegistry so that the router actually sends its messages to those shared channels. The shared channels themselves would have been created as long as a valid downstream flow has been defined.",16.0,0.8730203453319716,0.8730359590098373,0.5108802494781838,True,True,True,1376289678000.0,1377519782000.0,1377523382000.0,1377697799000.0,1377698699000.0,1376289678000.0,-1.0,1377009519000.0,1377013119000.0,1377519804000.0,1377523404000.0,1377519782000.0,1377523382000.0,"mark.fisher",1377013119000.0,1376989133000.0,43,1376989133000.0,-1.0,"Add support for dynamic routing","It should be possible to create streams like the following which rely upon named channel support and dynamic routing capabilities:

{code}
http | somerouter
:x > xtransformer | hdfs
:y > ytransformer | hdfs
{code}

The 'somerouter' processor could return ""x"" or ""y"" which determines the downstream path for each message.

This should be implemented in such a way that any developer adding a router module would only need to deal with existing Spring Integration semantics (in this case, only considering the return of ""x"" or ""y"" - whether it be SpEL or a POJO method invocation). Perhaps in the plugin that modifies a module context, we could simply add a new ChannelResolver implementation (by adding that ChannelResolver as a bean and/or a BeanPostProcessor that configures that as the resolver for any router, if necessary). That ChannelResolver would have a reference to the ChannelRegistry so that the router actually sends its messages to those shared channels. The shared channels themselves would have been created as long as a valid downstream flow has been defined.",16.0,False,16.0,0,0.0,0,1,False
"XD-670","Story","Sprint 12","2013-08-12T03:40:27.000+0000","eric.bottard","eric.bottard","TAB completion for existing entities","Provide Shell TAB completion when referencing an existing entity",4.0,0.0024464233290928,0.0,0.0015167824640375,True,True,True,1376278827000.0,1376278877000.0,1376282477000.0,1376298365000.0,1376299265000.0,1376278827000.0,-1.0,1376278858000.0,1376282458000.0,1376278827000.0,1376282427000.0,1376278877000.0,1376282477000.0,"eric.bottard",1376282458000.0,1376278827000.0,43,1376278827000.0,-1.0,"TAB completion for existing entities","Provide Shell TAB completion when referencing an existing entity",4.0,False,4.0,0,0.0,-1,0,False
"XD-666","Story","Sprint 12","2013-08-10T01:54:10.000+0000","grussell","grussell","Remove Redis Transport Headers from Tapped Stream","Redis transport headers are not removed in taps.",2.0,0.0010354068539873,0.0011166152346922,0.0008120838070488,True,True,True,1376099650000.0,1376099701000.0,1376103301000.0,1376148006000.0,1376148906000.0,1376099650000.0,-1.0,1376099690000.0,1376103290000.0,1376099705000.0,1376103305000.0,1376099701000.0,1376103301000.0,"grussell",1376103290000.0,1376099650000.0,43,1376099650000.0,-1.0,"Remove Redis Transport Headers from Tapped Stream","Redis transport headers are not removed in taps.",2.0,False,2.0,0,0.0,0,0,False
"XD-665","Story","Sprint 12","2013-08-09T17:13:03.000+0000","iperumal","iperumal","AggregateCounter display command options with ""lastHours"" and ""lastDays""","It would be nice to have ""lastHours"" and ""lastDays"" options for aggregatecounter display command.",1.0,0.0280384199924094,0.0276880857151198,0.027950836423087,True,True,True,1376068383000.0,1376073185000.0,1376076785000.0,1376238748000.0,1376239648000.0,1376068383000.0,1376239661000.0,1376073170000.0,1376076770000.0,1376073125000.0,1376076725000.0,1376073185000.0,1376076785000.0,"iperumal",1376076770000.0,1376068383000.0,43,1376068383000.0,1376239661000.0,"AggregateCounter display command options with ""lastHours"" and ""lastDays""","It would be nice to have ""lastHours"" and ""lastDays"" options for aggregatecounter display command.",2.0,True,2.0,1,0.0,0,0,False
"XD-664","Story","Sprint 12","2013-08-09T16:33:36.000+0000","mark.pollack","","File sink filename should default to having a '.out' suffix.","",1.0,0.6841250385709877,0.6841464668975211,0.0001157129632804,True,True,True,1376066016000.0,1376225647000.0,1376229247000.0,1376298452000.0,1376299352000.0,1376066016000.0,-1.0,1376066043000.0,1376069643000.0,1376225652000.0,1376229252000.0,1376225647000.0,1376229247000.0,"mark.pollack",1376069643000.0,1376066016000.0,43,1376066016000.0,-1.0,"File sink filename should default to having a '.out' suffix.","",1.0,False,1.0,0,0.0,0,0,False
"XD-663","Story","Sprint 12","2013-08-09T15:59:59.000+0000","thomas.risberg","thomas.risberg","Use correct FS_DEFAULT_NAME_KEY constant based on Hadoop version used","Keep getting the following warning:

WARN Spring Shell conf.Configuration:817 - fs.default.name is deprecated. Instead, use fs.defaultFS

Should switch to use the runtime value of the FS_DEFAULT_NAME_KEY constant based on Hadoop version used.",3.0,0.0002454760031471,0.0,0.0002077104642014,True,True,True,1376063999000.0,1376064038000.0,1376067638000.0,1376221974000.0,1376222874000.0,1376063999000.0,-1.0,1376064032000.0,1376067632000.0,1376063999000.0,1376067599000.0,1376064038000.0,1376067638000.0,"thomas.risberg",1376067632000.0,1376063999000.0,43,1376063999000.0,-1.0,"Use correct FS_DEFAULT_NAME_KEY constant based on Hadoop version used","Keep getting the following warning:

WARN Spring Shell conf.Configuration:817 - fs.default.name is deprecated. Instead, use fs.defaultFS

Should switch to use the runtime value of the FS_DEFAULT_NAME_KEY constant based on Hadoop version used.",3.0,False,3.0,0,0.0,0,0,False
"XD-662","Bug","Sprint 13","2013-08-09T14:11:34.000+0000","jencompgeek","iperumal","No indication of failure in shell when deploying job referencing nonexistent trigger","I see the following output on the shell if I create a job and reference a non-existent trigger. There's a big stack trace in the server log, but nothing on the shell side indicating failure. A subsequent ""jobs list"" also shows the job. The same thing happens if I deploy an undeployed Job after deleting its associated Trigger.

$ job create --name helloWorldJob --definition ""myjob --trigger=nonexistenttrigger""
Successfully created and deployed job 'helloWorldJob'",2.0,0.9723427063890612,0.9658270578083245,0.9723255999604652,True,True,True,1376057494000.0,1376569061000.0,1376572661000.0,1376582712000.0,1376583612000.0,1376057494000.0,-1.0,1376569052000.0,1376572652000.0,1376565633000.0,1376569233000.0,1376569061000.0,1376572661000.0,"jencompgeek",1376572652000.0,1376057494000.0,43,1376057494000.0,-1.0,"No indication of failure in shell when deploying job referencing nonexistent trigger","I see the following output on the shell if I create a job and reference a non-existent trigger. There's a big stack trace in the server log, but nothing on the shell side indicating failure. A subsequent ""jobs list"" also shows the job. The same thing happens if I deploy an undeployed Job after deleting its associated Trigger.

$ job create --name helloWorldJob --definition ""myjob --trigger=nonexistenttrigger""
Successfully created and deployed job 'helloWorldJob'",2.0,False,2.0,0,0.0,0,0,False
"XD-661","Story","Sprint 13","2013-08-09T13:53:35.000+0000","hillert","","Batch Jobs: Add the ability to provide JobParameters","",4.0,0.3875531946641201,0.387568712883106,0.2902766420961533,True,True,True,1376056415000.0,1376381078000.0,1376384678000.0,1376893240000.0,1376894140000.0,1376056415000.0,-1.0,1376299587000.0,1376303187000.0,1376381091000.0,1376384691000.0,1376381078000.0,1376384678000.0,"hillert",1376303187000.0,1376056415000.0,43,1376056415000.0,-1.0,"Batch Jobs: Add the ability to provide JobParameters","",4.0,False,4.0,0,0.0,-1,0,False
"XD-660","Story","Sprint 12","2013-08-09T13:47:05.000+0000","mark.pollack","","Rename spring-xd-shell to xd-shell","",1.0,0.9148623081774228,0.9133907925162916,0.0014715156611309,True,True,True,1376056025000.0,1376060377000.0,1376060782000.0,1376059882000.0,1376060782000.0,1376056025000.0,-1.0,1376056032000.0,1376059632000.0,1376060370000.0,1376060782000.0,1376060377000.0,1376060782000.0,"mark.pollack",1376059632000.0,1376056025000.0,43,1376056025000.0,-1.0,"Rename spring-xd-shell to xd-shell","",1.0,False,1.0,0,0.0,-1,0,False
"XD-656","Story","Sprint 13","2013-08-09T11:27:57.000+0000","grussell","","Intra-Module ""Pipe"" Naming","Consider 2 Admins/Containers that are using discrete Redis instances but a shared Rabbit instance for the transport.

If two different streams are deployed on each, but with the same stream name, the Rabbit queues will be common (e.g. foo.0), causing crosstalk.

Stream names must be unique across all container instances sharing Rabbit infrastructure.

I am not sure what the solution is; two instances of the *same* stream *do* need common queues but instances of different streams need a qualifier of some kind (container name?). I guess it's not *that* big of an issue because, if they're sharing infrastructure, they're likely to be sharing a stream repo too - in which case you'd need unique stream names.",3.0,0.9882975493089412,0.9735779361608,0.2864882753726531,True,True,True,1376047677000.0,1376916690000.0,1376920290000.0,1376926080000.0,1376926980000.0,1376047677000.0,1376903761000.0,1376299587000.0,1376303187000.0,1376903747000.0,1376907347000.0,1376916690000.0,1376920290000.0,"grussell",1376303187000.0,1376047677000.0,43,1376903761000.0,1376903761000.0,"Intra-Module ""Pipe"" Naming","Consider 2 Admins/Containers that are using discrete Redis instances but a shared Rabbit instance for the transport.

If two different streams are deployed on each, but with the same stream name, the Rabbit queues will be common (e.g. foo.0), causing crosstalk.

Stream names must be unique across all container instances sharing Rabbit infrastructure.

I am not sure what the solution is; two instances of the *same* stream *do* need common queues but instances of different streams need a qualifier of some kind (container name?). I guess it's not *that* big of an issue because, if they're sharing infrastructure, they're likely to be sharing a stream repo too - in which case you'd need unique stream names.",8.0,True,8.0,1,-62.5,0,0,False
"XD-655","Story","Sprint 12","2013-08-09T11:23:48.000+0000","mark.pollack","dturanski","Trim output from http post shell command to two lines","Instead of 

xd:>http post --target http://localhost:9898 --data ""hello world""
> POST (text/plain;charset=UTF-8) http://localhost:9898 hello world
> 200 OK
> Content-Length: 0
> Connection: keep-alive
> 
Success sending data 'hello world' to target 'http://localhost:9898'


have

xd:>http post --target http://localhost:9898 --data ""hello world""
> POST (text/plain;charset=UTF-8) http://localhost:9898 hello world
> 200 OK


or better yet

xd:>http post --target http://localhost:9898 --data ""hello world""
> 200 OK POST (text/plain;charset=UTF-8) http://localhost:9898 hello world




 
",1.0,-1.0,0.419345579793341,0.011768082663605,False,True,True,1376047428000.0,-1.0,-1.0,1376050012000.0,1376050912000.0,1376047428000.0,-1.0,1376047469000.0,1376050912000.0,1376048889000.0,1376050912000.0,-1.0,-1.0,"mark.pollack",1376050912000.0,1376047428000.0,43,1376047428000.0,-1.0,"Trim output from http post shell command to two lines","Instead of 

xd:>http post --target http://localhost:9898 --data ""hello world""
> POST (text/plain;charset=UTF-8) http://localhost:9898 hello world
> 200 OK
> Content-Length: 0
> Connection: keep-alive
> 
Success sending data 'hello world' to target 'http://localhost:9898'


have

xd:>http post --target http://localhost:9898 --data ""hello world""
> POST (text/plain;charset=UTF-8) http://localhost:9898 hello world
> 200 OK


or better yet

xd:>http post --target http://localhost:9898 --data ""hello world""
> 200 OK POST (text/plain;charset=UTF-8) http://localhost:9898 hello world




 
",1.0,False,1.0,0,0.0,-1,0,False
"XD-653","Story","Sprint 12","2013-08-09T09:39:47.000+0000","dturanski","dturanski","Configure Jackson ObjectMappers to Allow Single Quotes ","Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.",1.0,0.0555220277610138,0.0,0.0114665057332528,True,True,True,1376041187000.0,1376041279000.0,1376042844000.0,1376041944000.0,1376042844000.0,1376041187000.0,-1.0,1376041206000.0,1376042844000.0,1376041187000.0,1376042844000.0,1376041279000.0,1376042844000.0,"dturanski",1376042844000.0,1376041187000.0,43,1376041187000.0,-1.0,"Configure Jackson ObjectMappers to Allow Single Quotes ","Allow Json payloads from external sources, e.g., http post to contain single quoted field names and values. This is required where XD uses Jackson to convert payloads from Json to object or Tuples.",1.0,False,1.0,0,0.0,-1,0,False
"XD-652","Bug","Sprint 12","2013-08-09T09:12:44.000+0000","jencompgeek","","File Source Name and Duplicates options not working as documented","The doc says the name option is ""the absolute path to the directory to monitor for files"" but it actually seems to be the name of a dir in /tmp/xd/input. Not sure which is the correct behavior. Also, ""name"" as an option name seems a little vague. Maybe something like ""--directory""?

Also, if I set --duplicates=true, it actually prevents duplicates (setting prevents-duplicates to true)",2.0,-1.0,-1.0,0.0023218691046292,False,True,False,1376039564000.0,-1.0,-1.0,1376052446000.0,1376053346000.0,1376039564000.0,-1.0,1376039596000.0,1376043196000.0,-1.0,-1.0,-1.0,-1.0,"jencompgeek",1376043196000.0,1376039564000.0,43,1376039564000.0,-1.0,"File Source Name and Duplicates options not working as documented","The doc says the name option is ""the absolute path to the directory to monitor for files"" but it actually seems to be the name of a dir in /tmp/xd/input. Not sure which is the correct behavior. Also, ""name"" as an option name seems a little vague. Maybe something like ""--directory""?

Also, if I set --duplicates=true, it actually prevents duplicates (setting prevents-duplicates to true)",2.0,False,2.0,0,0.0,0,0,False
"XD-650","Story","Sprint 12","2013-08-09T08:48:15.000+0000","aeisenberg","","Eclipse build path error after running gradle -> refresh source folders in Eclipse","After running gradle -> refresh source folders on the spring-xd-module project in Eclipse, there is an error because the {{src/test/java}} folder is missing.

Solution is to add a placeholder file.",5.0,0.9459349039901948,-1.0,0.9449135230832084,True,True,False,1376038095000.0,1376051987000.0,1376052781000.0,1376051881000.0,1376052781000.0,1376038095000.0,-1.0,1376051972000.0,1376052781000.0,-1.0,-1.0,1376051987000.0,1376052781000.0,"aeisenberg",1376052781000.0,1376038095000.0,43,1376038095000.0,-1.0,"Eclipse build path error after running gradle -> refresh source folders in Eclipse","After running gradle -> refresh source folders on the spring-xd-module project in Eclipse, there is an error because the {{src/test/java}} folder is missing.

Solution is to add a placeholder file.",5.0,False,5.0,0,0.0,0,0,False
"XD-649","Story","Sprint 12","2013-08-09T08:03:01.000+0000","mark.pollack","","Remove 'substream' from the documentation","This will come back in M3 once we iron out the issues.",1.0,0.9892945374691188,0.9890749382377162,0.0011528959648641,True,True,True,1376035381000.0,1376053401000.0,1376053596000.0,1376052696000.0,1376053596000.0,1376035381000.0,-1.0,1376035402000.0,1376039002000.0,1376053397000.0,1376053596000.0,1376053401000.0,1376053596000.0,"mark.pollack",1376039002000.0,1376035381000.0,43,1376035381000.0,-1.0,"Remove 'substream' from the documentation","This will come back in M3 once we iron out the issues.",1.0,False,1.0,0,0.0,-1,0,False
"XD-648","Story","Sprint 12","2013-08-09T06:55:47.000+0000","eric.bottard","eric.bottard","Regression test on file source","As we overwrote changes to file source by mistake, let's add some regression tests, esp. to the file location.

Plan on extending the utility source and sink functionality",3.0,0.0137043478260869,0.0,0.0131478260869565,True,True,True,1376031347000.0,1376031544000.0,1376035144000.0,1376044822000.0,1376045722000.0,1376031347000.0,-1.0,1376031536000.0,1376035136000.0,1376031347000.0,1376034947000.0,1376031544000.0,1376035144000.0,"eric.bottard",1376035136000.0,1376031347000.0,43,1376031347000.0,-1.0,"Regression test on file source","As we overwrote changes to file source by mistake, let's add some regression tests, esp. to the file location.

Plan on extending the utility source and sink functionality",3.0,False,3.0,0,0.0,0,0,False
"XD-647","Story","Sprint 21","2013-08-09T06:53:36.000+0000","eric.bottard","Eric Bottard","HTTP source should emit raw payload","Current implementation converts to a String.

See if we can emit raw payload (given that we also emit content-type header)

Setting to 8 points, as this may have lots of implications down the line though",8.0,0.8751919138300829,0.9355957433367864,0.8751908725493168,True,True,True,1376031216000.0,1391160137000.0,1391163737000.0,1393316720000.0,1393317620000.0,1376031216000.0,-1.0,1391160119000.0,1391163719000.0,1392204302000.0,1392207902000.0,1391160137000.0,1391163737000.0,"eric.bottard",1391163719000.0,1376031216000.0,43,1376031216000.0,-1.0,"HTTP source should emit raw payload","Current implementation converts to a String.

See if we can emit raw payload (given that we also emit content-type header)

Setting to 8 points, as this may have lots of implications down the line though",8.0,False,8.0,0,0.0,0,0,False
"XD-645","Improvement","Sprint 13","2013-08-08T19:56:37.000+0000","jencompgeek","","Find a way to contribute redis.properties to Rabbit Container PPC","Need to undo the recent add of rabbit.properties to xd-common.xml. Tried to work around this by configuring rabbit-container with only the PPC it needs (pointing to only rabbit.properties), but this caused issues with redis-analytics later requiring redis.properties. Would be nice to have a way for redis-analytics to contribute redis.properties or something similar...


Also, strictly speaking, the local admin server does not even need redis.properties, let alone rabbit.properties, so we should find a cleaner way to configure this.",3.0,0.455914892110527,0.4559093604517374,0.3405178517692457,True,True,True,1375991797000.0,1376403893000.0,1376407493000.0,1376894785000.0,1376895685000.0,1375991797000.0,-1.0,1376299587000.0,1376303187000.0,1376403888000.0,1376407488000.0,1376403893000.0,1376407493000.0,"jencompgeek",1376303187000.0,1375991797000.0,43,1375991797000.0,-1.0,"Find a way to contribute redis.properties to Rabbit Container PPC","Need to undo the recent add of rabbit.properties to xd-common.xml. Tried to work around this by configuring rabbit-container with only the PPC it needs (pointing to only rabbit.properties), but this caused issues with redis-analytics later requiring redis.properties. Would be nice to have a way for redis-analytics to contribute redis.properties or something similar...


Also, strictly speaking, the local admin server does not even need redis.properties, let alone rabbit.properties, so we should find a cleaner way to configure this.",3.0,False,3.0,0,0.0,0,1,False
"XD-644","Bug","Sprint 12","2013-08-08T18:50:53.000+0000","jencompgeek","jencompgeek","Connection props in rabbit.properties ignored by xd-admin and xd-container","I modified rabbit.hostname in rabbit.properties and xd-container still attempted to find Rabbit at localhost with --transport rabbit. Looks like the PPC for xd-container and xd-admin is not pointing to rabbit.properties",2.0,0.0224690734662963,0.0239838424640242,0.0176723049734915,True,True,True,1375987853000.0,1375987942000.0,1375991542000.0,1375990914000.0,1375991814000.0,1375987853000.0,-1.0,1375987923000.0,1375991523000.0,1375987948000.0,1375991548000.0,1375987942000.0,1375991542000.0,"jencompgeek",1375991523000.0,1375987853000.0,43,1375987853000.0,-1.0,"Connection props in rabbit.properties ignored by xd-admin and xd-container","I modified rabbit.hostname in rabbit.properties and xd-container still attempted to find Rabbit at localhost with --transport rabbit. Looks like the PPC for xd-container and xd-admin is not pointing to rabbit.properties",2.0,False,2.0,0,0.0,0,0,False
"XD-643","Story","Sprint 12","2013-08-08T16:03:17.000+0000","thomas.risberg","thomas.risberg","Map column names with underscore to camelCase style keys for JDBC sink","We need to add support for matching column names with underscores like ""user_name"" and map them to camel case style keys like ""userName"" in the JdbcMessagePayloadTransformer.",3.0,0.4443879496631499,0.0,0.0036862844794712,True,True,True,1375977797000.0,1375981293000.0,1375984893000.0,1375984764000.0,1375985664000.0,1375977797000.0,-1.0,1375977826000.0,1375981426000.0,1375977797000.0,1375981397000.0,1375981293000.0,1375984893000.0,"thomas.risberg",1375981426000.0,1375977797000.0,43,1375977797000.0,-1.0,"Map column names with underscore to camelCase style keys for JDBC sink","We need to add support for matching column names with underscores like ""user_name"" and map them to camel case style keys like ""userName"" in the JdbcMessagePayloadTransformer.",3.0,False,3.0,0,0.0,-1,0,False
"XD-642","Story","Sprint 12","2013-08-08T15:29:54.000+0000","mark.pollack","mark.pollack","Update architecture diagram to show rabbit in addition to redis to communicate between admin and containers","Figure 1 here https://github.com/SpringSource/spring-xd/wiki/Architecture should also show Rabbit as an option, since otherwise people will think we are tied to redis.",1.0,-1.0,0.0,0.0026395939086294,False,True,True,1375975794000.0,-1.0,-1.0,1375984744000.0,1375985644000.0,1375975794000.0,-1.0,1375975820000.0,1375979420000.0,1375975794000.0,1375979394000.0,-1.0,-1.0,"mark.pollack",1375979420000.0,1375975794000.0,43,1375975794000.0,-1.0,"Update architecture diagram to show rabbit in addition to redis to communicate between admin and containers","Figure 1 here https://github.com/SpringSource/spring-xd/wiki/Architecture should also show Rabbit as an option, since otherwise people will think we are tied to redis.",1.0,False,1.0,0,0.0,-1,0,False
"XD-640","Bug","Sprint 12","2013-08-08T14:12:17.000+0000","jencompgeek","thomas.risberg","Cannot start xd-container with the --hadoopDistro option","Trying to use xd-container with PHD, and therefore need to start with --hadoopDistro. I get the following error:

$ bin/xd-container --hadoopDistro phd1
17:11:20,305 ERROR main server.ContainerMain:59 - ""--hadoopDistro"" is not a valid option
",3.0,0.1077645218071582,0.1074711519655779,0.0985722667709759,True,True,True,1375971137000.0,1375972239000.0,1375975839000.0,1375980463000.0,1375981363000.0,1375971137000.0,-1.0,1375972145000.0,1375975745000.0,1375972236000.0,1375975836000.0,1375972239000.0,1375975839000.0,"jencompgeek",1375975745000.0,1375971137000.0,43,1375971137000.0,-1.0,"Cannot start xd-container with the --hadoopDistro option","Trying to use xd-container with PHD, and therefore need to start with --hadoopDistro. I get the following error:

$ bin/xd-container --hadoopDistro phd1
17:11:20,305 ERROR main server.ContainerMain:59 - ""--hadoopDistro"" is not a valid option
",3.0,False,3.0,0,0.0,-1,0,False
"XD-639","Improvement","","2013-08-08T13:25:14.000+0000","jencompgeek","","Error using hadoop rm with --recursive option in the wrong order","I had the following interaction with the shell. It does work if I do ""hadoop fs rm /xd/tweets --recursive"". Either the order shouldn't matter or the doc should be more clear on placement of the option.

xd:>hadoop fs rm /xd/tweets
To remove directory, please use fs rm --recursive instead
xd:>hadoop fs rm --recursive /xd/tweets
java.lang.IllegalArgumentException: Failed to convert '/xd/tweets' to type boolean for option 'recursive'
Cannot convert /xd/tweets to type Boolean.
",2.0,-1.0,1.0000009933346583,-1.0,False,False,True,1375968314000.0,-1.0,-1.0,1391068065000.0,1391068965000.0,1375968314000.0,-1.0,-1.0,-1.0,1391068980000.0,1391068965000.0,-1.0,-1.0,"jencompgeek",-1.0,-1.0,0,1375968314000.0,-1.0,"Update error message for usage of hadoop rm with --recursive option","I had the following interaction with the shell. It does work if I do ""hadoop fs rm /xd/tweets --recursive"". Either the order shouldn't matter or the doc should be more clear on placement of the option.

xd:>hadoop fs rm /xd/tweets
To remove directory, please use fs rm --recursive instead
xd:>hadoop fs rm --recursive /xd/tweets
java.lang.IllegalArgumentException: Failed to convert '/xd/tweets' to type boolean for option 'recursive'
Cannot convert /xd/tweets to type Boolean.
",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-637","Story","","2013-08-08T11:28:16.000+0000","grussell","grussell","Channel Registry Refactoring","Factor out common Redis/Rabbit {{ChannelRegistry}} code.

Change transport nternals to Use AbstractTransformer instead of {{ARPMH}} and {{BridgeHandler}}.",4.0,-1.0,0.0,-1.0,False,False,True,1375961296000.0,-1.0,-1.0,1378916532000.0,1378917432000.0,1375961296000.0,-1.0,-1.0,-1.0,1375961296000.0,1375964896000.0,-1.0,-1.0,"grussell",-1.0,-1.0,0,1375961296000.0,-1.0,"Channel Registry Refactoring","Factor out common Redis/Rabbit {{ChannelRegistry}} code.

Also, factor out common inbound/tap code (very similar).

Change transport nternals to Use AbstractTransformer instead of {{ARPMH}} and {{BridgeHandler}}.",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-636","Story","Sprint 12","2013-08-08T09:50:25.000+0000","grussell","grussell","x-xd-* Transport Content-Type Leakage","The {{AbstractReplyProducingMessageHandler}} in the Rabbit transport exposes the internal transport content-type, if none existed on the original transported message.",1.0,0.0266982088543426,0.0,0.0158837445082798,True,True,True,1375955425000.0,1375955504000.0,1375958384000.0,1375957484000.0,1375958384000.0,1375955425000.0,-1.0,1375955472000.0,1375958384000.0,1375955425000.0,1375958384000.0,1375955504000.0,1375958384000.0,"grussell",1375958384000.0,1375955425000.0,43,1375955425000.0,-1.0,"x-xd-* Transport Content-Type Leakage","The {{AbstractReplyProducingMessageHandler}} in the Rabbit transport exposes the internal transport content-type, if none existed on the original transported message.",1.0,False,1.0,0,0.0,0,0,False
"XD-635","Story","Sprint 12","2013-08-08T09:27:32.000+0000","dturanski","","OOTB source modules with poller should use fixed-delay","pollers should standardize on fixed-delay vs fixed-rate. The value should accept a property with a standard name like 'interval'",1.0,-1.0,0.9787844351985924,0.746531278065947,False,True,True,1375954052000.0,-1.0,-1.0,1376062270000.0,1376063170000.0,1375954052000.0,-1.0,1376035512000.0,1376039112000.0,1376060855000.0,1376063170000.0,-1.0,-1.0,"dturanski",1376039112000.0,1375954052000.0,43,1375954052000.0,-1.0,"OOTB source modules with poller should use fixed-delay","pollers should standardize on fixed-delay vs fixed-rate. The value should accept a property with a standard name like 'interval'",1.0,False,1.0,0,0.0,-1,0,False
"XD-634","Story","Sprint 12","2013-08-08T08:34:46.000+0000","thomas.risberg","","Fix guava dependency for hadoop20 and phd1 ","Spring Xd currently ships with Guava 12.0 while Hadoop 2.0.5 and Pivotal HD 1.0 depends on 11.0.2 - this could lead to classpath problems if we include both.",3.0,0.1290494364072069,0.6157510132836307,0.0018046803349013,True,True,True,1375950886000.0,1375955248000.0,1375958848000.0,1375983787000.0,1375984687000.0,1375950886000.0,-1.0,1375950947000.0,1375954547000.0,1375971699000.0,1375975299000.0,1375955248000.0,1375958848000.0,"thomas.risberg",1375954547000.0,1375950886000.0,43,1375950886000.0,-1.0,"Fix guava dependency for hadoop20 and phd1 ","Spring Xd currently ships with Guava 12.0 while Hadoop 2.0.5 and Pivotal HD 1.0 depends on 11.0.2 - this could lead to classpath problems if we include both.",3.0,False,3.0,0,0.0,-1,0,False
"XD-633","Story","Sprint 12","2013-08-08T07:19:25.000+0000","grussell","grussell","Rabbit Source - Make Default QueueName == Stream Name","Consistency with JMS Source.",1.0,0.0252475247524752,0.0,0.0198019801980198,True,True,True,1375946365000.0,1375946416000.0,1375948385000.0,1375947485000.0,1375948385000.0,1375946365000.0,-1.0,1375946405000.0,1375948385000.0,1375946365000.0,1375948385000.0,1375946416000.0,1375948385000.0,"grussell",1375948385000.0,1375946365000.0,43,1375946365000.0,-1.0,"Rabbit Source - Make Default QueueName == Stream Name","Consistency with JMS Source.",1.0,False,1.0,0,0.0,0,0,False
"XD-632","Story","Sprint 12","2013-08-08T06:23:59.000+0000","hillert","eric.bottard","Shell: HTTP Post - Allow posting of local file contents","E.g. allow for posting of JSON data stored in local files.

* Allow users to specify the *content-type*.
* Ensure that Unicode data (UTF) posts correctly.",4.0,0.7676764737265858,0.7674048637584999,0.7673175605544723,True,True,True,1375943039000.0,1376022178000.0,1376025778000.0,1376045228000.0,1376046128000.0,1375943039000.0,-1.0,1376022141000.0,1376025741000.0,1376022150000.0,1376025750000.0,1376022178000.0,1376025778000.0,"hillert",1376025741000.0,1375943039000.0,43,1375943039000.0,-1.0,"Shell: HTTP Post - Allow posting of local file contents","E.g. allow for posting of JSON data stored in local files.

* Allow users to specify the *content-type*.
* Ensure that Unicode data (UTF) posts correctly.",4.0,False,4.0,0,0.0,0,0,False
"XD-631","Story","Sprint 53","2013-08-08T06:17:54.000+0000","hillert","hillert","Pluralize test classes in package org.springframework.xd.shell.command","The classes under test are pluralized. Therefore, the test classes themselves should reflect that. E.g. rename *JobCommandTests* to *JobCommandsTests* as it tests class *JobCommands*. Please check all tests in that package for correct naming.",1.0,0.9939463134082626,0.9873079310039332,0.9873079310039332,True,True,True,1375942674000.0,1436773236000.0,1436776836000.0,1437142828000.0,1437143728000.0,1375942674000.0,-1.0,1436366960000.0,1436370560000.0,1436366960000.0,1436370560000.0,1436773236000.0,1436776836000.0,"hillert",1436370560000.0,1375942674000.0,43,1375942674000.0,-1.0,"Pluralize test classes in package org.springframework.xd.shell.command","The classes under test are pluralized. Therefore, the test classes themselves should reflect that. E.g. rename *JobCommandTests* to *JobCommandsTests* as it tests class *JobCommands*. Please check all tests in that package for correct naming.",1.0,False,1.0,0,0.0,0,0,False
"XD-630","Bug","Sprint 21","2013-08-07T16:17:37.000+0000","aclement","eric.bottard","StreamCommandTests - asserting sink contents sometimes failing","There are some asserts in StreamCommandTests that are commented out (see the TODOs in there). These asserts are verifying the contents of the various sinks employed in the tests. I am finding that if the tests are run all together with the asserts enabled (run all StreamCommandTests), some of the assertions fail, with something like:

{code}
org.junit.ComparisonFailure: expected:<[DRACARYS!
]> but was:<[]>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)

{code}

When run individually the tests succeed. Not sure if it is timing (checking sinks before they've been written to) or something else...",2.0,0.9943164707969778,0.9943162708369964,0.9943150710771084,True,True,True,1375892257000.0,1390809989000.0,1390813589000.0,1390894359000.0,1390895259000.0,1375892257000.0,-1.0,1390809968000.0,1390813568000.0,1390809986000.0,1390813586000.0,1390809989000.0,1390813589000.0,"aclement",1390813568000.0,1375892257000.0,43,1375892257000.0,-1.0,"StreamCommandTests - asserting sink contents sometimes failing","There are some asserts in StreamCommandTests that are commented out (see the TODOs in there). These asserts are verifying the contents of the various sinks employed in the tests. I am finding that if the tests are run all together with the asserts enabled (run all StreamCommandTests), some of the assertions fail, with something like:

{code}
org.junit.ComparisonFailure: expected:<[DRACARYS!
]> but was:<[]>
	at org.junit.Assert.assertEquals(Assert.java:115)
	at org.junit.Assert.assertEquals(Assert.java:144)

{code}

When run individually the tests succeed. Not sure if it is timing (checking sinks before they've been written to) or something else...",2.0,False,2.0,0,0.0,0,0,False
"XD-629","Bug","","2013-08-07T16:13:42.000+0000","aclement","","Problem with tapping on a module using a named sink channel","{code}
mystream = http | transform --payload=expression.toUpperCase() > :foo
tap mystream.transform | log
{code}
This appears to fail because we can't tap into whatever was created to represent the named channel 'foo'. There is an @Ignore test in StreamCommandTests called testTappingModulesVariationsWithSinkChannel() which checks this.

(The parser is currently resolving 'tap mystream.transform' to 'tap --channel=foo'.)",2.0,-1.0,-1.0,-1.0,False,False,False,1375892022000.0,-1.0,-1.0,1382337573000.0,1382338473000.0,1375892022000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"aclement",-1.0,-1.0,0,1375892022000.0,-1.0,"Problem with tapping on a module using a named sink channel","{code}
mystream = http | transform --payload=expression.toUpperCase() > :foo
tap mystream.transform | log
{code}
This appears to fail because we can't tap into whatever was created to represent the named channel 'foo'. There is an @Ignore test in StreamCommandTests called testTappingModulesVariationsWithSinkChannel() which checks this.

(The parser is currently resolving 'tap mystream.transform' to 'tap --channel=foo'.)",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-628","Bug","","2013-08-07T14:40:09.000+0000","grenfro","","Streams created without a '|' (substreams) are being typed by the parser as a Job","",3.0,-1.0,-1.0,-1.0,False,False,False,1375886409000.0,-1.0,-1.0,1382337598000.0,1382338498000.0,1375886409000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1375886409000.0,-1.0,"Streams created without a '|' (substreams) are being typed by the parser as a Job","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-626","Bug","Sprint 12","2013-08-07T12:54:03.000+0000","hillert","hillert","Shell: RestTemplate not posting using UTF-8 ","The *http post* command uses the default MediaType by the RestTemplate, which in return triggers the default *StringHttpMessageConverter* which itself uses the default charset *ISO-8859-1*.

This creates issues when posting special characters.",1.0,0.9874134144281128,0.0,0.9871262037506336,True,True,True,1375880043000.0,1375938488000.0,1375939233000.0,1375938333000.0,1375939233000.0,1375880043000.0,-1.0,1375938471000.0,1375939233000.0,1375880043000.0,1375883643000.0,1375938488000.0,1375939233000.0,"hillert",1375939233000.0,1375880043000.0,43,1375880043000.0,-1.0,"Shell: RestTemplate not posting using UTF-8 ","The *http post* command uses the default MediaType by the RestTemplate, which in return triggers the default *StringHttpMessageConverter* which itself uses the default charset *ISO-8859-1*.

This creates issues when posting special characters.",1.0,False,1.0,0,0.0,0,1,False
"XD-625","Bug","","2013-08-07T10:58:54.000+0000","grussell","","Revert XD-364 When SI 3.0.M3 is Available","",1.0,-1.0,-1.0,-1.0,False,False,False,1375873134000.0,-1.0,-1.0,1381457363000.0,1381458263000.0,1375873134000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grussell",-1.0,-1.0,0,1375873134000.0,-1.0,"Revert XD-624 When SI 3.0.M3 is Available","",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-624","Bug","Sprint 12","2013-08-07T10:58:16.000+0000","grussell","grussell","Use External Connection Factory in TCP Syslog Source","WARN log emitted because the embedded connection factory does not get an application event publisher.

Will be fixed in SI M3 (INT-3107).",1.0,0.9887295081967212,0.0,0.1301229508196721,True,True,True,1375873096000.0,1375874061000.0,1375874072000.0,1375873172000.0,1375874072000.0,1375873096000.0,-1.0,1375873223000.0,1375874072000.0,1375873096000.0,1375874072000.0,1375874061000.0,1375874072000.0,"grussell",1375874072000.0,1375873096000.0,43,1375873096000.0,-1.0,"Use External Connection Factory in TCP Syslog Source","WARN log emitted because the embedded connection factory does not get an application event publisher.

Will be fixed in SI M3 (INT-3107).",1.0,False,1.0,0,0.0,-1,0,False
"XD-622","Story","Sprint 12","2013-08-07T07:50:03.000+0000","grussell","grussell","Fix JavaDoc Warnings","/home/gpr/Documents/github/spring-xd/spring-xd-analytics/src/main/java/org/springframework/xd/store/AbstractRedisRepository.java:196: warning - @param argument ""the"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-rest-domain/src/main/java/org/springframework/xd/rest/client/util/RestTemplateMessageConverterUtil.java:63: warning - Tag @link: reference not found: StreamDefinitionResource.Page
/home/gpr/Documents/github/spring-xd/spring-xd-rest-client/src/main/java/org/springframework/xd/rest/client/TapOperations.java:40: warning - @param argument ""control"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/OptionUtils.java:29: warning - @parame is an unknown tag.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/MissingRequiredDefinitionException.java:38: warning - @param argument ""name"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/StreamServer.java:102: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/TapDefinition.java:57: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @param argument ""group"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @param argument ""index"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:101: warning - @param argument ""group"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:101: warning - @param argument ""index"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/json/TypedJsonMapper.java:133: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-shell/src/main/java/org/springframework/xd/shell/util/UiUtils.java:60: warning - @CloudApplication is an unknown tag.
/home/gpr/Documents/github/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleJsonMarshaller.java:26: warning - @param argument ""tupleToStringConverter"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleJsonMarshaller.java:26: warning - @param argument ""stringToTupleConverter"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/dsl/DSLException.java:89: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/dsl/StreamLookupEnvironment.java:60: warning - @return tag has no arguments.
",1.0,0.002666074205732,0.0,0.0022217285047767,True,True,True,1375861803000.0,1375861875000.0,1375865475000.0,1375887909000.0,1375888809000.0,1375861803000.0,-1.0,1375861863000.0,1375865463000.0,1375861803000.0,1375865403000.0,1375861875000.0,1375865475000.0,"grussell",1375865463000.0,1375861803000.0,43,1375861803000.0,-1.0,"Fix JavaDoc Warnings","/home/gpr/Documents/github/spring-xd/spring-xd-analytics/src/main/java/org/springframework/xd/store/AbstractRedisRepository.java:196: warning - @param argument ""the"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-rest-domain/src/main/java/org/springframework/xd/rest/client/util/RestTemplateMessageConverterUtil.java:63: warning - Tag @link: reference not found: StreamDefinitionResource.Page
/home/gpr/Documents/github/spring-xd/spring-xd-rest-client/src/main/java/org/springframework/xd/rest/client/TapOperations.java:40: warning - @param argument ""control"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/server/options/OptionUtils.java:29: warning - @parame is an unknown tag.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/MissingRequiredDefinitionException.java:38: warning - @param argument ""name"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/StreamServer.java:102: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/TapDefinition.java:57: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @param argument ""group"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:93: warning - @param argument ""index"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:101: warning - @param argument ""group"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-module/src/main/java/org/springframework/xd/module/AbstractPlugin.java:101: warning - @param argument ""index"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/integration/x/json/TypedJsonMapper.java:133: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-shell/src/main/java/org/springframework/xd/shell/util/UiUtils.java:60: warning - @CloudApplication is an unknown tag.
/home/gpr/Documents/github/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleJsonMarshaller.java:26: warning - @param argument ""tupleToStringConverter"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-tuple/src/main/java/org/springframework/xd/tuple/TupleJsonMarshaller.java:26: warning - @param argument ""stringToTupleConverter"" is not a parameter name.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/dsl/DSLException.java:89: warning - @return tag has no arguments.
/home/gpr/Documents/github/spring-xd/spring-xd-dirt/src/main/java/org/springframework/xd/dirt/stream/dsl/StreamLookupEnvironment.java:60: warning - @return tag has no arguments.
",1.0,False,1.0,0,0.0,0,0,False
"XD-620","Story","Sprint 12","2013-08-06T22:19:45.000+0000","mark.pollack","mark.pollack","Refactor test cases to move away from inheritance model of utility methods for streams, counters ","Model the API more akin to SpringXDOperations api.",2.0,0.0255653883972468,0.0,0.015732546705998,True,True,True,1375827585000.0,1375827611000.0,1375828602000.0,1375827702000.0,1375828602000.0,1375827585000.0,-1.0,1375827601000.0,1375828602000.0,1375827585000.0,1375828602000.0,1375827611000.0,1375828602000.0,"mark.pollack",1375828602000.0,1375827585000.0,43,1375827585000.0,-1.0,"Refactor test cases to move away from inheritance model of utility methods for streams, counters ","Model the API more akin to SpringXDOperations api.",2.0,False,2.0,0,0.0,-1,0,False
"XD-619","Story","Sprint 12","2013-08-06T20:47:22.000+0000","mark.pollack","iperumal","Add list command for AggregateCounter","",2.0,0.0053256238587948,0.0,0.0038040170419963,True,True,True,1375822042000.0,1375822077000.0,1375825677000.0,1375827714000.0,1375828614000.0,1375822042000.0,-1.0,1375822067000.0,1375825667000.0,1375822042000.0,1375825642000.0,1375822077000.0,1375825677000.0,"mark.pollack",1375825667000.0,1375822042000.0,43,1375822042000.0,-1.0,"Add list command for AggregateCounter","",2.0,False,2.0,0,0.0,-1,0,False
"XD-618","Story","Sprint 12","2013-08-06T15:57:29.000+0000","mark.pollack","mark.pollack","Update to Spring Shell 1.1.0.M1 release","",1.0,0.0395908751364707,0.0,0.0132735735218065,True,True,True,1375804649000.0,1375805338000.0,1375808938000.0,1375821152000.0,1375822052000.0,1375804649000.0,-1.0,1375804880000.0,1375808480000.0,1375804649000.0,1375808249000.0,1375805338000.0,1375808938000.0,"mark.pollack",1375808480000.0,1375804649000.0,43,1375804649000.0,-1.0,"Update to Spring Shell 1.1.0.M1 release","",1.0,False,1.0,0,0.0,-1,0,False
"XD-617","Story","Sprint 12","2013-08-06T15:55:47.000+0000","mark.pollack","","Making an http post with json double quoted will hang the shell.","Was following the (now updated) directions for gemfire-cq source.

xd:> stream create --name stocks --definition ""http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')""
xd:> stream create --name cqtest --definition ""gemfire-cq --query='Select * from /Stocks where symbol=''VMW''' | file""
xd:> http post --target http://localhost:9090 --data ""{""symbol"":""VMW"", ""price"":73}""

The double quotes were causing a problem with xd-singlenode

Aug 06, 2013 5:38:15 PM org.jboss.netty.channel.SimpleChannelUpstreamHandler
WARNING: EXCEPTION, please implement org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.exceptionCaught() for proper handling.
org.springframework.integration.transformer.MessageTransformationException: org.springframework.integration.MessageHandlingException: org.springframework.integration.transformer.MessageTransformationException: Expected a ':' after a key at 22 [character 23 line 1]
	at org.springframework.integration.transformer.MessageTransformingHandler.handleRequestMessage(MessageTransformingHandler.java:73)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)

Using single quotes inside the json brackets worked, need to investigate.",2.0,-1.0,-1.0,0.9973103232259954,False,True,False,1375804547000.0,-1.0,-1.0,1376298502000.0,1376299402000.0,1375804547000.0,-1.0,1376298071000.0,1376299402000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1376299402000.0,1375804547000.0,43,1375804547000.0,-1.0,"Making an http post with json double quoted will hang the shell.","Was following the (now updated) directions for gemfire-cq source.

xd:> stream create --name stocks --definition ""http --port=9090 | gemfire-json-server --regionName=Stocks --keyExpression=payload.getField('symbol')""
xd:> stream create --name cqtest --definition ""gemfire-cq --query='Select * from /Stocks where symbol=''VMW''' | file""
xd:> http post --target http://localhost:9090 --data ""{""symbol"":""VMW"", ""price"":73}""

The double quotes were causing a problem with xd-singlenode

Aug 06, 2013 5:38:15 PM org.jboss.netty.channel.SimpleChannelUpstreamHandler
WARNING: EXCEPTION, please implement org.springframework.integration.x.http.NettyHttpInboundChannelAdapter$Handler.exceptionCaught() for proper handling.
org.springframework.integration.transformer.MessageTransformationException: org.springframework.integration.MessageHandlingException: org.springframework.integration.transformer.MessageTransformationException: Expected a ':' after a key at 22 [character 23 line 1]
	at org.springframework.integration.transformer.MessageTransformingHandler.handleRequestMessage(MessageTransformingHandler.java:73)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)

Using single quotes inside the json brackets worked, need to investigate.",2.0,False,2.0,0,0.0,0,0,False
"XD-614","Story","Sprint 12","2013-08-06T11:14:44.000+0000","grussell","grussell","Conversion Enhancements","Content-Type during transport transit is not the same as the content-type within modules.

""Real"" transports always use byte[] which may contain raw byte[] from a source, a byte[] converted from a String (which may or may not already contain JSON), or a byte[] containing JSON converted by the transport on the outbound side.

The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message.

Retain any content-type header that already exists in the message, and restore it.

For Rabbit, use normal SI/Rabbit headers to convey this information.

For Redis, add the information to the byte[].",8.0,0.0004177234074295,0.0,0.0002784822716196,True,True,True,1375787684000.0,1375787726000.0,1375791326000.0,1375887329000.0,1375888229000.0,1375787684000.0,-1.0,1375787712000.0,1375791312000.0,1375787684000.0,1375791284000.0,1375787726000.0,1375791326000.0,"grussell",1375791312000.0,1375787684000.0,43,1375787684000.0,-1.0,"Conversion Enhancements","Content-Type during transport transit is not the same as the content-type within modules.

""Real"" transports always use byte[] which may contain raw byte[] from a source, a byte[] converted from a String (which may or may not already contain JSON), or a byte[] containing JSON converted by the transport on the outbound side.

The transport needs to convey which of these was applied on the outbound side so it can properly reconstruct the message.

Retain any content-type header that already exists in the message, and restore it.

For Rabbit, use normal SI/Rabbit headers to convey this information.

For Redis, add the information to the byte[].",8.0,False,8.0,0,0.0,0,0,False
"XD-613","Story","Sprint 16","2013-08-06T10:14:09.000+0000","jencompgeek","iperumal","Deployed streams should be restarted on container start","When using Redis store, stored deployed streams should be deployed on container restart.",5.0,0.1999341498634403,0.1999336467716899,0.1999334638292353,True,True,True,1375784049000.0,1380155569000.0,1380159169000.0,1397647948000.0,1397648848000.0,1375784049000.0,-1.0,1380155554000.0,1380159154000.0,1380155558000.0,1380159158000.0,1380155569000.0,1380159169000.0,"jencompgeek",1380159154000.0,1375784049000.0,43,1375784049000.0,-1.0,"Deployed streams should be restarted on container start","When using Redis store, stored deployed streams should be deployed on container restart.",5.0,False,5.0,0,0.0,0,0,False
"XD-612","Story","Sprint 12","2013-08-06T08:33:56.000+0000","mark.pollack","mark.fisher","Create a rabbit sink module and documentation","https://github.com/springsource/spring-xd/wiki/Sinks

should have 'rabbit' added to the list and also the corresponding section that shows some basic usage.",2.0,0.0035537657854935,0.0,0.0017908010355098,True,True,True,1375778036000.0,1375778419000.0,1375782019000.0,1375884909000.0,1375885809000.0,1375778036000.0,-1.0,1375778229000.0,1375781829000.0,1375778036000.0,1375781636000.0,1375778419000.0,1375782019000.0,"mark.pollack",1375781829000.0,1375778036000.0,43,1375778036000.0,-1.0,"Create a rabbit sink module and documentation","https://github.com/springsource/spring-xd/wiki/Sinks

should have 'rabbit' added to the list and also the corresponding section that shows some basic usage.",2.0,False,2.0,0,0.0,0,0,False
"XD-611","Story","Sprint 12","2013-08-06T08:32:57.000+0000","mark.pollack","","Documentation for file source","http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources

should have 'file' added to the list and also the corresponding section that shows some basic usage.",1.0,0.8473826864391081,0.858136632640128,0.0012333401581405,True,True,True,1375777977000.0,1375951804000.0,1375955404000.0,1375982211000.0,1375983111000.0,1375777977000.0,-1.0,1375778230000.0,1375781830000.0,1375954010000.0,1375957610000.0,1375951804000.0,1375955404000.0,"mark.pollack",1375781830000.0,1375777977000.0,43,1375777977000.0,-1.0,"Documentation for file source","http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources

should have 'file' added to the list and also the corresponding section that shows some basic usage.",1.0,False,1.0,0,0.0,-1,0,False
"XD-610","Story","Sprint 12","2013-08-06T08:32:05.000+0000","mark.pollack","","Documentation for jms source","http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources

should have 'jms' added to the list and also the corresponding section that shows some basic usage.",1.0,0.9682398047297764,0.9681995072190112,0.0017558201119119,True,True,True,1375777925000.0,1375946116000.0,1375949716000.0,1375950733000.0,1375951633000.0,1375777925000.0,-1.0,1375778230000.0,1375781830000.0,1375946109000.0,1375949709000.0,1375946116000.0,1375949716000.0,"mark.pollack",1375781830000.0,1375777925000.0,43,1375777925000.0,-1.0,"Documentation for jms source","http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources

should have 'jms' added to the list and also the corresponding section that shows some basic usage.",1.0,False,1.0,0,0.0,-1,0,False
"XD-609","Story","Sprint 12","2013-08-06T08:31:46.000+0000","mark.pollack","","Documentation for rabbit source","http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources

should have 'rabbit' added to the list and also the corresponding section that shows some basic usage.",1.0,0.8092907061808333,0.8093374009681386,0.0024748237271779,True,True,True,1375777906000.0,1375881895000.0,1375885495000.0,1375905500000.0,1375906400000.0,1375777906000.0,-1.0,1375778224000.0,1375781824000.0,1375881901000.0,1375885501000.0,1375881895000.0,1375885495000.0,"mark.pollack",1375781824000.0,1375777906000.0,43,1375777906000.0,-1.0,"Documentation for rabbit source","http://static.springsource.org/spring-xd/docs/1.0.0.BUILD-SNAPSHOT/reference/html/#sources

should have 'rabbit' added to the list and also the corresponding section that shows some basic usage.",1.0,False,1.0,0,0.0,0,0,False
"XD-608","Bug","","2013-08-06T08:23:25.000+0000","hillert","hillert","Fix text-table rendering","",1.0,-1.0,0.0,-1.0,False,False,True,1375777405000.0,-1.0,-1.0,1376271996000.0,1376272896000.0,1375777405000.0,-1.0,-1.0,-1.0,1375777405000.0,1375781005000.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1375777405000.0,-1.0,"Fix text-table rendering","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-606","Story","Sprint 12","2013-08-06T03:51:18.000+0000","eric.bottard","","Shell quoting behavior unclear","This may be an issue following the search/replace from curl to Shell, but for example, this documentation line does not work:

http post --target http://localhost:9000 --data ""{\""symbol\"":\""VMW\"",\""price\"":72.04}""

The backslash prior to quote is left in the payload (and hence Jackson chokes on it)

We need clear rules about quoting at the shell level",3.0,-1.0,-1.0,0.0858261211470579,False,True,False,1375761078000.0,-1.0,-1.0,1375986531000.0,1375987431000.0,1375761078000.0,-1.0,1375780505000.0,1375784105000.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",1375784105000.0,1375761078000.0,43,1375761078000.0,-1.0,"Document JSON quoting behavior in shell","This may be an issue following the search/replace from curl to Shell, but for example, this documentation line does not work:

http post --target http://localhost:9000 --data ""{\""symbol\"":\""VMW\"",\""price\"":72.04}""

The backslash prior to quote is left in the payload (and hence Jackson chokes on it)

We need clear rules about quoting at the shell level",3.0,False,3.0,0,0.0,0,0,True
"XD-605","Story","Sprint 12","2013-08-06T03:04:16.000+0000","eric.bottard","","Rich Gauge doco is outdated","The RichGauge section does not mention the ""alpha"" parameter in redis output, nor does it explain its meaning.",2.0,0.6969735879597204,0.6969580239996265,0.000140075640846,True,True,True,1375758256000.0,1375937381000.0,1375940981000.0,1376014360000.0,1376015260000.0,1375758256000.0,-1.0,1375758292000.0,1375761892000.0,1375937377000.0,1375940977000.0,1375937381000.0,1375940981000.0,"eric.bottard",1375761892000.0,1375758256000.0,43,1375758256000.0,-1.0,"Rich Gauge doco is outdated","The RichGauge section does not mention the ""alpha"" parameter in redis output, nor does it explain its meaning.",2.0,False,2.0,0,0.0,0,0,False
"XD-603","Bug","Sprint 12","2013-08-05T15:48:37.000+0000","dturanski","","NPE on stream destroy","xd:>stream create ticktock --definition ""time | log"" --deploy true
18:45:13,310  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/streams"" resulted in 400 (Bad Request); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: There is already a stream named 'ticktock'

xd:>stream destroy ticktock
18:45:16,505  WARN Spring Shell client.RestTemplate:524 - DELETE request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.NullPointerException


Caused by: java.lang.NullPointerException
	at org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:143)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:97)


",2.0,0.9219213441713562,0.986035624388758,0.000480977345967,True,True,True,1375717717000.0,1375775220000.0,1375778820000.0,1375779190000.0,1375780090000.0,1375717717000.0,-1.0,1375717747000.0,1375721347000.0,1375779219000.0,1375780090000.0,1375775220000.0,1375778820000.0,"dturanski",1375721347000.0,1375717717000.0,43,1375717717000.0,-1.0,"NPE on stream destroy","xd:>stream create ticktock --definition ""time | log"" --deploy true
18:45:13,310  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/streams"" resulted in 400 (Bad Request); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: There is already a stream named 'ticktock'

xd:>stream destroy ticktock
18:45:16,505  WARN Spring Shell client.RestTemplate:524 - DELETE request for ""http://localhost:8080/streams/ticktock"" resulted in 500 (Internal Server Error); invoking error handler
Command failed org.springframework.xd.rest.client.impl.SpringXDException: java.lang.NullPointerException


Caused by: java.lang.NullPointerException
	at org.springframework.xd.dirt.module.ModuleDeployer.undeploy(ModuleDeployer.java:143)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:97)


",2.0,False,2.0,0,0.0,0,0,False
"XD-600","Bug","Sprint 13","2013-08-05T09:27:30.000+0000","dturanski","iperumal","Add stream destroy all, stream undeploy all, stream deploy all commands","",5.0,0.8058569562326084,0.776482158067605,0.7765226657184876,True,True,True,1375694850000.0,1376411032000.0,1376414632000.0,1376582671000.0,1376583571000.0,1375694850000.0,-1.0,1376384962000.0,1376388562000.0,1376384926000.0,1376388526000.0,1376411032000.0,1376414632000.0,"dturanski",1376388562000.0,1375694850000.0,43,1375694850000.0,-1.0,"Add deploy/undeploy/destroy 'all' commands for all applicable resources (streams, tap, job & trigger,)","",5.0,False,5.0,0,0.0,0,0,True
"XD-599","Bug","Sprint 12","2013-08-05T07:46:17.000+0000","grussell","thomas.risberg","Gradle Import Broken by Hadoop Pseudo Projects","When importing Spring-XD as a gradle project, in STS, while building the model, we get

Root exception:
java.lang.IllegalArgumentException: Project location doesn't exist: 
.../spring-xd/spring-xd-hadoop/hadoop11

./gradlew eclipse creates these directories, but the plugin needs them before running that task

The problem seems to be that these ""projects"" are not really projects.

Perhaps a quick fix would be to commit these directories (with a dummy file) ??",1.0,0.9734174116684264,0.649486871284886,0.6498960590107156,True,True,True,1375688777000.0,1375955214000.0,1375958814000.0,1375961590000.0,1375962490000.0,1375688777000.0,-1.0,1375866662000.0,1375870262000.0,1375866550000.0,1375870150000.0,1375955214000.0,1375958814000.0,"grussell",1375870262000.0,1375688777000.0,43,1375688777000.0,-1.0,"Gradle Import Broken by Hadoop Pseudo Projects","When importing Spring-XD as a gradle project, in STS, while building the model, we get

Root exception:
java.lang.IllegalArgumentException: Project location doesn't exist: 
.../spring-xd/spring-xd-hadoop/hadoop11

./gradlew eclipse creates these directories, but the plugin needs them before running that task

The problem seems to be that these ""projects"" are not really projects.

Perhaps a quick fix would be to commit these directories (with a dummy file) ??",1.0,False,1.0,0,0.0,-1,1,False
"XD-598","Story","Sprint 12","2013-08-04T09:07:30.000+0000","dturanski","","Gemfire cached closed when a gemfire module is undeployed","Need to investigate why this is happening, normally setting 
{code:xml}
<gfe:client-cache close=""false""/>
{code}
prevents the (singleton) cache from closing when the application context is closed. ",4.0,0.9911998783943714,0.9901521156122084,0.0001954376173982,True,True,True,1375607250000.0,1375789831000.0,1375791452000.0,1375790552000.0,1375791452000.0,1375607250000.0,1375789689000.0,1375607286000.0,1375610886000.0,1375789638000.0,1375791452000.0,1375789831000.0,1375791452000.0,"dturanski",1375610886000.0,1375607250000.0,43,1375789689000.0,-1.0,"Gemfire cache closed when a gemfire module is undeployed","Need to investigate why this is happening, normally setting 
{code:xml}
<gfe:client-cache close=""false""/>
{code}
prevents the (singleton) cache from closing when the application context is closed. ",0.0,False,0.0,1,-1.0,0,0,True
"XD-597","Story","Sprint 13","2013-08-02T12:08:55.000+0000","grenfro","","Ad-Hoc Job needs to have option for launch and forget","When running an ad-hoc job without the use of a trigger (adhoc or named).  The user has to wait for job to complete before receiving a success.  We need to launch a job and get a success back to the user letting them know the job has been launched.

for example --immediate",4.0,0.6851332788521681,0.6851312676524673,0.5726904555836602,True,True,True,1375445335000.0,1376467312000.0,1376470912000.0,1376936082000.0,1376936982000.0,1375445335000.0,-1.0,1376299587000.0,1376303187000.0,1376467309000.0,1376470909000.0,1376467312000.0,1376470912000.0,"grenfro",1376303187000.0,1375445335000.0,43,1375445335000.0,-1.0,"Ad-Hoc Job needs to have option for launch and forget","When running an ad-hoc job without the use of a trigger (adhoc or named).  The user has to wait for job to complete before receiving a success.  We need to launch a job and get a success back to the user letting them know the job has been launched.

for example --immediate",4.0,False,4.0,0,0.0,0,0,False
"XD-596","Story","Sprint 12","2013-08-02T11:01:57.000+0000","hillert","hillert","Add CONTRIBUTING.md file","Add CONTRIBUTING.md file, use the Spring Integration file as the basis.",1.0,4.163526673633634e-05,0.0,0.0236363409262181,True,True,True,1375441317000.0,1375441327000.0,1375444927000.0,1375680598000.0,1375681498000.0,1375441317000.0,-1.0,1375446994000.0,1375450594000.0,1375441317000.0,1375444917000.0,1375441327000.0,1375444927000.0,"hillert",1375450594000.0,1375441317000.0,43,1375441317000.0,-1.0,"Add CONTRIBUTING.md file","Add CONTRIBUTING.md file, use the Spring Integration file as the basis.",1.0,False,1.0,0,0.0,0,0,False
"XD-595","Story","Sprint 12","2013-08-02T10:11:46.000+0000","iperumal","eric.bottard","Fix wiki documentation to use xd shell command prompt to read ""xd:>""","We need to fix the github wiki to use the xd shell command prompt ""xd:>"".",1.0,0.9979616548940464,0.9979455095862764,0.9978607467204844,True,True,True,1375438306000.0,1375685551000.0,1375686056000.0,1375685156000.0,1375686056000.0,1375438306000.0,-1.0,1375685526000.0,1375686056000.0,1375685547000.0,1375686056000.0,1375685551000.0,1375686056000.0,"iperumal",1375686056000.0,1375438306000.0,43,1375438306000.0,-1.0,"Fix wiki documentation to use xd shell command prompt to read ""xd:>""","We need to fix the github wiki to use the xd shell command prompt ""xd:>"".",1.0,False,1.0,0,0.0,-1,0,False
"XD-594","Story","Sprint 12","2013-08-02T09:54:56.000+0000","iperumal","iperumal","Create delete commands for all the metrics","We need to add delete commands for the metrics:

InMemoryAggregateCounter
FieldValueCounter
Gauge
RichGauge

Currently, the AbstractMetricsController class has the delete method to delete the metric from the repository. We can probably use the same for all the metrics.",2.0,0.0046091445427728,0.0,0.0045476892822025,True,True,True,1375437296000.0,1375438421000.0,1375442021000.0,1375680476000.0,1375681376000.0,1375437296000.0,-1.0,1375438406000.0,1375442006000.0,1375437296000.0,1375440896000.0,1375438421000.0,1375442021000.0,"iperumal",1375442006000.0,1375437296000.0,43,1375437296000.0,-1.0,"Create list/delete commands for all the metrics","We need to add list/delete commands for the metrics:

InMemoryAggregateCounter
FieldValueCounter
Gauge
RichGauge

Currently, the AbstractMetricsController class has the delete method to delete the metric from the repository. We can probably use the same for all the metrics.",2.0,False,2.0,0,0.0,0,0,True
"XD-593","Story","Sprint 12","2013-08-02T07:44:21.000+0000","iperumal","iperumal","Add ""counter delete"" shell command","Add ""counter delete"" shell command. This also requires implementation of DELETE rest end point at CountersController.",1.0,0.00037306472673,0.0003055955740235,0.0002460639686943,True,True,True,1375429461000.0,1375429555000.0,1375433155000.0,1375680528000.0,1375681428000.0,1375429461000.0,-1.0,1375429523000.0,1375433123000.0,1375429538000.0,1375433138000.0,1375429555000.0,1375433155000.0,"iperumal",1375433123000.0,1375429461000.0,43,1375429461000.0,-1.0,"Add ""counter delete"" shell command","Add ""counter delete"" shell command. This also requires implementation of DELETE rest end point at CountersController.",1.0,False,1.0,0,0.0,0,0,False
"XD-592","Bug","Sprint 12","2013-08-01T11:44:17.000+0000","aclement","","Problems with tapping and '>' syntax","Start of a test program that can be placed in StreamCommandTests:

@Test
public void testTappingAndChannels() {
  executeStreamCreate(""myhttp"",""http --port=9314 | transform --expression=payload.toUpperCase() | log"",true);
  executeStreamCreate(""tap"",""tap @myhttp.1 | log"",true);		
  executeStreamCreate(""tap_new"",""tap myhttp.1 > log"",true);				
  executeCommand(""http post --data Dracarys! --target http://localhost:9314"");
  // TODO verify both logs output DRACARYS!
}

In the test program see two taps. One using the older style and one using the newer style and '>' so that there is no real tap module source, the log module just gets its input channel wired directly to myhttp.1 (the output of transform).  They should be doing the same thing.  However when run the output for tap_new is missing, all I see is:

{code}
11:39:36,055  WARN New I/O worker #28 logger.tap:141 - DRACARYS!
11:39:36,059  WARN New I/O worker #28 logger.myhttp:141 - DRACARYS!
{code}

No errors are reported, there is just no output for tap_new.",8.0,0.7288880780213819,-1.0,0.125378940286667,True,True,False,1375357457000.0,1375804909000.0,1375808509000.0,1375970440000.0,1375971340000.0,1375357457000.0,1375971224000.0,1375434425000.0,1375438025000.0,-1.0,-1.0,1375804909000.0,1375808509000.0,"aclement",1375438025000.0,1375357457000.0,43,1375971224000.0,1375971224000.0,"Problems with advanced tapping","Start of a test program that can be placed in StreamCommandTests:

{code}
@Test
public void testTappingAndChannels() {
  executeStreamCreate(""myhttp"",""http --port=9314 | transform --expression=payload.toUpperCase() | log"",true);
  executeStreamCreate(""tap"",""tap @myhttp.1 | log"",true);		
  executeStreamCreate(""tap_new"",""tap myhttp.1 > log"",true);				
  executeCommand(""http post --data Dracarys! --target http://localhost:9314"");
  // TODO verify both logs output DRACARYS!
}

{code}

In the test program see two taps. One using the older style and one using the newer style and '>' so that there is no real tap module source, the log module just gets its input channel wired directly to myhttp.1 (the output of transform).  They should be doing the same thing.  However when run the output for tap_new is missing, all I see is:

{code}
11:39:36,055  WARN New I/O worker #28 logger.tap:141 - DRACARYS!
11:39:36,059  WARN New I/O worker #28 logger.myhttp:141 - DRACARYS!
{code}

No errors are reported, there is just no output for tap_new.",3.0,True,3.0,1,166.66666666666669,0,0,True
"XD-591","Story","Sprint 12","2013-07-31T10:00:26.000+0000","mark.pollack","thomas.risberg","Improve build file distribution tasks","The current flow of gradle tasks is confusing.  Suggest the following changes to simplify the flow.

1. Move the current task logic in zipXD to distZip
2. Have distZip depend on dist
3. Update the 'how to build docs' on the wiki
4. Make sure that the distZip task only shows up once in the list of gradle target. ",1.0,0.4165662081028696,0.0,5.909327282182196e-05,True,True,True,1375264826000.0,1375335319000.0,1375338919000.0,1375433150000.0,1375434050000.0,1375264826000.0,-1.0,1375264836000.0,1375268436000.0,1375264826000.0,1375268426000.0,1375335319000.0,1375338919000.0,"mark.pollack",1375268436000.0,1375264826000.0,43,1375264826000.0,-1.0,"Improve build file distribution tasks","The current flow of gradle tasks is confusing.  Suggest the following changes to simplify the flow.

1. Move the current task logic in zipXD to distZip
2. Have distZip depend on dist
3. Update the 'how to build docs' on the wiki
4. Make sure that the distZip task only shows up once in the list of gradle target. ",1.0,False,1.0,0,0.0,0,0,False
"XD-589","Story","Sprint 12","2013-07-30T23:13:49.000+0000","mark.pollack","mark.pollack","Create AbstractStreamIntegrationTest that will destory streams that were created during test method execution","Keep track of named streams that were create and use @After to destroy them.",1.0,0.0011815841667721,0.0,0.0006329915179136,True,True,True,1375226029000.0,1375226057000.0,1375229657000.0,1375248826000.0,1375249726000.0,1375226029000.0,-1.0,1375226044000.0,1375229644000.0,1375226029000.0,1375229629000.0,1375226057000.0,1375229657000.0,"mark.pollack",1375229644000.0,1375226029000.0,43,1375226029000.0,-1.0,"Create AbstractStreamIntegrationTest that will destory streams that were created during test method execution","Keep track of named streams that were create and use @After to destroy them.",1.0,False,1.0,0,0.0,-1,0,False
"XD-588","Story","Sprint 13","2013-07-30T12:06:09.000+0000","eric.bottard","","RedisAggregateCounterRepository doesn't give proper results back","Both Luke's original code and my refactored PR[1] (which uses same code snippet) seem to behave strangely.

Stored values seem fine, but the getCounts() method seems phony.

To test:
1) stream create foo --definition ""time|log""
2) tap create bar --definition ""tap@foo | aggregatecounter""
3) curl -H ""application/json"" http://localhost:8080/metrics/aggregate-counters/bar

this gives default bucketing (hourly) but chances are that they are empty. ",3.0,-1.0,0.986497731726304,0.9341680535795774,False,True,True,1375185969000.0,-1.0,-1.0,1376377165000.0,1376378065000.0,1375185969000.0,-1.0,1376299587000.0,1376303187000.0,1376361969000.0,1376365569000.0,-1.0,-1.0,"eric.bottard",1376303187000.0,1375185969000.0,43,1375185969000.0,-1.0,"RedisAggregateCounterRepository doesn't give proper results back","Both Luke's original code and my refactored PR[1] (which uses same code snippet) seem to behave strangely.

Stored values seem fine, but the getCounts() method seems phony.

To test:
1) stream create foo --definition ""time|log""
2) tap create bar --definition ""tap@foo | aggregatecounter""
3) curl -H ""application/json"" http://localhost:8080/metrics/aggregate-counters/bar

this gives default bucketing (hourly) but chances are that they are empty. ",3.0,False,3.0,0,0.0,0,0,False
"XD-587","Story","Sprint 12","2013-07-30T11:15:29.000+0000","grenfro","grenfro","Create a abstract base class for rest controllers","",3.0,0.4463747134756907,0.4463626492942454,0.4919592230667149,True,True,True,1375182929000.0,1375256929000.0,1375260529000.0,1375347809000.0,1375348709000.0,1375182929000.0,-1.0,1375264486000.0,1375268086000.0,1375256927000.0,1375260527000.0,1375256929000.0,1375260529000.0,"grenfro",1375268086000.0,1375182929000.0,43,1375182929000.0,-1.0,"Create a abstract base class for rest controllers","",3.0,False,3.0,0,0.0,-1,0,False
"XD-586","Story","","2013-07-30T09:32:39.000+0000","luke","","Make channel queue size configurable when using local transport","",3.0,-1.0,0.9999523750620076,-1.0,False,False,True,1375176759000.0,-1.0,-1.0,1390923911000.0,1390924811000.0,1375176759000.0,-1.0,-1.0,-1.0,1390924061000.0,1390924811000.0,-1.0,-1.0,"luke",-1.0,-1.0,0,1375176759000.0,-1.0,"Document queue channel capacity configurable when using local transport","",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-585","Bug","Sprint 12","2013-07-30T09:28:55.000+0000","dturanski","dturanski","Deploying with twittersearch source throws Jackson ClassDefNotFound exception","The upgrade to Jackson 2.2 included the following change to the build script
{code}
project('spring-xd-dirt') {
	description = 'Spring XD DIRT'
	configurations {
	  [runtime,testRuntime]*.exclude group: 'org.codehaus.jackson'
	}
{code}

Spring social twitter template depends on these classes
",1.0,0.8944647201946472,0.8929440389294404,0.0139902676399026,True,True,True,1375176535000.0,1375179476000.0,1375179823000.0,1375178923000.0,1375179823000.0,1375176535000.0,1375179492000.0,1375176581000.0,1375179823000.0,1375179471000.0,1375179823000.0,1375179476000.0,1375179823000.0,"dturanski",1375179823000.0,1375179492000.0,43,1375179492000.0,-1.0,"Deploying with twittersearch source throws Jackson ClassDefNotFound exception","The upgrade to Jackson 2.2 included the following change to the build script
{code}
project('spring-xd-dirt') {
	description = 'Spring XD DIRT'
	configurations {
	  [runtime,testRuntime]*.exclude group: 'org.codehaus.jackson'
	}
{code}

Spring social twitter template depends on these classes
",1.0,False,1.0,0,0.0,0,0,False
"XD-583","Bug","Sprint 12","2013-07-30T08:38:18.000+0000","dturanski","","Dispatcher Has No Subscriber Error when posting a message to a stream","This has been observed intermittently with Redis transport by myself and others when sending a message to a valid stream. Not sure how to recreate it yet.

11:27:10,082 ERROR ThreadPoolTaskScheduler-1 redis.RedisQueueInboundChannelAdapter:126 - Error sending message
org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'org.springframework.context.support.GenericApplicationContext@3f73865d.input'.
",5.0,0.888520341540258,0.8885336491676911,0.8645549679868387,True,True,True,1375173498000.0,1375707640000.0,1375711240000.0,1375773757000.0,1375774657000.0,1375173498000.0,1375707709000.0,1375693233000.0,1375696833000.0,1375707648000.0,1375711248000.0,1375707640000.0,1375711240000.0,"dturanski",1375696833000.0,1375173498000.0,43,1375774633000.0,1375707709000.0,"Dispatcher Has No Subscriber Error when posting a message to a stream","This has been observed intermittently with Redis transport by myself and others when sending a message to a valid stream. Not sure how to recreate it yet.

11:27:10,082 ERROR ThreadPoolTaskScheduler-1 redis.RedisQueueInboundChannelAdapter:126 - Error sending message
org.springframework.integration.MessageDeliveryException: Dispatcher has no subscribers for channel 'org.springframework.context.support.GenericApplicationContext@3f73865d.input'.
",0.0,True,0.0,2,-1.0,0,0,False
"XD-582","Story","Sprint 12","2013-07-30T07:31:34.000+0000","luke","eric.bottard","Support named channels when using local transport","Sending data to an incomplete stream which is created using a named sink channel only works when using Redis (or Rabbit?, not tested). Since the in-memory version doesn't use a queue, it will fail if you are using xd-singlenode.

We should use a queue channel with unlimited capacity to allow messages to be sent before the full stream is created.",4.0,0.1611932252122737,0.1562846098402795,0.1554837613315181,True,True,True,1375169494000.0,1375252018000.0,1375255618000.0,1375680551000.0,1375681451000.0,1375169494000.0,-1.0,1375249095000.0,1375252695000.0,1375249505000.0,1375253105000.0,1375252018000.0,1375255618000.0,"luke",1375252695000.0,1375169494000.0,43,1375169494000.0,-1.0,"Support named channels when using local transport","Sending data to an incomplete stream which is created using a named sink channel only works when using Redis (or Rabbit?, not tested). Since the in-memory version doesn't use a queue, it will fail if you are using xd-singlenode.

We should use a queue channel with unlimited capacity to allow messages to be sent before the full stream is created.",4.0,False,4.0,0,0.0,0,1,False
"XD-581","Story","Sprint 12","2013-07-30T02:18:20.000+0000","mark.pollack","","configuration conflict when using ""--transport"", ""local"", ""--store"", ""redis"", ""--disableJmx"", ""true"", ""--analytics"", ""redis""","results in both in-memory and redis based definitions of RichGaugeService - can't satisfy autowiring because there are two candidates.

Had to change --analytics=memory to get the application context to load.",2.0,-1.0,0.999796535753487,0.0031269242095681,False,True,True,1375150700000.0,-1.0,-1.0,1375336565000.0,1375337465000.0,1375150700000.0,-1.0,1375151284000.0,1375154884000.0,1375337427000.0,1375337465000.0,-1.0,-1.0,"mark.pollack",1375154884000.0,1375150700000.0,43,1375150700000.0,-1.0,"configuration conflict when using ""--transport"", ""local"", ""--store"", ""redis"", ""--disableJmx"", ""true"", ""--analytics"", ""redis""","results in both in-memory and redis based definitions of RichGaugeService - can't satisfy autowiring because there are two candidates.

Had to change --analytics=memory to get the application context to load.",2.0,False,2.0,0,0.0,0,1,False
"XD-580","Story","","2013-07-30T01:32:16.000+0000","eric.bottard","","XD Shell needs to support multiple Hadoop distros","From https://github.com/SpringSource/spring-xd/pull/161:

""The command shell needs to also support different hadoop distribution options. Perhaps the shell just uses a relative path to the location of xd/lib/""",3.0,-1.0,-1.0,-1.0,False,False,False,1375147936000.0,-1.0,-1.0,1375175095000.0,1375175995000.0,1375147936000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1375147936000.0,-1.0,"XD Shell needs to support multiple Hadoop distros","From https://github.com/SpringSource/spring-xd/pull/161:

""The command shell needs to also support different hadoop distribution options. Perhaps the shell just uses a relative path to the location of xd/lib/""",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-579","Story","Sprint 12","2013-07-30T00:00:09.000+0000","mark.pollack","thomas.risberg","Modify startup script of xd shell to allow specifying hadoop distro to use","",1.0,0.1886748844375963,0.0059257832562917,0.0001476630713918,True,True,True,1375142409000.0,1375171797000.0,1375175397000.0,1375297269000.0,1375298169000.0,1375142409000.0,-1.0,1375142432000.0,1375146032000.0,1375143332000.0,1375146932000.0,1375171797000.0,1375175397000.0,"mark.pollack",1375146032000.0,1375142409000.0,43,1375142409000.0,-1.0,"Modify startup script of xd shell to allow specifying hadoop distro to use","",1.0,False,1.0,0,0.0,-1,0,False
"XD-578","Story","Sprint 16","2013-07-29T23:54:54.000+0000","mark.pollack","thomas.risberg","Can't access HDFS using webhdfs protocol","http://localhost:8080:>hadoop config fs --namenode webhdfs://localhost:50070
http://localhost:8080:>hadoop fs ls /
Hadoop configuration changed, re-initializing shell...
run HDFS shell failed. Message is: org/mortbay/util/ajax/JSON

This was on a hadoop 1.0.1 install

The hdfs http interface was available

$ curl -i ""http://localhost:50070/webhdfs/v1/tmp?op=GETFILESTATUS""
HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
Server: Jetty(6.1.26)

{""FileStatus"":{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1365015846724,""owner"":""mpollack"",""pathSuffix"":"""",""permission"":""777"",""replication"":0,""type"":""DIRECTORY""}}
",3.0,0.8409980658198046,0.8409729955410953,0.8409931500788812,True,True,True,1375142094000.0,1380274574000.0,1380278174000.0,1381244038000.0,1381244938000.0,1375142094000.0,-1.0,1380274544000.0,1380278144000.0,1380274421000.0,1380278021000.0,1380274574000.0,1380278174000.0,"mark.pollack",1380278144000.0,1375142094000.0,43,1375142094000.0,-1.0,"Can't access HDFS using webhdfs protocol","http://localhost:8080:>hadoop config fs --namenode webhdfs://localhost:50070
http://localhost:8080:>hadoop fs ls /
Hadoop configuration changed, re-initializing shell...
run HDFS shell failed. Message is: org/mortbay/util/ajax/JSON

This was on a hadoop 1.0.1 install

The hdfs http interface was available

$ curl -i ""http://localhost:50070/webhdfs/v1/tmp?op=GETFILESTATUS""
HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
Server: Jetty(6.1.26)

{""FileStatus"":{""accessTime"":0,""blockSize"":0,""group"":""supergroup"",""length"":0,""modificationTime"":1365015846724,""owner"":""mpollack"",""pathSuffix"":"""",""permission"":""777"",""replication"":0,""type"":""DIRECTORY""}}
",3.0,False,3.0,0,0.0,0,0,False
"XD-577","Story","Sprint 12","2013-07-29T23:29:33.000+0000","mark.pollack","","In documentation, replace usage of 'raw' hadoop command with shell 'hadoop' commands","",1.0,-1.0,-1.0,0.9897843359818388,False,True,False,1375140573000.0,-1.0,-1.0,1375141435000.0,1375142335000.0,1375140573000.0,-1.0,1375142317000.0,1375142335000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1375142335000.0,1375140573000.0,43,1375140573000.0,-1.0,"In documentation, replace usage of 'raw' hadoop command with shell 'hadoop' commands","",1.0,False,1.0,0,0.0,-1,0,False
"XD-576","Story","Sprint 12","2013-07-29T23:24:39.000+0000","mark.pollack","","Change banner of shell to say only 'xd'","",1.0,0.1688091519931145,0.1689167381190685,0.0076353547570998,True,True,True,1375140279000.0,1375192058000.0,1375195658000.0,1375446110000.0,1375447010000.0,1375140279000.0,-1.0,1375142621000.0,1375146221000.0,1375192091000.0,1375195691000.0,1375192058000.0,1375195658000.0,"mark.pollack",1375146221000.0,1375140279000.0,43,1375140279000.0,-1.0,"Change banner of shell to say only 'xd'","",1.0,False,1.0,0,0.0,0,0,False
"XD-575","Story","Sprint 12","2013-07-29T23:20:14.000+0000","mark.pollack","","Change http command to post data by putting 'http' as the main command option","The current http command is of the form

http://localhost:8080:>post httpsource --target http://localhost:9090 --data 10


It isn't intuitive to think 'post', rather the command can be 

http post --target http://localhost:9090 --data 10

which will allow us to have support for other http verbs and cleanly separate the namespace from 'hadoop' etc.

The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous.
",1.0,0.9850627344979224,0.985012304837213,0.0261629079759551,True,True,True,1375140014000.0,1375237681000.0,1375239162000.0,1375238262000.0,1375239162000.0,1375140014000.0,-1.0,1375142608000.0,1375146208000.0,1375237676000.0,1375239162000.0,1375237681000.0,1375239162000.0,"mark.pollack",1375146208000.0,1375140014000.0,43,1375140014000.0,-1.0,"Change http command to post data by putting 'http' as the main command option","The current http command is of the form

http://localhost:8080:>post httpsource --target http://localhost:9090 --data 10


It isn't intuitive to think 'post', rather the command can be 

http post --target http://localhost:9090 --data 10

which will allow us to have support for other http verbs and cleanly separate the namespace from 'hadoop' etc.

The RestShell from which this came was only concerned with http actions, so the leading command classification probably seemed superfluous.
",1.0,False,1.0,0,0.0,0,1,False
"XD-574","Story","Sprint 12","2013-07-29T23:17:22.000+0000","mark.pollack","","Replace usage of 'raw' curl with shell command to post http data in documentation","e.g. http://localhost:8080:>post httpsource --target http://localhost:9090 --data 10

I believe this will also help to avoid ugly syntax to escaping quotes for json as in the gemfire example.",1.0,0.3174284843331355,0.3173883745928861,0.0223571692149721,True,True,True,1375139842000.0,1375179412000.0,1375183012000.0,1375263600000.0,1375264500000.0,1375139842000.0,1375275927000.0,1375142629000.0,1375146229000.0,1375179407000.0,1375183007000.0,1375179412000.0,1375183012000.0,"mark.pollack",1375146229000.0,1375140393000.0,43,1375140393000.0,1375275927000.0,"Replace usage of 'raw' curl with shell command to post http data in documentation","e.g. http://localhost:8080:>post httpsource --target http://localhost:9090 --data 10

I believe this will also help to avoid ugly syntax to escaping quotes for json as in the gemfire example.",2.0,True,2.0,1,0.0,0,0,False
"XD-572","Story","Sprint 12","2013-07-29T10:47:53.000+0000","mark.pollack","","Prepare Blog post for XD M2","",5.0,0.7421156024058373,0.7421263961262168,0.5898527404425757,True,True,True,1375094873000.0,1375988680000.0,1375992280000.0,1376298377000.0,1376299277000.0,1375094873000.0,-1.0,1375805294000.0,1375808894000.0,1375988693000.0,1375992293000.0,1375988680000.0,1375992280000.0,"mark.pollack",1375808894000.0,1375094873000.0,43,1375094873000.0,-1.0,"Prepare Blog post for XD M2","",5.0,False,5.0,0,0.0,-1,0,False
"XD-571","Story","Sprint 12","2013-07-29T10:36:35.000+0000","mark.pollack","","Create shell integration test for named chanels","Expected usage (ATM) would be

// sink channel called foo
http | transform --expression=payload.toUppercase() > :foo
// source channel called foo
:foo > count | file",3.0,0.7524936209696126,0.7524786555024281,0.0005612050194176,True,True,True,1375094195000.0,1375194759000.0,1375198359000.0,1375226936000.0,1375227836000.0,1375094195000.0,-1.0,1375094270000.0,1375097870000.0,1375194757000.0,1375198357000.0,1375194759000.0,1375198359000.0,"mark.pollack",1375097870000.0,1375094195000.0,43,1375094195000.0,-1.0,"Create shell integration test for named chanels","Expected usage (ATM) would be

// sink channel called foo
http | transform --expression=payload.toUppercase() > :foo
// source channel called foo
:foo > count | file",3.0,False,3.0,0,0.0,0,0,False
"XD-569","Story","Sprint 12","2013-07-29T10:30:39.000+0000","mark.pollack","luke","Investigate failures to start a stream when using named channels.","Create a reproducible series of steps or shell integration test.
",2.0,0.91655769103758,0.0,0.0056931510468264,True,True,True,1375093839000.0,1375163227000.0,1375166827000.0,1375168644000.0,1375169544000.0,1375093839000.0,-1.0,1375094270000.0,1375097870000.0,1375093839000.0,1375097439000.0,1375163227000.0,1375166827000.0,"mark.pollack",1375097870000.0,1375093839000.0,43,1375093839000.0,-1.0,"Investigate failures to start a stream when using named channels.","Create a reproducible series of steps or shell integration test.
",2.0,False,2.0,0,0.0,0,0,False
"XD-568","Story","Sprint 12","2013-07-29T10:22:02.000+0000","mark.pollack","","Documentation for deleting triggers","",1.0,0.1952121203666243,0.1802011690685118,0.0219165469231735,True,True,True,1375093322000.0,1375107315000.0,1375110915000.0,1375164103000.0,1375165003000.0,1375093322000.0,-1.0,1375094893000.0,1375098493000.0,1375106239000.0,1375109839000.0,1375107315000.0,1375110915000.0,"mark.pollack",1375098493000.0,1375093322000.0,43,1375093322000.0,-1.0,"Documentation for deleting triggers","",1.0,False,1.0,0,0.0,-1,0,False
"XD-567","Story","Sprint 12","2013-07-29T10:21:21.000+0000","mark.pollack","","Documentation for fixed rate triggers","",1.0,0.9790797291695454,0.9791488185712311,0.0222744231034959,True,True,True,1375093281000.0,1375164137000.0,1375165651000.0,1375164751000.0,1375165651000.0,1375093281000.0,-1.0,1375094893000.0,1375098493000.0,1375164142000.0,1375165651000.0,1375164137000.0,1375165651000.0,"mark.pollack",1375098493000.0,1375093281000.0,43,1375093281000.0,-1.0,"Documentation for fixed rate triggers","",1.0,False,1.0,0,0.0,-1,0,False
"XD-566","Story","Sprint 12","2013-07-29T10:19:24.000+0000","mark.pollack","thomas.risberg","Document JDBC module","",1.0,0.9986109357242076,0.0,0.0017678999873721,True,True,True,1375093164000.0,1375101072000.0,1375101083000.0,1375100183000.0,1375101083000.0,1375093164000.0,-1.0,1375093178000.0,1375096778000.0,1375093164000.0,1375096764000.0,1375101072000.0,1375101083000.0,"mark.pollack",1375096778000.0,1375093164000.0,43,1375093164000.0,-1.0,"Document JDBC module","",1.0,False,1.0,0,0.0,-1,0,False
"XD-565","Story","Sprint 12","2013-07-29T10:19:00.000+0000","mark.pollack","thomas.risberg","Documentation for using a specific Hadoop distribution","Show how to select a specific hadoop distribution when starting embedded/standalone XDContainer.",1.0,0.0327385499729518,0.0,0.0001569203959349,True,True,True,1375093140000.0,1375101068000.0,1375104668000.0,1375334401000.0,1375335301000.0,1375093140000.0,-1.0,1375093178000.0,1375096778000.0,1375093140000.0,1375096740000.0,1375101068000.0,1375104668000.0,"mark.pollack",1375096778000.0,1375093140000.0,43,1375093140000.0,-1.0,"Documentation for using a specific Hadoop distribution","Show how to select a specific hadoop distribution when starting embedded/standalone XDContainer.",1.0,False,1.0,0,0.0,-1,0,False
"XD-563","Story","Sprint 12","2013-07-29T10:15:30.000+0000","mark.pollack","","Regression test existing functionality of stream/taps based on introduction of new conversion functionality","make sure nothing is broken - spot check using.

1) ticktock
2) twitter
3) gemfire


",2.0,0.0998615797090454,0.0998486967450475,0.0001116523546479,True,True,True,1375092930000.0,1375162693000.0,1375166293000.0,1375790627000.0,1375791527000.0,1375092930000.0,-1.0,1375093008000.0,1375096608000.0,1375162684000.0,1375166284000.0,1375162693000.0,1375166293000.0,"mark.pollack",1375096608000.0,1375092930000.0,43,1375092930000.0,-1.0,"Regression test existing functionality of stream/taps based on introduction of new conversion functionality","make sure nothing is broken - spot check using.

1) ticktock
2) twitter
3) gemfire


",2.0,False,2.0,0,0.0,0,0,False
"XD-561","Story","Sprint 12","2013-07-29T10:01:51.000+0000","mark.pollack","","Failure when creating/deploying stream leaves invalid stream registry/definitions in the Repository implementations.","reproduce
1) Create a bad stream definition name 'bad'
Try to recreate with the same name, but correct stream definitions.  The system will report that the stream already exists.

",4.0,0.6313858999527845,0.6313842432676458,0.000270039677609,True,True,True,1375092111000.0,1375854339000.0,1375857939000.0,1376298441000.0,1376299341000.0,1375092111000.0,-1.0,1375092437000.0,1375096037000.0,1375854337000.0,1375857937000.0,1375854339000.0,1375857939000.0,"mark.pollack",1375096037000.0,1375092111000.0,43,1375092111000.0,-1.0,"Failure when creating/deploying stream leaves invalid stream registry/definitions in the Repository implementations.","reproduce
1) Create a bad stream definition name 'bad'
Try to recreate with the same name, but correct stream definitions.  The system will report that the stream already exists.

",4.0,False,4.0,0,0.0,0,1,False
"XD-560","Bug","Sprint 12","2013-07-29T09:43:13.000+0000","grenfro","","Ad-hoc Jobs do not start","After XD-554 the ModuleJobExecutor.start does not get called.  Thus jobs that are ""run one time"" do not fire.",3.0,0.6867062580130143,0.6866881153390261,0.0235250006047558,True,True,True,1375090993000.0,1375204544000.0,1375208144000.0,1375255449000.0,1375256349000.0,1375090993000.0,-1.0,1375094883000.0,1375098483000.0,1375204541000.0,1375208141000.0,1375204544000.0,1375208144000.0,"grenfro",1375098483000.0,1375090993000.0,43,1375090993000.0,-1.0,"Ad-hoc Jobs do not start","After XD-554 the ModuleJobExecutor.start does not get called.  Thus jobs that are ""run one time"" do not fire.",3.0,False,3.0,0,0.0,0,0,False
"XD-559","Story","","2013-07-29T09:12:09.000+0000","mark.pollack","hillert","Send failing sonar build message to spring-xd mailing list.","",1.0,0.9363057324840764,0.0,-1.0,False,False,False,1375089129000.0,1375089570000.0,1375089600000.0,1375089129000.0,1375089600000.0,1375089129000.0,-1.0,-1.0,-1.0,1375089129000.0,1375089600000.0,1375089570000.0,1375089600000.0,"mark.pollack",-1.0,-1.0,0,1375089129000.0,-1.0,"Send failing sonar build message to spring-xd mailing list.","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-556","Bug","Sprint 12","2013-07-26T09:52:09.000+0000","aeisenberg","","CORS support","XD instances will not accept xhr requests from browsers whose page origin does not match the XD instance.  

As an example, the Kodiak UI is served from a different process (and url) than the XD instance.  When users open the Kodiak UI in a browser, requests from the browser to the XD instance, but these requests will fail due to cross-site scripting limitations.

CORS (Cross-Origin Resource Sharing) is a way to get around this.  We can configure the server to accept requests from browsers whose origins are not the XD instance.

I have this working in a local branch and will submit a pull request.


More information:
CORS Spec: http://www.w3.org/TR/cors/
SPR-9278 CORS support for SpringFramework
",5.0,0.8252356044193077,-1.0,0.8251753353655635,True,True,False,1374832329000.0,1375092487000.0,1375096087000.0,1375146682000.0,1375147582000.0,1374832329000.0,-1.0,1375092468000.0,1375096068000.0,-1.0,-1.0,1375092487000.0,1375096087000.0,"aeisenberg",1375096068000.0,1374832329000.0,43,1374832329000.0,-1.0,"CORS support","XD instances will not accept xhr requests from browsers whose page origin does not match the XD instance.  

As an example, the Kodiak UI is served from a different process (and url) than the XD instance.  When users open the Kodiak UI in a browser, requests from the browser to the XD instance, but these requests will fail due to cross-site scripting limitations.

CORS (Cross-Origin Resource Sharing) is a way to get around this.  We can configure the server to accept requests from browsers whose origins are not the XD instance.

I have this working in a local branch and will submit a pull request.


More information:
CORS Spec: http://www.w3.org/TR/cors/
SPR-9278 CORS support for SpringFramework
",5.0,False,5.0,0,0.0,-1,0,False
"XD-555","Story","Sprint 12","2013-07-26T06:50:53.000+0000","mark.fisher","eric.bottard","make application/json the default output type for the REST API?","the top-level URL works via simple curl (without an Accept header) or the browser:

{code}
> curl http://localhost:8080

{""links"":[{""rel"":""streams"",""href"":""http://localhost:8080/streams""},{""rel"":""triggers"",""href"":""http://localhost:8080/triggers""},{""rel"":""jobs"",""href"":""http://localhost:8080/jobs""},{""rel"":""taps"",""href"":""http://localhost:8080/taps""},{""rel"":""counters"",""href"":""http://localhost:8080/metrics/counters""}]}
{code}

However, trying any of those links then fails, e.g.:

{code}
> curl http://localhost:8080/streams

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><errors xmlns:ns2=""http://www.w3.org/2005/Atom""><error logref=""HttpMessageNotWritableException""><message>Could not marshal [PagedResource { content: [links: [&lt;http://localhost:8080/streams/mqttdemo&gt;;rel=&quot;self&quot;]], metadata: Metadata { number: 0, total pages: 1, total elements: 1, size: 20 }, links: [] }]: null; nested exception is javax.xml.bind.MarshalException
 - with linked exception:
[com.sun.istack.internal.SAXException2: unable to marshal type &quot;org.springframework.xd.rest.client.domain.StreamDefinitionResource&quot; as an element because it is not known to this context.]</message></error></errors>
{code}
",2.0,-1.0,0.9999936573980108,0.9999735724917124,False,True,True,1374821453000.0,-1.0,-1.0,1375766537000.0,1375767437000.0,1374821453000.0,-1.0,1375767412000.0,1375767437000.0,1375767431000.0,1375767437000.0,-1.0,-1.0,"mark.fisher",1375767437000.0,1374821453000.0,43,1374821453000.0,-1.0,"make application/json the default output type for the REST API?","the top-level URL works via simple curl (without an Accept header) or the browser:

{code}
> curl http://localhost:8080

{""links"":[{""rel"":""streams"",""href"":""http://localhost:8080/streams""},{""rel"":""triggers"",""href"":""http://localhost:8080/triggers""},{""rel"":""jobs"",""href"":""http://localhost:8080/jobs""},{""rel"":""taps"",""href"":""http://localhost:8080/taps""},{""rel"":""counters"",""href"":""http://localhost:8080/metrics/counters""}]}
{code}

However, trying any of those links then fails, e.g.:

{code}
> curl http://localhost:8080/streams

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><errors xmlns:ns2=""http://www.w3.org/2005/Atom""><error logref=""HttpMessageNotWritableException""><message>Could not marshal [PagedResource { content: [links: [&lt;http://localhost:8080/streams/mqttdemo&gt;;rel=&quot;self&quot;]], metadata: Metadata { number: 0, total pages: 1, total elements: 1, size: 20 }, links: [] }]: null; nested exception is javax.xml.bind.MarshalException
 - with linked exception:
[com.sun.istack.internal.SAXException2: unable to marshal type &quot;org.springframework.xd.rest.client.domain.StreamDefinitionResource&quot; as an element because it is not known to this context.]</message></error></errors>
{code}
",2.0,False,2.0,0,0.0,-1,1,False
"XD-554","Story","Sprint 12","2013-07-26T05:45:50.000+0000","grussell","grussell","Separate Module Context Refresh from Context Start","Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start.

In the Stream plugin, wire the module into the {{ChannelRegistry}} during post processing, instead of using the {{ChannelRegistrar}}.",5.0,0.0024685411523872,0.0,0.0020470829068577,True,True,True,1374817550000.0,1374817591000.0,1374821191000.0,1374833259000.0,1374834159000.0,1374817550000.0,-1.0,1374817584000.0,1374821184000.0,1374817550000.0,1374821150000.0,1374817591000.0,1374821191000.0,"grussell",1374821184000.0,1374817550000.0,43,1374817550000.0,-1.0,"Separate Module Context Refresh from Context Start","Split plugin module processing into pre and post processing where preprocessing is done before the context is refreshed and post processing is done after the refresh, but before the start.

In the Stream plugin, wire the module into the {{ChannelRegistry}} during post processing, instead of using the {{ChannelRegistrar}}.",5.0,False,5.0,0,0.0,0,0,False
"XD-553","Story","Sprint 12","2013-07-26T02:28:59.000+0000","eric.bottard","eric.bottard","File source needs improvements","Seems like the current file source results from an initial POC.
Very few things can be parameterized, including the polled directory that needs to be in /tmp/xxx

To be useful in production, we might want to revisit",3.0,0.9592008860546616,0.9591889949836668,0.9591949405191642,True,True,True,1374805739000.0,1375935058000.0,1375938658000.0,1375982193000.0,1375983093000.0,1374805739000.0,-1.0,1375935051000.0,1375938651000.0,1375935044000.0,1375938644000.0,1375935058000.0,1375938658000.0,"eric.bottard",1375938651000.0,1374805739000.0,43,1374805739000.0,-1.0,"Add additional options to File source","Seems like the current file source results from an initial POC.
Very few things can be parameterized, including the polled directory that needs to be in /tmp/xxx

To be useful in production, we might want to revisit",3.0,False,3.0,0,0.0,-1,1,True
"XD-552","Improvement","Sprint 13","2013-07-25T10:33:50.000+0000","kparikh","","Add status column for 'stream list'  shell command result","{{stream list}} shell command should display status of the stream (deployed, undeployed)",1.0,0.7528904428035338,0.7528839181019457,0.7229168958079725,True,True,True,1374748430000.0,1376363901000.0,1376367501000.0,1376893222000.0,1376894122000.0,1374748430000.0,-1.0,1376299587000.0,1376303187000.0,1376363887000.0,1376367487000.0,1376363901000.0,1376367501000.0,"kparikh",1376303187000.0,1374748430000.0,43,1374748430000.0,-1.0,"Add status column for 'stream list'  shell command result","{{stream list}} shell command should display status of the stream (deployed, undeployed)",1.0,False,1.0,0,0.0,0,0,False
"XD-551","Story","Sprint 12","2013-07-25T08:32:33.000+0000","hillert","hillert","Add ""trigger list"" support to Spring XD Shell","",1.0,0.0054386379758808,0.0,0.0021281626862142,True,True,True,1374741153000.0,1374741245000.0,1374744845000.0,1374757169000.0,1374758069000.0,1374741153000.0,-1.0,1374741189000.0,1374744789000.0,1374741153000.0,1374744753000.0,1374741245000.0,1374744845000.0,"hillert",1374744789000.0,1374741153000.0,43,1374741153000.0,-1.0,"Add ""trigger list"" support to Spring XD Shell","",1.0,False,1.0,0,0.0,0,0,False
"XD-549","Story","Sprint 12","2013-07-25T05:40:31.000+0000","eric.bottard","iperumal","Display a Rich Gauge","",2.0,0.8645808273741072,0.8645649654783463,0.8645711339933645,True,True,True,1374730831000.0,1375711953000.0,1375715553000.0,1375864726000.0,1375865626000.0,1374730831000.0,-1.0,1375711942000.0,1375715542000.0,1375711935000.0,1375715535000.0,1375711953000.0,1375715553000.0,"eric.bottard",1375715542000.0,1374730831000.0,43,1374730831000.0,-1.0,"Display a Rich Gauge","",2.0,False,2.0,0,0.0,0,0,False
"XD-548","Story","Sprint 12","2013-07-25T05:40:06.000+0000","eric.bottard","iperumal","Display a Gauge","",1.0,0.7605042770016173,0.7604871300247462,0.760493365289063,True,True,True,1374730806000.0,1375706552000.0,1375710152000.0,1376012931000.0,1376013831000.0,1374730806000.0,1375711885000.0,1375706538000.0,1375710138000.0,1375706530000.0,1375710130000.0,1375706552000.0,1375710152000.0,"eric.bottard",1375710138000.0,1374730806000.0,43,1375711885000.0,1375711885000.0,"Display a Gauge","",2.0,True,2.0,1,-50.0,0,0,False
"XD-547","Story","Sprint 12","2013-07-25T05:39:47.000+0000","eric.bottard","iperumal","Display an Aggregate Counter","",3.0,0.8676657723672071,0.8391420822684807,0.8539384524024135,True,True,True,1374730787000.0,1376039935000.0,1376043535000.0,1376238703000.0,1376239603000.0,1374730787000.0,1376069703000.0,1376019223000.0,1376022823000.0,1375996898000.0,1376000498000.0,1376039935000.0,1376043535000.0,"eric.bottard",1376022823000.0,1374730787000.0,43,1376069703000.0,1376069703000.0,"Display an Aggregate Counter","",2.0,True,2.0,1,50.0,0,0,False
"XD-546","Story","Sprint 12","2013-07-25T05:39:17.000+0000","eric.bottard","","Display a Field Value Counter","",2.0,0.8789018247426272,0.8788925326095794,0.7806506816166763,True,True,True,1374730757000.0,1375865784000.0,1375869384000.0,1376021272000.0,1376022172000.0,1374730757000.0,-1.0,1375738901000.0,1375742501000.0,1375865772000.0,1375869372000.0,1375865784000.0,1375869384000.0,"eric.bottard",1375742501000.0,1374730757000.0,43,1374730757000.0,-1.0,"Display a Field Value Counter","",2.0,False,2.0,0,0.0,0,0,False
"XD-545","Story","Sprint 12","2013-07-25T05:38:49.000+0000","eric.bottard","eric.bottard","Display a counter","",2.0,0.0005853016868153,0.0,0.0003801443945295,True,True,True,1374730729000.0,1374730923000.0,1374734523000.0,1375061282000.0,1375062182000.0,1374730729000.0,-1.0,1374730855000.0,1374734455000.0,1374730729000.0,1374734329000.0,1374730923000.0,1374734523000.0,"eric.bottard",1374734455000.0,1374730729000.0,43,1374730729000.0,-1.0,"Display a counter","",2.0,False,2.0,0,0.0,0,0,False
"XD-544","Story","Sprint 12","2013-07-25T03:34:08.000+0000","eric.bottard","mark.pollack","Fix In-Memory Analytics","Most of the infrastructure and code cleanup has been done for In-Memory Analytics. The only remaining issue is that, by including memory-analytics.xml from common.xml, we're actually creating e.g. a new InMemoryCounterRepository that is different from the one present in the Admin process space.

This story involves fixing that. It may actually be done as part of XD-353, handling the ""local"" transport as a special case (context inheritance) rather than import based on xd.transport",3.0,0.9046333731799366,0.904621952179,0.9046105311780636,True,True,True,1374723248000.0,1376148990000.0,1376152590000.0,1376298392000.0,1376299292000.0,1374723248000.0,-1.0,1376148954000.0,1376152554000.0,1376148972000.0,1376152572000.0,1376148990000.0,1376152590000.0,"eric.bottard",1376152554000.0,1374723248000.0,43,1374723248000.0,-1.0,"Fix In-Memory Analytics","Most of the infrastructure and code cleanup has been done for In-Memory Analytics. The only remaining issue is that, by including memory-analytics.xml from common.xml, we're actually creating e.g. a new InMemoryCounterRepository that is different from the one present in the Admin process space.

This story involves fixing that. It may actually be done as part of XD-353, handling the ""local"" transport as a special case (context inheritance) rather than import based on xd.transport",3.0,False,3.0,0,0.0,0,0,False
"XD-543","Story","","2013-07-24T19:47:40.000+0000","hillert","","Handle Pagination in Spring XD Shell","",5.0,-1.0,-1.0,-1.0,False,False,False,1374695260000.0,-1.0,-1.0,1381419378000.0,1381420278000.0,1374695260000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1374695260000.0,-1.0,"Handle Pagination in Spring XD Shell","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-542","Story","Sprint 12","2013-07-24T14:24:08.000+0000","grussell","grussell","Refactor Module to Encapsulate Group and Index","Currently many methods take module, group, index - defining a module instance; group and index can be encapsulated in {{Module}} so one arg can be passed around.",2.0,0.0006717910261217,0.0,0.0003593300837395,True,True,True,1374675848000.0,1374675891000.0,1374679491000.0,1374738956000.0,1374739856000.0,1374675848000.0,-1.0,1374675871000.0,1374679471000.0,1374675848000.0,1374679448000.0,1374675891000.0,1374679491000.0,"grussell",1374679471000.0,1374675848000.0,43,1374675848000.0,-1.0,"Refactor Module to Encapsulate Group and Index","Currently many methods take module, group, index - defining a module instance; group and index can be encapsulated in {{Module}} so one arg can be passed around.",2.0,False,2.0,0,0.0,0,0,False
"XD-541","Story","","2013-07-24T14:09:34.000+0000","grenfro","","Add Message Source for error messages returned to users","",3.0,-1.0,-1.0,-1.0,False,False,False,1374674974000.0,-1.0,-1.0,1426364692000.0,1426365592000.0,1374674974000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1374674974000.0,-1.0,"Add Message Source for error messages returned to users","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-540","Story","Sprint 12","2013-07-24T10:48:47.000+0000","grussell","Gary Russell","Broadcast Undeploy Requests","Use an 'undeploy' topic to broadcast undeploy requests to all containers.

Applies to Redis and Rabbit transports, not local.

Also, rename {{ModuleDeploymentRequest}} to {{ModuleOperationRequest}} with an enum {{DEPLOY}}, {{UNDEPLOY}}.",5.0,0.0001410450755412,0.2443544312117689,0.0001246126395558,True,True,True,1374662927000.0,1374663030000.0,1374666630000.0,1375392290000.0,1375393190000.0,1374662927000.0,-1.0,1374663018000.0,1374666618000.0,1374841370000.0,1374844970000.0,1374663030000.0,1374666630000.0,"grussell",1374666618000.0,1374662927000.0,43,1374662927000.0,-1.0,"Broadcast Undeploy Requests","Use an 'undeploy' topic to broadcast undeploy requests to all containers.

Applies to Redis and Rabbit transports, not local.

Also, rename {{ModuleDeploymentRequest}} to {{ModuleOperationRequest}} with an enum {{DEPLOY}}, {{UNDEPLOY}}.",5.0,False,5.0,0,0.0,0,0,False
"XD-539","Story","Sprint 12","2013-07-24T09:20:00.000+0000","jencompgeek","jencompgeek","Investigate using Redis txs and pipeline for Inbound/Outbound Q Adapter perf improvements","Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre-release of SDR 1.1 M2.",5.0,0.0005400979145251,0.0004878303744098,0.0003658727808073,True,True,True,1374657600000.0,1374657662000.0,1374661262000.0,1374771494000.0,1374772394000.0,1374657600000.0,-1.0,1374657642000.0,1374661242000.0,1374657656000.0,1374661256000.0,1374657662000.0,1374661262000.0,"jencompgeek",1374661242000.0,1374657600000.0,43,1374657600000.0,-1.0,"Investigate using Redis txs and pipeline for Inbound/Outbound Q Adapter perf improvements","Investigate using transactions and pipelining to improve performance of both the inbound and outbound RedisQueue channel adapters. Involves testing against a pre-release of SDR 1.1 M2.",5.0,False,5.0,0,0.0,0,0,False
"XD-538","Story","Sprint 12","2013-07-23T22:46:14.000+0000","mark.pollack","mark.pollack","Develop infrastructure to enable testability of commands","This requires to boostrap the singlenode admin server in process, submit commands to the shell programmatically, and assert on the results of executing the command.",5.0,0.000438785377071,0.0,0.0002762722744521,True,True,True,1374619574000.0,1374619628000.0,1374623228000.0,1374741741000.0,1374742641000.0,1374619574000.0,-1.0,1374619608000.0,1374623208000.0,1374619574000.0,1374623174000.0,1374619628000.0,1374623228000.0,"mark.pollack",1374623208000.0,1374619574000.0,43,1374619574000.0,-1.0,"Develop infrastructure to enable testability of commands","This requires to boostrap the singlenode admin server in process, submit commands to the shell programmatically, and assert on the results of executing the command.",5.0,False,5.0,0,0.0,-1,0,False
"XD-536","Story","Sprint 12","2013-07-22T09:11:40.000+0000","mark.pollack","","Create shell integration tests for job lifeycle","creating job defs, deploying jobs, undeploying jobs, deleting job defs",4.0,0.2845486297952275,0.7140400739646418,0.0027084495145662,True,True,True,1374484300000.0,1374833308000.0,1374836908000.0,1375709932000.0,1375710832000.0,1374484300000.0,-1.0,1374487622000.0,1374491222000.0,1375360093000.0,1375363693000.0,1374833308000.0,1374836908000.0,"mark.pollack",1374491222000.0,1374484300000.0,43,1374484300000.0,-1.0,"Create shell integration tests for job lifeycle","creating job defs, deploying jobs, undeploying jobs, deleting job defs",4.0,False,4.0,0,0.0,0,0,False
"XD-534","Story","Sprint 12","2013-07-22T09:10:50.000+0000","mark.pollack","","Create shell integration tests for tap lifeycle","creating taps, deleting",2.0,0.6403941653436618,0.640330267894153,0.0027623358941465,True,True,True,1374484250000.0,1375265983000.0,1375269583000.0,1375704056000.0,1375704956000.0,1374484250000.0,1375275942000.0,1374487622000.0,1374491222000.0,1375265905000.0,1375269505000.0,1375265983000.0,1375269583000.0,"mark.pollack",1374491222000.0,1374484250000.0,43,1375275942000.0,1375275942000.0,"Create shell integration tests for tap lifeycle","creating taps, deleting",4.0,True,4.0,1,-50.0,0,0,False
"XD-533","Story","Sprint 12","2013-07-22T09:09:03.000+0000","mark.pollack","","Create shell integration tests for stream lifeycle","create, delete, deploy streams...",2.0,0.4007623787797989,0.4007790909491449,0.0052856033776813,True,True,True,1374484143000.0,1374747926000.0,1374751526000.0,1375141446000.0,1375142346000.0,1374484143000.0,-1.0,1374487622000.0,1374491222000.0,1374747937000.0,1374751537000.0,1374747926000.0,1374751526000.0,"mark.pollack",1374491222000.0,1374484143000.0,43,1374484143000.0,-1.0,"Create shell integration tests for stream lifeycle","create, delete, deploy streams...",2.0,False,2.0,0,0.0,-1,0,False
"XD-532","Story","Sprint 12","2013-07-22T09:03:48.000+0000","mark.pollack","","Adapt SpringOne 2012 UI code from keynote demo of election results to use XD","",6.0,0.8616475372240039,0.7468910501039716,0.0008595558681854,True,True,True,1374483828000.0,1378286058000.0,1378289658000.0,1378895672000.0,1378896572000.0,1374483828000.0,-1.0,1374487621000.0,1374491221000.0,1377779667000.0,1377783267000.0,1378286058000.0,1378289658000.0,"mark.pollack",1374491221000.0,1374483828000.0,43,1374483828000.0,-1.0,"Adapt SpringOne 2012 UI code from keynote demo of election results to use XD","Existing code: https://github.com/ghillert/springone2012",6.0,False,6.0,0,0.0,-1,2,True
"XD-531","Story","Sprint 12","2013-07-22T09:02:37.000+0000","mark.pollack","","Integration test for syslog-tcp-reactor module","",2.0,0.9846719671522284,0.9846694709041784,0.000688964461808,True,True,True,1374483757000.0,1380006208000.0,1380009808000.0,1380091274000.0,1380092174000.0,1374483757000.0,-1.0,1374487621000.0,1374491221000.0,1380006194000.0,1380009794000.0,1380006208000.0,1380009808000.0,"mark.pollack",1374491221000.0,1374483757000.0,43,1374483757000.0,-1.0,"Check for high CPU usage with syslog-tcp-reactor module","",2.0,False,2.0,0,0.0,0,0,True
"XD-530","Story","Sprint 12","2013-07-22T09:02:22.000+0000","mark.pollack","","Create XD module for syslog-tcp-reactor ","Still keep existing one.",3.0,-1.0,0.9999970235408157,0.0012828539084381,False,True,True,1374483742000.0,-1.0,-1.0,1377506569000.0,1377507469000.0,1374483742000.0,-1.0,1374487621000.0,1374491221000.0,1377507460000.0,1377507469000.0,-1.0,-1.0,"mark.pollack",1374491221000.0,1374483742000.0,43,1374483742000.0,-1.0,"Create XD module for syslog-tcp-reactor ","Still keep existing one.",3.0,False,3.0,0,0.0,-1,0,False
"XD-528","Story","Sprint 12","2013-07-22T09:01:20.000+0000","mark.pollack","","Create SI components that wrap Reactor's TCP server","",4.0,-1.0,0.5091481496842695,0.0002553969702311,False,True,True,1374483680000.0,-1.0,-1.0,1389913660000.0,1389914560000.0,1374483680000.0,-1.0,1374487621000.0,1374491221000.0,1382340284000.0,1382343884000.0,-1.0,-1.0,"mark.pollack",1374491221000.0,1374483680000.0,43,1374483680000.0,-1.0,"Create SI components that wrap Reactor's TCP server","",4.0,False,4.0,0,0.0,-1,0,False
"XD-526","Story","Sprint 12","2013-07-22T08:41:34.000+0000","eric.bottard","","Make module files classpath aware","Currently, living at the root of the project, those files don't benefit from IDE SI awareness.
Make it so that they belong to a java project which sees the correct version of the SI jars used.
Has impact on the build.gradle file",5.0,0.0766588882905522,0.0766349548255848,0.0306647519894692,True,True,True,1374482494000.0,1374495306000.0,1374498906000.0,1374648724000.0,1374649624000.0,1374482494000.0,-1.0,1374487619000.0,1374491219000.0,1374495302000.0,1374498902000.0,1374495306000.0,1374498906000.0,"eric.bottard",1374491219000.0,1374482494000.0,43,1374482494000.0,-1.0,"Make module files classpath aware","Currently, living at the root of the project, those files don't benefit from IDE SI awareness.
Make it so that they belong to a java project which sees the correct version of the SI jars used.
Has impact on the build.gradle file",5.0,False,5.0,0,0.0,0,0,False
"XD-525","Story","Sprint 12","2013-07-22T08:39:52.000+0000","eric.bottard","eric.bottard","Provide .settings formatting rules so that they're shared","Thinking about using the official SpringSource rules as a template",3.0,0.0450101126673544,0.0,0.0038082789813891,True,True,True,1374482392000.0,1374544170000.0,1374547770000.0,1375854028000.0,1375854928000.0,1374482392000.0,-1.0,1374487619000.0,1374491219000.0,1374482392000.0,1374485992000.0,1374544170000.0,1374547770000.0,"eric.bottard",1374491219000.0,1374482392000.0,43,1374482392000.0,-1.0,"Provide .settings formatting rules so that they're shared","Thinking about using the official SpringSource rules as a template",3.0,False,3.0,0,0.0,-1,0,False
"XD-524","Story","Sprint 11","2013-07-22T06:07:12.000+0000","eric.bottard","eric.bottard","Update jobs section to use shell","",1.0,0.0675039246467817,0.0,0.0298273155416012,False,False,False,1374473232000.0,1374473275000.0,1374473869000.0,1374473232000.0,1374473869000.0,1374473232000.0,-1.0,1374473251000.0,1374473869000.0,1374473232000.0,1374473869000.0,1374473275000.0,1374473869000.0,"eric.bottard",1374473869000.0,1374473232000.0,43,1374473232000.0,-1.0,"Update jobs section to use shell","",1.0,False,1.0,0,0.0,-1,0,False
"XD-523","Bug","Sprint 11","2013-07-22T02:41:09.000+0000","eric.bottard","aclement","Parser blows on modules names with '-'","Tried to create a module named ""tcp-poll"" and got this:
XD108E:(pos 3): missing expected character '-'
tcp-poll --host=54.208.22.193 --port=8081 | log


I believe this should be supported, and indeed we have several module names of this form already",5.0,0.6562594134238847,0.0,0.0001061149601555,True,True,True,1374460869000.0,1374652586000.0,1374656186000.0,1374752105000.0,1374753005000.0,1374460869000.0,-1.0,1374460900000.0,1374464500000.0,1374460869000.0,1374464469000.0,1374652586000.0,1374656186000.0,"eric.bottard",1374464500000.0,1374460869000.0,43,1374460869000.0,-1.0,"Parser blows on modules names with '-'","Tried to create a module named ""tcp-poll"" and got this:
XD108E:(pos 3): missing expected character '-'
tcp-poll --host=54.208.22.193 --port=8081 | log


I believe this should be supported, and indeed we have several module names of this form already",5.0,False,5.0,0,0.0,0,0,False
"XD-521","Bug","Sprint 19","2013-07-19T09:43:50.000+0000","grenfro","grenfro","The parser should be able to handle a parameter name with a '-' hyphen embedded.","Right now it treats it as a new parameter start and fails.",1.0,0.2025650954562174,0.9475619050680172,0.9534808450564874,True,True,True,1374227030000.0,1376483229000.0,1376486829000.0,1385364273000.0,1385365173000.0,1374227030000.0,-1.0,1384847036000.0,1384850636000.0,1384781110000.0,1384784710000.0,1376483229000.0,1376486829000.0,"grenfro",1384850636000.0,1384781651000.0,43,1384781651000.0,-1.0,"The parser should be able to handle a parameter name with a '-' hyphen embedded.","Right now it treats it as a new parameter start and fails.",1.0,False,1.0,0,0.0,0,0,False
"XD-518","Story","Sprint 11","2013-07-19T09:25:26.000+0000","dturanski","dturanski","Upgrade to Jackson 2.2.2","",2.0,-1.0,0.0,0.9973024008632316,False,True,True,1374225926000.0,-1.0,-1.0,1374232440000.0,1374233340000.0,1374225926000.0,-1.0,1374233320000.0,1374233340000.0,1374225926000.0,1374229526000.0,-1.0,-1.0,"dturanski",1374233340000.0,1374225926000.0,43,1374225926000.0,-1.0,"Upgrade to Jackson 2.2.2","",2.0,False,2.0,0,0.0,-1,0,False
"XD-517","Improvement","","2013-07-19T02:39:55.000+0000","eric.bottard","","Admin Server Banner and Info","Standalone Admin currently has no shiny banner as container has.
More importantly, it does not say which port it's listening on, the transport used, etc.",3.0,-1.0,-1.0,-1.0,False,False,False,1374201595000.0,-1.0,-1.0,1384762197000.0,1384763097000.0,1374201595000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1374201595000.0,-1.0,"Add Server Runtime Info to Banner","Standalone Admin currently has no shiny banner as container has.
More importantly, it does not say which port it's listening on, the transport used, etc.",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-515","Bug","","2013-07-19T02:22:11.000+0000","eric.bottard","","Address already in use for tomcat should fail completly","Currently, process is left running and if logs / sysout are not monitored, you have no clue",3.0,-1.0,-1.0,-1.0,False,False,False,1374200531000.0,-1.0,-1.0,1389966827000.0,1389967727000.0,1374200531000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1374200531000.0,-1.0,"Address already in use for tomcat/hsqldb should fail completly","Currently, process is left running and if logs / sysout are not monitored, you have no clue",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-514","Story","","2013-07-18T12:40:10.000+0000","hillert","","Create proper test coverage for Controllers","Create proper test coverage for Controllers",8.0,-1.0,-1.0,-1.0,False,False,False,1374151210000.0,-1.0,-1.0,1382338904000.0,1382339804000.0,1374151210000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1374151210000.0,-1.0,"Create proper test coverage for Controllers","Create proper test coverage for Controllers",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-513","Story","","2013-07-18T10:14:35.000+0000","kparikh","kparikh","Add CI job in bamboo to run XD integration tests","CI job will run integration tests that are tagged for CI build.",1.0,-1.0,0.0,-1.0,False,False,True,1374142475000.0,-1.0,-1.0,1381484011000.0,1381484911000.0,1374142475000.0,-1.0,-1.0,-1.0,1374142475000.0,1374146075000.0,-1.0,-1.0,"kparikh",-1.0,-1.0,0,1374142475000.0,-1.0,"Add CI job in bamboo to run XD integration tests","CI job will run integration tests that are tagged for CI build.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-510","Story","","2013-07-18T08:33:38.000+0000","hillert","","Ensure that each controller's list() returns PagedResources","",4.0,-1.0,-1.0,-1.0,False,False,False,1374136418000.0,-1.0,-1.0,1376272217000.0,1376273117000.0,1374136418000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"hillert",-1.0,-1.0,0,1374136418000.0,-1.0,"Ensure that each controller's list() returns PagedResources","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-509","Bug","Sprint 11","2013-07-18T03:26:11.000+0000","eric.bottard","","Investigate intermittent failure of RedisStreamDefinitionRepositoryTests","the test for findAll often fails for me when running inside gradle. (Could not reproduce inside eclipse)

I already tried fixing it by using a different redis key space, but to no avail.
One explanation would be if gradle runs tests concurrently, but my understanding is that it does not.",3.0,-1.0,0.9988280596015404,0.0093755231876778,False,True,True,1374117971000.0,-1.0,-1.0,1374123044000.0,1374123944000.0,1374117971000.0,-1.0,1374118027000.0,1374121627000.0,1374123937000.0,1374123944000.0,-1.0,-1.0,"eric.bottard",1374121627000.0,1374117971000.0,43,1374117971000.0,-1.0,"Investigate intermittent failure of RedisStreamDefinitionRepositoryTests","the test for findAll often fails for me when running inside gradle. (Could not reproduce inside eclipse)

I already tried fixing it by using a different redis key space, but to no avail.
One explanation would be if gradle runs tests concurrently, but my understanding is that it does not.",3.0,False,3.0,0,0.0,-1,0,False
"XD-508","Story","Sprint 12","2013-07-17T21:14:52.000+0000","mark.pollack","","Support pagination in list() command for triggers","See XD-477",1.0,0.9670039495615624,0.9670145179602386,0.591724641882262,True,True,True,1374095692000.0,1374736189000.0,1374739789000.0,1374757144000.0,1374758044000.0,1374095692000.0,-1.0,1374487622000.0,1374491222000.0,1374736196000.0,1374739796000.0,1374736189000.0,1374739789000.0,"mark.pollack",1374491222000.0,1374095692000.0,43,1374095692000.0,-1.0,"Support pagination in list() command for triggers","See XD-477",1.0,False,1.0,0,0.0,0,0,False
"XD-507","Story","Sprint 12","2013-07-17T21:14:31.000+0000","mark.pollack","","Support pagination in list() command for taps","See XD-477",1.0,0.7619868387157209,0.7620165023457687,0.6119310242539207,True,True,True,1374095671000.0,1374583735000.0,1374587335000.0,1374735286000.0,1374736186000.0,1374095671000.0,-1.0,1374487622000.0,1374491222000.0,1374583754000.0,1374587354000.0,1374583735000.0,1374587335000.0,"mark.pollack",1374491222000.0,1374095671000.0,43,1374095671000.0,-1.0,"Support pagination in list() command for taps","See XD-477",1.0,False,1.0,0,0.0,-1,0,False
"XD-506","Story","Sprint 12","2013-07-17T21:13:29.000+0000","mark.pollack","","Support pagination in list() command for jobs","See XD-477",1.0,0.7272136882129278,0.7272106463878327,0.5962174904942966,True,True,True,1374095609000.0,1374573752000.0,1374577352000.0,1374752209000.0,1374753109000.0,1374095609000.0,-1.0,1374487622000.0,1374491222000.0,1374573750000.0,1374577350000.0,1374573752000.0,1374577352000.0,"mark.pollack",1374491222000.0,1374095609000.0,43,1374095609000.0,-1.0,"Support pagination in list() command for jobs","See XD-477",1.0,False,1.0,0,0.0,-1,0,False
"XD-505","Story","","2013-07-17T15:12:15.000+0000","iperumal","","Fix XD modules parameters with ""-"" to use camel case","XD-482 addresses the use of camel case in 'fixed-delay' job module parameter name. and, we need to fix the same for other module parameters wherever '-' is being used. ",2.0,-1.0,-1.0,-1.0,False,False,False,1374073935000.0,-1.0,-1.0,1381458499000.0,1381459399000.0,1374073935000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"iperumal",-1.0,-1.0,0,1374073935000.0,-1.0,"Fix XD modules parameters with ""-"" to use camel case","XD-482 addresses the use of camel case in 'fixed-delay' job module parameter name. and, we need to fix the same for other module parameters wherever '-' is being used. ",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-504","Story","Sprint 11","2013-07-17T11:14:23.000+0000","iperumal","iperumal","Add ""How to Build Spring-XD"" instructions to the documentation","We need to determine where this information could fit in.
It can be either in ""README"" at the project home page or ""Getting started"" wiki page.",1.0,0.99936638766426,0.9444391199710348,0.9443965241837582,True,True,True,1374059663000.0,1374247356000.0,1374247475000.0,1374246575000.0,1374247475000.0,1374059663000.0,-1.0,1374237032000.0,1374240632000.0,1374237040000.0,1374240640000.0,1374247356000.0,1374247475000.0,"iperumal",1374240632000.0,1374059663000.0,43,1374059663000.0,-1.0,"Add ""How to Build Spring-XD"" instructions to the documentation","We need to determine where this information could fit in.
It can be either in ""README"" at the project home page or ""Getting started"" wiki page.",1.0,False,1.0,0,0.0,0,0,False
"XD-503","Bug","Sprint 12","2013-07-17T10:33:07.000+0000","grenfro","","Jobs are created even though they have an invalid definition","Also, if you deploy the Job it will fail, but then you can't delete the job.",2.0,0.9557751551772998,0.955773326873326,0.9487197301423336,True,True,True,1374057187000.0,1375102719000.0,1375106319000.0,1375150197000.0,1375151097000.0,1374057187000.0,-1.0,1375095001000.0,1375098601000.0,1375102717000.0,1375106317000.0,1375102719000.0,1375106319000.0,"grenfro",1375098601000.0,1374057187000.0,43,1374057187000.0,-1.0,"Jobs are created even though they have an invalid definition","Also, if you deploy the Job it will fail, but then you can't delete the job.",2.0,False,2.0,0,0.0,-1,1,False
"XD-502","Story","Sprint 12","2013-07-17T07:03:24.000+0000","iperumal","Ilayaperumal Gopinathan","Delete a trigger from Shell","Shell command to delete a trigger. 

Note: this command will only remove the trigger definition not modifying the jobs that use the trigger.",1.0,0.5854349290681449,0.5854310902449745,0.4251669408226214,True,True,True,1374044604000.0,1374654619000.0,1374658219000.0,1375085690000.0,1375086590000.0,1374044604000.0,-1.0,1374487622000.0,1374491222000.0,1374654615000.0,1374658215000.0,1374654619000.0,1374658219000.0,"iperumal",1374491222000.0,1374044604000.0,43,1374044604000.0,-1.0,"Delete a trigger from Shell","Shell command to delete a trigger. 

Note: this command will only remove the trigger definition not modifying the jobs that use the trigger.",1.0,False,1.0,0,0.0,0,0,False
"XD-500","Bug","Sprint 12","2013-07-16T09:55:23.000+0000","grenfro","","If a job is created that uses a trigger that has not been created and deployed it throws a 500 error instead of a 404","",2.0,0.8665480709668196,0.8665365194376138,0.7495455195228063,True,True,True,1373968523000.0,1374568650000.0,1374572250000.0,1374660172000.0,1374661072000.0,1373968523000.0,-1.0,1374487620000.0,1374491220000.0,1374568642000.0,1374572242000.0,1374568650000.0,1374572250000.0,"grenfro",1374491220000.0,1373968523000.0,43,1373968523000.0,-1.0,"If a job is created that uses a trigger that has not been created and deployed it throws a 500 error instead of a 400","",2.0,False,2.0,0,0.0,0,0,True
"XD-499","Bug","Sprint 12","2013-07-16T09:54:39.000+0000","grenfro","grenfro","Cron Jobs stop firing when a named trigger is created and deployed","",3.0,0.3760237924698701,0.3760209613811183,0.7348671228494342,True,True,True,1373968479000.0,1374234118000.0,1374237718000.0,1374674021000.0,1374674921000.0,1373968479000.0,-1.0,1374487620000.0,1374491220000.0,1374234116000.0,1374237716000.0,1374234118000.0,1374237718000.0,"grenfro",1374491220000.0,1373968479000.0,43,1373968479000.0,-1.0,"Cron Jobs stop firing when a named trigger is created and deployed","",3.0,False,3.0,0,0.0,0,1,False
"XD-498","Story","","2013-07-16T09:31:25.000+0000","eric.bottard","eric.bottard","Better UX when admin is not running","Current behavior is to just have a prompt of ""unknown:>""

I think any return value of a @CliCommand method is not shown b/c the whole infrastructure is not up at that time",1.0,-1.0,0.0,-1.0,False,False,True,1373967085000.0,-1.0,-1.0,1375337623000.0,1375338523000.0,1373967085000.0,-1.0,-1.0,-1.0,1373967085000.0,1373970685000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1373967085000.0,-1.0,"Better UX when admin is not running","Current behavior is to just have a prompt of ""unknown:>""

I think any return value of a @CliCommand method is not shown b/c the whole infrastructure is not up at that time",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-497","Story","Sprint 11","2013-07-16T08:47:29.000+0000","eric.bottard","","Fix Sonar build!","Caused by that weird annotation dependency problem that I worked around for compile. But Sonar complains.

One solution would be to add Jackson 2 to the Sonar ""classpath"", but I did not manage to do that.

",3.0,0.0830968587036363,0.9999765156229818,5.088281687274208e-05,True,True,True,1373964449000.0,1374028140000.0,1374031740000.0,1374730016000.0,1374730916000.0,1373964449000.0,-1.0,1373964488000.0,1373968088000.0,1374730898000.0,1374730916000.0,1374028140000.0,1374031740000.0,"eric.bottard",1373968088000.0,1373964449000.0,43,1373964449000.0,-1.0,"Fix Sonar build!","Caused by that weird annotation dependency problem that I worked around for compile. But Sonar complains.

One solution would be to add Jackson 2 to the Sonar ""classpath"", but I did not manage to do that.

",3.0,False,3.0,0,0.0,0,0,False
"XD-496","Story","Sprint 10","2013-07-15T10:50:59.000+0000","dturanski","dturanski","Disable Collection to Object conversion in DefaultTuple","DefaultFormattingConversionService provides Collection -> Object conversion which will produce the first item if the target type matches. Here, this results in an unfortunate side effect, getTuple(List<Tuple> list) would return a Tuple which is misleading. In this case it is preferable to treat it as an error if the argument is not a Tuple.",3.0,0.9991774754824424,0.0,0.999509648845302,True,True,True,1373885459000.0,1373948627000.0,1373948679000.0,1373947779000.0,1373948679000.0,1373885459000.0,-1.0,1373948648000.0,1373948679000.0,1373885459000.0,1373889059000.0,1373948627000.0,1373948679000.0,"dturanski",1373948679000.0,1373885459000.0,43,1373885459000.0,-1.0,"Disable Collection to Object conversion in DefaultTuple","DefaultFormattingConversionService provides Collection -> Object conversion which will produce the first item if the target type matches. Here, this results in an unfortunate side effect, getTuple(List<Tuple> list) would return a Tuple which is misleading. In this case it is preferable to treat it as an error if the argument is not a Tuple.",3.0,False,3.0,0,0.0,-1,0,False
"XD-495","Story","Sprint 11","2013-07-15T09:08:55.000+0000","grussell","grussell","Document MQTT Source and Sink","",1.0,-1.0,0.0,0.2205104741606482,False,True,True,1373879335000.0,-1.0,-1.0,1374223761000.0,1374224661000.0,1373879335000.0,-1.0,1373955483000.0,1373959083000.0,1373879335000.0,1373882935000.0,-1.0,-1.0,"grussell",1373959083000.0,1373879335000.0,43,1373879335000.0,-1.0,"Document MQTT Source and Sink","",1.0,False,1.0,0,0.0,-1,0,False
"XD-494","Story","Sprint 11","2013-07-15T08:42:21.000+0000","mark.pollack","hillert","Implement common set of controller methods","Save : Save a XYZDefinition - method used to be 'create'
Delete : Delete a XYZDefinition - method used to be called 'destroy'
Deploy : Deploy a XYZDefinition
Undeploy : Undeploy a XYZDefinition
List : List a XYZDefinition
       Returns PagedResources<XYZDefinitionResource>
Display : Get specific information about a XYZDefinition

Create other stories for each Controller and include in this weeks sprint
",2.0,0.2103291414687546,0.1343866394328968,0.134398320584191,True,True,True,1373877741000.0,1374003782000.0,1374007382000.0,1374476097000.0,1374476997000.0,1373877741000.0,-1.0,1373958280000.0,1373961880000.0,1373958273000.0,1373961873000.0,1374003782000.0,1374007382000.0,"mark.pollack",1373961880000.0,1373877741000.0,43,1373877741000.0,-1.0,"Implement common set of controller methods","Save : Save a XYZDefinition - method used to be 'create'
Delete : Delete a XYZDefinition - method used to be called 'destroy'
Deploy : Deploy a XYZDefinition
Undeploy : Undeploy a XYZDefinition
List : List a XYZDefinition
       Returns PagedResources<XYZDefinitionResource>
Display : Get specific information about a XYZDefinition

Create other stories for each Controller and include in this weeks sprint
",2.0,False,2.0,0,0.0,0,0,False
"XD-493","Story","Sprint 11","2013-07-15T08:39:45.000+0000","mark.pollack","luke","Remove duplicate code in ResourceDeployer implementations, create abstract base class.","Only create in TapDeployer has some additional code to check if the stream exists, could take place in another location.",4.0,0.319544470998579,0.2567578478232786,0.3194839168066141,True,True,True,1373877585000.0,1373956740000.0,1373960340000.0,1374124397000.0,1374125297000.0,1373877585000.0,-1.0,1373956725000.0,1373960325000.0,1373941187000.0,1373944787000.0,1373956740000.0,1373960340000.0,"mark.pollack",1373960325000.0,1373877585000.0,43,1373877585000.0,-1.0,"Remove duplicate code in ResourceDeployer implementations, create abstract base class.","Only create in TapDeployer has some additional code to check if the stream exists, could take place in another location.",4.0,False,4.0,0,0.0,-1,1,False
"XD-492","Story","Sprint 11","2013-07-15T08:39:26.000+0000","mark.pollack","luke","Rename create to 'save' in ResourceDeployer","To be consistent with Spring Data Repository method names.",1.0,0.5775024945439738,0.1070373903957112,0.1337799113911058,True,True,True,1373877566000.0,1374220774000.0,1374224374000.0,1374470963000.0,1374471863000.0,1373877566000.0,-1.0,1373957071000.0,1373960671000.0,1373941178000.0,1373944778000.0,1374220774000.0,1374224374000.0,"mark.pollack",1373960671000.0,1373877566000.0,43,1373877566000.0,-1.0,"Rename create to 'save' in ResourceDeployer","To be consistent with Spring Data Repository method names.",1.0,False,1.0,0,0.0,-1,0,False
"XD-491","Story","Sprint 11","2013-07-15T08:39:00.000+0000","mark.pollack","","Rerview usage of Assert.Null","Favor using custom  exceptions instead of using Assert.notNull, review usage and make changes.  Eg. is a stream can't be found (or another definition) a 'XYZNotFoundException' instead of Assert.Null on the return value of a findOne method
",3.0,0.7338529024528737,0.7338404228102907,0.2748953269978347,True,True,True,1373877540000.0,1374112756000.0,1374116356000.0,1374197162000.0,1374198062000.0,1373877540000.0,-1.0,1373965650000.0,1373969250000.0,1374112752000.0,1374116352000.0,1374112756000.0,1374116356000.0,"mark.pollack",1373969250000.0,1373877540000.0,43,1373877540000.0,-1.0,"Exception Consistency","Favor using custom  exceptions instead of using Assert.notNull, review usage and make changes.  Eg. if a stream can't be found (or another definition) a 'XYZNotFoundException' instead of Assert.Null on the return value of a findOne method
",3.0,False,3.0,0,0.0,0,0,True
"XD-490","Story","Sprint 11","2013-07-15T08:37:51.000+0000","mark.pollack","","StreamDeployer to implement ResourceDeployer","StreamController to not access the repository instance directly, all access to go through StreamDeployer
",4.0,0.4483171288576486,0.448324824914702,0.1136156076037043,True,True,True,1373877471000.0,1374226988000.0,1374230588000.0,1374656191000.0,1374657091000.0,1373877471000.0,-1.0,1373966048000.0,1373969648000.0,1374226994000.0,1374230594000.0,1374226988000.0,1374230588000.0,"mark.pollack",1373969648000.0,1373877471000.0,43,1373877471000.0,-1.0,"StreamDeployer to implement ResourceDeployer","StreamController to not access the repository instance directly, all access to go through StreamDeployer
",4.0,False,4.0,0,0.0,-1,0,False
"XD-489","Story","Sprint 12","2013-07-15T08:37:07.000+0000","mark.pollack","","Introduce distinction between TapDefinition and Tap (the instance)","Rename existing Tap class to something else.",3.0,0.6788324606466597,0.6788387417923092,0.4790904587042153,True,True,True,1373877427000.0,1374742024000.0,1374745624000.0,1375150180000.0,1375151080000.0,1373877427000.0,-1.0,1374487622000.0,1374491222000.0,1374742032000.0,1374745632000.0,1374742024000.0,1374745624000.0,"mark.pollack",1374491222000.0,1373877427000.0,43,1373877427000.0,-1.0,"Introduce distinction between TapDefinition and Tap (the instance)","Rename existing Tap class to something else.",3.0,False,3.0,0,0.0,0,0,False
"XD-488","Story","Sprint 11","2013-07-15T08:36:36.000+0000","mark.pollack","hillert","Rename controllers to have pluralized named (e.g. JobsController)","See implementation used for Steams and apply to jobs, taps, triggers.",1.0,0.7756597428288823,0.639082096933729,0.6391216617210682,True,True,True,1373877396000.0,1373975420000.0,1373979020000.0,1374002871000.0,1374003771000.0,1373877396000.0,-1.0,1373958165000.0,1373961765000.0,1373958160000.0,1373961760000.0,1373975420000.0,1373979020000.0,"mark.pollack",1373961765000.0,1373877396000.0,43,1373877396000.0,-1.0,"Rename controllers to have pluralized named (e.g. JobsController)","See implementation used for Steams and apply to jobs, taps, triggers.",1.0,False,1.0,0,0.0,0,0,False
"XD-487","Story","Sprint 11","2013-07-15T08:36:17.000+0000","mark.pollack","","All contollers to return paged results for list() ","See implementation used for Steams and apply to jobs, taps, triggers.",4.0,0.2500696549756871,0.2500613627166767,0.1469819494105862,True,True,True,1373877377000.0,1374028162000.0,1374031762000.0,1374479449000.0,1374480349000.0,1373877377000.0,-1.0,1373966003000.0,1373969603000.0,1374028157000.0,1374031757000.0,1374028162000.0,1374031762000.0,"mark.pollack",1373969603000.0,1373877377000.0,43,1373877377000.0,-1.0,"StreamsController to return paged results for list() ","See implementation used for Steams and apply to jobs, taps, triggers.",4.0,False,4.0,0,0.0,0,0,True
"XD-486","Story","Sprint 11","2013-07-15T08:35:32.000+0000","mark.pollack","eric.bottard","All controllers to return XYZResource objects not the raw domain objects.","Resource objects should be returned from all controller methods.
MVC Tests should be added to check returned values.",2.0,0.9838524763148738,0.5119808807843897,0.5031057317720068,True,True,True,1373877332000.0,1374049822000.0,1374052653000.0,1374051753000.0,1374052653000.0,1373877332000.0,-1.0,1373965537000.0,1373969137000.0,1373967093000.0,1373970693000.0,1374049822000.0,1374052653000.0,"mark.pollack",1373969137000.0,1373877332000.0,43,1373877332000.0,-1.0,"All controllers to return XYZResource objects not the raw domain objects.","Resource objects should be returned from all controller methods.
MVC Tests should be added to check returned values.",2.0,False,2.0,0,0.0,0,0,False
"XD-485","Story","Sprint 11","2013-07-15T08:34:36.000+0000","mark.pollack","kparikh","Create stories to enable the use of Spirng Shell's 2.0 branch testing facilities ","We need a few steps
1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch
2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes.  Create stories related to Shell release.
3. Determine and document the basic recipe for doing integration tests.
4. Create stories to provide integration tests for each existing command",5.0,0.3165075034106412,0.3164959698413639,0.3175998971864681,True,True,True,1373877276000.0,1374069372000.0,1374072972000.0,1374483300000.0,1374484200000.0,1373877276000.0,-1.0,1374070035000.0,1374073635000.0,1374069365000.0,1374072965000.0,1374069372000.0,1374072972000.0,"mark.pollack",1374073635000.0,1373877276000.0,43,1373877276000.0,-1.0,"Create stories to enable the use of Spring Shell's 2.0 branch testing facilities ","We need a few steps
1. Investigate if we need to move off Spring Shell 1.0 dependency, e.g. need to use code in Spring Shell 2.0 branch
2. If we need to use code in Spring Shell 2.0 branch, we need to release a Spring Shell 1.1 M1 release with appropriate code changes.  Create stories related to Shell release.
3. Determine and document the basic recipe for doing integration tests.
4. Create stories to provide integration tests for each existing command",5.0,False,5.0,0,0.0,0,0,True
"XD-483","Story","","2013-07-15T07:12:31.000+0000","grenfro","","Job Delete/Destroy Command for shell","",2.0,-1.0,0.0077597230870173,-1.0,False,False,True,1373872351000.0,-1.0,-1.0,1374468895000.0,1374469795000.0,1373872351000.0,-1.0,-1.0,-1.0,1373876987000.0,1373880587000.0,-1.0,-1.0,"grenfro",-1.0,-1.0,0,1373872351000.0,-1.0,"Job Delete/Destroy Command for shell","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-482","Bug","Sprint 11","2013-07-15T06:39:10.000+0000","grenfro","grenfro","When creating a job with the fixed-delay parameter in the shell command fails","Need to change the name from fixed-delay to fixed_delay or fixedDelay.  System rejects the '-'.",1.0,0.0351445996158267,0.0351369619760101,0.3744696613852387,True,True,True,1373870350000.0,1373879553000.0,1373883153000.0,1374131311000.0,1374132211000.0,1373870350000.0,-1.0,1373968409000.0,1373972009000.0,1373879551000.0,1373883151000.0,1373879553000.0,1373883153000.0,"grenfro",1373972009000.0,1373870350000.0,43,1373870350000.0,-1.0,"When creating a job with the fixed-delay parameter in the shell command fails","Need to change the name from fixed-delay to fixed_delay or fixedDelay.  System rejects the '-'.",1.0,False,1.0,0,0.0,-1,0,False
"XD-481","Bug","Sprint 11","2013-07-15T06:33:16.000+0000","grenfro","grenfro","Running Job with time delay (non cron) launches 2 instances before job is supposed to fire","",2.0,0.0363190492898659,0.0363114216411648,0.3753298958063187,True,True,True,1373869996000.0,1373879519000.0,1373883119000.0,1374131300000.0,1374132200000.0,1373869996000.0,-1.0,1373968409000.0,1373972009000.0,1373879517000.0,1373883117000.0,1373879519000.0,1373883119000.0,"grenfro",1373972009000.0,1373869996000.0,43,1373869996000.0,-1.0,"Running Job with time delay (non cron) launches 2 instances before job is supposed to fire","",2.0,False,2.0,0,0.0,-1,0,False
"XD-480","Bug","Sprint 11","2013-07-15T06:10:45.000+0000","grenfro","grenfro","In certain scenarios a job can be redeployed more than once","In a scenario where we are using the same job definition i.e. Job.xml and we create Job Instance Foo.  If I create and deploy Foo2 using Job.xml  I will see only 2 job definitions(correct), but I will see the job run 3 times.  If I create Foo3 & deploy, I will see 3 job definitions(correct), but the jobs will run 5 times.  ",2.0,0.0403090244707614,0.0402938464515688,0.378554976682768,True,True,True,1373868645000.0,1373879268000.0,1373882868000.0,1374131284000.0,1374132184000.0,1373868645000.0,-1.0,1373968409000.0,1373972009000.0,1373879264000.0,1373882864000.0,1373879268000.0,1373882868000.0,"grenfro",1373972009000.0,1373868645000.0,43,1373868645000.0,-1.0,"In certain scenarios a job can be redeployed more than once","In a scenario where we are using the same job definition i.e. Job.xml and we create Job Instance Foo.  If I create and deploy Foo2 using Job.xml  I will see only 2 job definitions(correct), but I will see the job run 3 times.  If I create Foo3 & deploy, I will see 3 job definitions(correct), but the jobs will run 5 times.  ",2.0,False,2.0,0,0.0,-1,0,False
"XD-479","Story","Sprint 11","2013-07-15T04:45:57.000+0000","dturanski","David Turanski","Add conversion support to ChannelRegistrar and ChannelRegistry ","Implements automatic conversion. Provide APIs to channel registry to register Payload conversion. Includes Redis and Rabbit transport.",8.0,0.0742462016622271,0.0977880566921662,0.0741560309959634,True,True,True,1373863557000.0,1373954954000.0,1373958554000.0,1375093656000.0,1375094556000.0,1373863557000.0,-1.0,1373954843000.0,1373958443000.0,1373983934000.0,1373987534000.0,1373954954000.0,1373958554000.0,"dturanski",1373958443000.0,1373863557000.0,43,1373863557000.0,-1.0,"Add conversion support to ChannelRegistrar and ChannelRegistry ","Implements automatic conversion. Provide APIs to channel registry to register Payload conversion. Includes Redis and Rabbit transport.",8.0,False,8.0,0,0.0,-1,1,False
"XD-478","Story","Sprint 10","2013-07-15T04:35:17.000+0000","dturanski","dturanski","Add accepted type logic to module","A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion.  This can be done in the module XML configuration and processed by StreamPlugin",3.0,0.7618681926656262,0.1897341888175985,0.699399674651737,True,True,True,1373862917000.0,1373955180000.0,1373958780000.0,1373983118000.0,1373984018000.0,1373862917000.0,-1.0,1373947615000.0,1373951215000.0,1373885894000.0,1373889494000.0,1373955180000.0,1373958780000.0,"dturanski",1373951215000.0,1373862917000.0,43,1373862917000.0,-1.0,"Add accepted type logic to module","A module can declare one ore more payload types it will accept. This will inform the runtime re. automatic payload conversion.  This can be done in the module XML configuration and processed by StreamPlugin",3.0,False,3.0,0,0.0,0,0,False
"XD-477","Story","","2013-07-15T02:19:38.000+0000","eric.bottard","eric.bottard","Support pagination in list() commands","Spring HATEOAS is here to help.
Nonetheless, there are currently a number of outstanding issues, namely:

https://github.com/SpringSource/spring-hateoas/pull/98
https://jira.springsource.org/browse/SPR-10262#comment-91685
https://github.com/SpringSource/spring-hateoas/pull/94

Creating a issue here for future reference",5.0,-1.0,0.0,-1.0,False,False,True,1373854778000.0,-1.0,-1.0,1373939273000.0,1373940173000.0,1373854778000.0,-1.0,-1.0,-1.0,1373854778000.0,1373858378000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1373854778000.0,-1.0,"Support pagination in list() command for streams","Spring HATEOAS is here to help.
Nonetheless, there are currently a number of outstanding issues, namely:

https://github.com/SpringSource/spring-hateoas/pull/98
https://jira.springsource.org/browse/SPR-10262#comment-91685
https://github.com/SpringSource/spring-hateoas/pull/94

Creating a issue here for future reference",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-476","Bug","","2013-07-13T08:14:32.000+0000","dsz0111@gmail.com","","The stream definition is not deleted in redis container when the stream is undeployed","This only happened with distributed mode that uses redis as store. The single mode which uses in memory store works fine.

Step to reproduce:

Create stream:
curl -X POST -d ""name=httptest"" -d ""definition=http|log"" http://localhost:8080/streams

redis 127.0.0.1:6379> keys *httptest*
1) ""modules:httptest""
2) ""streams.httptest""
3) ""stream.definitions.httptest""

Delete Stream:
curl -X DELETE http://localhost:8080/streams/httptest

redis 127.0.0.1:6379> keys *httptest*
1) ""streams.httptest""
2) ""stream.definitions.httptest""

stream still there not deleted

Recreate the stream
curl -X POST -d ""name=httptest"" -d ""definition=http|log"" http://localhost:8080/streams

Got:
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><errors xmlns:ns2=""http://www.w3.org/2005/Atom""><error logref=""StreamAlreadyExistsException""><message>There is already a stream with name 'httptest'</message></error>",2.0,-1.0,-1.0,-1.0,False,False,False,1373703272000.0,-1.0,-1.0,1373939424000.0,1373940324000.0,1373703272000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"dsz0111@gmail.com",-1.0,-1.0,0,1373703272000.0,-1.0,"The stream definition is not deleted in redis container when the stream is destroyed","This only happened with distributed mode that uses redis as store. The single mode which uses in memory store works fine.

Step to reproduce:

Create stream:
curl -X POST -d ""name=httptest"" -d ""definition=http|log"" http://localhost:8080/streams

redis 127.0.0.1:6379> keys *httptest*
1) ""modules:httptest""
2) ""streams.httptest""
3) ""stream.definitions.httptest""

Delete Stream:
curl -X DELETE http://localhost:8080/streams/httptest

redis 127.0.0.1:6379> keys *httptest*
1) ""streams.httptest""
2) ""stream.definitions.httptest""

stream still there not deleted

Recreate the stream
curl -X POST -d ""name=httptest"" -d ""definition=http|log"" http://localhost:8080/streams

Got:
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""yes""?><errors xmlns:ns2=""http://www.w3.org/2005/Atom""><error logref=""StreamAlreadyExistsException""><message>There is already a stream with name 'httptest'</message></error>",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-475","Improvement","","2013-07-12T11:14:54.000+0000","thomas.risberg","","Investigate what is needed for an Avro sink for HDFS","We need a sink that can write data in Avro serialized format. This story is for investigating what we would need to do to support that. The Spring Integration Kafka adapter provides Avro support for Kafka.",8.0,-1.0,-1.0,-1.0,False,False,False,1373627694000.0,-1.0,-1.0,1389915986000.0,1389916886000.0,1373627694000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"thomas.risberg",-1.0,-1.0,0,1373627694000.0,-1.0,"Avro sink for HDFS","We need a sink that can write data in Avro serialized format. This story is for investigating what we would need to do to support that. The Spring Integration Kafka adapter provides Avro support for Kafka.",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-473","Story","Sprint 12","2013-07-12T11:01:25.000+0000","thomas.risberg","","Modify startup script to allow specifying hadoop distro to use","we need to modify startup script to use hadoop 1.1.2 as default or phd1 when specified with --hadoopDistro=phd1",5.0,0.6081057114008517,0.6081103940764154,0.5038184292550156,True,True,True,1373626885000.0,1374665788000.0,1374669388000.0,1375334410000.0,1375335310000.0,1373626885000.0,-1.0,1374487621000.0,1374491221000.0,1374665796000.0,1374669396000.0,1374665788000.0,1374669388000.0,"thomas.risberg",1374491221000.0,1373626885000.0,43,1373626885000.0,-1.0,"Modify startup script of xdadmin/xdcontainer to allow specifying hadoop distro to use","we need to modify startup script to use hadoop 1.1.2 as default or phd1 when specified with --hadoopDistro=phd1",5.0,False,5.0,0,0.0,-1,0,True
"XD-472","Story","Sprint 11","2013-07-12T10:58:59.000+0000","thomas.risberg","","Add spring-xd-hadoop distro specific sub-projects","we need to modify build adding two sub-projects for spring-xd-hadoop: one for hadoop 1.1.2 and one for phd1 (Pivotal HD) to pull in transitive dependencies for correct Hadoop distro",8.0,0.3333353113496599,0.6220867940377314,0.223378043095701,True,True,True,1373626739000.0,1374132299000.0,1374135899000.0,1375142510000.0,1375143410000.0,1373626739000.0,-1.0,1373965530000.0,1373969130000.0,1374570240000.0,1374573840000.0,1374132299000.0,1374135899000.0,"thomas.risberg",1373969130000.0,1373626739000.0,43,1373626739000.0,-1.0,"Add spring-xd-hadoop distro specific sub-projects","we need to modify build adding two sub-projects for spring-xd-hadoop: one for hadoop 1.1.2 and one for phd1 (Pivotal HD) to pull in transitive dependencies for correct Hadoop distro",8.0,False,8.0,0,0.0,0,0,False
"XD-470","Story","Sprint 11","2013-07-12T10:55:37.000+0000","thomas.risberg","thomas.risberg","Create JDBC sink","we need a JDBC sink for writing to HAWQ (using int-jdbc:outbound-channel-adapter and postgresql JDBC driver)
",3.0,0.7152218931917678,0.4508742166789209,0.5715697450875903,True,True,True,1373626537000.0,1374050655000.0,1374054255000.0,1374218625000.0,1374219525000.0,1373626537000.0,-1.0,1373965471000.0,1373969071000.0,1373893900000.0,1373897500000.0,1374050655000.0,1374054255000.0,"thomas.risberg",1373969071000.0,1373626537000.0,43,1373626537000.0,-1.0,"Create JDBC sink","we need a JDBC sink for writing to HAWQ (using int-jdbc:outbound-channel-adapter and postgresql JDBC driver)
",3.0,False,3.0,0,0.0,0,0,False
"XD-469","Story","Sprint 11","2013-07-12T10:54:27.000+0000","thomas.risberg","thomas.risberg","Upgrade to spring-data-hadoop 1.0.1.RC1","spring-data-hadoop 1.0.1.RC1 provides flavors for commonly used Hadoop distros/versions and we should make use of that.",1.0,-1.0,0.6270185149387786,0.7991338950207214,False,True,True,1373626467000.0,-1.0,-1.0,1374049765000.0,1374050665000.0,1373626467000.0,-1.0,1373965458000.0,1373969058000.0,1373892447000.0,1373896047000.0,-1.0,-1.0,"thomas.risberg",1373969058000.0,1373626467000.0,43,1373626467000.0,-1.0,"Upgrade to spring-data-hadoop 1.0.1.RC1","spring-data-hadoop 1.0.1.RC1 provides flavors for commonly used Hadoop distros/versions and we should make use of that.",1.0,False,1.0,0,0.0,-1,0,False
"XD-467","Story","Sprint 12","2013-07-12T09:17:45.000+0000","luke","","JMX shouldn't register taps or streams if the creation fails","There's a lifecycle problem when a tap creation fails (e.g. because the DSL syntax is wrong). Subsequence attempts to create the tap will fail with an error:

 [{""links"":[],""logref"":""MessageHandlingException"",""message"":""org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#92e27d20-4c48-4131-866c-91b6cb642ca2'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=nullChannel, sends=0, receives=0]] with key 'xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log'; nested exception is javax.management.InstanceAlreadyExistsException: xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log""}]

Disabling JMX solves the issue.",3.0,-1.0,-1.0,0.0350668515742723,False,True,False,1373620665000.0,-1.0,-1.0,1398342743000.0,1398343643000.0,1373620665000.0,-1.0,1374487622000.0,1374491222000.0,-1.0,-1.0,-1.0,-1.0,"luke",1374491222000.0,1373620665000.0,43,1373620665000.0,-1.0,"JMX shouldn't register taps or streams if the creation fails","There's a lifecycle problem when a tap creation fails (e.g. because the DSL syntax is wrong). Subsequent attempts to create the tap will fail with an error:

 [{""links"":[],""logref"":""MessageHandlingException"",""message"":""org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#92e27d20-4c48-4131-866c-91b6cb642ca2'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=nullChannel, sends=0, receives=0]] with key 'xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log'; nested exception is javax.management.InstanceAlreadyExistsException: xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log""}]

Disabling JMX solves the issue.


reproduce
Create a bad stream definition name 'bad'
Try to recreate with the same name, but correct stream definitions.  The system will report that the stream already exists.",3.0,False,3.0,0,0.0,0,0,True
"XD-466","Story","Sprint 10","2013-07-12T07:00:16.000+0000","dturanski","dturanski","Add JSON conversion to tuple","Support toString() to emit JSON by default. Should be backed by a simple strategy to allow the possibility of other representations.  Also provide toTuple(String json). This supports seamless mapping JSON<->Tuple in XD",3.0,0.0337972885110891,0.0,0.0001204035928432,True,True,True,1373612416000.0,1373620837000.0,1373624437000.0,1373860678000.0,1373861578000.0,1373612416000.0,-1.0,1373612446000.0,1373616046000.0,1373612416000.0,1373616016000.0,1373620837000.0,1373624437000.0,"dturanski",1373616046000.0,1373612416000.0,43,1373612416000.0,-1.0,"Add JSON conversion to tuple","Support toString() to emit JSON by default. Should be backed by a simple strategy to allow the possibility of other representations.  Also provide toTuple(String json). This supports seamless mapping JSON<->Tuple in XD",3.0,False,3.0,0,0.0,0,0,False
"XD-465","Story","","2013-07-12T06:43:37.000+0000","luke","","Shell should display error messages returned from the server","For example, using tcpdump I can see both an exception and message information:

'HTTP/1.1 500 Internal Server Error
Server: Apache-Coyote/1.1
Content-Type: application/json;charset=UTF-8
Transfer-Encoding: chunked
Date: Fri, 12 Jul 2013 13:38:26 GMT
Connection: close

275
[{""links"":[],""logref"":""MessageHandlingException"",""message"":""org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#92e27d20-4c48-4131-866c-91b6cb642ca2'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=nullChannel, sends=0, receives=0]] with key 'xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log'; nested exception is javax.management.InstanceAlreadyExistsException: xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log""}]

However the client only shows:

http://localhost:8080:>tap create --name ""tap1"" --definition ""tap@test1.file | log"" --deploy true
14:38:26,113  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/taps"" resulted in 500 (Internal Server Error); invoking error handler
Error creating tap 'tap1'

The error doesn't seem to be logged in the XD Admin server either, so the information is effectively lost.

",2.0,-1.0,0.9999497577486112,-1.0,False,False,True,1373611417000.0,-1.0,-1.0,1374207624000.0,1374208524000.0,1373611417000.0,-1.0,-1.0,-1.0,1374208494000.0,1374208524000.0,-1.0,-1.0,"luke",-1.0,-1.0,0,1373611417000.0,-1.0,"Shell should display error messages returned from the server","For example, using tcpdump I can see both an exception and message information:

'HTTP/1.1 500 Internal Server Error
Server: Apache-Coyote/1.1
Content-Type: application/json;charset=UTF-8
Transfer-Encoding: chunked
Date: Fri, 12 Jul 2013 13:38:26 GMT
Connection: close

275
[{""links"":[],""logref"":""MessageHandlingException"",""message"":""org.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.integration.monitor.IntegrationMBeanExporter#0#92e27d20-4c48-4131-866c-91b6cb642ca2'; nested exception is org.springframework.jmx.export.UnableToRegisterMBeanException: Unable to register MBean [MessageChannelMonitor: [name=nullChannel, sends=0, receives=0]] with key 'xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log'; nested exception is javax.management.InstanceAlreadyExistsException: xd.tap1:type=MessageChannel,name=nullChannel,index=1,module=log""}]

However the client only shows:

http://localhost:8080:>tap create --name ""tap1"" --definition ""tap@test1.file | log"" --deploy true
14:38:26,113  WARN Spring Shell client.RestTemplate:524 - POST request for ""http://localhost:8080/taps"" resulted in 500 (Internal Server Error); invoking error handler
Error creating tap 'tap1'

The error doesn't seem to be logged in the XD Admin server either, so the information is effectively lost.

",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-451","Story","","2013-07-11T14:47:06.000+0000","aeisenberg","","Add aAccess-Control-Allow-Origin header to responses in order to support cross-origin requests","If we want spring-xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin.  This way, the UI could remain a separate project and be served from wherever we want.

So, we would need to add the header to *all* outgoing REST responses. We may also need to add a {{Access-Control-Allow-Methods}} header as well.

In the short term, the {{Access-Control-Allow-Origin}} header could be hard coded to a specific url (I'm using http://localhost:9889 for now), but in the long term we would need this configurable.",5.0,-1.0,-1.0,-1.0,False,False,False,1373554026000.0,-1.0,-1.0,1375146928000.0,1375147828000.0,1373554026000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"aeisenberg",-1.0,-1.0,0,1373554026000.0,-1.0,"Add a Access-Control-Allow-Origin header to responses in order to support cross-origin requests","If we want spring-xd to support interactions through a UI, then it would make sense that we should support the UI coming from a separate origin.  This way, the UI could remain a separate project and be served from wherever we want.

So, we would need to add the header to *all* outgoing REST responses. We may also need to add a {{Access-Control-Allow-Methods}} header as well.

In the short term, the {{Access-Control-Allow-Origin}} header could be hard coded to a specific url (I'm using http://localhost:9889 for now), but in the long term we would need this configurable.",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-447","Story","Sprint 10","2013-07-11T11:45:27.000+0000","grussell","grussell","Add an MQTT Sink","",1.0,0.0066817802171578,0.0,0.0057272401861353,True,True,True,1373543127000.0,1373543183000.0,1373546783000.0,1373550608000.0,1373551508000.0,1373543127000.0,-1.0,1373543175000.0,1373546775000.0,1373543127000.0,1373546727000.0,1373543183000.0,1373546783000.0,"grussell",1373546775000.0,1373543127000.0,43,1373543127000.0,-1.0,"Add an MQTT Sink","",1.0,False,1.0,0,0.0,0,0,False
"XD-444","Story","Sprint 10","2013-07-10T09:25:37.000+0000","grussell","grussell","Make Redis/Rabbit @Rules Conditional","Dependent servers should be required on the CI server, but optional on developer systems.",1.0,0.9987579182710222,0.0,0.0029809961495466,True,True,True,1373448337000.0,1373464419000.0,1373464439000.0,1373463539000.0,1373464439000.0,1373448337000.0,1373448442000.0,1373448385000.0,1373451985000.0,1373448337000.0,1373451937000.0,1373464419000.0,1373464439000.0,"grussell",1373451985000.0,1373448442000.0,43,1373448442000.0,-1.0,"Make Redis/Rabbit @Rules Conditional","Dependent servers should be required on the CI server, but optional on developer systems.",1.0,False,1.0,0,0.0,0,0,False
"XD-443","Story","Sprint 10","2013-07-10T09:24:36.000+0000","grussell","grussell","Add MQTT Source","",3.0,0.1865986394557823,0.0,0.001235827664399,True,True,True,1373448276000.0,1373464734000.0,1373468334000.0,1373535576000.0,1373536476000.0,1373448276000.0,-1.0,1373448385000.0,1373451985000.0,1373448276000.0,1373451876000.0,1373464734000.0,1373468334000.0,"grussell",1373451985000.0,1373448276000.0,43,1373448276000.0,-1.0,"Add MQTT Source","",3.0,False,3.0,0,0.0,0,0,False
"XD-442","Story","","2013-07-10T07:59:53.000+0000","aclement","aclement","More DSL work: Documentation updates for new format","Once issues like XD-438 have been completed the wiki doc will need updates to reflect the current behaviour and syntax options.",2.0,-1.0,0.0,-1.0,False,False,True,1373443193000.0,-1.0,-1.0,1382337502000.0,1382338402000.0,1373443193000.0,-1.0,-1.0,-1.0,1373443193000.0,1373446793000.0,-1.0,-1.0,"aclement",-1.0,-1.0,0,1373443193000.0,-1.0,"More DSL work: Documentation updates for new format","Once issues like XD-438 have been completed the wiki doc will need updates to reflect the current behaviour and syntax options.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-441","Story","","2013-07-10T07:58:23.000+0000","aclement","","More DSL work: checking behaviour for non-deployable streams","With support for substreams/parameterized streams now in the parser it will be possible to create a stream that cannot be deployed: it may not fit the source/processor*/sink structure or it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these - after creating them, attempting to 'deploy' them should return appropriate errors. (They should exist in the stream directory).",3.0,-1.0,-1.0,-1.0,False,False,False,1373443103000.0,-1.0,-1.0,1381835405000.0,1381836305000.0,1373443103000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"aclement",-1.0,-1.0,0,1373443103000.0,-1.0,"More DSL work: checking behaviour for non-deployable streams","With support for substreams/parameterized streams now in the parser it will be possible to create a stream that cannot be deployed: it may not fit the source/processor*/sink structure or it is a parameterized stream with no default values for parameters. Need to check how XD is going to handle these - after creating them, attempting to 'deploy' them should return appropriate errors. (They should exist in the stream directory).",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-439","Story","Sprint 12","2013-07-10T07:53:14.000+0000","aclement","","More DSL work: hooking up stream directory","Following stream parsing there is now a stream resolution stage that chases down substream references and fills in parameterization. The 'lookup' of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory.  The parser implementation doesn't know about stream deletions, for example, so may still resolve streams that no longer exist.",2.0,-1.0,-1.0,0.999989355855596,False,True,False,1373442794000.0,-1.0,-1.0,1375226913000.0,1375227813000.0,1373442794000.0,-1.0,1375227794000.0,1375227813000.0,-1.0,-1.0,-1.0,-1.0,"aclement",1375227813000.0,1373442794000.0,43,1373442794000.0,-1.0,"More DSL work: hooking up stream directory","Following stream parsing there is now a stream resolution stage that chases down substream references and fills in parameterization. The 'lookup' of streams is done through implementors of the StreamLookupEnvironment interface. Currently the parser implements this itself but it is really a job for the stream directory.  The parser implementation doesn't know about stream deletions, for example, so may still resolve streams that no longer exist.",2.0,False,2.0,0,0.0,0,0,False
"XD-438","Story","Sprint 11","2013-07-10T07:51:08.000+0000","aclement","luke","More DSL work: exploiting source/sink channels","The DSL changes under XD-369 now build stream Ast objects that can include a source and sink channel:

{code}
// Source Channel
:mystream.foo > count | log

// Sink Channel
http | count > :foo
{code}

These new fields in the Ast object need to be copied into the module deployment request objects and then used at the destination as the channels for wiring things together.  Currently the only channels used are the .NNN numeric channels where NNN is the index of the module in the stream definition. The source/sink channels are 'extra' channels that need creating - the source channel acting as a real source for the next module in the chain whilst the sink channel acts as a sink output for the last channel in the chain.  

I can think of two ways to handle the implementation:
- In order to police the stream structure as ""source | processor* | sink"" maybe special SourceChannel and SinkChannel modules are created to represent these channels and when deployment happens the deployer understands that they don't represent a real request to deploy a module but simply the channels to wire up to the adjacent module.

- Carry source/sink channel info in the existing module definitions. But then the verification of source/processor*/sink structure will need modification to say a source isn't necessary if the first processor has a source channel attached and a sink isn't necessary if the last processor has a sink channel attached.
",4.0,0.7568252186131773,0.252540163547664,0.2970938784845129,True,True,True,1373442668000.0,1374753098000.0,1374756698000.0,1375173251000.0,1375174151000.0,1373442668000.0,-1.0,1373957081000.0,1373960681000.0,1373879937000.0,1373883537000.0,1374753098000.0,1374756698000.0,"aclement",1373960681000.0,1373442668000.0,43,1373442668000.0,-1.0,"More DSL work: exploiting source/sink channels","The DSL changes under XD-369 now build stream Ast objects that can include a source and sink channel:

{code}
// Source Channel
:mystream.foo > count | log

// Sink Channel
http | count > :foo
{code}

These new fields in the Ast object need to be copied into the module deployment request objects and then used at the destination as the channels for wiring things together.  Currently the only channels used are the .NNN numeric channels where NNN is the index of the module in the stream definition. The source/sink channels are 'extra' channels that need creating - the source channel acting as a real source for the next module in the chain whilst the sink channel acts as a sink output for the last channel in the chain.  

I can think of two ways to handle the implementation:
- In order to police the stream structure as ""source | processor* | sink"" maybe special SourceChannel and SinkChannel modules are created to represent these channels and when deployment happens the deployer understands that they don't represent a real request to deploy a module but simply the channels to wire up to the adjacent module.

- Carry source/sink channel info in the existing module definitions. But then the verification of source/processor*/sink structure will need modification to say a source isn't necessary if the first processor has a source channel attached and a sink isn't necessary if the last processor has a sink channel attached.
",4.0,False,4.0,0,0.0,0,0,False
"XD-437","Story","","2013-07-10T07:35:51.000+0000","hillert","","Fix Package Tangles","Looking at the latest Sonar run we have 3 package tangles in Spring XD:

https://sonar.springsource.org/drilldown/measures/7717?metric=87",4.0,0.8826358054938015,0.8826331472884977,-1.0,True,False,True,1373441751000.0,1374437877000.0,1374441477000.0,1374569432000.0,1374570332000.0,1373441751000.0,-1.0,-1.0,-1.0,1374437874000.0,1374441474000.0,1374437877000.0,1374441477000.0,"hillert",-1.0,-1.0,0,1373441751000.0,-1.0,"Fix Package Tangles","Looking at the latest Sonar run we have 3 package tangles in Spring XD:

https://sonar.springsource.org/drilldown/measures/7717?metric=87",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-435","Story","Sprint 10","2013-07-10T04:55:39.000+0000","luke","luke","Create tests to load the standard runtime app context configurations","The basic launch configurations should be tested automatically to ensure that startup scripts and launch aren't broken by changes.",2.0,0.5862516945323091,0.5850090375056485,0.5844441934026209,True,True,True,1373432139000.0,1373442518000.0,1373446118000.0,1373448943000.0,1373449843000.0,1373432139000.0,1373442620000.0,1373442486000.0,1373446086000.0,1373442496000.0,1373446096000.0,1373442518000.0,1373446118000.0,"luke",1373446086000.0,1373442620000.0,43,1373442620000.0,-1.0,"Create tests to load the standard runtime app context configurations","The basic launch configurations should be tested automatically to ensure that startup scripts and launch aren't broken by changes.",2.0,False,2.0,0,0.0,-1,0,False
"XD-433","Story","","2013-07-09T10:40:19.000+0000","grussell","","Homogenize Container Initialization Failures and Add Spring Retry","If Redis is not running, the container fails to initialize in {{ContainerMain.launch()}} because the connection factory attempts to eagerly connect.

If RabbitMQ is not running, the container fails to initialize in {{AbstractContainerLauncher.launch()}}.

Make the failure behavior consistent from a user perspective and add a spring-retry {{RetryTemplate}} to retry container startup.",5.0,-1.0,-1.0,-1.0,False,False,False,1373366419000.0,-1.0,-1.0,1398344255000.0,1398345155000.0,1373366419000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"grussell",-1.0,-1.0,0,1373366419000.0,-1.0,"Homogenize Container Initialization Failures ","If Redis is not running, the container fails to initialize in {{ContainerMain.launch()}} because the connection factory attempts to eagerly connect.

If RabbitMQ is not running, the container fails to initialize in {{AbstractContainerLauncher.launch()}}.

Make the failure behavior consistent from a user perspective and add a spring-retry {{RetryTemplate}} to retry container startup.",-1.0,False,-1.0,0,-1.0,0,0,True
"XD-431","Story","Sprint 10","2013-07-09T10:02:59.000+0000","dturanski","dturanski","Make String conversion optional with local transport","",5.0,0.0025685499540314,0.0025022036452557,0.0023505549394826,True,True,True,1373364179000.0,1373364450000.0,1373368050000.0,1373468786000.0,1373469686000.0,1373364179000.0,1373364468000.0,1373364427000.0,1373368027000.0,1373364443000.0,1373368043000.0,1373364450000.0,1373368050000.0,"dturanski",1373368027000.0,1373364468000.0,43,1373364468000.0,-1.0,"Make String conversion optional with local transport","",5.0,False,5.0,0,0.0,-1,0,False
"XD-430","Story","Sprint 12","2013-07-09T08:45:27.000+0000","grussell","mark.fisher","Use a Different Default Jolokia Port for Admin Vs. Container","Avoid the need for {{--jmxPort=xxxx}} when running both a {{Container}} and {{Admin}} on the same server",1.0,0.9966904521279312,0.9966890910142678,0.996680243775457,True,True,True,1373359527000.0,1374824049000.0,1374827649000.0,1374828012000.0,1374828912000.0,1373359527000.0,-1.0,1374824034000.0,1374827634000.0,1374824047000.0,1374827647000.0,1374824049000.0,1374827649000.0,"grussell",1374827634000.0,1373359527000.0,43,1373359527000.0,-1.0,"Use a Different Default Jolokia Port for Admin Vs. Container","Avoid the need for {{--jmxPort=xxxx}} when running both a {{Container}} and {{Admin}} on the same server",1.0,False,1.0,0,0.0,0,0,False
"XD-429","Bug","Sprint 11","2013-07-08T23:06:10.000+0000","mark.pollack","Eric Bottard","Document time source","time source is used in some examples, but it isn't documented explicitly, eg. --interval option in seconds.",1.0,-1.0,0.9999834881425812,0.8100145685388149,False,True,True,1373324770000.0,-1.0,-1.0,1374111183000.0,1374112083000.0,1373324770000.0,-1.0,1373962505000.0,1373966105000.0,1374112070000.0,1374112083000.0,-1.0,-1.0,"mark.pollack",1373966105000.0,1373324770000.0,43,1373324770000.0,-1.0,"Document time source","time source is used in some examples, but it isn't documented explicitly, eg. --interval option in seconds.",1.0,False,1.0,0,0.0,-1,0,False
"XD-428","Story","","2013-07-08T16:47:54.000+0000","mark.pollack","","Update Creating a Sink Module section to use Shell commands instead of curl ","See http://static.springsource.org/spring-xd/docs/1.0.x-SNAPSHOT/reference/html/#_test_the_deployed_module_3
",1.0,-1.0,-1.0,-1.0,False,False,False,1373302074000.0,-1.0,-1.0,1374471613000.0,1374472513000.0,1373302074000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373302074000.0,-1.0,"Update Creating a Sink Module section to use Shell commands instead of curl ","See http://static.springsource.org/spring-xd/docs/1.0.x-SNAPSHOT/reference/html/#_test_the_deployed_module_3
",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-427","Story","","2013-07-08T16:47:05.000+0000","mark.pollack","","Update Creating a Source Module section to use Shell commands instead of curl ","See http://static.springsource.org/spring-xd/docs/1.0.x-SNAPSHOT/reference/html/#_test_the_deployed_module",1.0,-1.0,-1.0,-1.0,False,False,False,1373302025000.0,-1.0,-1.0,1374471563000.0,1374472463000.0,1373302025000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373302025000.0,-1.0,"Update Creating a Source Module section to use Shell commands instead of curl ","See http://static.springsource.org/spring-xd/docs/1.0.x-SNAPSHOT/reference/html/#_test_the_deployed_module",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-426","Story","","2013-07-08T16:45:40.000+0000","mark.pollack","","Update Creating a Processor Module section to use Shell commands instead of curl ","""test the deployed module"" sub-section uses curl.",1.0,-1.0,-1.0,-1.0,False,False,False,1373301940000.0,-1.0,-1.0,1374471579000.0,1374472479000.0,1373301940000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301940000.0,-1.0,"Update Creating a Processor Module section to use Shell commands instead of curl ","""test the deployed module"" sub-section uses curl.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-425","Story","","2013-07-08T16:44:40.000+0000","mark.pollack","","Update Samples syslog ingestion section to use Shell commands instead of curl ","",1.0,-1.0,-1.0,-1.0,False,False,False,1373301880000.0,-1.0,-1.0,1374471519000.0,1374472419000.0,1373301880000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301880000.0,-1.0,"Update Samples syslog ingestion section to use Shell commands instead of curl ","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-424","Story","","2013-07-08T16:44:01.000+0000","mark.pollack","","Update Analytics Rich Gauge section to use Shell commands instead of curl ","",1.0,-1.0,-1.0,-1.0,False,False,False,1373301841000.0,-1.0,-1.0,1374471461000.0,1374472361000.0,1373301841000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301841000.0,-1.0,"Update Analytics Rich Gauge section to use Shell commands instead of curl ","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-423","Story","","2013-07-08T16:43:44.000+0000","mark.pollack","","Update Analytics Gauge section to use Shell commands instead of curl ","",1.0,-1.0,-1.0,-1.0,False,False,False,1373301824000.0,-1.0,-1.0,1374471470000.0,1374472370000.0,1373301824000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301824000.0,-1.0,"Update Analytics Gauge section to use Shell commands instead of curl ","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-422","Story","","2013-07-08T16:43:25.000+0000","mark.pollack","","Update Analytics Field Value Counter section to use Shell commands instead of curl ","",1.0,-1.0,-1.0,-1.0,False,False,False,1373301805000.0,-1.0,-1.0,1374471477000.0,1374472377000.0,1373301805000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301805000.0,-1.0,"Update Analytics Field Value Counter section to use Shell commands instead of curl ","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-421","Story","","2013-07-08T16:43:03.000+0000","mark.pollack","","Update Analytics Counter section to use Shell commands instead of curl ","",1.0,-1.0,-1.0,-1.0,False,False,False,1373301783000.0,-1.0,-1.0,1374471484000.0,1374472384000.0,1373301783000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301783000.0,-1.0,"Update Analytics Counter section to use Shell commands instead of curl ","",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-420","Story","Sprint 11","2013-07-08T16:40:40.000+0000","mark.pollack","luke","Documentation for AggregateCounter","Add section to Analytics chapter on use of AggregateCounter.  The example should show the use of the shell to create the tap that uses the AggregateCounter.",3.0,0.8961742807123295,0.0499821014526656,0.8961242300632488,True,True,True,1373301640000.0,1374125286000.0,1374128886000.0,1374219809000.0,1374220709000.0,1373301640000.0,-1.0,1374125240000.0,1374128840000.0,1373347577000.0,1373351177000.0,1374125286000.0,1374128886000.0,"mark.pollack",1374128840000.0,1373301640000.0,43,1373301640000.0,-1.0,"Documentation for AggregateCounter","Add section to Analytics chapter on use of AggregateCounter.  The example should show the use of the shell to create the tap that uses the AggregateCounter.",3.0,False,3.0,0,0.0,-1,0,False
"XD-419","Story","","2013-07-08T16:38:57.000+0000","mark.pollack","","Taps introduction section should show use of shell to create a real stream and a real tap using the shell","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#taps

The existing docs should be made to show a real stream being created with filter and/or transformer and then a tap that goes to logging.  

The shell syntax to also stop/undeploy a tap should be shown here as well since the lifecycle is discussed.",2.0,-1.0,-1.0,-1.0,False,False,False,1373301537000.0,-1.0,-1.0,1374471978000.0,1374472878000.0,1373301537000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301537000.0,-1.0,"Taps introduction section should show use of shell to create a real stream and a real tap using the shell","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#taps

The existing docs should be made to show a real stream being created with filter and/or transformer and then a tap that goes to logging.  

The shell syntax to also stop/undeploy a tap should be shown here as well since the lifecycle is discussed.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-418","Story","","2013-07-08T16:34:56.000+0000","mark.pollack","","Update Sink's GemFire section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#gemfire
",1.0,-1.0,-1.0,-1.0,False,False,False,1373301296000.0,-1.0,-1.0,1374471377000.0,1374472277000.0,1373301296000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301296000.0,-1.0,"Update Sink's GemFire section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#gemfire
",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-417","Story","","2013-07-08T16:34:20.000+0000","mark.pollack","","Update Sink's TCP section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tcp_sinks",1.0,-1.0,-1.0,-1.0,False,False,False,1373301260000.0,-1.0,-1.0,1374471386000.0,1374472286000.0,1373301260000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301260000.0,-1.0,"Update Sink's TCP section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tcp_sinks",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-416","Story","","2013-07-08T16:33:30.000+0000","mark.pollack","","Update Sink's HDFS section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#hdfs",1.0,-1.0,-1.0,-1.0,False,False,False,1373301210000.0,-1.0,-1.0,1374471392000.0,1374472292000.0,1373301210000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301210000.0,-1.0,"Update Sink's HDFS section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#hdfs",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-415","Story","","2013-07-08T16:32:27.000+0000","mark.pollack",""," Update Sink's File section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#file_sinks",1.0,-1.0,-1.0,-1.0,False,False,False,1373301147000.0,-1.0,-1.0,1374471399000.0,1374472299000.0,1373301147000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301147000.0,-1.0," Update Sink's File section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#file_sinks",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-414","Story","","2013-07-08T16:31:49.000+0000","mark.pollack","","Update Sink's Log section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#log_sinks",1.0,-1.0,-1.0,-1.0,False,False,False,1373301109000.0,-1.0,-1.0,1374471406000.0,1374472306000.0,1373301109000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301109000.0,-1.0,"Update Sink's Log section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#log_sinks",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-413","Story","","2013-07-08T16:30:05.000+0000","mark.pollack","","Update Processors Script section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#script",1.0,-1.0,-1.0,-1.0,False,False,False,1373301005000.0,-1.0,-1.0,1374471229000.0,1374472129000.0,1373301005000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373301005000.0,-1.0,"Update Processors Script section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#script",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-412","Story","","2013-07-08T16:29:33.000+0000","mark.pollack","","Update Processors JSON Field Extractor section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#json-field-extractor",1.0,-1.0,-1.0,-1.0,False,False,False,1373300973000.0,-1.0,-1.0,1374471237000.0,1374472137000.0,1373300973000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373300973000.0,-1.0,"Update Processors JSON Field Extractor section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#json-field-extractor",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-411","Story","","2013-07-08T16:28:44.000+0000","mark.pollack","","Update Processors Transform section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#transform",1.0,-1.0,-1.0,-1.0,False,False,False,1373300924000.0,-1.0,-1.0,1374471245000.0,1374472145000.0,1373300924000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373300924000.0,-1.0,"Update Processors Transform section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#transform",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-410","Story","","2013-07-08T16:27:35.000+0000","mark.pollack","","Update Processors Filter & JSon Filed Value Filter section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#filter
http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#json-value-filter",1.0,-1.0,-1.0,-1.0,False,False,False,1373300855000.0,-1.0,-1.0,1374471254000.0,1374472154000.0,1373300855000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373300855000.0,-1.0,"Update Processors Filter & JSon Filed Value Filter section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#filter
http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#json-value-filter",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-409","Story","","2013-07-08T16:17:49.000+0000","mark.pollack","","Update Sources TCP section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tcp",1.0,-1.0,-1.0,-1.0,False,False,False,1373300269000.0,-1.0,-1.0,1374471186000.0,1374472086000.0,1373300269000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373300269000.0,-1.0,"Update Sources TCP section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tcp",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-408","Story","","2013-07-08T16:16:43.000+0000","mark.pollack","","Update Sources twitter Syslog section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#syslog",1.0,-1.0,-1.0,-1.0,False,False,False,1373300203000.0,-1.0,-1.0,1374471139000.0,1374472039000.0,1373300203000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373300203000.0,-1.0,"Update Source Syslog section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#syslog",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-407","Story","","2013-07-08T16:16:12.000+0000","mark.pollack","","Update Sources twitter Gemfire CQ section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#gemfire-cq",1.0,-1.0,-1.0,-1.0,False,False,False,1373300172000.0,-1.0,-1.0,1374471057000.0,1374471957000.0,1373300172000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373300172000.0,-1.0,"Update Sources Gemfire CQ section to use Shell commands instead of curl ","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#gemfire-cq",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-406","Story","","2013-07-08T16:15:16.000+0000","mark.pollack","","Update Sources twitter search section to use Shell commands instead of curl","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#twittersearch",1.0,-1.0,-1.0,-1.0,False,False,False,1373300116000.0,-1.0,-1.0,1374471074000.0,1374471974000.0,1373300116000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373300116000.0,-1.0,"Update Sources twitter search section to use Shell commands instead of curl","See 

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#twittersearch",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-405","Story","","2013-07-08T16:14:39.000+0000","mark.pollack","","Update Sources tail section to use Shell commands instead of curl","http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tail",1.0,-1.0,-1.0,-1.0,False,False,False,1373300079000.0,-1.0,-1.0,1374471012000.0,1374471912000.0,1373300079000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1373300079000.0,-1.0,"Update Sources tail section to use Shell commands instead of curl","http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#tail",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-404","Story","Sprint 10","2013-07-08T15:36:08.000+0000","mark.pollack","","Update documentation section ""Running in Distributed Mode"" to show use of RabbitMQ in addition to Redis","The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes.  This functionality is provided the core ChannelRegistry abstraction.  A new intro paragraph should should convey that it isn't a 'redis' only or 'rabbitmq' only system.

There should be ""Installing RabbitMQ"" and ""Starting RabbitMQ"" sections to match those for Redis.

""Starting Spring XD in Distributed Mode"" should cover how to configure the system to select to use Redis or Rabbit.",3.0,0.9195127175662242,0.9195083236759286,0.0023254664389166,True,True,True,1373297768000.0,1374134851000.0,1374138451000.0,1374207223000.0,1374208123000.0,1373297768000.0,-1.0,1373299885000.0,1373303485000.0,1374134847000.0,1374138447000.0,1374134851000.0,1374138451000.0,"mark.pollack",1373303485000.0,1373297768000.0,43,1373297768000.0,-1.0,"Update documentation section ""Running in Distributed Mode"" to show use of RabbitMQ in addition to Redis","The documentation in the Running in Distributed Mode chapter should discuss that the distributed runtime can use essentially any middleware to communicate between nodes.  This functionality is provided by the core ChannelRegistry abstraction.  A new intro paragraph shoul convey that it isn't a 'redis' only or 'rabbitmq' only system.

There should be ""Installing RabbitMQ"" and ""Starting RabbitMQ"" sections to match those for Redis.

""Starting Spring XD in Distributed Mode"" should cover how to configure the system to select to use Redis or Rabbit.",3.0,False,3.0,0,0.0,-1,0,True
"XD-403","Story","Sprint 10","2013-07-08T15:28:33.000+0000","mark.pollack","","Update Sources HTTP section to use Shell commands instead of curl","See http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#http



",1.0,0.979876725410192,0.9790259571143328,0.0111641635558642,True,True,True,1373297313000.0,1373523057000.0,1373526657000.0,1373526793000.0,1373527693000.0,1373297313000.0,-1.0,1373299885000.0,1373303485000.0,1373522861000.0,1373526461000.0,1373523057000.0,1373526657000.0,"mark.pollack",1373303485000.0,1373297313000.0,43,1373297313000.0,-1.0,"Update Sources section to use Shell commands instead of curl","See http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#http



",1.0,False,1.0,0,0.0,-1,0,True
"XD-402","Story","Sprint 10","2013-07-08T09:04:05.000+0000","mark.pollack","","Create a command to browse the HDFS file system","There are existing commands that can be taken from 

https://github.com/SpringSource/spring-hadoop-samples

or 

https://github.com/SpringSource/impala

that can be used for this",4.0,0.2057365370605566,0.2057300900645344,0.0065849617370786,True,True,True,1373274245000.0,1373433805000.0,1373437405000.0,1374048900000.0,1374049800000.0,1373274245000.0,-1.0,1373279352000.0,1373282952000.0,1373433800000.0,1373437400000.0,1373433805000.0,1373437405000.0,"mark.pollack",1373282952000.0,1373274245000.0,43,1373274245000.0,-1.0,"Create a command to browse the HDFS file system","There are existing commands that can be taken from 

https://github.com/SpringSource/spring-hadoop-samples

or 

https://github.com/SpringSource/impala

that can be used for this",4.0,False,4.0,0,0.0,0,0,False
"XD-401","Story","Sprint 10","2013-07-08T09:02:43.000+0000","mark.pollack","","Create a shell command to post data to an http port for use with the http source module","the current streams chapter

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#streams

shows using curl to post some data to a http source module, 

curl -d ""hello"" http://localhost:9000

create a shell command so curl doesn't have to be used.

https://github.com/SpringSource/rest-shell

has a command already developed for this.",1.0,0.0472787713702017,0.0472294899824379,0.02324737464607,True,True,True,1373274163000.0,1373284716000.0,1373288316000.0,1373496471000.0,1373497371000.0,1373274163000.0,-1.0,1373279352000.0,1373282952000.0,1373284705000.0,1373288305000.0,1373284716000.0,1373288316000.0,"mark.pollack",1373282952000.0,1373274163000.0,43,1373274163000.0,-1.0,"Create a shell command to post data to an http port for use with the http source module","the current streams chapter

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#streams

shows using curl to post some data to a http source module, 

curl -d ""hello"" http://localhost:9000

create a shell command so curl doesn't have to be used.

https://github.com/SpringSource/rest-shell

has a command already developed for this.",1.0,False,1.0,0,0.0,0,0,False
"XD-400","Story","Sprint 10","2013-07-08T09:00:53.000+0000","mark.pollack","","Update Streams Chapter to use shell commands instead of curl","the current streams chapter

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#streams

shows creation and deleting streams using CURL - switch to use shell.  Also add listing of a stream.

there is also an example of creating a stream, this should be replaced as well.
",1.0,0.3208006752682986,0.3207805779975079,0.0212990875839061,True,True,True,1373274053000.0,1373353865000.0,1373357465000.0,1373521943000.0,1373522843000.0,1373274053000.0,-1.0,1373279352000.0,1373282952000.0,1373353860000.0,1373357460000.0,1373353865000.0,1373357465000.0,"mark.pollack",1373282952000.0,1373274053000.0,43,1373274053000.0,-1.0,"Update Streams Chapter to use shell commands instead of curl","the current streams chapter

http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#streams

shows creation and deleting streams using CURL - switch to use shell.  Also add listing of a stream.

there is also an example of creating a stream, this should be replaced as well.
",1.0,False,1.0,0,0.0,-1,0,False
"XD-399","Story","Sprint 10","2013-07-08T08:54:28.000+0000","mark.pollack","","Update Getting Started chapter to include a section on starting the shell.","The chapter on how to start up the shell should ocme right after ""start the runtime"" and before ""create the stream""",1.0,-1.0,0.9993343278315128,0.0227932566607317,False,True,True,1373273668000.0,-1.0,-1.0,1373522140000.0,1373523040000.0,1373273668000.0,-1.0,1373279352000.0,1373282952000.0,1373522874000.0,1373523040000.0,-1.0,-1.0,"mark.pollack",1373282952000.0,1373273668000.0,43,1373273668000.0,-1.0,"Update Getting Started chapter to include a section on starting the shell.","The chapter on how to start up the shell should ocme right after ""start the runtime"" and before ""create the stream""",1.0,False,1.0,0,0.0,0,0,False
"XD-398","Story","Sprint 10","2013-07-08T08:53:45.000+0000","mark.pollack","","Update Getting Started chapter to use Shell commands instead of curl","See http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#getting-started

",1.0,0.3242551163420022,0.3242349215837537,0.023131076097888,True,True,True,1373273625000.0,1373353907000.0,1373357507000.0,1373520314000.0,1373521214000.0,1373273625000.0,-1.0,1373279352000.0,1373282952000.0,1373353902000.0,1373357502000.0,1373353907000.0,1373357507000.0,"mark.pollack",1373282952000.0,1373273625000.0,43,1373273625000.0,-1.0,"Update Getting Started chapter to use Shell commands instead of curl","See http://static.springsource.org/spring-xd/docs/1.0.0.M1/reference/html/#getting-started

",1.0,False,1.0,0,0.0,-1,0,False
"XD-397","Story","Sprint 12","2013-07-08T08:52:18.000+0000","mark.pollack","","Add a chapter on Monitoring","This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia.

in particular showing how some existing metrics for inbound message channel adapters or the 'inbound' channel of the stream, that indicate the number of messages processed per section.  

",2.0,-1.0,0.9999997607228658,0.0484170501548003,False,True,True,1373273538000.0,-1.0,-1.0,1398348164000.0,1398349064000.0,1373273538000.0,-1.0,1374487621000.0,1374491221000.0,1398349058000.0,1398349064000.0,-1.0,-1.0,"mark.pollack",1374491221000.0,1373273538000.0,43,1373273538000.0,-1.0,"Document Monitoring & Management Features","This section should discuss what is exposed via JMX, how you can view it in JConsole, and how you can view it over http via Jolikia.

in particular showing how some existing metrics for inbound message channel adapters or the 'inbound' channel of the stream, that indicate the number of messages processed per section.  

",2.0,False,2.0,0,0.0,0,0,True
"XD-396","Story","Sprint 12","2013-07-08T08:49:24.000+0000","mark.pollack","","Add section to documentation that shows how to enable JMX and pass in Jolokia http options.","This should likely be in the ""start the runtime"" section of Getting Started section.",1.0,0.9997782955401808,0.9997780412919473,0.05145378292557,True,True,True,1373273364000.0,1396867116000.0,1396870716000.0,1396871448000.0,1396872348000.0,1373273364000.0,-1.0,1374487621000.0,1374491221000.0,1396867110000.0,1396870710000.0,1396867116000.0,1396870716000.0,"mark.pollack",1374491221000.0,1373273364000.0,43,1373273364000.0,-1.0,"Add section to documentation that shows command line options available for each server","This should likely be in the ""start the runtime"" section of Getting Started section.",1.0,False,1.0,0,0.0,0,0,True
"XD-393","Improvement","","2013-07-06T10:33:33.000+0000","jvalkeal","","Allow Streamserver/tomcat to chooce free port","Currently you need to define a port which tomcat will use to bind to. If XD is embedded or run in a Hadoop it is not possible to know which port should be used. Current embedded tomcat version is able to use the 0-port trick for it to choose free port.

Real binded port can be asked from a connector using below example.

org/springframework/xd/dirt/stream/StreamServer.java:
public int getLocalPort() {
  return this.tomcat.getConnector().getLocalPort();
}


Also I believe Streamserver should provide a method to ask what is the real url to connect to or atleast what streamserver thinks what it is. This will make life much easier for components empedding spring-xd.
",1.0,-1.0,-1.0,-1.0,False,False,False,1373106813000.0,-1.0,-1.0,1389909445000.0,1389910345000.0,1373106813000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"jvalkeal",-1.0,-1.0,0,1373106813000.0,-1.0,"Allow Streamserver/tomcat to chooce free port","Currently you need to define a port which tomcat will use to bind to. If XD is embedded or run in a Hadoop it is not possible to know which port should be used. Current embedded tomcat version is able to use the 0-port trick for it to choose free port.

Real binded port can be asked from a connector using below example.

org/springframework/xd/dirt/stream/StreamServer.java:
public int getLocalPort() {
  return this.tomcat.getConnector().getLocalPort();
}


Also I believe Streamserver should provide a method to ask what is the real url to connect to or atleast what streamserver thinks what it is. This will make life much easier for components empedding spring-xd.
",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-392","Story","Sprint 9","2013-07-05T11:59:43.000+0000","grenfro","grenfro","Create a stubbed out job controller ","",1.0,0.0009628065406118,0.0,0.0006027654392905,True,True,True,1373025583000.0,1373025821000.0,1373029421000.0,1373271877000.0,1373272777000.0,1373025583000.0,-1.0,1373025732000.0,1373029332000.0,1373025583000.0,1373029183000.0,1373025821000.0,1373029421000.0,"grenfro",1373029332000.0,1373025583000.0,43,1373025583000.0,-1.0,"Create a stubbed out job controller ","",1.0,False,1.0,0,0.0,-1,0,False
"XD-388","Story","Sprint 10","2013-07-05T08:39:02.000+0000","mark.pollack","","Create design document for implementation strategy for ingesting data from twitter into HDFS that can be analyzed by HAWQ","As part of the Hadoop World demonstration work, the flow of data using XD from twitter to be analyzed by HAWQ as done.  Part of this work had the data going into HDFS that HAWQ was able to query using external tables.

The work for this story is to identify the concrete technical tasks/stories to be created do deliver and document this functionality in XD.

",3.0,-1.0,0.9995770665989974,0.4323843358481604,False,True,True,1373013542000.0,-1.0,-1.0,1373627396000.0,1373628296000.0,1373013542000.0,1373299918000.0,1373279352000.0,1373282952000.0,1373628036000.0,1373628296000.0,-1.0,-1.0,"mark.pollack",1373282952000.0,1373013542000.0,43,1373299918000.0,-1.0,"Create design document for implementation strategy for ingesting data from twitter into HDFS that can be analyzed by HAWQ","As part of the Hadoop World demonstration work, the flow of data using XD from twitter to be analyzed by HAWQ as done.  Part of this work had the data going into HDFS that HAWQ was able to query using external tables.

The work for this story is to identify the concrete technical tasks/stories to be created do deliver and document this functionality in XD.

",0.0,False,0.0,1,-1.0,0,1,False
"XD-386","Story","Sprint 31","2013-07-04T01:34:54.000+0000","eric.bottard","eric.bottard","Automate copyright header management","Some (java) files are currently missing headers.

The plugin at https://github.com/hierynomus/license-gradle-plugin can help, but initial trial revealed that:

- skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001-${current} or force all files to have ${current} year. Don't know the legal implications of this
- Default source sets encompass all files ""in the classpath"" basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I don't think this is what we do on other projetcs.",3.0,0.9867634402974832,0.986763624356308,0.9999969016764508,True,True,True,1372901694000.0,1405068479000.0,1405072079000.0,1405499068000.0,1405499968000.0,1372901694000.0,-1.0,1405499867000.0,1405499968000.0,1405068485000.0,1405072085000.0,1405068479000.0,1405072079000.0,"eric.bottard",1405499968000.0,1372901694000.0,43,1372901694000.0,-1.0,"Automate copyright header management","Some (java) files are currently missing headers.

The plugin at https://github.com/hierynomus/license-gradle-plugin can help, but initial trial revealed that:

- skipExistingHeaders does not seem to be honored. We may then need to use a year construction like 2001-${current} or force all files to have ${current} year. Don't know the legal implications of this
- Default source sets encompass all files ""in the classpath"" basically, so that means .xml as well as .properties files for example. It would seem logical to add header to those as well, but I don't think this is what we do on other projetcs.",3.0,False,3.0,0,0.0,0,0,False
"XD-385","Story","","2013-07-04T01:28:53.000+0000","eric.bottard","eric.bottard","Error handling on Streams","Have proper exceptions for common error cases on Stream creation/deployment and propagate those to clients correctly.",5.0,-1.0,0.0,-1.0,False,False,True,1372901333000.0,-1.0,-1.0,1377747768000.0,1377748668000.0,1372901333000.0,-1.0,-1.0,-1.0,1372901333000.0,1372904933000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1372901333000.0,-1.0,"Error handling on Streams","Have proper exceptions for common error cases on Stream creation/deployment and propagate those to clients correctly.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-384","Story","Sprint 9","2013-07-03T14:22:19.000+0000","dturanski","","Implement list() method on TapDeployer()","",1.0,0.8948432502750127,0.8948612347544069,0.0005290434355151,True,True,True,1372861339000.0,1373458416000.0,1373462016000.0,1373527681000.0,1373528581000.0,1372861339000.0,-1.0,1372861692000.0,1372865292000.0,1373458428000.0,1373462028000.0,1373458416000.0,1373462016000.0,"dturanski",1372865292000.0,1372861339000.0,43,1372861339000.0,-1.0,"Implement list() method on TapDeployer()","",1.0,False,1.0,0,0.0,-1,0,False
"XD-383","Story","Sprint 9","2013-07-03T14:21:42.000+0000","dturanski","","Implement list() method on TapController","",1.0,0.7313252785915748,0.731347460128687,0.0006654461133681,True,True,True,1372861302000.0,1373289912000.0,1373293512000.0,1373446475000.0,1373447375000.0,1372861302000.0,-1.0,1372861692000.0,1372865292000.0,1373289925000.0,1373293525000.0,1373289912000.0,1373293512000.0,"dturanski",1372865292000.0,1372861302000.0,43,1372861302000.0,-1.0,"Implement list() method on TapController","",1.0,False,1.0,0,0.0,0,0,False
"XD-382","Story","Sprint 9","2013-07-03T14:20:38.000+0000","dturanski","","Create TapRepository","see StreamsRepository as an example. This includes in memory and Redis implementations",1.0,0.999057612497508,0.9991723904625552,0.0027425892700725,True,True,True,1372861238000.0,1373026619000.0,1373026775000.0,1373025875000.0,1373026775000.0,1372861238000.0,-1.0,1372861692000.0,1372865292000.0,1373026638000.0,1373026775000.0,1373026619000.0,1373026775000.0,"dturanski",1372865292000.0,1372861238000.0,43,1372861238000.0,-1.0,"Create TapRepository","see StreamsRepository as an example. This includes in memory and Redis implementations",1.0,False,1.0,0,0.0,-1,0,False
"XD-381","Story","Sprint 9","2013-07-03T14:19:33.000+0000","dturanski","","add create() and deploy() methods to TapDeployer","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit",1.0,0.8915998189453567,0.8916752583800639,0.003262755551085,True,True,True,1372861173000.0,1373002998000.0,1373006598000.0,1373019341000.0,1373020241000.0,1372861173000.0,-1.0,1372861692000.0,1372865292000.0,1373003010000.0,1373006610000.0,1373002998000.0,1373006598000.0,"dturanski",1372865292000.0,1372861173000.0,43,1372861173000.0,-1.0,"add create() and deploy() methods to TapDeployer","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit",1.0,False,1.0,0,0.0,-1,0,False
"XD-379","Story","Sprint 9","2013-07-03T14:17:37.000+0000","dturanski","","add create() and deploy() methods to StreamDeployer","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit

Refactor current DefaultStreamDeployer",1.0,0.1061752706361717,0.0966626339651232,0.0011068107266369,True,True,True,1372861057000.0,1372921876000.0,1372925476000.0,1373432974000.0,1373433874000.0,1372861057000.0,-1.0,1372861691000.0,1372865291000.0,1372916427000.0,1372920027000.0,1372921876000.0,1372925476000.0,"dturanski",1372865291000.0,1372861057000.0,43,1372861057000.0,-1.0,"add create() and deploy() methods to StreamDeployer","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit

Refactor current DefaultStreamDeployer",1.0,False,1.0,0,0.0,0,0,False
"XD-378","Story","Sprint 9","2013-07-03T14:16:24.000+0000","dturanski","","add create() and deploy() methods to StreamsController","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit

create optionally deploys",1.0,0.7185502480371724,0.0957290826639297,0.0012340765720948,True,True,True,1372860984000.0,1373272640000.0,1373276240000.0,1373432982000.0,1373433882000.0,1372860984000.0,-1.0,1372861691000.0,1372865291000.0,1372915827000.0,1372919427000.0,1373272640000.0,1373276240000.0,"dturanski",1372865291000.0,1372860984000.0,43,1372860984000.0,-1.0,"add create() and deploy() methods to StreamsController","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit

create optionally deploys",1.0,False,1.0,0,0.0,0,0,False
"XD-377","Story","Sprint 9","2013-07-03T14:15:09.000+0000","dturanski","","add create() and deploy() methods to JobDeployer","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit

Create the deployer if it doesn't exist.",1.0,0.999983597121444,0.9999761412675549,0.0011660955482587,True,True,True,1372860909000.0,1373531512000.0,1373531523000.0,1373530623000.0,1373531523000.0,1372860909000.0,-1.0,1372861691000.0,1372865291000.0,1373531507000.0,1373531523000.0,1373531512000.0,1373531523000.0,"dturanski",1372865291000.0,1372860909000.0,43,1372860909000.0,-1.0,"add create() and deploy() methods to JobDeployer","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit

Create the deployer if it doesn't exist.",1.0,False,1.0,0,0.0,-1,0,False
"XD-376","Story","Sprint 9","2013-07-03T14:14:07.000+0000","dturanski","","add create() and deploy() methods to JobsController","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit

Create the controller if it doesn't exist. Test with MvcTest",1.0,-1.0,0.9999940408440549,0.001257381904427,False,True,True,1372860847000.0,-1.0,-1.0,1373531183000.0,1373532083000.0,1372860847000.0,-1.0,1372861691000.0,1372865291000.0,1373532079000.0,1373532083000.0,-1.0,-1.0,"dturanski",1372865291000.0,1372860847000.0,43,1372860847000.0,-1.0,"add create() and deploy() methods to JobsController","see https://docs.google.com/a/gopivotal.com/drawings/d/1kCNbVspRBjGc10itF9cSkwST8WgCjgg3wGzFKqLCAzU/edit

Create the controller if it doesn't exist. Test with MvcTest",1.0,False,1.0,0,0.0,-1,0,False
"XD-375","Story","Sprint 9","2013-07-03T14:11:49.000+0000","dturanski","","Command to delete tap","",1.0,0.3963423903014228,0.3314256045189965,0.0006142932656067,True,True,True,1372860709000.0,1373494296000.0,1373497896000.0,1374458394000.0,1374459294000.0,1372860709000.0,-1.0,1372861691000.0,1372865291000.0,1373390521000.0,1373394121000.0,1373494296000.0,1373497896000.0,"dturanski",1372865291000.0,1372860709000.0,43,1372860709000.0,-1.0,"Command to delete tap","",1.0,False,1.0,0,0.0,0,0,False
"XD-374","Story","Sprint 9","2013-07-03T14:11:25.000+0000","dturanski","","Command to list taps","",1.0,0.865705588019175,0.8657120677835188,0.0013037285859987,True,True,True,1372860685000.0,1373528692000.0,1373532292000.0,1373631418000.0,1373632318000.0,1372860685000.0,-1.0,1372861691000.0,1372865291000.0,1373528697000.0,1373532297000.0,1373528692000.0,1373532292000.0,"dturanski",1372865291000.0,1372860685000.0,43,1372860685000.0,-1.0,"Command to list taps","",1.0,False,1.0,0,0.0,0,0,False
"XD-373","Story","Sprint 9","2013-07-03T14:10:37.000+0000","dturanski","","Command to create a tap","To store it's definition and optionally deploy with --autostart flag",1.0,0.8205934129143793,0.8205681290236941,0.0016655762988808,True,True,True,1372860637000.0,1373379920000.0,1373383520000.0,1373492551000.0,1373493451000.0,1372860637000.0,-1.0,1372861691000.0,1372865291000.0,1373379904000.0,1373383504000.0,1373379920000.0,1373383520000.0,"dturanski",1372865291000.0,1372860637000.0,43,1372860637000.0,-1.0,"Command to create a tap","To store it's definition and optionally deploy with --autostart flag",1.0,False,1.0,0,0.0,0,0,False
"XD-372","Story","Sprint 9","2013-07-03T14:06:50.000+0000","dturanski","","Command to deploy a job","Deploy an existing job. Must exist in the JobsRepository",1.0,0.4930972251866776,0.4930988862044995,0.001063881914921,True,True,True,1372860410000.0,1373454139000.0,1373457739000.0,1374063591000.0,1374064491000.0,1372860410000.0,-1.0,1372861691000.0,1372865291000.0,1373454141000.0,1373457741000.0,1373454139000.0,1373457739000.0,"dturanski",1372865291000.0,1372860410000.0,43,1372860410000.0,-1.0,"Command to deploy a job","Deploy an existing job. Must exist in the JobsRepository",1.0,False,1.0,0,0.0,-1,0,False
"XD-371","Story","Sprint 9","2013-07-03T14:05:33.000+0000","dturanski","","Command for creating a job","optional --autostart switch to also deploy the job",1.0,0.4930876379711115,0.4931158737263806,0.0011277692839827,True,True,True,1372860333000.0,1373454083000.0,1373457683000.0,1374063580000.0,1374064480000.0,1372860333000.0,-1.0,1372861691000.0,1372865291000.0,1373454117000.0,1373457717000.0,1373454083000.0,1373457683000.0,"dturanski",1372865291000.0,1372860333000.0,43,1372860333000.0,-1.0,"Command for creating a job","optional --autostart switch to also deploy the job",1.0,False,1.0,0,0.0,-1,0,False
"XD-370","Story","Sprint 9","2013-07-03T14:04:01.000+0000","dturanski","","Add command for deploying a Stream","Deploy a named stream. The stream must exist in the StreamRepository",3.0,0.7189490640177678,0.0968616963784513,0.0025277970510196,True,True,True,1372860241000.0,1373272646000.0,1373276246000.0,1373432963000.0,1373433863000.0,1372860241000.0,-1.0,1372861691000.0,1372865291000.0,1372915803000.0,1372919403000.0,1373272646000.0,1373276246000.0,"dturanski",1372865291000.0,1372860241000.0,43,1372860241000.0,-1.0,"Add command for deploying a Stream","Deploy a named stream. The stream must exist in the StreamRepository",3.0,False,3.0,0,0.0,0,0,False
"XD-369","Story","Sprint 9","2013-07-03T13:56:04.000+0000","aclement","aclement","Further DSL extensions","Extend the DSL in the following ways:

- stream naming, use <name>'='
{code}
mystream = http | file
{code}

- module aliasing (for later referencing) use <label>':'
{code}
mystream = http | t1: transform --expression=payload.toUpperCase() | t2: transform --expression=payload.toLowerCase()
{code}

- sequence of job steps with '&'
{code}
mybigjobby = step1 --option=value & step2 --option=value
{code}

- Channels for sources and sinks, use '>', channels references prefixed ':'
{code}
// sink channel called foo
http | transform --expression=payload.toUppercase() > :foo
// source channel called foo
:foo > count | file
{code}

- Qualify channels with a stream ':'<stream>'.'<channel>
{code}
mystream = http | transform --expression=payload.toUppercase() > :foo
:mystream.foo > count | log
{code}


- Reusable substreams, define then reuse
{code}
myIntricateFlow = transform --expression=payload.toUppercase() | transform --expression=payload.toLowercase()

http | myIntricateFlow | file
{code}

- Parameterized substreams, use $XX or ${XX} to indicate parameters
{code}
obfuscateName = transform --expression=payload.replaceAll(""${name}"",XXX)

twitter --query=Bieber | obfuscateName --name=Justin | file
{code}

- Tapping (still not 100% happy about the format here)
{code}
mystream = http | filter | t1: transform XXX | t2: transform YYY | file

// These are then equivalent (tapping on a module is effectively tapping a channel, hence the '>')
tap filter > count | file
tap mystream.filter > count | file

mystream = http | filter > :foo

tap :foo > count | file
tap :mystream.foo > count | file
{code}
I'd still like to thing about removing 'tap' and using a symbol (perhaps '@', but @:mystream.foo is a little odd)

still not covered, topological constraints on the stream components.",3.0,0.1910784137416991,0.0,0.0007488579721718,True,True,True,1372859764000.0,1373351712000.0,1373355312000.0,1375433451000.0,1375434351000.0,1372859764000.0,-1.0,1372861692000.0,1372865292000.0,1372859764000.0,1372863364000.0,1373351712000.0,1373355312000.0,"aclement",1372865292000.0,1372859764000.0,43,1372859764000.0,-1.0,"Further DSL extensions","Extend the DSL in the following ways:

- stream naming, use <name>'='
{code}
mystream = http | file
{code}

- module aliasing (for later referencing) use <label>':'
{code}
mystream = http | t1: transform --expression=payload.toUpperCase() | t2: transform --expression=payload.toLowerCase()
{code}

- sequence of job steps with '&'
{code}
mybigjobby = step1 --option=value & step2 --option=value
{code}

- Channels for sources and sinks, use '>', channels references prefixed ':'
{code}
// sink channel called foo
http | transform --expression=payload.toUppercase() > :foo
// source channel called foo
:foo > count | file
{code}

- Qualify channels with a stream ':'<stream>'.'<channel>
{code}
mystream = http | transform --expression=payload.toUppercase() > :foo
:mystream.foo > count | log
{code}


- Reusable substreams, define then reuse
{code}
myIntricateFlow = transform --expression=payload.toUppercase() | transform --expression=payload.toLowercase()

http | myIntricateFlow | file
{code}

- Parameterized substreams, use $XX or ${XX} to indicate parameters
{code}
obfuscateName = transform --expression=payload.replaceAll(""${name}"",XXX)

twitter --query=Bieber | obfuscateName --name=Justin | file
{code}

- Tapping (still not 100% happy about the format here)
{code}
mystream = http | filter | t1: transform XXX | t2: transform YYY | file

// These are then equivalent (tapping on a module is effectively tapping a channel, hence the '>')
tap filter > count | file
tap mystream.filter > count | file

mystream = http | filter > :foo

tap :foo > count | file
tap :mystream.foo > count | file
{code}
I'd still like to thing about removing 'tap' and using a symbol (perhaps '@', but @:mystream.foo is a little odd)

still not covered, topological constraints on the stream components.",3.0,False,3.0,0,0.0,0,0,False
"XD-368","Story","Sprint 9","2013-07-03T10:11:27.000+0000","luke","","Improve connection handling in RedisAggregateCounterService.","This is currently too chatty. It should be possible to use a single connection for each ""increment"" operation.",2.0,-1.0,0.9122680993796196,0.0914541838582326,False,True,True,1372846287000.0,-1.0,-1.0,1373013832000.0,1373014732000.0,1372846287000.0,-1.0,1372861692000.0,1372865292000.0,1372999954000.0,1373003554000.0,-1.0,-1.0,"luke",1372865292000.0,1372846287000.0,43,1372846287000.0,-1.0,"Improve connection handling in RedisAggregateCounterService.","This is currently too chatty. It should be possible to use a single connection for each ""increment"" operation.",2.0,False,2.0,0,0.0,-1,0,False
"XD-367","Story","Sprint 9","2013-07-01T10:55:42.000+0000","mark.pollack","","Final review of REST API structure document for streams, taps and jobs","Get closure on open discussion points for REST API wrt to streams, taps and jobs.
",3.0,0.1879294721739787,0.3337118166756244,0.0001726772412506,True,True,True,1372676142000.0,1373014612000.0,1373018212000.0,1374476290000.0,1374477190000.0,1372676142000.0,-1.0,1372676453000.0,1372680053000.0,1373277173000.0,1373280773000.0,1373014612000.0,1373018212000.0,"mark.pollack",1372680053000.0,1372676142000.0,43,1372676142000.0,-1.0,"Final review of REST API structure document for streams, taps and jobs","Get closure on open discussion points for REST API wrt to streams, taps and jobs.
",3.0,False,3.0,0,0.0,-1,0,False
"XD-366","Story","Sprint 9","2013-07-01T10:54:23.000+0000","mark.pollack","","Create a XD job definition","when posting the DSL to create a spring batch job
e.g. ""trigger job.xml --option1=foo""

it should be stored (in redis) so that a listing of XD job definitions can be retrieved.",3.0,-1.0,0.3055768205663006,0.0006459765953878,False,True,True,1372676063000.0,-1.0,-1.0,1373277352000.0,1373278252000.0,1372676063000.0,-1.0,1372676452000.0,1372680052000.0,1372860078000.0,1372863678000.0,-1.0,-1.0,"mark.pollack",1372680052000.0,1372676063000.0,43,1372676063000.0,-1.0,"Create a XD job definition","when posting the DSL to create a spring batch job
e.g. ""trigger job.xml --option1=foo""

it should be stored (in redis) so that a listing of XD job definitions can be retrieved.",3.0,False,3.0,0,0.0,-1,0,False
"XD-365","Story","Sprint 9","2013-07-01T10:32:29.000+0000","mark.pollack","dturanski","Support having multiple property placeholders defined in different modules","Right now the PPC for jolokia mgmt is conflicting with the PPC used to resolve redis properties.  Need to determine a strategy such that multiple PPCs can be used.",4.0,0.4249006622516556,0.4283443708609271,0.2252980132450331,True,True,True,1372674749000.0,1372677957000.0,1372681557000.0,1372681399000.0,1372682299000.0,1372674749000.0,-1.0,1372676450000.0,1372680050000.0,1372677983000.0,1372681583000.0,1372677957000.0,1372681557000.0,"mark.pollack",1372680050000.0,1372674749000.0,43,1372674749000.0,-1.0,"Support having multiple property placeholders defined in different modules","Right now the PPC for jolokia mgmt is conflicting with the PPC used to resolve redis properties.  Need to determine a strategy such that multiple PPCs can be used.",4.0,False,4.0,0,0.0,0,0,False
"XD-364","Story","Sprint 9","2013-07-01T10:21:27.000+0000","mark.pollack","","Create trigger registry for storing trigger definitions","Redis based.",1.0,0.099959140185369,0.1001880068685865,0.0030580209367759,True,True,True,1372674087000.0,1372751393000.0,1372754993000.0,1373446563000.0,1373447463000.0,1372674087000.0,-1.0,1372676452000.0,1372680052000.0,1372751570000.0,1372755170000.0,1372751393000.0,1372754993000.0,"mark.pollack",1372680052000.0,1372675673000.0,43,1372675673000.0,-1.0,"Create TriggerDefinition Repository ","Redis based.",1.0,False,1.0,0,0.0,0,0,True
"XD-363","Story","Sprint 9","2013-07-01T10:19:53.000+0000","mark.pollack","","Add support for creating fixed delay triggers","",1.0,0.0688966439060868,0.0688934644620825,0.001563650561299,True,True,True,1372673993000.0,1372782340000.0,1372785940000.0,1374245695000.0,1374246595000.0,1372673993000.0,-1.0,1372676452000.0,1372680052000.0,1372782335000.0,1372785935000.0,1372782340000.0,1372785940000.0,"mark.pollack",1372680052000.0,1372675666000.0,43,1372675666000.0,-1.0,"Add support for creating fixed delay/ fixed rate triggers","",1.0,False,1.0,0,0.0,0,0,True
"XD-361","Story","Sprint 9","2013-07-01T10:18:13.000+0000","mark.pollack","","Create a trigger","",1.0,0.0556707629288274,0.0556688428059395,0.0016378648233486,True,True,True,1372673893000.0,1372760873000.0,1372764473000.0,1374235393000.0,1374236293000.0,1372673893000.0,1372781709000.0,1372676452000.0,1372680052000.0,1372760870000.0,1372764470000.0,1372760873000.0,1372764473000.0,"mark.pollack",1372680052000.0,1372676351000.0,43,1374057503000.0,1372781709000.0,"Create a trigger from Shell","",1.0,True,1.0,2,0.0,0,0,True
"XD-360","Story","Sprint 9","2013-07-01T10:16:59.000+0000","mark.pollack","","Add support for creating a spring batch job that references a named trigger","",2.0,0.4446112159307643,0.4446073706445725,0.0033748795143659,True,True,True,1372673819000.0,1373020694000.0,1373024294000.0,1373453095000.0,1373453995000.0,1372673819000.0,-1.0,1372676452000.0,1372680052000.0,1373020691000.0,1373024291000.0,1373020694000.0,1373024294000.0,"mark.pollack",1372680052000.0,1372676358000.0,43,1372676358000.0,-1.0,"Add support for creating a spring batch job that references a named trigger","",2.0,False,2.0,0,0.0,-1,0,False
"XD-359","Story","Sprint 9","2013-07-01T10:16:22.000+0000","mark.pollack","","Add support for creating a spring batch job that has an embedded trigger expression","",1.0,-1.0,0.9999953377236436,0.0031120694679177,False,True,True,1372673782000.0,-1.0,-1.0,1373530832000.0,1373531732000.0,1372673782000.0,-1.0,1372676452000.0,1372680052000.0,1373531728000.0,1373531732000.0,-1.0,-1.0,"mark.pollack",1372680052000.0,1372676402000.0,43,1372676402000.0,-1.0,"Add support for creating a spring batch job that has an embedded trigger expression","",1.0,False,1.0,0,0.0,-1,1,False
"XD-358","Story","Sprint 9","2013-07-01T10:14:53.000+0000","mark.pollack","","Add support for creating named cron triggers","Simple cron based triggers",2.0,-1.0,-1.0,0.0045654012769218,False,True,False,1372673693000.0,-1.0,-1.0,1373276902000.0,1373277802000.0,1372673693000.0,-1.0,1372676451000.0,1372680051000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1372680051000.0,1372675660000.0,43,1372675660000.0,-1.0,"Add support for creating named cron triggers","Simple cron based triggers",2.0,False,2.0,0,0.0,-1,1,False
"XD-357","Story","Sprint 9","2013-07-01T09:57:48.000+0000","hillert","","Creating a base class for Plugins ","It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub-classes.""",4.0,0.1013075576216172,0.1012998781524221,0.0048444651505687,True,True,True,1372672668000.0,1372751820000.0,1372755420000.0,1373453072000.0,1373453972000.0,1372672668000.0,-1.0,1372676453000.0,1372680053000.0,1372751814000.0,1372755414000.0,1372751820000.0,1372755420000.0,"hillert",1372680053000.0,1372672668000.0,43,1372672668000.0,-1.0,"Creating a base class for Plugins ","It might be worth creating a base class for Plugins that combines common concerns across plugins. E.g. That would allow us to hide the commonApplicationContext and BeanDefinitionAddingPostProcessor for common cases, instead exposing a simple addBeanDefinition method to sub-classes.""",4.0,False,4.0,0,0.0,-1,0,False
"XD-356","Story","Sprint 9","2013-07-01T09:02:48.000+0000","mark.pollack","iperumal","Test startup scripts on windows","startup scripts on windows should be tested, xd-admin, xd-container, xd-shell.",1.0,0.4784360458517762,0.4783225513562592,0.4768471229145386,True,True,True,1372669368000.0,1372677799000.0,1372681399000.0,1372686090000.0,1372686990000.0,1372669368000.0,-1.0,1372677771000.0,1372681371000.0,1372677797000.0,1372681397000.0,1372677799000.0,1372681399000.0,"mark.pollack",1372681371000.0,1372669368000.0,43,1372669368000.0,-1.0,"Test startup scripts on windows","startup scripts on windows should be tested, xd-admin, xd-container, xd-shell.",1.0,False,1.0,0,0.0,0,0,False
"XD-353","Story","Sprint 9","2013-07-01T08:55:57.000+0000","eric.bottard","Eric Bottard","Clean up configuration management","There are more and more concepts in XD, which are arguably orthogonal.
To name a few:
 - messaging middleware
 - storage of definitions
 - storage of counters

These are currently handled throuh ${xd.stuff} System properties which impact which xml files are loaded.

This is a good candidate for Spring profiles.

Moreover, using @Configuration classes would allow us to capture the contract (in terms of visible beans) that each concern need to fulfil. Ex:
public interface Storage {
   public StreamDefinitionRepository streamDefinitionRepository();
}

",5.0,0.8624625148181795,0.8624580898299167,0.0426420631419275,True,True,True,1372668957000.0,1376567103000.0,1376570703000.0,1377187843000.0,1377188743000.0,1372668957000.0,-1.0,1372861690000.0,1372865290000.0,1376567083000.0,1376570683000.0,1376567103000.0,1376570703000.0,"eric.bottard",1372865290000.0,1372668957000.0,43,1372668957000.0,-1.0,"Clean up Spring Configuration","- Container context should be separate from Admin Context in local mode (consistency across transports). Provide a LocalChannelRegistry to bridge deploy and undeploy channels to the ModuleDeployer

- Verify Plugins are not in common module context. They are only needed by the ModuleDeployer not the Modules.

- Add global-beans Config for beans to be shared among admin and container (and available to modules). This would be set as the parent context where needed. (Currently analytics-context.xml and JobRepository shared by Admin and modules)

- Fix ModuleDeployer sets parentContext twice
- Rename common.xml to module-common.xml

NOTE: Analytics parent only required in local mode

- Write unit tests to verify configurations are as expected. Correct bean implementations and no stray beans or redundant instances where not needed

- Decouple Command options from System properties (in general XD property names), this gives us some flexibility in mapping to properties or profiles where appropriate
",5.0,False,5.0,0,0.0,0,0,True
"XD-352","Story","Sprint 9","2013-07-01T07:33:44.000+0000","luke","luke","In-memory implementation of aggregate counter","",3.0,0.4961497115025998,0.00015357276058,0.0681588819903029,True,True,True,1372664024000.0,1372754484000.0,1372758084000.0,1372845448000.0,1372846348000.0,1372664024000.0,-1.0,1372676451000.0,1372680051000.0,1372664052000.0,1372667652000.0,1372754484000.0,1372758084000.0,"luke",1372680051000.0,1372664024000.0,43,1372664024000.0,-1.0,"In-memory implementation of aggregate counter","",3.0,False,3.0,0,0.0,-1,0,False
"XD-351","Story","Sprint 9","2013-07-01T07:33:09.000+0000","luke","luke","Support daily query resolution in redis aggregate counter","",4.0,0.9880436824372504,1.0303054290185524e-05,0.0248405382466337,True,True,True,1372663989000.0,1380527636000.0,1380531236000.0,1380621894000.0,1380622794000.0,1372663989000.0,1380518887000.0,1372861690000.0,1372865290000.0,1372664071000.0,1372667671000.0,1380527636000.0,1380531236000.0,"luke",1372865290000.0,1372663989000.0,43,1380518887000.0,1380518887000.0,"Support daily query resolution in redis aggregate counter","",1.0,True,1.0,1,300.0,0,0,False
"XD-350","Story","Sprint 9","2013-07-01T07:32:35.000+0000","luke","luke","Support hourly resolution in redis aggregate counter","",4.0,0.0689623039691283,0.0005865166937999,0.0684963794927452,True,True,True,1372663955000.0,1372676536000.0,1372680136000.0,1372845488000.0,1372846388000.0,1372663955000.0,1372846379000.0,1372676451000.0,1372680051000.0,1372664062000.0,1372667662000.0,1372676536000.0,1372680136000.0,"luke",1372680051000.0,1372663955000.0,43,1372846379000.0,1372846379000.0,"Support hourly resolution in redis aggregate counter","",2.0,True,2.0,1,100.0,-1,0,False
"XD-348","Story","Sprint 9","2013-07-01T07:22:24.000+0000","hillert","iperumal","Trigger - Add support for fixed-delay interval","Trigger - Add support for fixed-delay interval",1.0,0.2094956129669702,0.2094869154391555,0.2094208142277639,True,True,True,1372663344000.0,1372783778000.0,1372787378000.0,1373237320000.0,1373238220000.0,1372663344000.0,1372874084000.0,1372783735000.0,1372787335000.0,1372783773000.0,1372787373000.0,1372783778000.0,1372787378000.0,"hillert",1372787335000.0,1372663344000.0,43,1372874084000.0,1372874084000.0,"Trigger - Add support for fixed-delay interval","Trigger - Add support for fixed-delay interval",4.0,True,4.0,1,-75.0,-1,0,False
"XD-345","Story","Sprint 9","2013-06-28T07:16:02.000+0000","luke","luke","Replace ""gardenhose"" doc with new ""twitterstream""","",1.0,0.9947687149708098,0.0,0.5303025588568183,True,True,True,1372403762000.0,1372915286000.0,1372917976000.0,1372917076000.0,1372917976000.0,1372403762000.0,-1.0,1372676451000.0,1372680051000.0,1372403762000.0,1372407362000.0,1372915286000.0,1372917976000.0,"luke",1372680051000.0,1372403762000.0,43,1372403762000.0,-1.0,"Replace ""gardenhose"" doc with new ""twitterstream""","",1.0,False,1.0,0,0.0,-1,0,False
"XD-343","Story","Sprint 9","2013-06-28T06:34:54.000+0000","dturanski","","Revisit JMX object naming","The object naming is still not ideal for XD since SI conventions add some noise. Likely  need to design and implement a custom naming strategy",5.0,-1.0,0.9651715247395172,0.0262388879380744,False,True,True,1372401294000.0,-1.0,-1.0,1389946755000.0,1389947655000.0,1372401294000.0,-1.0,1372861691000.0,1372865291000.0,1389336542000.0,1389340142000.0,-1.0,-1.0,"dturanski",1372865291000.0,1372401294000.0,43,1372401294000.0,-1.0,"Investigate JMX object naming of deployed modules and inbound/outbound channel adapters.","The object naming is still not ideal for XD since SI conventions add some noise. Likely  need to design and implement a custom naming strategy",5.0,False,5.0,0,0.0,0,0,True
"XD-342","Story","Sprint 8","2013-06-27T12:50:13.000+0000","thomas.risberg","thomas.risberg","Fix classpath error caused by multiple conflicting servlet-api jars","There is some conflicting Servlet API jars on the claspath that needs cleanup. Building and running with xd-singlenode script gave this error:

Jun 27, 2013 3:18:16 PM org.apache.coyote.http11.AbstractHttp11Processor process
SEVERE: Error processing request
java.lang.NoSuchMethodError: javax.servlet.ServletContext.getEffectiveSessionTrackingModes()Ljava/util/Set;
	at org.apache.catalina.connector.CoyoteAdapter.postParseRequest(CoyoteAdapter.java:674)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:402)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:310)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
",3.0,0.8140858430163623,0.8178800094854162,0.8079203225041499,True,True,True,1372337413000.0,1372340846000.0,1372341630000.0,1372340730000.0,1372341630000.0,1372337413000.0,-1.0,1372340820000.0,1372341630000.0,1372340862000.0,1372341630000.0,1372340846000.0,1372341630000.0,"thomas.risberg",1372341630000.0,1372337413000.0,43,1372337413000.0,-1.0,"Fix classpath error caused by multiple conflicting servlet-api jars","There is some conflicting Servlet API jars on the claspath that needs cleanup. Building and running with xd-singlenode script gave this error:

Jun 27, 2013 3:18:16 PM org.apache.coyote.http11.AbstractHttp11Processor process
SEVERE: Error processing request
java.lang.NoSuchMethodError: javax.servlet.ServletContext.getEffectiveSessionTrackingModes()Ljava/util/Set;
	at org.apache.catalina.connector.CoyoteAdapter.postParseRequest(CoyoteAdapter.java:674)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:402)
	at org.apache.coyote.http11.AbstractHttp11Processor.process(AbstractHttp11Processor.java:1004)
	at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process(AbstractProtocol.java:589)
	at org.apache.tomcat.util.net.JIoEndpoint$SocketProcessor.run(JIoEndpoint.java:310)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
",3.0,False,3.0,0,0.0,-1,1,False
"XD-339","Story","Sprint 8","2013-06-26T10:39:57.000+0000","iperumal","iperumal","Investigate using profiler when doing the performance testing","Investigate how efficiently we can integrate profiler into the performance test.",1.0,0.2524615027393944,0.0,0.2524145699224867,True,True,True,1372243197000.0,1372345402000.0,1372349002000.0,1372647131000.0,1372648031000.0,1372243197000.0,-1.0,1372345383000.0,1372348983000.0,1372243197000.0,1372246797000.0,1372345402000.0,1372349002000.0,"iperumal",1372348983000.0,1372243197000.0,43,1372243197000.0,-1.0,"Investigate using profiler when doing the performance testing","Investigate how efficiently we can integrate profiler into the performance test.",1.0,False,1.0,0,0.0,0,0,False
"XD-338","Story","Sprint 9","2013-06-25T16:23:46.000+0000","iperumal","iperumal","Create tcp/udp load generator script for XD performance testing","Create a load generator script which can generate messages at specific

1) Rate
2) Payload
3) Concurrency

to a specific tcp/udp port where a syslog adapter is listening.",4.0,-1.0,0.0,0.010660302629325,False,True,True,1372177426000.0,-1.0,-1.0,1436364291000.0,1436365191000.0,1372177426000.0,-1.0,1372861687000.0,1372865287000.0,1372177426000.0,1372181026000.0,-1.0,-1.0,"iperumal",1372865287000.0,1372177426000.0,43,1372177426000.0,-1.0,"Create tcp/udp load generator script for XD performance testing","Create a load generator script which can generate messages at specific

1) Rate
2) Payload
3) Concurrency

to a specific tcp/udp port where a syslog adapter is listening.",4.0,False,4.0,0,0.0,0,0,False
"XD-337","Story","Sprint 8","2013-06-25T10:01:26.000+0000","mark.fisher","iperumal","Test connection pooling on Redis blocking/nonblocking operations","",2.0,0.0045361451890905,0.0,0.0354739937138801,True,True,True,1372154486000.0,1372156723000.0,1372160323000.0,1372646736000.0,1372647636000.0,1372154486000.0,-1.0,1372171980000.0,1372175580000.0,1372154486000.0,1372158086000.0,1372156723000.0,1372160323000.0,"mark.fisher",1372175580000.0,1372154486000.0,43,1372154486000.0,-1.0,"Test connection pooling on Redis blocking/nonblocking operations","",2.0,False,2.0,0,0.0,0,1,False
"XD-336","Story","Sprint 8","2013-06-25T09:47:22.000+0000","mark.fisher","","Document Splunk source module","",2.0,0.8929133773638646,0.8933755367228602,0.0985474223867843,True,True,True,1372153642000.0,1372319798000.0,1372323398000.0,1372338825000.0,1372339725000.0,1372153642000.0,-1.0,1372171980000.0,1372175580000.0,1372319884000.0,1372323484000.0,1372319798000.0,1372323398000.0,"mark.fisher",1372175580000.0,1372153642000.0,43,1372153642000.0,-1.0,"Document Splunk source sink","",2.0,False,2.0,0,0.0,0,0,True
"XD-335","Story","Sprint 8","2013-06-25T09:44:17.000+0000","mark.fisher","","Review DSL","updated story points to 14 since 5 of us just participated in a 2 hour call, and we still need to discuss ""topology"" support after some dev spikes later this week",14.0,0.3195879493446592,0.9954030938771388,0.0719728615449179,True,True,True,1372153457000.0,1372235702000.0,1372239302000.0,1372409904000.0,1372410804000.0,1372153457000.0,1372172909000.0,1372171979000.0,1372175579000.0,1372409621000.0,1372410804000.0,1372235702000.0,1372239302000.0,"mark.fisher",1372175579000.0,1372172909000.0,43,1372172909000.0,-1.0,"Review DSL","updated story points to 14 since 5 of us just participated in a 2 hour call, and we still need to discuss ""topology"" support after some dev spikes later this week",14.0,False,14.0,0,0.0,-1,0,False
"XD-334","Story","Sprint 8","2013-06-25T09:26:31.000+0000","mark.fisher","","Document the structure of the REST API","",5.0,0.2856811163481855,0.2856772465888071,0.0379004233516759,True,True,True,1372152391000.0,1372300039000.0,1372303639000.0,1372668319000.0,1372669219000.0,1372152391000.0,-1.0,1372171979000.0,1372175579000.0,1372300037000.0,1372303637000.0,1372300039000.0,1372303639000.0,"mark.fisher",1372175579000.0,1372152391000.0,43,1372152391000.0,-1.0,"Document the structure of the REST API","",5.0,False,5.0,0,0.0,0,0,False
"XD-332","Story","Sprint 9","2013-06-25T09:03:22.000+0000","mark.pollack","","Retrieve information for an aggregate counter","",3.0,0.9348372745452386,0.9348359905885978,0.228122181313937,True,True,True,1372151002000.0,1375063366000.0,1375066966000.0,1375265472000.0,1375266372000.0,1372151002000.0,1374487523000.0,1372861687000.0,1372865287000.0,1375063362000.0,1375066962000.0,1375063366000.0,1375066966000.0,"mark.pollack",1372865287000.0,1372151002000.0,43,1374652573000.0,1374487523000.0,"Retrieve information for an aggregate counter","TODO as part of this (see XD-537): 

* Get rid of so-called Service layer in analytics project (doesn't do much right now, and logic would better live in the 'Handler' IMO)
* Have REST controllers depend on XRepository in all cases",0.0,True,0.0,2,-1.0,-1,0,True
"XD-331","Story","Sprint 9","2013-06-25T09:03:01.000+0000","mark.pollack","","Retrieve information for a Rich Gauge","",3.0,0.9763772808573644,0.8864496189765733,0.2368958407081416,True,True,True,1372150981000.0,1375080189000.0,1375083789000.0,1375150159000.0,1375151059000.0,1372150981000.0,1374487535000.0,1372861687000.0,1372865287000.0,1374810399000.0,1374813999000.0,1375080189000.0,1375083789000.0,"mark.pollack",1372865287000.0,1372150981000.0,43,1374652595000.0,1374487535000.0,"Retrieve information for a Rich Gauge","TODO as part of this (see XD-537): 

* Get rid of so-called Service layer in analytics project (doesn't do much right now, and logic would better live in the 'Handler' IMO)
* Have REST controllers depend on XRepository in all cases",0.0,True,0.0,2,-1.0,-1,0,True
"XD-330","Story","Sprint 9","2013-06-25T09:02:35.000+0000","mark.pollack","","Retrieve information for a Gauge","",3.0,0.9846271944685452,0.9846249726437826,0.2631869928453539,True,True,True,1372150955000.0,1374809924000.0,1374813524000.0,1374850538000.0,1374851438000.0,1372150955000.0,1374487529000.0,1372861687000.0,1372865287000.0,1374809918000.0,1374813518000.0,1374809924000.0,1374813524000.0,"mark.pollack",1372865287000.0,1372150955000.0,43,1374652584000.0,1374487529000.0,"Retrieve information for a Gauge","TODO as part of this (see XD-537): 

* Get rid of so-called Service layer in analytics project (doesn't do much right now, and logic would better live in the 'Handler' IMO)
* Have REST controllers depend on XRepository in all cases",0.0,True,0.0,2,-1.0,-1,0,True
"XD-329","Story","Sprint 9","2013-06-25T09:02:14.000+0000","mark.pollack","","Retrieve informaiton for a Field Value Counter","",3.0,0.8277201712132667,0.8277191407185228,0.2441434407463804,True,True,True,1372150934000.0,1374560612000.0,1374564212000.0,1375061257000.0,1375062157000.0,1372150934000.0,1374487516000.0,1372861690000.0,1372865290000.0,1374560609000.0,1374564209000.0,1374560612000.0,1374564212000.0,"mark.pollack",1372865290000.0,1372150934000.0,43,1374652562000.0,1374487516000.0,"Retrieve information for a Field Value Counter","TODO as part of this (see XD-537): 

* Get rid of so-called Service layer in analytics project (doesn't do much right now, and logic would better live in the 'Handler' IMO)
* Have REST controllers depend on XRepository in all cases",0.0,True,0.0,2,-1.0,0,0,True
"XD-328","Story","Sprint 9","2013-06-25T09:01:55.000+0000","mark.pollack","","Retrieve information for a Counter","",5.0,0.9575781392732626,0.9575769399444072,0.284150989106496,True,True,True,1372150915000.0,1374546200000.0,1374549800000.0,1374651414000.0,1374652314000.0,1372150915000.0,1374487488000.0,1372861690000.0,1372865290000.0,1374546197000.0,1374549797000.0,1374546200000.0,1374549800000.0,"mark.pollack",1372865290000.0,1372150915000.0,43,1374487488000.0,-1.0,"Retrieve information for a Counter","",0.0,False,0.0,1,-1.0,0,0,False
"XD-323","Story","Sprint 9","2013-06-25T08:49:54.000+0000","mark.pollack","","Add command for listing of taps","",1.0,-1.0,-1.0,0.4663167763064621,False,True,False,1372150194000.0,-1.0,-1.0,1373277838000.0,1373278738000.0,1372150194000.0,-1.0,1372676453000.0,1372680053000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1372680053000.0,1372676159000.0,43,1372676159000.0,-1.0,"Add command for listing of taps","",1.0,False,1.0,0,0.0,-1,0,False
"XD-321","Story","Sprint 9","2013-06-25T08:45:31.000+0000","mark.pollack","","Support for creation (POST) of tap","POST?",1.0,0.604174332325793,0.6041833618138436,0.5942768847363277,True,True,True,1372149931000.0,1372685221000.0,1372688821000.0,1373035017000.0,1373035917000.0,1372149931000.0,-1.0,1372676452000.0,1372680052000.0,1372685229000.0,1372688829000.0,1372685221000.0,1372688821000.0,"mark.pollack",1372680052000.0,1372676363000.0,43,1372676363000.0,-1.0,"Add create() and deploy() to TapsController","POST?",1.0,False,1.0,0,0.0,0,1,True
"XD-319","Story","Sprint 9","2013-06-25T08:42:28.000+0000","mark.pollack","","Add command for deleting a tap","",1.0,-1.0,-1.0,0.4664883511870231,False,True,False,1372149748000.0,-1.0,-1.0,1373277933000.0,1373278833000.0,1372149748000.0,-1.0,1372676453000.0,1372680053000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1372680053000.0,1372676077000.0,43,1372676077000.0,-1.0,"Add command for deleting a tap","",1.0,False,1.0,0,0.0,-1,2,False
"XD-318","Story","Sprint 8","2013-06-25T08:40:47.000+0000","mark.pollack","","Create Splunk sink module","This would be based off the spring-integration-extenstions splunk project.  The use of this adapter for storing tweet data is in
https://github.com/markpollack/springone

We should be able to reproduce the use case as done in that demo",10.0,0.3668904047691926,0.3666644803008439,0.0813765359219904,True,True,True,1372149647000.0,1372250332000.0,1372253932000.0,1372423175000.0,1372424075000.0,1372149647000.0,-1.0,1372171979000.0,1372175579000.0,1372250270000.0,1372253870000.0,1372250332000.0,1372253932000.0,"mark.pollack",1372175579000.0,1372153065000.0,43,1372153065000.0,-1.0,"Create Splunk sink module","This would be based off the spring-integration-extenstions splunk project.  The use of this adapter for storing tweet data is in
https://github.com/markpollack/springone

We should be able to reproduce the use case as done in that demo",10.0,False,10.0,0,0.0,0,0,False
"XD-317","Story","Sprint 8","2013-06-25T07:46:37.000+0000","hillert","","Add Documentation Chapter on Executing Batch Jobs","",4.0,0.3611939162743301,0.3612518155560482,0.0423194121403213,True,True,True,1372146397000.0,1372364738000.0,1372368338000.0,1372749995000.0,1372750895000.0,1372146397000.0,-1.0,1372171979000.0,1372175579000.0,1372364773000.0,1372368373000.0,1372364738000.0,1372368338000.0,"hillert",1372175579000.0,1372146397000.0,43,1372146397000.0,-1.0,"Add Documentation Chapter on Executing Batch Jobs","",4.0,False,4.0,0,0.0,-1,0,False
"XD-316","Story","Sprint 9","2013-06-25T07:30:23.000+0000","grenfro","","Create a common exception framework for XD","Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions.  An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ",5.0,-1.0,-1.0,0.0769004975231207,False,True,False,1372145423000.0,-1.0,-1.0,1381458663000.0,1381459563000.0,1372145423000.0,-1.0,1372861685000.0,1372865285000.0,-1.0,-1.0,-1.0,-1.0,"grenfro",1372865285000.0,1372145423000.0,43,1372145423000.0,-1.0,"Create a common exception framework for XD","Need to capture exceptions from the various projects that make up XD and wrap them in XD Specific exceptions.  An example of this is when leaving out the channels in the module definitions, we see NoSuchBeanExceptions and IllegalArgumentExceptions thrown based on which module and what channel is missing. ",5.0,False,5.0,0,0.0,-1,0,False
"XD-315","Story","Sprint 8","2013-06-25T06:41:05.000+0000","eric.bottard","","Package Shell ""binary"" next to xd-admin and xd-container","The shell should be an 'executable' delivered out of the box in much the same way that xd-container and xd-admin are right now. If we follow how redis/mongo distribut the shell, it sits side by side with the other binaries",3.0,0.2983523202677479,0.2983466412245532,0.0558685339360691,True,True,True,1372142465000.0,1372300072000.0,1372303672000.0,1372669823000.0,1372670723000.0,1372142465000.0,-1.0,1372171978000.0,1372175578000.0,1372300069000.0,1372303669000.0,1372300072000.0,1372303672000.0,"eric.bottard",1372175578000.0,1372142465000.0,43,1372142465000.0,-1.0,"Package Shell ""binary"" next to xd-admin and xd-container","The shell should be an 'executable' delivered out of the box in much the same way that xd-container and xd-admin are right now. If we follow how redis/mongo distribut the shell, it sits side by side with the other binaries",3.0,False,3.0,0,0.0,-1,0,False
"XD-313","Story","Sprint 8","2013-06-25T04:19:11.000+0000","grussell","","Add Spring/Integration MBean Exporters to Module ApplicationContexts","Global option?

Override for individual modules? module types?",8.0,0.6844867341438582,0.6844680182068635,0.1423459304066598,True,True,True,1372133951000.0,1372316813000.0,1372320413000.0,1372400203000.0,1372401103000.0,1372133951000.0,-1.0,1372171979000.0,1372175579000.0,1372316808000.0,1372320408000.0,1372316813000.0,1372320413000.0,"grussell",1372175579000.0,1372152756000.0,43,1372152756000.0,-1.0,"Add Spring/Integration MBean Exporters to Module ApplicationContexts","Global option?

Override for individual modules? module types?",8.0,False,8.0,0,0.0,0,1,False
"XD-312","Story","Sprint 8","2013-06-25T04:11:05.000+0000","grussell","","Add Jolokia Agent Depending on Run Mode","WAR Vs. JVM Jolokia Agent

Jolokia Vs. JVM MBeanServer

Probably needs support for Spring Profiles.",5.0,0.3322161317924172,0.3321526119723651,0.1439060205580029,True,True,True,1372133465000.0,1372222377000.0,1372225977000.0,1372400198000.0,1372401098000.0,1372133465000.0,-1.0,1372171979000.0,1372175579000.0,1372222360000.0,1372225960000.0,1372222377000.0,1372225977000.0,"grussell",1372175579000.0,1372152620000.0,43,1372152620000.0,-1.0,"Add Jolokia Agent Depending on Run Mode","WAR Vs. JVM Jolokia Agent

Jolokia Vs. JVM MBeanServer

Probably needs support for Spring Profiles.",5.0,False,5.0,0,0.0,0,0,False
"XD-308","Story","Sprint 9","2013-06-24T13:12:31.000+0000","grenfro","","User wants ability to test multiple processors in a chain","",8.0,-1.0,-1.0,0.0261808189178463,False,True,False,1372079551000.0,-1.0,-1.0,1401953003000.0,1401953903000.0,1372079551000.0,-1.0,1372861686000.0,1372865286000.0,-1.0,-1.0,-1.0,-1.0,"grenfro",1372865286000.0,1372079551000.0,43,1372079551000.0,-1.0,"User wants ability to test multiple processors in a chain","",8.0,False,8.0,0,0.0,0,0,False
"XD-307","Story","Sprint 9","2013-06-24T13:12:01.000+0000","grenfro","","User to send a message directly to module and receive a message a module","",8.0,-1.0,-1.0,0.0369898015885214,False,True,False,1372079521000.0,-1.0,-1.0,1393224071000.0,1393224971000.0,1372079521000.0,-1.0,1372861687000.0,1372865287000.0,-1.0,-1.0,-1.0,-1.0,"grenfro",1372865287000.0,1372079521000.0,43,1372079521000.0,-1.0,"User to send a message directly to module and receive a message from a module","",8.0,False,8.0,0,0.0,0,0,True
"XD-305","Story","Sprint 9","2013-06-24T13:09:58.000+0000","grenfro","","User wants ability to test sinks","",8.0,-1.0,-1.0,0.0369954451821998,False,True,False,1372079398000.0,-1.0,-1.0,1393224020000.0,1393224920000.0,1372079398000.0,-1.0,1372861686000.0,1372865286000.0,-1.0,-1.0,-1.0,-1.0,"grenfro",1372865286000.0,1372079398000.0,43,1372079398000.0,-1.0,"User wants ability to test sinks","Handled by 1245",8.0,False,8.0,0,0.0,-1,0,True
"XD-303","Story","Sprint 9","2013-06-24T13:08:03.000+0000","grenfro","","User wants ability to create a in-process sink or tap","",8.0,-1.0,-1.0,0.0261896302717223,False,True,False,1372079283000.0,-1.0,-1.0,1401952917000.0,1401953817000.0,1372079283000.0,-1.0,1372861686000.0,1372865286000.0,-1.0,-1.0,-1.0,-1.0,"grenfro",1372865286000.0,1372079283000.0,43,1372079283000.0,-1.0,"User wants ability to create a in-process sink or tap","So that we can validate the message content in the stream",8.0,False,8.0,0,0.0,0,0,True
"XD-302","Story","Sprint 9","2013-06-24T13:06:17.000+0000","grenfro","","User wants ability to create a mock source","",8.0,-1.0,-1.0,0.0370037826179272,False,True,False,1372079177000.0,-1.0,-1.0,1393225007000.0,1393225907000.0,1372079177000.0,-1.0,1372861686000.0,1372865286000.0,-1.0,-1.0,-1.0,-1.0,"grenfro",1372865286000.0,1372079177000.0,43,1372079177000.0,-1.0,"User wants ability to create a mock source","To send a pre-set message to process(es)",8.0,False,8.0,0,0.0,-1,0,True
"XD-301","Story","Sprint 8","2013-06-24T09:38:28.000+0000","iperumal","iperumal","Request to create a repo for Spring XD performance testing","It would be nice if we have a git repo for Spring XD performance testing.

This would enable us to have a common repository (rather than inside spring-xd as a subproject) for all performance related code specific to any module, message middleware etc., 
",4.0,0.5206739740556009,0.520663968502519,0.5266522920220722,True,True,True,1372066708000.0,1372170785000.0,1372174385000.0,1372265697000.0,1372266597000.0,1372066708000.0,1372241564000.0,1372171980000.0,1372175580000.0,1372170783000.0,1372174383000.0,1372170785000.0,1372174385000.0,"iperumal",1372175580000.0,1372066708000.0,43,1372241564000.0,1372241564000.0,"Request to create a repo for Spring XD performance testing","It would be nice if we have a git repo for Spring XD performance testing.

This would enable us to have a common repository (rather than inside spring-xd as a subproject) for all performance related code specific to any module, message middleware etc., 
",1.0,True,1.0,1,300.0,0,0,False
"XD-299","Bug","Sprint 9","2013-06-21T09:44:41.000+0000","eric.bottard","","Creating a tap with same name as existing streams results in infinite loop","See http://stackoverflow.com/questions/17157068/counter-analytics-in-springxd

The underlying issue is stream creation with a name already taken though",5.0,0.9564459251014092,0.8594288134032776,0.0857825103012811,True,True,True,1371807881000.0,1383557472000.0,1383561072000.0,1384091618000.0,1384092518000.0,1371807881000.0,-1.0,1372861688000.0,1372865288000.0,1382365652000.0,1382369252000.0,1383557472000.0,1383561072000.0,"eric.bottard",1372865288000.0,1371807881000.0,43,1371807881000.0,-1.0,"Creating a tap with same name as existing streams results in infinite loop","See http://stackoverflow.com/questions/17157068/counter-analytics-in-springxd

The underlying issue is stream creation with a name already taken though",5.0,False,5.0,0,0.0,0,0,False
"XD-298","Story","Sprint 8","2013-06-21T09:30:18.000+0000","luke","","Refactor gardenhose into more generic twitterstream source","Twitter's streaming APIs have more capabilities than just the plain statuses/sample.json. In particular we should support the filter.json option and the use of ""track"" (https://dev.twitter.com/docs/streaming-apis/parameters#track) as well as other request parameters (delimited, language etc).",2.0,0.8672500796325169,0.8672383443142383,0.611847642039263,True,True,True,1371807018000.0,1372324324000.0,1372327924000.0,1372402608000.0,1372403508000.0,1371807018000.0,-1.0,1372171979000.0,1372175579000.0,1372324317000.0,1372327917000.0,1372324324000.0,1372327924000.0,"luke",1372175579000.0,1372067344000.0,43,1372067344000.0,-1.0,"Refactor gardenhose into more generic twitterstream source","Twitter's streaming APIs have more capabilities than just the plain statuses/sample.json. In particular we should support the filter.json option and the use of ""track"" (https://dev.twitter.com/docs/streaming-apis/parameters#track) as well as other request parameters (delimited, language etc).",2.0,False,2.0,0,0.0,-1,0,False
"XD-297","Story","Sprint 9","2013-06-21T09:27:50.000+0000","jencompgeek","","File source should support rollover","I wanted to have a rollover feature when I was streaming tweets to a file overnight, just to avoid dealing with a single enormous file (in case I collected more data than my demo could handle and needed to split it up).",2.0,-1.0,0.4111386695061068,0.0163390228551209,False,True,True,1371806870000.0,-1.0,-1.0,1436364237000.0,1436365137000.0,1371806870000.0,-1.0,1372861689000.0,1372865289000.0,1398349270000.0,1398352870000.0,-1.0,-1.0,"jencompgeek",1372865289000.0,1371806870000.0,43,1371806870000.0,-1.0,"File sink should support rollover","I wanted to have a rollover feature when I was streaming tweets to a file overnight, just to avoid dealing with a single enormous file (in case I collected more data than my demo could handle and needed to split it up).",2.0,False,2.0,0,0.0,0,0,True
"XD-296","Story","Sprint 8","2013-06-21T07:13:31.000+0000","luke","iperumal","Add log config file to gemfire in final distro","The changes for XD-144 mean that log4j files are no longer in the library jars. The admin server already has a logging configuration which should be activated by the startup scripts, but the separate gemfire app doesn't.",1.0,0.5282909644863373,0.5282852518328097,0.7105950680757879,True,True,True,1371798811000.0,1372076243000.0,1372079843000.0,1372323061000.0,1372323961000.0,1371798811000.0,-1.0,1372171980000.0,1372175580000.0,1372076240000.0,1372079840000.0,1372076243000.0,1372079843000.0,"luke",1372175580000.0,1371798811000.0,43,1371798811000.0,-1.0,"Add log config file to gemfire in final distro","The changes for XD-144 mean that log4j files are no longer in the library jars. The admin server already has a logging configuration which should be activated by the startup scripts, but the separate gemfire app doesn't.",1.0,False,1.0,0,0.0,0,0,False
"XD-294","Story","Sprint 7","2013-06-21T00:46:28.000+0000","grussell","grussell","Apply JavaDoc HotFix","",1.0,0.0010710112943009,0.0,0.0006166428664156,True,True,True,1371775588000.0,1371775621000.0,1371779221000.0,1371805500000.0,1371806400000.0,1371775588000.0,-1.0,1371775607000.0,1371779207000.0,1371775588000.0,1371779188000.0,1371775621000.0,1371779221000.0,"grussell",1371779207000.0,1371775588000.0,43,1371775588000.0,-1.0,"Apply JavaDoc HotFix","",1.0,False,1.0,0,0.0,0,0,False
"XD-293","Improvement","Sprint 9","2013-06-20T14:31:15.000+0000","hillert","","Ensure package-info.java is present for each package","We should ensure that each Package of Spring XD is documented. Right now the created JavaDoc looks barren:

http://static.springsource.org/spring-xd/docs/1.0.0.M1/api/

 ",4.0,0.9954428337251432,0.9954428337251432,0.0326827943479951,True,True,True,1371738675000.0,1405943132000.0,1405946732000.0,1406098821000.0,1406099721000.0,1371738675000.0,-1.0,1372861690000.0,1372865290000.0,1405943132000.0,1405946732000.0,1405943132000.0,1405946732000.0,"hillert",1372865290000.0,1371738675000.0,43,1371738675000.0,-1.0,"Ensure package-info.java is present for each package","We should ensure that each Package of Spring XD is documented. Right now the created JavaDoc looks barren:

http://static.springsource.org/spring-xd/docs/1.0.0.M1/api/

 ",4.0,False,4.0,0,0.0,0,0,False
"XD-292","Bug","Sprint 8","2013-06-20T06:25:26.000+0000","verrol","iperumal","Redis 'install-redis' script fails on Ubuntu64","The installation script for redis fails on Ubuntu64 when trying to untar the redis distribution.  The script uses REDIS_ZIPNAME instead of REDIS_ZIP_PATH.  

This bug will be seen on any Linux 64 bits platform and looking at the code, even Linux 32 bits platform.",1.0,0.7756520536339508,0.7756455698391155,0.9999632584959324,True,True,True,1371709526000.0,1372068414000.0,1372072014000.0,1372171318000.0,1372172218000.0,1371709526000.0,-1.0,1372172201000.0,1372172218000.0,1372068411000.0,1372072011000.0,1372068414000.0,1372072014000.0,"verrol",1372172218000.0,1371709526000.0,43,1371709526000.0,-1.0,"Redis 'install-redis' script fails on Ubuntu64","The installation script for redis fails on Ubuntu64 when trying to untar the redis distribution.  The script uses REDIS_ZIPNAME instead of REDIS_ZIP_PATH.  

This bug will be seen on any Linux 64 bits platform and looking at the code, even Linux 32 bits platform.",1.0,False,1.0,0,0.0,0,0,False
"XD-291","Bug","Sprint 9","2013-06-19T11:45:28.000+0000","franktylerva","","HTTP Source still listens on port 9000 after removal.","Steps to reproduce:

1.  curl -d ""http | log"" http://localhost:8080/streams/testHttp

2.  curl -X DELETE http://localhost:8080/streams/testHttp

3.  curl -d ""http | log"" http://localhost:8080/streams/testHttp

org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:9000",1.0,0.9999779898456151,0.999982942130352,0.6709586467716881,True,True,True,1371642328000.0,1373459631000.0,1373459671000.0,1373458771000.0,1373459671000.0,1371642328000.0,-1.0,1372861690000.0,1372865290000.0,1373459640000.0,1373459671000.0,1373459631000.0,1373459671000.0,"franktylerva",1372865290000.0,1371642328000.0,43,1371642328000.0,-1.0,"HTTP Source still listens on port 9000 after removal.","Steps to reproduce:

1.  curl -d ""http | log"" http://localhost:8080/streams/testHttp

2.  curl -X DELETE http://localhost:8080/streams/testHttp

3.  curl -d ""http | log"" http://localhost:8080/streams/testHttp

org.jboss.netty.channel.ChannelException: Failed to bind to: 0.0.0.0/0.0.0.0:9000",1.0,False,1.0,0,0.0,0,0,False
"XD-282","Story","Sprint 9","2013-06-19T08:21:36.000+0000","mark.pollack","","Support writing to HDFS using a Avro Serialization to Avro container files","",10.0,0.9260469446185828,0.9110749692012946,0.1030955743869619,True,True,True,1371630096000.0,1382692763000.0,1382696363000.0,1383575315000.0,1383576215000.0,1371630096000.0,1382453697000.0,1372861688000.0,1372865288000.0,1382513906000.0,1382517506000.0,1382692763000.0,1382696363000.0,"mark.pollack",1372865288000.0,1371630096000.0,43,1382503419000.0,1382453697000.0,"The HDFS Sink should support writing POJOs to HDFS using Avro Serialization","Writing POJOs using CDK Data (Avro)

We should support both partitioned and un-partitioned.  This story addresses only un-partitioned.

Document limitations in terms of which Java types are supported and not supported by the Avro serialization
",0.0,True,0.0,2,-1.0,0,0,True
"XD-271","Story","Sprint 9","2013-06-19T08:04:47.000+0000","mark.pollack","","File rollover option to supported size of file, number of events, and time with no activity.","A strategy to roll over files that allows the user to choose between 
1) the size of the file
2) the number of events/items in the file
3) an idle timeout value that if exceeded, will close the file",8.0,-1.0,-1.0,0.0636497244278659,False,True,False,1371629087000.0,-1.0,-1.0,1390993534000.0,1390994434000.0,1371629087000.0,1382453692000.0,1372861686000.0,1372865286000.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",1372865286000.0,1371629087000.0,43,1382453692000.0,-1.0,"The HDFS Sink should support a number of rollover options","A strategy to roll over files that allows the user to choose between 
1) the size of the file
2) the number of events/items in the file
3) an idle timeout value that if exceeded, will close the file",0.0,False,0.0,1,-1.0,-1,0,True
"XD-270","Story","Sprint 9","2013-06-19T08:02:23.000+0000","mark.pollack","","File naming strategy should distinguish between file currently being written and completed files based on a filename suffix","A file that is in the process of being written to should have a customized suffix added to the name, e.g. 'temp'.

Once the file is closed, the suffix is removed and replaced with another value - default value can be dependent on the serialization format used, but can be customized",8.0,0.5746680340233431,0.5744630131499459,0.0636619764569935,True,True,True,1371628943000.0,1382756747000.0,1382760347000.0,1390991925000.0,1390992825000.0,1371628943000.0,1382453685000.0,1372861686000.0,1372865286000.0,1382752777000.0,1382756377000.0,1382756747000.0,1382760347000.0,"mark.pollack",1372865286000.0,1371628943000.0,43,1382453685000.0,-1.0,"The HDFS Sink should support a file naming strategy to distinguish between file currently being written and completed files","A file that is in the process of being written to should have a customized suffix added to the name, e.g. 'temp'.

Once the file is closed, the suffix is removed and replaced with another value - default value can be dependent on the serialization format used, but can be customized",0.0,False,0.0,1,-1.0,-1,0,True
"XD-269","Story","Sprint 9","2013-06-19T07:51:36.000+0000","mminella","","Create job module registry","h2. Narrative
As XD, I need a persistent way to register job definitions (beyond the map registry implementation provided by Spring Batch).  This implementation should live in Spring Batch.

h2.  Acceptance Criteria
# XD should be able to register, unregister, and find job definitions via the registry.
# The registry should be backed by Redis so that it is persistent.",4.0,0.5113282608511251,0.8497581842995916,0.4679416177923496,True,True,True,1371628296000.0,1372773635000.0,1372777235000.0,1373867325000.0,1373868225000.0,1371628296000.0,-1.0,1372676452000.0,1372680052000.0,1373531694000.0,1373535294000.0,1372773635000.0,1372777235000.0,"mminella",1372680052000.0,1371628296000.0,43,1371628296000.0,-1.0,"Create JobDefinition repository","h2. Narrative
As XD, I need a persistent way to register job definitions (beyond the map registry implementation provided by Spring Batch).  

h2.  Acceptance Criteria
# XD should be able to register, unregister, and find job definitions via the registry.
# The registry should be backed by Redis so that it is persistent.",4.0,False,4.0,0,0.0,0,1,True
"XD-268","Story","Sprint 7","2013-06-19T06:58:31.000+0000","eric.bottard","eric.bottard","Streamline command-line arg management","Command line arguments (and especially their default values) are currently scattered around different places.

The aim is to regroup those in a common place (*Options classes make sense).

Also, not very happy with how System properties are used as a vehicle for options.transport / options.home",2.0,0.994018065442364,0.0,0.9939411548551944,True,True,True,1371625111000.0,1371741430000.0,1371742130000.0,1371741230000.0,1371742130000.0,1371625111000.0,-1.0,1371741421000.0,1371742130000.0,1371625111000.0,1371628711000.0,1371741430000.0,1371742130000.0,"eric.bottard",1371742130000.0,1371625111000.0,43,1371625111000.0,-1.0,"Streamline command-line arg management","Command line arguments (and especially their default values) are currently scattered around different places.

The aim is to regroup those in a common place (*Options classes make sense).

Also, not very happy with how System properties are used as a vehicle for options.transport / options.home",2.0,False,2.0,0,0.0,0,0,False
"XD-264","Story","Sprint 7","2013-06-18T07:22:19.000+0000","eric.bottard","eric.bottard","Spring MVC infrastructure tests","",8.0,0.7062852182021334,0.0,0.7062168918755979,True,True,True,1371540139000.0,1371715867000.0,1371719467000.0,1371788045000.0,1371788945000.0,1371540139000.0,-1.0,1371715850000.0,1371719450000.0,1371540139000.0,1371543739000.0,1371715867000.0,1371719467000.0,"eric.bottard",1371719450000.0,1371540139000.0,43,1371540139000.0,-1.0,"Spring MVC infrastructure tests","",8.0,False,8.0,0,0.0,0,0,False
"XD-263","Story","Sprint 8","2013-06-18T07:13:05.000+0000","eric.bottard","","Support GET /streams","Pagination support, maybe querying by name as well",5.0,0.7003480786706477,0.7003448331818772,0.5131069064001038,True,True,True,1371539585000.0,1372402750000.0,1372406350000.0,1372771165000.0,1372772065000.0,1371539585000.0,-1.0,1372171979000.0,1372175579000.0,1372402746000.0,1372406346000.0,1372402750000.0,1372406350000.0,"eric.bottard",1372175579000.0,1371539585000.0,43,1371539585000.0,-1.0,"Support GET /streams","Pagination support, maybe querying by name as well",5.0,False,5.0,0,0.0,-1,0,False
"XD-262","Story","Sprint 7","2013-06-18T07:09:44.000+0000","eric.bottard","eric.bottard","Convert current REST servlet to Spring MVC","",3.0,-1.0,0.0,0.9996343176817024,False,True,True,1371539384000.0,-1.0,-1.0,1371694357000.0,1371695257000.0,1371539384000.0,-1.0,1371695200000.0,1371695257000.0,1371539384000.0,1371542984000.0,-1.0,-1.0,"eric.bottard",1371695257000.0,1371539384000.0,43,1371539384000.0,-1.0,"Convert current REST servlet to Spring MVC","",3.0,False,3.0,0,0.0,0,0,False
"XD-261","Story","Sprint 9","2013-06-18T05:49:31.000+0000","thomas.risberg","eric.bottard","Add command for deleting a stream","",1.0,-1.0,0.4336887607362163,0.9337881190369688,False,True,True,1371534571000.0,-1.0,-1.0,1372756518000.0,1372757418000.0,1371534571000.0,-1.0,1372676451000.0,1372680051000.0,1372064906000.0,1372068506000.0,-1.0,-1.0,"thomas.risberg",1372680051000.0,1372674223000.0,43,1372674223000.0,-1.0,"Add command for deleting a stream","",1.0,False,1.0,0,0.0,-1,1,False
"XD-260","Story","Sprint 8","2013-06-18T05:49:07.000+0000","thomas.risberg","","Add command for listing streams","",3.0,0.6630402986642532,0.663030601772915,0.5150924032937108,True,True,True,1371534547000.0,1372355066000.0,1372358666000.0,1372771157000.0,1372772057000.0,1371534547000.0,-1.0,1372171979000.0,1372175579000.0,1372355054000.0,1372358654000.0,1372355066000.0,1372358666000.0,"thomas.risberg",1372175579000.0,1372152305000.0,43,1372152305000.0,-1.0,"Add command for listing streams","",3.0,False,3.0,0,0.0,-1,1,False
"XD-259","Story","Sprint 9","2013-06-18T05:48:40.000+0000","thomas.risberg","","Add command for tap creation","",1.0,0.6597499739126147,0.6597625875934129,0.6547257155110433,True,True,True,1371534520000.0,1372685215000.0,1372688815000.0,1373277758000.0,1373278658000.0,1371534520000.0,-1.0,1372676452000.0,1372680052000.0,1372685237000.0,1372688837000.0,1372685215000.0,1372688815000.0,"thomas.risberg",1372680052000.0,1372674232000.0,43,1372674232000.0,-1.0,"Add command for tap creation","",1.0,False,1.0,0,0.0,0,0,False
"XD-258","Story","Sprint 8","2013-06-18T05:48:16.000+0000","thomas.risberg","eric.bottard","Add command for stream creation","",3.0,-1.0,0.6965369961031849,0.8372610130668736,False,True,True,1371534496000.0,-1.0,-1.0,1372294987000.0,1372295887000.0,1371534496000.0,-1.0,1372171979000.0,1372175579000.0,1372064833000.0,1372068433000.0,-1.0,-1.0,"thomas.risberg",1372175579000.0,1372152446000.0,43,1372152446000.0,-1.0,"Add command for stream creation","",3.0,False,3.0,0,0.0,-1,0,False
"XD-257","Story","Sprint 8","2013-06-18T05:47:07.000+0000","thomas.risberg","eric.bottard","Create the base implementation for XDCommands for the shell","This is the basic setup of the commands file - no specific command implementations",3.0,0.8951717544219081,0.91622014194233,0.9143920171885752,True,True,True,1371534427000.0,1372216043000.0,1372219643000.0,1372294963000.0,1372295863000.0,1371534427000.0,1372231625000.0,1372230678000.0,1372234278000.0,1372232070000.0,1372235670000.0,1372216043000.0,1372219643000.0,"thomas.risberg",1372234278000.0,1372231625000.0,43,1372231625000.0,-1.0,"Create the base implementation for XDCommands for the shell","This is the basic setup of the commands file - no specific command implementations",3.0,False,3.0,0,0.0,-1,0,False
"XD-256","Story","Sprint 8","2013-06-18T05:45:07.000+0000","thomas.risberg","eric.bottard","Create a banner page for XD Shell","",2.0,0.7983814843702811,0.7983749189166476,0.914349718079421,True,True,True,1371534307000.0,1372142324000.0,1372145924000.0,1372294969000.0,1372295869000.0,1371534307000.0,1372231700000.0,1372230641000.0,1372234241000.0,1372142319000.0,1372145919000.0,1372142324000.0,1372145924000.0,"thomas.risberg",1372234241000.0,1372231700000.0,43,1372231700000.0,-1.0,"Create a banner page for XD Shell","",2.0,False,2.0,0,0.0,-1,0,False
"XD-255","Story","Sprint 8","2013-06-18T05:44:30.000+0000","thomas.risberg","eric.bottard","Set up a project for XD Shell","Set up a basic Spring Shell project for XD Shell",3.0,0.8951859735000729,0.6966282250090928,0.9142225379372284,True,True,True,1371534270000.0,1372216032000.0,1372219632000.0,1372294957000.0,1372295857000.0,1371534270000.0,1372231739000.0,1372230530000.0,1372234130000.0,1372064813000.0,1372068413000.0,1372216032000.0,1372219632000.0,"thomas.risberg",1372234130000.0,1372231739000.0,43,1372231739000.0,-1.0,"Set up a project for XD Shell","Set up a basic Spring Shell project for XD Shell",3.0,False,3.0,0,0.0,-1,0,False
"XD-251","Story","","2013-06-18T05:37:57.000+0000","thomas.risberg","","Add support for stream creation","For stream creation we need to be able to specify:

source
sink
processor
 - filter
 - transformer
 - script
 etc.
",10.0,0.6791803379227868,0.678992517862071,-1.0,True,False,True,1371533877000.0,1372065447000.0,1372069047000.0,1372315641000.0,1372316541000.0,1371533877000.0,-1.0,-1.0,-1.0,1372065300000.0,1372068900000.0,1372065447000.0,1372069047000.0,"thomas.risberg",-1.0,-1.0,0,1371533877000.0,-1.0,"Add support for stream creation","For stream creation we need to be able to specify:

source
sink
processor
 - filter
 - transformer
 - script
 etc.
",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-250","Story","","2013-06-18T05:32:58.000+0000","thomas.risberg","","Set up a project for XD REST client library","",5.0,0.3332924579878089,0.6794734233612818,-1.0,True,False,True,1371533578000.0,1371794502000.0,1371798102000.0,1372315546000.0,1372316446000.0,1371533578000.0,-1.0,-1.0,-1.0,1372065516000.0,1372069116000.0,1371794502000.0,1371798102000.0,"thomas.risberg",-1.0,-1.0,0,1371533578000.0,-1.0,"Set up a project for XD REST client library","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-247","Story","Sprint 9","2013-06-18T05:21:57.000+0000","thomas.risberg","jencompgeek","Need to be able to specify password for Redis","Running on Cloud Foundry (and other managed environments) we need to be able to specify a Redis password in addition to host and port.",2.0,0.929570346092064,0.9295662896918758,0.9277263065665006,True,True,True,1371532917000.0,1372678724000.0,1372682324000.0,1372764637000.0,1372765537000.0,1371532917000.0,-1.0,1372676451000.0,1372680051000.0,1372678719000.0,1372682319000.0,1372678724000.0,1372682324000.0,"thomas.risberg",1372680051000.0,1371532917000.0,43,1371532917000.0,-1.0,"Need to be able to specify password for Redis","Running on Cloud Foundry (and other managed environments) we need to be able to specify a Redis password in addition to host and port.",2.0,False,2.0,0,0.0,0,0,False
"XD-246","Story","Sprint 9","2013-06-17T12:26:12.000+0000","mminella","mminella","MessageChannelItemWriter","h2. Narrative
As a user of XD, I want to be able to use a job as a source.  To do so, we need the output of a job to be written to a message channel

h2.  Acceptance Criteria
# Create a new ItemWriter in the Spring Batch project to write to a Spring Integration message channel.",4.0,-1.0,0.0099473244935356,0.1278116097643972,False,True,True,1371471972000.0,-1.0,-1.0,1382344247000.0,1382345147000.0,1371471972000.0,-1.0,1372861690000.0,1372865290000.0,1371580131000.0,1371583731000.0,-1.0,-1.0,"mminella",1372865290000.0,1371471972000.0,43,1371471972000.0,-1.0,"MessageChannelItemWriter","h2. Narrative
As a user of XD, I want to be able to use a job as a source.  To do so, we need the output of a job to be written to a message channel

h2.  Acceptance Criteria
# Create a new ItemWriter in the Spring Batch project to write to a Spring Integration message channel.",4.0,False,4.0,0,0.0,0,2,False
"XD-245","Story","Sprint 7","2013-06-17T11:31:25.000+0000","mminella","mminella","Deploy Batch Jobs on XD","h2. Narrative
As a developer, I need a way to deploy job configurations as well as the related custom code to XD.

h2.  Acceptance Criteria
# Provide the ability to deploy a job definition via the XD DSL:
{{job --jobDefinition=/some/local/path.xml}}
# Confirm that both ""regular"" jobs and Spring Hadoop based jobs can be packaged/run.",8.0,0.2155222296918622,0.1435518874855501,0.2133077776289629,True,True,True,1371468685000.0,1371634430000.0,1371638030000.0,1372236824000.0,1372237724000.0,1371468685000.0,-1.0,1371632727000.0,1371636327000.0,1371579082000.0,1371582682000.0,1371634430000.0,1371638030000.0,"mminella",1371636327000.0,1371580880000.0,43,1371580880000.0,-1.0,"Deploy Batch Jobs on XD","h2. Narrative
As a developer, I need a way to deploy job configurations as well as the related custom code to XD.

h2.  Acceptance Criteria
# Provide the ability to register jobs that have been deployed as modules via something like {{curl -d ""job"" http://localhost:8080/streams/myJob}} where job is the name of the job definition located in /modules/job and myJob is the name of the resulting registered job
# Confirm that both ""regular"" jobs and Spring Hadoop based jobs can be packaged/run.",8.0,False,8.0,0,0.0,0,1,True
"XD-244","Story","Sprint 8","2013-06-17T11:19:32.000+0000","mminella","Michael Minella","Create a Trigger","h2. Narrative
As the XD system, I need to be able to execute a job (or potentially a stream) based on a given condition (time, data existence, etc).  This story is intended is for a local trigger implementation but remote triggers will also need to exist.

h2.  Acceptance Criteria
# Implement the ability to register a time based trigger {{trigger <CRON_STRING>}} for example
# Implement the ability to register a file existence based trigger {{trigger <PATH>}} for example
# Implement the ability to execute a job via an anonymous trigger: {{job_name @ <CRON_STRING OR PATH>}}
# Implement the ability to execute a job via a job via the previously registered trigger: {{job_name @ trigger_name}}
",8.0,0.5951324760642008,0.5951394913598839,0.5487582536902404,True,True,True,1371467972000.0,1372231474000.0,1372235074000.0,1372749983000.0,1372750883000.0,1371467972000.0,-1.0,1372171980000.0,1372175580000.0,1372231483000.0,1372235083000.0,1372231474000.0,1372235074000.0,"mminella",1372175580000.0,1371467972000.0,43,1371467972000.0,-1.0,"Create a Trigger","h2. Narrative
As the XD system, I need to be able to execute a job (or potentially a stream) based on a given condition (time, data existence, etc).  This story is intended is for a local trigger implementation but remote triggers will also need to exist.

h2.  Acceptance Criteria
# Implement the ability to register a time based trigger {{trigger <CRON_STRING>}} for example
# Implement the ability to register a file existence based trigger {{trigger <PATH>}} for example
# Implement the ability to execute a job via an anonymous trigger: {{job_name @ <CRON_STRING OR PATH>}}
# Implement the ability to execute a job via a job via the previously registered trigger: {{job_name @ trigger_name}}
",8.0,False,8.0,0,0.0,0,2,False
"XD-242","Story","Sprint 7","2013-06-17T10:28:14.000+0000","mark.pollack","","Add JMS source module","",2.0,0.5747886486078877,0.5747769185008923,0.1095474692294157,True,True,True,1371464894000.0,1371807902000.0,1371811502000.0,1372060749000.0,1372061649000.0,1371464894000.0,-1.0,1371530267000.0,1371533867000.0,1371807895000.0,1371811495000.0,1371807902000.0,1371811502000.0,"mark.pollack",1371533867000.0,1371464894000.0,43,1371464894000.0,-1.0,"Add JMS source module","",2.0,False,2.0,0,0.0,0,0,False
"XD-236","Story","Sprint 8","2013-06-17T08:51:52.000+0000","mark.pollack","Mark Pollack","Create an Aggregate Counter","An aggregate counter rolls up counts into discrete time buckets.  There is an existing POC implementation in Java based off the library
https://github.com/thheller/timed-counter 

The README there has a good description of the desired feature set.",8.0,0.6433265297822062,0.6414996144469908,0.5917064186068132,True,True,True,1371459112000.0,1372234168000.0,1372237768000.0,1372662975000.0,1372663875000.0,1371459112000.0,-1.0,1372171978000.0,1372175578000.0,1372231967000.0,1372235567000.0,1372234168000.0,1372237768000.0,"mark.pollack",1372175578000.0,1372151844000.0,43,1372151844000.0,-1.0,"Create an Aggregate Counter","An aggregate counter rolls up counts into discrete time buckets.  There is an existing POC implementation in Java based off the library
https://github.com/thheller/timed-counter 

The README there has a good description of the desired feature set.",8.0,False,8.0,0,0.0,0,0,False
"XD-231","Story","Sprint 7","2013-06-17T08:04:43.000+0000","mark.fisher","Mark Fisher","Add Twitter gardenhose source module","we have a prototype gardenhose adapter that was built directly upon RestTemplate (streaming on a background thread), but Spring Social Twitter has an issue on its 1.1 roadmap that is relevant:
https://jira.springsource.org/browse/SOCIALTW-2
",3.0,0.6411956881756045,0.6411655776700491,0.2784581915630363,True,True,True,1371456283000.0,1371626641000.0,1371630241000.0,1371721071000.0,1371721971000.0,1371456283000.0,-1.0,1371530266000.0,1371533866000.0,1371626633000.0,1371630233000.0,1371626641000.0,1371630241000.0,"mark.fisher",1371533866000.0,1371456283000.0,43,1371456283000.0,-1.0,"Add Twitter gardenhose source module","we have a prototype gardenhose adapter that was built directly upon RestTemplate (streaming on a background thread), but Spring Social Twitter has an issue on its 1.1 roadmap that is relevant:
https://jira.springsource.org/browse/SOCIALTW-2
",3.0,False,3.0,0,0.0,-1,1,False
"XD-230","Story","Sprint 7","2013-06-17T07:53:27.000+0000","mark.fisher","","Add RabbitMQ source module","configurable parameters should include the queue-name(s) and optional binding key pattern

connection info, such as host and port, should also be configurable but with defaults (localhost and default port), and that should likely fallback to a rabbit.properties file in the $XD_HOME/config directory",3.0,0.6615125881673737,0.6615941665202439,0.468510755792063,True,True,True,1371455607000.0,1371561023000.0,1371564623000.0,1371614063000.0,1371614963000.0,1371455607000.0,-1.0,1371530267000.0,1371533867000.0,1371561036000.0,1371564636000.0,1371561023000.0,1371564623000.0,"mark.fisher",1371533867000.0,1371455607000.0,43,1371455607000.0,-1.0,"Add RabbitMQ source module","configurable parameters should include the queue-name(s) and optional binding key pattern

connection info, such as host and port, should also be configurable but with defaults (localhost and default port), and that should likely fallback to a rabbit.properties file in the $XD_HOME/config directory",3.0,False,3.0,0,0.0,0,1,False
"XD-229","Story","Sprint 9","2013-06-17T07:50:30.000+0000","mark.fisher","","Add RabbitMQ-based implementation of ChannelRegistry","",8.0,0.6883237371407183,0.6883167314030191,0.6486812699600973,True,True,True,1371455430000.0,1372830950000.0,1372834550000.0,1373452892000.0,1373453792000.0,1371455430000.0,1372830947000.0,1372751730000.0,1372755330000.0,1372830936000.0,1372834536000.0,1372830950000.0,1372834550000.0,"mark.fisher",1372755330000.0,1371455430000.0,43,1372830947000.0,1372830947000.0,"Add RabbitMQ-based implementation of ChannelRegistry","",5.0,True,5.0,1,60.0,0,1,False
"XD-228","Bug","","2013-06-14T15:26:07.000+0000","pikus","Mark Pollack","Missing '=' in example of http stream","In documentation attached to M1, in Streams/Introduction section, there's
{noformat}
http --port 8091 | file --dir=/tmp/httpdata/
{noformat}
while it should be:
{noformat}
http --port=8091 | file --dir=/tmp/httpdata/
{noformat}
missing ""{{=}}"" in {{http}}",1.0,-1.0,0.9996644250704376,-1.0,False,False,True,1371223567000.0,-1.0,-1.0,1371523643000.0,1371524543000.0,1371223567000.0,-1.0,-1.0,-1.0,1371524442000.0,1371524543000.0,-1.0,-1.0,"pikus",-1.0,-1.0,0,1371223567000.0,-1.0,"Missing '=' in example of http stream","In documentation attached to M1, in Streams/Introduction section, there's
{noformat}
http --port 8091 | file --dir=/tmp/httpdata/
{noformat}
while it should be:
{noformat}
http --port=8091 | file --dir=/tmp/httpdata/
{noformat}
missing ""{{=}}"" in {{http}}",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-227","Story","Sprint 7","2013-06-13T13:19:51.000+0000","mark.pollack","","Add jetty-util-6.1.26.jar and jsr311-api-1.1.1.jar as required jars so they will be on the XD classpath","This is needed for the use of the webhdfs:// scheme to talk to HDFS over http.",1.0,0.99120751723266,0.9237947858773148,0.148121763908677,True,True,True,1371129591000.0,1373810845000.0,1373814445000.0,1373833729000.0,1373834629000.0,1371129591000.0,-1.0,1371530266000.0,1371533866000.0,1373628491000.0,1373632091000.0,1373810845000.0,1373814445000.0,"mark.pollack",1371533866000.0,1371129591000.0,43,1371129591000.0,-1.0,"Add jetty-util-6.1.26.jar and jsr311-api-1.1.1.jar as required jars so they will be on the XD classpath","This is needed for the use of the webhdfs:// scheme to talk to HDFS over http.",1.0,False,1.0,0,0.0,-1,0,False
"XD-226","Story","Sprint 7","2013-06-11T21:15:18.000+0000","iperumal","","Cleanup and Optimize gradle tasks to bundle spring-xd distribution","We need to cleanup some of the duplicate gradle tasks that bundle spring-xd distributions. 

Currently, distXD does the copy of distributions from ""spring-xd-dirt"", ""redis"" and ""spring-xd-gemfire-server"" projects into ""$rootDir/dist/spring-xd"".

And, the task ""zipXD"" makes the zip archive.

These tasks should be combined with the ""distZip"" & ""docZip"" tasks.

We also need to remove the duplicate artifacts configuration from these tasks.",2.0,0.6806311471589055,0.6806376730527199,0.5080399011793222,True,True,True,1370985318000.0,1371715397000.0,1371718997000.0,1372057068000.0,1372057968000.0,1370985318000.0,-1.0,1371530267000.0,1371533867000.0,1371715404000.0,1371719004000.0,1371715397000.0,1371718997000.0,"iperumal",1371533867000.0,1370985318000.0,43,1370985318000.0,-1.0,"Cleanup and Optimize gradle tasks to bundle spring-xd distribution","We need to cleanup some of the duplicate gradle tasks that bundle spring-xd distributions. 

Currently, distXD does the copy of distributions from ""spring-xd-dirt"", ""redis"" and ""spring-xd-gemfire-server"" projects into ""$rootDir/dist/spring-xd"".

And, the task ""zipXD"" makes the zip archive.

These tasks should be combined with the ""distZip"" & ""docZip"" tasks.

We also need to remove the duplicate artifacts configuration from these tasks.",2.0,False,2.0,0,0.0,-1,0,False
"XD-225","Story","Sprint 6","2013-06-11T15:35:47.000+0000","mark.fisher","mark.fisher","field-value-counter should support nested fieldNames","for example, when using the 'twittersearch' source module, the ""hashTags"" are nested within ""entities"", and the value for hashTags is itself an object with a ""text"" field, so the following would be needed to count the actual value of interest:

{code}
tap @ tweets | field-value-counter --fieldName=entities.hashTags.text
{code}
",2.0,0.0083862770012706,0.0,0.0043202033036848,True,True,True,1370964947000.0,1370964980000.0,1370968580000.0,1370967982000.0,1370968882000.0,1370964947000.0,-1.0,1370964964000.0,1370968564000.0,1370964947000.0,1370968547000.0,1370964980000.0,1370968580000.0,"mark.fisher",1370968564000.0,1370964947000.0,43,1370964947000.0,-1.0,"field-value-counter should support nested fieldNames","for example, when using the 'twittersearch' source module, the ""hashTags"" are nested within ""entities"", and the value for hashTags is itself an object with a ""text"" field, so the following would be needed to count the actual value of interest:

{code}
tap @ tweets | field-value-counter --fieldName=entities.hashTags.text
{code}
",2.0,False,2.0,0,0.0,-1,0,False
"XD-224","Improvement","Sprint 6","2013-06-11T12:23:46.000+0000","aclement","aclement","Reduce necessity for quoting in parameter values in DSL expressions","parameter values that include spaces need to be quoted, but this becomes overly complex in this kind of case.  Here is what you want to say:

{code}
http --port=9995 | filter --expression=payload.matches('hello world')
{code}

With the rule 'parameter values that contain spaces must be quoted' it would be this:

{code}
http --port=9995 | filter --expression='payload.matches('hello world')'
{code}

But then to include single quotes within a single quoted string you need to use two of them: '' - so it becomes

{code}
http --port=9995 | filter --expression='payload.matches(''hello world'')'
{code}

Less than ideal.



",2.0,0.9994183792167508,0.0,0.9978673904614191,True,True,True,1370953426000.0,1370968891000.0,1370968900000.0,1370968000000.0,1370968900000.0,1370953426000.0,-1.0,1370968867000.0,1370968900000.0,1370953426000.0,1370957026000.0,1370968891000.0,1370968900000.0,"aclement",1370968900000.0,1370953426000.0,43,1370953426000.0,-1.0,"Reduce necessity for quoting in parameter values in DSL expressions","parameter values that include spaces need to be quoted, but this becomes overly complex in this kind of case.  Here is what you want to say:

{code}
http --port=9995 | filter --expression=payload.matches('hello world')
{code}

With the rule 'parameter values that contain spaces must be quoted' it would be this:

{code}
http --port=9995 | filter --expression='payload.matches('hello world')'
{code}

But then to include single quotes within a single quoted string you need to use two of them: '' - so it becomes

{code}
http --port=9995 | filter --expression='payload.matches(''hello world'')'
{code}

Less than ideal.



",2.0,False,2.0,0,0.0,0,0,False
"XD-223","Story","Sprint 6","2013-06-10T09:51:48.000+0000","jencompgeek","jencompgeek","Update Creating a Custom Source Module doc with twitter changes","https://github.com/SpringSource/spring-xd/wiki/Creating-a-Source-Module uses the SI twittersearch inbound channel adapter, which is no longer going to work once Twitter disallows anonymous searches. 

Ideally we update the example to use a new version of SI-twitter that adds support for this (as opposed to the XD workaround.)",2.0,0.0071693096572025,0.0054125913968284,0.000759661950432,True,True,True,1370857908000.0,1370858059000.0,1370861659000.0,1370878070000.0,1370878970000.0,1370857908000.0,-1.0,1370857924000.0,1370861524000.0,1370858022000.0,1370861622000.0,1370858059000.0,1370861659000.0,"jencompgeek",1370861524000.0,1370857908000.0,43,1370857908000.0,-1.0,"Update Creating a Custom Source Module doc with a different SI adapter due to Twitter issues","https://github.com/SpringSource/spring-xd/wiki/Creating-a-Source-Module uses the SI twittersearch inbound channel adapter, which is no longer going to work once Twitter disallows anonymous searches. 

Ideally we update the example to use a new version of SI-twitter that adds support for this (as opposed to the XD workaround.)",2.0,False,2.0,0,0.0,0,0,True
"XD-222","Story","Sprint 6","2013-06-10T09:37:53.000+0000","mark.pollack","luke","Add docs for Deleting a simple stream.","curl -X DELETE http://localhost:8080/streams/ticktock",1.0,0.0056608614772763,0.0,0.0003367651303762,True,True,True,1370857073000.0,1370857426000.0,1370861026000.0,1370918531000.0,1370919431000.0,1370857073000.0,-1.0,1370857094000.0,1370860694000.0,1370857073000.0,1370860673000.0,1370857426000.0,1370861026000.0,"mark.pollack",1370860694000.0,1370857073000.0,43,1370857073000.0,-1.0,"Add docs for Deleting a simple stream.","curl -X DELETE http://localhost:8080/streams/ticktock",1.0,False,1.0,0,0.0,-1,0,False
"XD-221","Story","Sprint 6","2013-06-10T09:33:28.000+0000","mark.pollack","","Links in asciidoctor generated HTML+docbook documentation are broken","The issue arises because the link:document[Label] asciidoc macro is meant for ""external documents"" and creates {{<ulink>}} in docbook / {{<a href=""document"">}} in html, whereas we want {{<link linkend=""anchor"">}} / {{<a href=""doc#anchor>}} resp. We also want it to continue working in github live view.

I guess what could work is to have the macro (either override the link macro or create our own if github supports that) that looks like :
{{link:document#anchor[Label]}}  (the #anchor works out of the box in asciidoc and should work in github) but override it for the html and docbook backends to render to the correct form.

The thing is, there are several ways to create/override macros (and templates they render to), some of which make sense to our setup:
- having asciidoc.conf in the directory of the processed document (http://asciidoc.org/userguide.html#X27)
- having docbook.conf/html.conf in the directory of the processed document (http://asciidoc.org/userguide.html#X27)
- defining macros using attributes (http://asciidoc.org/userguide.html#_setting_configuration_entries)

I tried all of those, but to no avail. These DO WORK with plain asciidoc, but not with our toolchain. Don't know if the problem is with asciidocTOR or with the gradle wrapper though.



",2.0,0.8476621034719224,0.8476113072410028,0.0002031849236786,True,True,True,1370856808000.0,1370923558000.0,1370927158000.0,1370934654000.0,1370935554000.0,1370856808000.0,-1.0,1370856824000.0,1370860424000.0,1370923554000.0,1370927154000.0,1370923558000.0,1370927158000.0,"mark.pollack",1370860424000.0,1370856808000.0,43,1370856808000.0,-1.0,"Links in asciidoctor generated HTML+docbook documentation are broken","The issue arises because the link:document[Label] asciidoc macro is meant for ""external documents"" and creates {{<ulink>}} in docbook / {{<a href=""document"">}} in html, whereas we want {{<link linkend=""anchor"">}} / {{<a href=""doc#anchor>}} resp. We also want it to continue working in github live view.

I guess what could work is to have the macro (either override the link macro or create our own if github supports that) that looks like :
{{link:document#anchor[Label]}}  (the #anchor works out of the box in asciidoc and should work in github) but override it for the html and docbook backends to render to the correct form.

The thing is, there are several ways to create/override macros (and templates they render to), some of which make sense to our setup:
- having asciidoc.conf in the directory of the processed document (http://asciidoc.org/userguide.html#X27)
- having docbook.conf/html.conf in the directory of the processed document (http://asciidoc.org/userguide.html#X27)
- defining macros using attributes (http://asciidoc.org/userguide.html#_setting_configuration_entries)

I tried all of those, but to no avail. These DO WORK with plain asciidoc, but not with our toolchain. Don't know if the problem is with asciidocTOR or with the gradle wrapper though.



",2.0,False,2.0,0,0.0,0,3,False
"XD-220","Story","Sprint 6","2013-06-10T09:29:38.000+0000","mark.fisher","mark.fisher","Add twitter oauth properties file to config dir","Those property keys should then be provided as defaults for the placeholders in source/twittersearch.xml",2.0,0.0272941176470588,0.0057647058823529,0.004235294117647,True,True,True,1370856578000.0,1370856810000.0,1370860410000.0,1370864178000.0,1370865078000.0,1370856578000.0,1370863094000.0,1370856614000.0,1370860214000.0,1370856627000.0,1370860227000.0,1370856810000.0,1370860410000.0,"mark.fisher",1370860214000.0,1370856578000.0,43,1370863094000.0,1370863094000.0,"Add twitter oauth properties file to config dir","Those property keys should then be provided as defaults for the placeholders in source/twittersearch.xml",1.0,True,1.0,1,100.0,0,0,False
"XD-219","Story","Sprint 6","2013-06-10T09:22:56.000+0000","dturanski","dturanski","Surpress tap WARNING message in local mode","",1.0,0.0216426193118756,0.0,0.0133185349611542,True,True,True,1370856176000.0,1370856215000.0,1370857978000.0,1370857078000.0,1370857978000.0,1370856176000.0,-1.0,1370856200000.0,1370857978000.0,1370856176000.0,1370857978000.0,1370856215000.0,1370857978000.0,"dturanski",1370857978000.0,1370856176000.0,43,1370856176000.0,-1.0,"Surpress tap WARNING message in local mode","",1.0,False,1.0,0,0.0,0,0,False
"XD-218","Story","","2013-06-10T09:13:45.000+0000","mark.pollack","","Add support to load a twitter-oauth.properties file in the sink","",1.0,-1.0,-1.0,-1.0,False,False,False,1370855625000.0,-1.0,-1.0,1371464078000.0,1371464978000.0,1370855625000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1370855625000.0,-1.0,"Add support to load a twitter.properties file in the source","",-1.0,False,-1.0,0,-1.0,-1,0,True
"XD-217","Story","Sprint 6","2013-06-10T09:12:14.000+0000","jencompgeek","jencompgeek","Add config dir to classpath to support custom properties-locations","The transform, filter, and script processor modules support passing in a properties-location for script variables. We need a default location on the classpath for users to provide custom properties files.",2.0,0.0204828090709583,0.0234089246525237,0.0146305779078273,True,True,True,1370855534000.0,1370855562000.0,1370856901000.0,1370856001000.0,1370856901000.0,1370855534000.0,-1.0,1370855554000.0,1370856901000.0,1370855566000.0,1370856901000.0,1370855562000.0,1370856901000.0,"jencompgeek",1370856901000.0,1370855534000.0,43,1370855534000.0,-1.0,"Add config dir to classpath to support custom properties-locations","The transform, filter, and script processor modules support passing in a properties-location for script variables. We need a default location on the classpath for users to provide custom properties files.",2.0,False,2.0,0,0.0,-1,0,False
"XD-216","Story","Sprint 6","2013-06-10T09:05:48.000+0000","mark.pollack","aclement","Add support for tap foo.bar syntax in the DSL","",2.0,-1.0,0.0,0.0423555479802284,False,True,True,1370855148000.0,-1.0,-1.0,1370865982000.0,1370866882000.0,1370855148000.0,-1.0,1370855645000.0,1370859245000.0,1370855148000.0,1370858748000.0,-1.0,-1.0,"mark.pollack",1370859245000.0,1370855148000.0,43,1370855148000.0,-1.0,"Add support for tap foo.bar syntax in the DSL","",2.0,False,2.0,0,0.0,0,0,False
"XD-215","Story","Sprint 6","2013-06-09T10:42:27.000+0000","luke","","Add authentication information to twittersearch source doc","Since the changes for XD-202, twittersearch requires authentication. Need to update the docs to reflect this.",1.0,-1.0,0.1221736017185138,0.999411249900549,False,True,True,1370774547000.0,-1.0,-1.0,1370836492000.0,1370837392000.0,1370774547000.0,-1.0,1370837355000.0,1370837392000.0,1370782225000.0,1370785825000.0,-1.0,-1.0,"luke",1370837392000.0,1370774547000.0,43,1370774547000.0,-1.0,"Add authentication information to twittersearch source doc","Since the changes for XD-202, twittersearch requires authentication. Need to update the docs to reflect this.",1.0,False,1.0,0,0.0,-1,0,False
"XD-214","Story","Sprint 9","2013-06-07T19:56:00.000+0000","mark.pollack","aclement","Create documentation on the general DSL syntax","The asciidoc wiki should have a section (included in the _Sidebar.asciidoc as well) that describes the general usage of the DSL syntax.",3.0,-1.0,0.0,0.1902629558843031,False,True,True,1370634960000.0,-1.0,-1.0,1382337468000.0,1382338368000.0,1370634960000.0,-1.0,1372861685000.0,1372865285000.0,1370634960000.0,1370638560000.0,-1.0,-1.0,"mark.pollack",1372865285000.0,1370634960000.0,43,1370634960000.0,-1.0,"Create documentation on the general DSL syntax","The asciidoc wiki should have a section (included in the _Sidebar.asciidoc as well) that describes the general usage of the DSL syntax.",3.0,False,3.0,0,0.0,0,0,False
"XD-213","Story","Sprint 6","2013-06-07T10:25:27.000+0000","grussell","grussell","Temporarily add toString() Logic in Local Mode Inter-Module Comms","",2.0,0.4500369731328568,0.0,0.4499482376140005,True,True,True,1370600727000.0,1370646372000.0,1370649972000.0,1370701252000.0,1370702152000.0,1370600727000.0,-1.0,1370646363000.0,1370649963000.0,1370600727000.0,1370604327000.0,1370646372000.0,1370649972000.0,"grussell",1370649963000.0,1370600727000.0,43,1370600727000.0,-1.0,"Temporarily add toString() Logic in Local Mode Inter-Module Comms","",2.0,False,2.0,0,0.0,0,0,False
"XD-212","Improvement","Sprint 6","2013-06-07T07:25:52.000+0000","dturanski","iperumal","Add http port command line option to AdminMain","Currently StreamServer has setPort, but no way for end user to set it. ",2.0,0.1298995388669301,0.1292819499341238,0.1305583003952569,True,True,True,1370589952000.0,1370593107000.0,1370596707000.0,1370613340000.0,1370614240000.0,1370589952000.0,-1.0,1370593123000.0,1370596723000.0,1370593092000.0,1370596692000.0,1370593107000.0,1370596707000.0,"dturanski",1370596723000.0,1370589952000.0,43,1370589952000.0,-1.0,"Add http port command line option to AdminMain","Currently StreamServer has setPort, but no way for end user to set it. ",2.0,False,2.0,0,0.0,0,0,False
"XD-211","Story","Sprint 6","2013-06-06T21:56:40.000+0000","mark.pollack","","Document the log sink","",1.0,0.9306793424440484,0.8863636363636364,0.0001485442661913,True,True,True,1370555800000.0,1370593392000.0,1370596192000.0,1370595292000.0,1370596192000.0,1370555800000.0,-1.0,1370555806000.0,1370559406000.0,1370591602000.0,1370595202000.0,1370593392000.0,1370596192000.0,"mark.pollack",1370559406000.0,1370555800000.0,43,1370555800000.0,-1.0,"Document the log sink","",1.0,False,1.0,0,0.0,-1,0,False
"XD-210","Story","","2013-06-06T20:21:33.000+0000","mark.pollack","","If output directory does not exist for a file sink, by default allow it to be created","There shouldn't be a need to do a mkdir -p before sending data to a file sink.",1.0,-1.0,-1.0,-1.0,False,False,False,1370550093000.0,-1.0,-1.0,1371464276000.0,1371465176000.0,1370550093000.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,-1.0,"mark.pollack",-1.0,-1.0,0,1370550093000.0,-1.0,"If output directory does not exist for a file sink, by default allow it to be created","There shouldn't be a need to do a mkdir -p before sending data to a file sink.",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-208","Story","Sprint 6","2013-06-06T16:23:39.000+0000","mark.pollack","","Document the file sink","",1.0,0.9633271372235638,0.9604607126105312,0.3468373781769539,True,True,True,1370535819000.0,1370591271000.0,1370593382000.0,1370592482000.0,1370593382000.0,1370535819000.0,-1.0,1370555784000.0,1370559384000.0,1370591106000.0,1370593382000.0,1370591271000.0,1370593382000.0,"mark.pollack",1370559384000.0,1370535819000.0,43,1370535819000.0,-1.0,"Document the file sink","",1.0,False,1.0,0,0.0,-1,0,False
"XD-207","Story","Sprint 6","2013-06-06T16:17:09.000+0000","mark.pollack","","Provide configurable properties for hdfs sink.","The config file modules/sink/hdfs.xml has a hardcoded value to locate the namenode.

	<hdp:configuration register-url-handler=""false"">
		fs.default.name=hdfs://localhost:9000
	</hdp:configuration>

the fs.default.name proprety should be configurable and we should also support loading an external configuration file using 

   <hdp:configuration properties-location=""${xd.home}/config/hadoop.properties"">

   </hdp:configuration>

",2.0,0.990569003458032,0.9906007576551432,0.1934719721579199,True,True,True,1370535429000.0,1370847378000.0,1370850348000.0,1370849448000.0,1370850348000.0,1370535429000.0,-1.0,1370596357000.0,1370599957000.0,1370847388000.0,1370850348000.0,1370847378000.0,1370850348000.0,"mark.pollack",1370599957000.0,1370535429000.0,43,1370535429000.0,-1.0,"Provide configurable properties for hdfs sink.","The config file modules/sink/hdfs.xml has a hardcoded value to locate the namenode.

	<hdp:configuration register-url-handler=""false"">
		fs.default.name=hdfs://localhost:9000
	</hdp:configuration>

the fs.default.name proprety should be configurable and we should also support loading an external configuration file using 

   <hdp:configuration properties-location=""${xd.home}/config/hadoop.properties"">

   </hdp:configuration>

",2.0,False,2.0,0,0.0,0,0,False
"XD-206","Bug","Sprint 6","2013-06-06T15:36:59.000+0000","iperumal","Mark Pollack","XD scripts should always prefer xd.home property from scripts","Currently, the system property xd.home is set as JVM_OPTS (via SPRING_XD_ADMIN_OPTS) into xd-admin & xd-container scripts.

Inside the ContainerMain & AdminMain, we need to check if this system property is set and use it. It seems like, this check is missing now.",1.0,0.0551564227017156,0.0551323946369359,0.0002282666154067,True,True,True,1370533019000.0,1370537610000.0,1370541210000.0,1370615355000.0,1370616255000.0,1370533019000.0,-1.0,1370533038000.0,1370536638000.0,1370537608000.0,1370541208000.0,1370537610000.0,1370541210000.0,"iperumal",1370536638000.0,1370533019000.0,43,1370533019000.0,-1.0,"XD AdminMain & ContainerMain should check xd.home property from scripts","Currently, the system property xd.home is set as JVM_OPTS (via SPRING_XD_ADMIN_OPTS) into xd-admin & xd-container scripts.

Inside the ContainerMain & AdminMain, we need to check if this system property is set and use it. It seems like, this check is missing now.",1.0,False,1.0,0,0.0,0,0,True
"XD-204","Story","Sprint 6","2013-06-06T12:52:10.000+0000","jencompgeek","jencompgeek","Document processor modules","Fill in https://github.com/SpringSource/spring-xd/wiki/Processors",2.0,0.4217292204832092,0.017352985868409,0.016532441878134,True,True,True,1370523130000.0,1370537007000.0,1370540607000.0,1370555135000.0,1370556035000.0,1370523130000.0,-1.0,1370523674000.0,1370527274000.0,1370523701000.0,1370527301000.0,1370537007000.0,1370540607000.0,"jencompgeek",1370527274000.0,1370523130000.0,43,1370523130000.0,-1.0,"Document processor modules","Fill in https://github.com/SpringSource/spring-xd/wiki/Processors",2.0,False,2.0,0,0.0,-1,0,False
"XD-202","Story","Sprint 6","2013-06-06T09:51:46.000+0000","mark.fisher","mark.fisher","Update twittersearch module for Twitter 1.0 API retirement","",3.0,0.007104145305405,0.0037351691811923,0.003002783067233,True,True,True,1370512306000.0,1370512403000.0,1370516003000.0,1370525060000.0,1370525960000.0,1370512306000.0,-1.0,1370512347000.0,1370515947000.0,1370512357000.0,1370515957000.0,1370512403000.0,1370516003000.0,"mark.fisher",1370515947000.0,1370512306000.0,43,1370512306000.0,-1.0,"Update twittersearch module for Twitter 1.0 API retirement","",3.0,False,3.0,0,0.0,0,0,False
"XD-201","Bug","Sprint 6","2013-06-06T09:40:52.000+0000","iperumal","iperumal","Fix XD scripts on windows","Currently the XD scripts are broken in windows. ",2.0,0.0658895279268151,0.0,0.0036669625969815,True,True,True,1370511652000.0,1370513359000.0,1370516959000.0,1370536659000.0,1370537559000.0,1370511652000.0,-1.0,1370511747000.0,1370515347000.0,1370511652000.0,1370515252000.0,1370513359000.0,1370516959000.0,"iperumal",1370515347000.0,1370511652000.0,43,1370511652000.0,-1.0,"Fix XD scripts on windows","Currently the XD scripts are broken in windows. ",2.0,False,2.0,0,0.0,0,0,False
"XD-200","Bug","Sprint 6","2013-06-06T09:40:50.000+0000","dturanski","dturanski","Creating a tap throws an exception","Creating a tap throws an exception. In local mode: Cannot resolve reference to bean 'redisConnectionFactory' while setting constructor argument; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'redisConnectionFactory' is defined  But also fails when using redis.
",4.0,0.0037884049552336,0.0,0.0007576809910467,True,True,True,1370511650000.0,1370511950000.0,1370515550000.0,1370589939000.0,1370590839000.0,1370511650000.0,-1.0,1370511710000.0,1370515310000.0,1370511650000.0,1370515250000.0,1370511950000.0,1370515550000.0,"dturanski",1370515310000.0,1370511650000.0,43,1370511650000.0,-1.0,"Creating a tap throws an exception","Creating a tap throws an exception. In local mode: Cannot resolve reference to bean 'redisConnectionFactory' while setting constructor argument; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'redisConnectionFactory' is defined  But also fails when using redis.
",4.0,False,4.0,0,0.0,0,0,False
"XD-199","Bug","Sprint 6","2013-06-06T08:23:17.000+0000","grussell","grussell","Ensure the DELETE Operation can Delete a Tap","While deleting a stream doesn't remove any taps right now, we should be able to explicitly delete a tap.

Determine whether the current DELETE works and, if not, make it so.",3.0,0.4827414160656779,0.0,9.071529006213996e-05,True,True,True,1370506997000.0,1370602784000.0,1370606384000.0,1370704520000.0,1370705420000.0,1370506997000.0,-1.0,1370507015000.0,1370510615000.0,1370506997000.0,1370510597000.0,1370602784000.0,1370606384000.0,"grussell",1370510615000.0,1370506997000.0,43,1370506997000.0,-1.0,"Ensure the DELETE Operation can Delete a Tap","While deleting a stream doesn't remove any taps right now, we should be able to explicitly delete a tap.

Determine whether the current DELETE works and, if not, make it so.",3.0,False,3.0,0,0.0,0,0,False
"XD-198","Improvement","Sprint 9","2013-06-06T07:50:29.000+0000","grussell","Mark Pollack","DEBUG Stack Trace When Deploying Transformer","{{curl -X POST -d ""time --interval=3 | transform | log"" http://localhost:8080/streams/test}}

results in the following stack trace in the DEBUG log. It's apparently benign, but ugly...

{code}
2013-06-06 10:43:36,875 [task-scheduler-1] DEBUG: org.springframework.scripting.support.ResourceScriptSource - class path resource [transform.groovy] could not be resolved in the file system - current timestamp not available for script modification check
java.io.FileNotFoundException: class path resource [transform.groovy] cannot be resolved to URL because it does not exist
	at org.springframework.core.io.ClassPathResource.getURL(ClassPathResource.java:177)
	at org.springframework.core.io.AbstractFileResolvingResource.lastModified(AbstractFileResolvingResource.java:170)
	at org.springframework.scripting.support.ResourceScriptSource.retrieveLastModifiedTime(ResourceScriptSource.java:101)
	at org.springframework.scripting.support.ResourceScriptSource.getScriptAsString(ResourceScriptSource.java:79)
	at org.springframework.integration.scripting.RefreshableResourceScriptSource.<init>(RefreshableResourceScriptSource.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:121)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:280)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1035)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:939)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:485)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:271)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:126)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:616)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:148)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1035)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:939)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:485)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:271)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:126)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1360)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1118)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:517)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:294)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:225)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:291)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:193)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:589)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:925)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:472)
	at org.springframework.xd.module.SimpleModule.start(SimpleModule.java:97)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:120)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:108)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:84)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:57)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:102)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:126)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:230)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:129)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)
	at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter.access$4(RedisQueueInboundChannelAdapter.java:1)
	at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter$ListenerTask.run(RedisQueueInboundChannelAdapter.java:110)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

",1.0,0.9997459384682524,0.9999362955206534,0.0660202240150927,True,True,True,1370505029000.0,1406192037000.0,1406195637000.0,1406200206000.0,1406201106000.0,1370505029000.0,1402056128000.0,1372861692000.0,1372865292000.0,1406198832000.0,1406201106000.0,1406192037000.0,1406195637000.0,"grussell",1372865292000.0,1370505029000.0,43,1402056128000.0,-1.0,"Documentation for developing streams in the IDE needs to mention including scripts dir to project classpath","{{curl -X POST -d ""time --interval=3 | transform | log"" http://localhost:8080/streams/test}}

results in the following stack trace in the DEBUG log. It's apparently benign, but ugly...

{code}
2013-06-06 10:43:36,875 [task-scheduler-1] DEBUG: org.springframework.scripting.support.ResourceScriptSource - class path resource [transform.groovy] could not be resolved in the file system - current timestamp not available for script modification check
java.io.FileNotFoundException: class path resource [transform.groovy] cannot be resolved to URL because it does not exist
	at org.springframework.core.io.ClassPathResource.getURL(ClassPathResource.java:177)
	at org.springframework.core.io.AbstractFileResolvingResource.lastModified(AbstractFileResolvingResource.java:170)
	at org.springframework.scripting.support.ResourceScriptSource.retrieveLastModifiedTime(ResourceScriptSource.java:101)
	at org.springframework.scripting.support.ResourceScriptSource.getScriptAsString(ResourceScriptSource.java:79)
	at org.springframework.integration.scripting.RefreshableResourceScriptSource.<init>(RefreshableResourceScriptSource.java:46)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.springframework.beans.BeanUtils.instantiateClass(BeanUtils.java:147)
	at org.springframework.beans.factory.support.SimpleInstantiationStrategy.instantiate(SimpleInstantiationStrategy.java:121)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:280)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1035)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:939)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:485)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:271)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:126)
	at org.springframework.beans.factory.support.ConstructorResolver.resolveConstructorArguments(ConstructorResolver.java:616)
	at org.springframework.beans.factory.support.ConstructorResolver.autowireConstructor(ConstructorResolver.java:148)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.autowireConstructor(AbstractAutowireCapableBeanFactory.java:1035)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBeanInstance(AbstractAutowireCapableBeanFactory.java:939)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:485)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveInnerBean(BeanDefinitionValueResolver.java:271)
	at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:126)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1360)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1118)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:517)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:294)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:225)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:291)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:193)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:589)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:925)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:472)
	at org.springframework.xd.module.SimpleModule.start(SimpleModule.java:97)
	at org.springframework.xd.dirt.module.ModuleDeployer.deployModule(ModuleDeployer.java:120)
	at org.springframework.xd.dirt.module.ModuleDeployer.handleMessageInternal(ModuleDeployer.java:108)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:69)
	at org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:84)
	at org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:57)
	at org.springframework.expression.spel.ast.SpelNodeImpl.getTypedValue(SpelNodeImpl.java:102)
	at org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:102)
	at org.springframework.integration.util.AbstractExpressionEvaluator.evaluateExpression(AbstractExpressionEvaluator.java:126)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.processInternal(MessagingMethodInvokerHelper.java:230)
	at org.springframework.integration.util.MessagingMethodInvokerHelper.process(MessagingMethodInvokerHelper.java:129)
	at org.springframework.integration.handler.MethodInvokingMessageProcessor.processMessage(MethodInvokingMessageProcessor.java:73)
	at org.springframework.integration.handler.ServiceActivatingHandler.handleRequestMessage(ServiceActivatingHandler.java:67)
	at org.springframework.integration.handler.AbstractReplyProducingMessageHandler.handleMessageInternal(AbstractReplyProducingMessageHandler.java:137)
	at org.springframework.integration.handler.AbstractMessageHandler.handleMessage(AbstractMessageHandler.java:73)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.doDispatch(UnicastingDispatcher.java:115)
	at org.springframework.integration.dispatcher.UnicastingDispatcher.dispatch(UnicastingDispatcher.java:102)
	at org.springframework.integration.channel.AbstractSubscribableChannel.doSend(AbstractSubscribableChannel.java:77)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:178)
	at org.springframework.integration.channel.AbstractMessageChannel.send(AbstractMessageChannel.java:149)
	at org.springframework.integration.core.MessagingTemplate.doSend(MessagingTemplate.java:304)
	at org.springframework.integration.core.MessagingTemplate.send(MessagingTemplate.java:165)
	at org.springframework.integration.endpoint.MessageProducerSupport.sendMessage(MessageProducerSupport.java:92)
	at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter.access$4(RedisQueueInboundChannelAdapter.java:1)
	at org.springframework.integration.x.redis.RedisQueueInboundChannelAdapter$ListenerTask.run(RedisQueueInboundChannelAdapter.java:110)
	at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:53)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

",0.0,False,0.0,1,-1.0,0,2,True
"XD-197","Story","Sprint 6","2013-06-06T05:26:25.000+0000","mark.pollack","","Create a scriptProcessor module that allows the execution of a groovy (potentially jruby,jython) based SI Service Activator","This will enable arbitrary processing logic to be used in a processing step.

See http://blog.springsource.org/2011/12/08/spring-integration-scripting-support-part-1/

<int:service-activator ...>
  <script:script lang=""groovy"" location=""file:scripts/groovy/myscript.groovy"">
</int:service-activator>

would be the essence of the module.  Probably 'lang' gets detected from the file extension.",4.0,0.1569835854631032,0.1570428437190481,0.0002031711632395,True,True,True,1370496385000.0,1370514929000.0,1370518529000.0,1370613612000.0,1370614512000.0,1370496385000.0,-1.0,1370496409000.0,1370500009000.0,1370514936000.0,1370518536000.0,1370514929000.0,1370518529000.0,"mark.pollack",1370500009000.0,1370496385000.0,43,1370496385000.0,-1.0,"Create a scriptProcessor module that allows the execution of a groovy (potentially jruby,jython) based SI Service Activator","This will enable arbitrary processing logic to be used in a processing step.

See http://blog.springsource.org/2011/12/08/spring-integration-scripting-support-part-1/

<int:service-activator ...>
  <script:script lang=""groovy"" location=""file:scripts/groovy/myscript.groovy"">
</int:service-activator>

would be the essence of the module.  Probably 'lang' gets detected from the file extension.",4.0,False,4.0,0,0.0,-1,0,False
"XD-196","Story","Sprint 6","2013-06-05T17:31:34.000+0000","aclement","aclement","replace the hacky parser with a good one","Replace the existing DSL parser that uses string indexing with a more robust one based on a derivative of SpEL.  This will provide a stable base on which to quickly iterate on syntax.
",4.0,0.9533350429196136,0.0,0.9532338174161852,True,True,True,1370453494000.0,1370594763000.0,1370598363000.0,1370600778000.0,1370601678000.0,1370453494000.0,-1.0,1370594748000.0,1370598348000.0,1370453494000.0,1370457094000.0,1370594763000.0,1370598363000.0,"aclement",1370598348000.0,1370453494000.0,43,1370453494000.0,-1.0,"replace the hacky parser with a good one","Replace the existing DSL parser that uses string indexing with a more robust one based on a derivative of SpEL.  This will provide a stable base on which to quickly iterate on syntax.
",4.0,False,4.0,0,0.0,0,0,False
"XD-195","Story","Sprint 6","2013-06-05T12:05:44.000+0000","grussell","grussell","The {{time}} Source Should Emit String by Default","When running in local mode (no Redis) {{time | tcp}} no longer works.

Change the {{time}} source to emit the date as a String, while allowing an option to emit a {{Date}} object.",1.0,0.0034647550776583,0.0,0.0026284348864994,True,True,True,1370433944000.0,1370433973000.0,1370437573000.0,1370441414000.0,1370442314000.0,1370433944000.0,-1.0,1370433966000.0,1370437566000.0,1370433944000.0,1370437544000.0,1370433973000.0,1370437573000.0,"grussell",1370437566000.0,1370433944000.0,43,1370433944000.0,-1.0,"The {{time}} Source Should Emit String by Default","When running in local mode (no Redis) {{time | tcp}} no longer works.

Change the {{time}} source to emit the date as a String, while allowing an option to emit a {{Date}} object.",1.0,False,1.0,0,0.0,0,0,False
"XD-194","Story","Sprint 9","2013-06-05T09:25:45.000+0000","jencompgeek","","Users should be able to package custom modules into a single jar","If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir.  This is a bit inconvenient to build from a project, since context files in src/main/resources typically get built into a jar.

Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.",5.0,-1.0,0.0023986246175665,0.3325151015670596,False,True,True,1370424345000.0,-1.0,-1.0,1377753479000.0,1377754379000.0,1370424345000.0,-1.0,1372861692000.0,1372865292000.0,1370441927000.0,1370445527000.0,-1.0,-1.0,"jencompgeek",1372865292000.0,1370424345000.0,43,1370424345000.0,-1.0,"Users should be able to package custom modules into a single jar","If a user needs to deploy a module containing custom code, they have to build a jar for the lib dir and somehow deploy the app context file separately to a modules dir.  This is a bit inconvenient to build from a project, since context files in src/main/resources typically get built into a jar.

Might be better to accept both jar and xml files in the modules dir, though that brings up the issue of classpath isolation.",5.0,False,5.0,0,0.0,0,0,False
"XD-193","Story","Sprint 6","2013-06-05T04:55:48.000+0000","dturanski","eric.bottard","Need more unique resource locations for XD internal configuration","Currently internal config files are in META-INF/spring with fairly generic names. To avoid potential collisions if users add their own configuration in the classpath, we should have a more unique location, e.g. META-INF/spring/xd",3.0,0.1861274803862757,0.0360869002670054,0.0368820353576343,True,True,True,1370408148000.0,1370487268000.0,1370490868000.0,1370832333000.0,1370833233000.0,1370408148000.0,-1.0,1370423826000.0,1370427426000.0,1370423488000.0,1370427088000.0,1370487268000.0,1370490868000.0,"dturanski",1370427426000.0,1370408148000.0,43,1370408148000.0,-1.0,"Need more unique resource locations for XD internal configuration","Currently internal config files are in META-INF/spring with fairly generic names. To avoid potential collisions if users add their own configuration in the classpath, we should have a more unique location, e.g. META-INF/spring/xd",3.0,False,3.0,0,0.0,0,0,False
"XD-192","Story","Sprint 6","2013-06-04T19:18:57.000+0000","mark.pollack","","Update getting started documentation to use xd-singlenode start script.","With the new option of starting without requiring redis, the getting started documentation should reflect this easier way to start processing data.",2.0,-1.0,0.6655198741594532,0.0001195853149233,False,True,True,1370373537000.0,-1.0,-1.0,1370590055000.0,1370590955000.0,1370373537000.0,-1.0,1370373563000.0,1370377163000.0,1370518233000.0,1370521833000.0,-1.0,-1.0,"mark.pollack",1370377163000.0,1370373537000.0,43,1370373537000.0,-1.0,"Update getting started documentation to use xd-singlenode start script.","With the new option of starting without requiring redis, the getting started documentation should reflect this easier way to start processing data.",2.0,False,2.0,0,0.0,-1,0,False
"XD-191","Story","Sprint 7","2013-06-04T10:16:43.000+0000","mark.fisher","","Move Redis connection metadata logging into the code closest to establishing that connection","XD-106 included detailed logging about the Redis metadata within the RedisContainerListener, but it seems as though that info could be logged somewhere closer to the establishment of a Redis connection for the XD runtime (and could be logged even if this listener, whose main role is to capture Container-related events, is not enabled).",2.0,0.945867777930222,0.9459107561914868,0.6552884474984724,True,True,True,1370341003000.0,1372057631000.0,1372061231000.0,1372154974000.0,1372155874000.0,1370341003000.0,-1.0,1371530267000.0,1371533867000.0,1372057709000.0,1372061309000.0,1372057631000.0,1372061231000.0,"mark.fisher",1371533867000.0,1370341003000.0,43,1370341003000.0,-1.0,"Move Redis connection metadata logging into the code closest to establishing that connection","XD-106 included detailed logging about the Redis metadata within the RedisContainerListener, but it seems as though that info could be logged somewhere closer to the establishment of a Redis connection for the XD runtime (and could be logged even if this listener, whose main role is to capture Container-related events, is not enabled).",2.0,False,2.0,0,0.0,0,0,False
"XD-190","Improvement","","2013-06-04T04:14:13.000+0000","eric.bottard","mark.pollack","Cleanup embedded container story","The --embeddedX options are a bit confusing in code right now, as the Admin can embed the Container and vice-versa.
I guess we should only keep the Admin>Container side of things.",1.0,-1.0,0.0,-1.0,False,False,True,1370319253000.0,-1.0,-1.0,1371624049000.0,1371624949000.0,1370319253000.0,-1.0,-1.0,-1.0,1370319253000.0,1370322853000.0,-1.0,-1.0,"eric.bottard",-1.0,-1.0,0,1370319253000.0,-1.0,"Cleanup embedded container story","The --embeddedX options are a bit confusing in code right now, as the Admin can embed the Container and vice-versa.
I guess we should only keep the Admin>Container side of things.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-188","Story","Sprint 6","2013-06-03T20:07:57.000+0000","mark.pollack","jencompgeek","Add  directory to classpath in server startup scripts so groovy based processors can be easily referenced by name without a resource uri prefix","a stream such as time | filter --script=oddMinuteFilter.groovy | file 

would load the groovy script 'oddMinuteFilter.groovy' that is located in the directory modules/processor or perhaps in modules/processor/scripts.

Not sure the benefit of having a subdirectory below processor just for scripts.",1.0,0.924892703862661,0.924959763948498,0.9232162017167382,True,True,True,1370290077000.0,1370345245000.0,1370348845000.0,1370348825000.0,1370349725000.0,1370290077000.0,-1.0,1370345145000.0,1370348745000.0,1370345249000.0,1370348849000.0,1370345245000.0,1370348845000.0,"mark.pollack",1370348745000.0,1370290077000.0,43,1370290077000.0,-1.0,"Add  directory to classpath in server startup scripts so groovy based processors can be easily referenced by name without a resource uri prefix","a stream such as time | filter --script=oddMinuteFilter.groovy | file 

would load the groovy script 'oddMinuteFilter.groovy' that is located in the directory modules/processor or perhaps in modules/processor/scripts.

Not sure the benefit of having a subdirectory below processor just for scripts.",1.0,False,1.0,0,0.0,-1,0,False
"XD-187","Story","Sprint 6","2013-06-03T17:32:06.000+0000","iperumal","dturanski","Create XD script for xd-single node","This script will launch XD admin along with the module container.

As part of this implementation, we will also remove the embedded options for XD admin & container scripts.",2.0,-1.0,0.999973962401708,0.9998394348105332,False,True,True,1370280726000.0,-1.0,-1.0,1370510262000.0,1370511162000.0,1370280726000.0,-1.0,1370511125000.0,1370511162000.0,1370511156000.0,1370511162000.0,-1.0,-1.0,"iperumal",1370511162000.0,1370280726000.0,43,1370280726000.0,-1.0,"Create XD script for xd-single node","This script will launch XD admin along with the module container.

As part of this implementation, we will also remove the embedded options for XD admin & container scripts.",2.0,False,2.0,0,0.0,0,0,False
"XD-186","Story","Sprint 6","2013-06-03T14:31:02.000+0000","dturanski","dturanski","Create a pipe protocol independent StreamDeployer","Create StreamDeployer that does not depend on an adapter implementation",2.0,-1.0,0.2539182880984839,0.9996695170818358,False,True,True,1370269862000.0,-1.0,-1.0,1370511032000.0,1370511932000.0,1370269862000.0,-1.0,1370511852000.0,1370511932000.0,1370331328000.0,1370334928000.0,-1.0,-1.0,"dturanski",1370511932000.0,1370269862000.0,43,1370269862000.0,-1.0,"Create a pipe protocol independent StreamDeployer","Create StreamDeployer that does not depend on an adapter implementation",2.0,False,2.0,0,0.0,-1,0,False
"XD-184","Story","Sprint 6","2013-06-03T13:10:36.000+0000","mark.pollack","","Add unregistration support to the channel registry","",4.0,0.2722564188787415,0.2723097301220827,0.000746357406777,True,True,True,1370265036000.0,1370331426000.0,1370335026000.0,1370507987000.0,1370508887000.0,1370265036000.0,-1.0,1370265218000.0,1370268818000.0,1370331439000.0,1370335039000.0,1370331426000.0,1370335026000.0,"mark.pollack",1370268818000.0,1370265036000.0,43,1370265036000.0,-1.0,"Add unregistration support to the channel registry","",4.0,False,4.0,0,0.0,-1,0,False
"XD-183","Story","Sprint 6","2013-06-03T13:10:05.000+0000","mark.pollack","","Create localChannelProtocol.xml that will load all the SI specific implementations to suppor the XD container runtime and administration","",3.0,0.6285561979743306,0.2968056751547956,0.0008734161643498,True,True,True,1370265005000.0,1370418291000.0,1370421891000.0,1370507975000.0,1370508875000.0,1370265005000.0,-1.0,1370265218000.0,1370268818000.0,1370337387000.0,1370340987000.0,1370418291000.0,1370421891000.0,"mark.pollack",1370268818000.0,1370265005000.0,43,1370265005000.0,-1.0,"Create localChannelProtocol.xml that will load all the SI specific implementations to suppor the XD container runtime and administration","",3.0,False,3.0,0,0.0,-1,0,False
"XD-182","Story","Sprint 6","2013-06-03T13:09:02.000+0000","mark.pollack","Mark Pollack","Create redisProtocol.xml that will load all the Redis specific implementations to suppor the XD container runtime and administration","The redis specific beans that are defined in the current launcher.xml should move into this configuration file.   ",3.0,0.6287055447371118,0.2926514297427487,0.0011314953366813,True,True,True,1370264942000.0,1370418299000.0,1370421899000.0,1370507967000.0,1370508867000.0,1370264942000.0,-1.0,1370265218000.0,1370268818000.0,1370336327000.0,1370339927000.0,1370418299000.0,1370421899000.0,"mark.pollack",1370268818000.0,1370264942000.0,43,1370264942000.0,-1.0,"Create redisProtocol.xml that will load all the Redis specific implementations to suppor the XD container runtime and administration","The redis specific beans that are defined in the current launcher.xml should move into this configuration file.   ",3.0,False,3.0,0,0.0,-1,0,False
"XD-181","Story","Sprint 6","2013-06-03T13:07:37.000+0000","mark.pollack","","Update launcher.xml to have protocol independent beans defined and an import statement to load protocol specific defintiions from a system property defined location.","launcher.xml can make use of the system property xd.pipeProtocol inside an import statement.  This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. 

File name conventions should be used, so if the option passed in from the command line is --pipeProtocol localChannel

then the XML filename looked for has the 'Protocol' suffix applied, e.g. localChannelProtocol, and is loaded via the classpath.

Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes (an advanced task).  ",3.0,0.6288026884694986,0.2965922829450216,0.0014794778795516,True,True,True,1370264857000.0,1370418288000.0,1370421888000.0,1370507962000.0,1370508862000.0,1370264857000.0,-1.0,1370265218000.0,1370268818000.0,1370337227000.0,1370340827000.0,1370418288000.0,1370421888000.0,"mark.pollack",1370268818000.0,1370264857000.0,43,1370264857000.0,-1.0,"Update launcher.xml to have protocol independent beans defined and an import statement to load protocol specific defintiions from a system property defined location.","launcher.xml can make use of the system property xd.pipeProtocol inside an import statement.  This determines which version of the XD infrastructure to load, for example what ChannelRegistry implementation, Local or Redis based, or specific message listener containers. 

File name conventions should be used, so if the option passed in from the command line is --pipeProtocol localChannel

then the XML filename looked for has the 'Protocol' suffix applied, e.g. localChannelProtocol, and is loaded via the classpath.

Redis and Local will not be the only options, other implementations will be provided in the future, e.g. Rabbit, and the user may be able to provide their own implementations of these infrastructure classes (an advanced task).  ",3.0,False,3.0,0,0.0,-1,1,False
"XD-180","Story","Sprint 6","2013-06-03T13:04:09.000+0000","mark.pollack","","The command line for xd-admin and xd-container to support an additional option, pipeProtocol, that is used to determine the middleware for sending admin requests and data between processing steps","The name 'pipeProtocol' is tentative.  

1. The command line scripts for xd-admin and xd-container would support a --pipeProtocol option, with the default being to use Redis.  (Otherwise use xd-singlenode).
2. The xd-admin and xd-container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app.
",1.0,0.6289961303003624,0.6290411744231281,0.0023300096230625,True,True,True,1370264649000.0,1370418253000.0,1370421853000.0,1370507954000.0,1370508854000.0,1370264649000.0,-1.0,1370265218000.0,1370268818000.0,1370418264000.0,1370421864000.0,1370418253000.0,1370421853000.0,"mark.pollack",1370268818000.0,1370264649000.0,43,1370264649000.0,-1.0,"The command line for xd-admin and xd-container to support an additional option, pipeProtocol, that is used to determine the middleware for sending admin requests and data between processing steps","The name 'pipeProtocol' is tentative.  

1. The command line scripts for xd-admin and xd-container would support a --pipeProtocol option, with the default being to use Redis.  (Otherwise use xd-singlenode).
2. The xd-admin and xd-container scripts will use the value of pipeProtocol to set the java system property xd.pipeProtocol when launching the app.
",1.0,False,1.0,0,0.0,0,1,False
"XD-179","Story","Sprint 6","2013-06-03T12:57:55.000+0000","mark.pollack","dturanski","Have three startup scripts, xd-singlenode, xd-admin, and xd-container","The xd-singlenode script will launch a main application that creates both the admin node (to process http admin requests) and the container node (to execute modules for data processing) within in the same process 

the xd-admin script will launch a main application that creates only the admin node (remove current embeddedContainer options)

the xd-container script will launch a main application that creates only the container node (as it is now)",1.0,0.0171851936656431,0.0171565721201614,0.0038557310556034,True,True,True,1370264275000.0,1370268478000.0,1370272078000.0,1370507946000.0,1370508846000.0,1370264275000.0,-1.0,1370265218000.0,1370268818000.0,1370268471000.0,1370272071000.0,1370268478000.0,1370272078000.0,"mark.pollack",1370268818000.0,1370264275000.0,43,1370264275000.0,-1.0,"Have three startup scripts, xd-singlenode, xd-admin, and xd-container","The xd-singlenode script will launch a main application that creates both the admin node (to process http admin requests) and the container node (to execute modules for data processing) within in the same process 

the xd-admin script will launch a main application that creates only the admin node (remove current embeddedContainer options)

the xd-container script will launch a main application that creates only the container node (as it is now)",1.0,False,1.0,0,0.0,-1,0,False
"XD-178","Story","Sprint 6","2013-06-03T12:55:23.000+0000","mark.pollack","dturanski","DefaultContainer should have a default constructor that generates a UUID","The current incrementAndGet approach based off redis will not easily be applicable in local model deployment",1.0,0.7355679702048417,0.5332091868404718,0.6797020484171322,True,True,True,1370264123000.0,1370265308000.0,1370265734000.0,1370264834000.0,1370265734000.0,1370264123000.0,-1.0,1370265218000.0,1370265734000.0,1370264982000.0,1370265734000.0,1370265308000.0,1370265734000.0,"mark.pollack",1370265734000.0,1370264123000.0,43,1370264123000.0,-1.0,"DefaultContainer should have a default constructor that generates a UUID","The current incrementAndGet approach based off redis will not easily be applicable in local model deployment",1.0,False,1.0,0,0.0,-1,0,False
"XD-176","Story","Sprint 7","2013-06-03T11:19:38.000+0000","luke","luke","Support exponential moving average in RichGauge ","This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter ""alpha"" to the gauge data (https://en.wikipedia.org/wiki/Exponential_moving_average). If not set it would default to the current behaviour (simple mean), otherwise it would calculate the exponential moving average in place of the mean.",2.0,0.9908381433529426,0.508551117483447,0.9783279900066458,True,True,True,1370258378000.0,1371546531000.0,1371550131000.0,1371557542000.0,1371558442000.0,1370258378000.0,-1.0,1371530267000.0,1371533867000.0,1370919527000.0,1370923127000.0,1371546531000.0,1371550131000.0,"luke",1371533867000.0,1370258378000.0,43,1370258378000.0,-1.0,"Support exponential moving average in RichGauge ","This could easily be supported in the existing gauge by adding a setAlpha method to RichGaugeService and adding the extra parameter ""alpha"" to the gauge data (https://en.wikipedia.org/wiki/Exponential_moving_average). If not set it would default to the current behaviour (simple mean), otherwise it would calculate the exponential moving average in place of the mean.",2.0,False,2.0,0,0.0,-1,0,False
"XD-175","Story","Sprint 6","2013-06-03T10:06:12.000+0000","mark.pollack","","Create and document a syslog aggregation example","2-3 containers (separate processes) that the stream: syslog | tcp 
1 container (separate process) that aggregates the data sent from those conainers, tcp | severityFilter | hdfs






",8.0,0.5031092232662576,0.5031006421532873,0.0136511205503353,True,True,True,1370253972000.0,1370605751000.0,1370609351000.0,1370952282000.0,1370953182000.0,1370253972000.0,-1.0,1370263517000.0,1370267117000.0,1370605745000.0,1370609345000.0,1370605751000.0,1370609351000.0,"mark.pollack",1370267117000.0,1370253972000.0,43,1370253972000.0,-1.0,"Create and document a syslog aggregation example","2-3 containers (separate processes) that the stream: syslog | tcp 
1 container (separate process) that aggregates the data sent from those conainers, tcp | severityFilter | hdfs






",8.0,False,8.0,0,0.0,0,0,False
"XD-174","Story","Sprint 6","2013-06-03T10:04:11.000+0000","mark.pollack","","Document how to create a custom processor module.","The use case is to write custom code that does processing on a specific domain class (perhaps from twitter adapter) or a tuple.  Need to package up this code so that it can be used inside XD.
",4.0,0.9950521062114714,0.5728881941762378,0.0160222249111948,True,True,True,1370253851000.0,1370854153000.0,1370857138000.0,1370856238000.0,1370857138000.0,1370253851000.0,-1.0,1370263517000.0,1370267117000.0,1370599467000.0,1370603067000.0,1370854153000.0,1370857138000.0,"mark.pollack",1370267117000.0,1370253851000.0,43,1370253851000.0,-1.0,"Document how to create a custom processor module.","The use case is to write custom code that does processing on a specific domain class (perhaps from twitter adapter) or a tuple.  Need to package up this code so that it can be used inside XD.
",4.0,False,4.0,0,0.0,0,0,False
"XD-173","Story","Sprint 6","2013-06-03T10:02:50.000+0000","mark.pollack","Mark Pollack","Document how to create a custom input/output module for existing SI channel adapters","Document how to take an existing input/output channel adapters in spring integration and add them as a XD source/sink module.

Should be as end-user focused, step by step guide as possible.

Consider including a getting started gradle/pom.xml ",6.0,0.4454652269380247,0.445523709268386,0.0518206612756591,True,True,True,1370253770000.0,1370337558000.0,1370341158000.0,1370440961000.0,1370441861000.0,1370253770000.0,-1.0,1370263517000.0,1370267117000.0,1370337569000.0,1370341169000.0,1370337558000.0,1370341158000.0,"mark.pollack",1370267117000.0,1370253770000.0,43,1370253770000.0,-1.0,"Document how to create a custom input/output module for existing SI channel adapters","Document how to take an existing input/output channel adapters in spring integration and add them as a XD source/sink module.

Should be as end-user focused, step by step guide as possible.

Consider including a getting started gradle/pom.xml ",6.0,False,6.0,0,0.0,0,1,False
"XD-172","Story","Sprint 6","2013-06-03T09:57:36.000+0000","mark.pollack","","Create links to SpringXD on other pages of springsource.org site","bottom home page - list of projects
data/integration category landing pages - related projects.
",4.0,0.8930057776589437,1.00001017193476,0.014619976517362,True,True,True,1370253456000.0,1370867994000.0,1370871594000.0,1370940724000.0,1370941624000.0,1370253456000.0,-1.0,1370263517000.0,1370267117000.0,1370941631000.0,1370941624000.0,1370867994000.0,1370871594000.0,"mark.pollack",1370267117000.0,1370253456000.0,43,1370253456000.0,-1.0,"Create links to SpringXD on other pages of springsource.org site","bottom home page - list of projects
data/integration category landing pages - related projects.
",4.0,False,4.0,0,0.0,-1,0,False
"XD-171","Story","Sprint 6","2013-06-03T09:56:34.000+0000","mark.pollack","","Create project home page for SpringXD on springsource.org/spring-xd","A minimal project page of a 'top level project' page that has basic information of docs and links to the github wiki page. 
No need to list maven coordinates.",4.0,0.986036301215359,0.9860610595465374,0.0278476207243737,True,True,True,1370253394000.0,1370611832000.0,1370615432000.0,1370616008000.0,1370616908000.0,1370253394000.0,-1.0,1370263517000.0,1370267117000.0,1370611841000.0,1370615441000.0,1370611832000.0,1370615432000.0,"mark.pollack",1370267117000.0,1370253394000.0,43,1370253394000.0,-1.0,"Create project home page for SpringXD on springsource.org/spring-xd","A minimal project page of a 'top level project' page that has basic information of docs and links to the github wiki page. 
No need to list maven coordinates.",4.0,False,4.0,0,0.0,-1,0,False
"XD-170","Story","Sprint 6","2013-06-03T09:50:44.000+0000","mark.pollack","Mark Pollack","Home wiki page improvements","Add more structure, more easily find the reference documentation.
",2.0,0.955875025525832,0.93794840378463,0.0285154176026138,True,True,True,1370253044000.0,1370604113000.0,1370607713000.0,1370619419000.0,1370620319000.0,1370253044000.0,-1.0,1370263517000.0,1370267117000.0,1370597529000.0,1370601129000.0,1370604113000.0,1370607713000.0,"mark.pollack",1370267117000.0,1370253044000.0,43,1370253044000.0,-1.0,"Home wiki page improvements","Add more structure, more easily find the reference guide.  The style that is here 
https://github.com/snowplow/snowplow/wiki
is nice.
",2.0,False,2.0,0,0.0,-1,0,True
"XD-169","Story","Sprint 6","2013-06-03T09:50:08.000+0000","jencompgeek","jencompgeek","XD should run offline","Trying to run XD offline results in an error in redis.xml because the cloudfoundry schema file is missing.  We need to add the cf-runtime jar to the classpath to resolve this.",2.0,0.2785537369455864,0.2784771340295687,0.2683400148098971,True,True,True,1370253008000.0,1370263917000.0,1370267517000.0,1370291271000.0,1370292171000.0,1370253008000.0,-1.0,1370263517000.0,1370267117000.0,1370263914000.0,1370267514000.0,1370263917000.0,1370267517000.0,"jencompgeek",1370267117000.0,1370253008000.0,43,1370253008000.0,-1.0,"XD should run offline","Trying to run XD offline results in an error in redis.xml because the cloudfoundry schema file is missing.  We need to add the cf-runtime jar to the classpath to resolve this.",2.0,False,2.0,0,0.0,0,1,False
"XD-168","Story","Sprint 6","2013-06-03T09:43:32.000+0000","mark.pollack","","Decide on location to host http reference documentation and automate upload in build scripts","",4.0,0.8277446014030174,0.8277748033441785,0.0299411062147358,True,True,True,1370252612000.0,1370554089000.0,1370557689000.0,1370615927000.0,1370616827000.0,1370252612000.0,-1.0,1370263517000.0,1370267117000.0,1370554100000.0,1370557700000.0,1370554089000.0,1370557689000.0,"mark.pollack",1370267117000.0,1370252612000.0,43,1370252612000.0,-1.0,"Decide on location to host http reference documentation and automate upload in build scripts","",4.0,False,4.0,0,0.0,-1,0,False
"XD-167","Story","Sprint 6","2013-06-03T09:42:27.000+0000","mark.pollack","","Script to generate reference documentation from wiki and include in .zip distribution","",5.0,0.2268571047186811,0.2269238470266301,0.0318331790522011,True,True,True,1370252547000.0,1370330724000.0,1370334324000.0,1370596256000.0,1370597156000.0,1370252547000.0,-1.0,1370263517000.0,1370267117000.0,1370330747000.0,1370334347000.0,1370330724000.0,1370334324000.0,"mark.pollack",1370267117000.0,1370252547000.0,43,1370252547000.0,-1.0,"Script to generate reference documentation from wiki and include in .zip distribution","",5.0,False,5.0,0,0.0,0,0,False
"XD-166","Story","","2013-05-31T14:58:00.000+0000","iperumal","","Create config support based on channel registry type","We need to have the XD container & admin reading the registry specific property based on the registry type selected. 

From Mark F, on one of the code review comments:

Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry (""redis"", ""rabbit"", or ""local"" being possible values), and then we could use something like:

<import resource=""config/${registry.type}.xml""/>

<context:property-placeholder location=""config/${registry.type}.properties""/>",3.0,0.0117120876064152,0.0116950074786559,-1.0,True,False,True,1370012280000.0,1370017080000.0,1370020680000.0,1370421213000.0,1370422113000.0,1370012280000.0,-1.0,-1.0,-1.0,1370017073000.0,1370020673000.0,1370017080000.0,1370020680000.0,"iperumal",-1.0,-1.0,0,1370012280000.0,-1.0,"Create config support based on channel registry type","We need to have the XD container & admin reading the registry specific property based on the registry type selected. 

From Mark F, on one of the code review comments:

Maybe rather than having redis, rabbit, etc. properties all within a container.properties we should rely upon naming conventions instead. Specifically, we could have a single configurable property for the type of channel registry (""redis"", ""rabbit"", or ""local"" being possible values), and then we could use something like:

<import resource=""config/${registry.type}.xml""/>

<context:property-placeholder location=""config/${registry.type}.properties""/>",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-165","Improvement","Sprint 6","2013-05-31T09:41:48.000+0000","jencompgeek","Mark Pollack","xd-container and xd-admin should log to a file out of the box","We should have an externally editable log4j config file in a conf dir and the default should log to a file (presumably in a logs dir)",2.0,0.6039242630743065,0.6039214471564862,0.3804431691465516,True,True,True,1369993308000.0,1370422244000.0,1370425844000.0,1370702656000.0,1370703556000.0,1369993308000.0,-1.0,1370263517000.0,1370267117000.0,1370422242000.0,1370425842000.0,1370422244000.0,1370425844000.0,"jencompgeek",1370267117000.0,1369993308000.0,43,1369993308000.0,-1.0,"xd-container and xd-admin should log to a file out of the box","We should have an externally editable log4j config file in a conf dir and the default should log to a file (presumably in a logs dir)",2.0,False,2.0,0,0.0,0,0,False
"XD-164","Story","Sprint 7","2013-05-31T07:46:30.000+0000","mark.fisher","","Validate processing modules declare the required channels","Validate that modules have required channels declared according to their type.  Currently the stream deployer accepts processors with no input, but the stream doesn't complete. We should fail earlier and more loudly.",2.0,0.847613397100821,0.8476110813146626,0.7150577973909427,True,True,True,1369986390000.0,1371816467000.0,1371820067000.0,1372144584000.0,1372145484000.0,1369986390000.0,-1.0,1371530267000.0,1371533867000.0,1371816462000.0,1371820062000.0,1371816467000.0,1371820067000.0,"mark.fisher",1371533867000.0,1369986390000.0,43,1369986390000.0,-1.0,"Validate processing modules declare the required channels","Validate that modules have required channels declared according to their type.  Currently the stream deployer accepts processors with no input, but the stream doesn't complete. We should fail earlier and more loudly.",2.0,False,2.0,0,0.0,0,2,False
"XD-163","Story","Sprint 15","2013-05-31T07:43:54.000+0000","mark.fisher","","Enable grouping of modules for co-located deployment","example:

{code}
a | (b | c) | d
{code}

...where b and c modules are deployed together as a composite module.

There are 2 options (maybe more) for how we could handle that. One would be defining a CompositeModule type that simply bridges the channels (b's output to c's input in this example). The second option would be to deploy those together on the same node as modules but using the LocalChannelRegistry between them.
",12.0,0.7385857874635241,0.7161656011041813,0.5983514749325323,True,True,True,1369986234000.0,1379615705000.0,1379619305000.0,1383023049000.0,1383023949000.0,1369986234000.0,1377787382000.0,1377787370000.0,1377790970000.0,1379323397000.0,1379326997000.0,1379615705000.0,1379619305000.0,"mark.fisher",1377790970000.0,1377787382000.0,43,1377787382000.0,-1.0,"Enable grouping of modules for co-located deployment","example:

{code}
a | (b | c) | d
{code}

...where b and c modules are deployed together as a composite module.

There are 2 options (maybe more) for how we could handle that. One would be defining a CompositeModule type that simply bridges the channels (b's output to c's input in this example). The second option would be to deploy those together on the same node as modules but using the LocalChannelRegistry between them.
",12.0,False,12.0,0,0.0,0,1,False
"XD-162","Story","Sprint 9","2013-05-31T07:40:13.000+0000","mark.fisher","","Add support for message conversion in ChannelRegistry","The conversion should be based on content-type headers, similar to the way Spring's HttpMessageConverters work (with mime types).

Also, the map of available converters should be extensible while including the most common defaults (for JSON, XML, etc). We most likely want to add a few of our own content types also (e.g. for Tuples).

Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations (i.e. the logic should be the same regardless of the transport used after-serialization/before-deserialization).",5.0,0.7514009584451297,0.7513982886815372,0.5985694516750542,True,True,True,1369986013000.0,1373363395000.0,1373366995000.0,1374479893000.0,1374480793000.0,1369986013000.0,1373277628000.0,1372676451000.0,1372680051000.0,1373363383000.0,1373366983000.0,1373363395000.0,1373366995000.0,"mark.fisher",1372680051000.0,1372675559000.0,43,1373277628000.0,1373277628000.0,"Create design document for implementation strategy to support message conversion in ChannelRegistry","The conversion should be based on content-type headers, similar to the way Spring's HttpMessageConverters work (with mime types).

Also, the map of available converters should be extensible while including the most common defaults (for JSON, XML, etc). We most likely want to add a few of our own content types also (e.g. for Tuples).

Most likely, this logic and the configuration methods for extending the converter map, belong in AbstractChannelRegistry since it should be common across all implementations (i.e. the logic should be the same regardless of the transport used after-serialization/before-deserialization).",20.0,True,20.0,1,-75.0,0,1,True
"XD-161","Story","Sprint 5","2013-05-30T15:21:51.000+0000","grussell","grussell","Add ReST Delete Operation","",5.0,9.049166163124357e-05,0.0,5.5462631322375086e-05,True,True,True,1369927311000.0,1369927342000.0,1369930942000.0,1370268984000.0,1370269884000.0,1369927311000.0,-1.0,1369927330000.0,1369930930000.0,1369927311000.0,1369930911000.0,1369927342000.0,1369930942000.0,"grussell",1369930930000.0,1369927311000.0,43,1369927311000.0,-1.0,"Add HTTP Delete Stream Operation","",5.0,False,5.0,0,0.0,-1,0,True
"XD-160","Story","Sprint 7","2013-05-30T14:34:33.000+0000","jencompgeek","","Publish golo themed docs documentation to static.springsource.org as part of nightly build","The wiki repo contains a script, gen-docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. 

We should consider using maven (or gradle, but there is currently an issue documented in build.gradle) to generate this and other reference docs and publish them automatically as part of a nightly build.",3.0,0.992939057165686,0.9971055422386867,0.9331264618290114,True,True,True,1369924473000.0,1371633197000.0,1371636797000.0,1371644448000.0,1371645348000.0,1369924473000.0,-1.0,1371530267000.0,1371533867000.0,1371640367000.0,1371643967000.0,1371633197000.0,1371636797000.0,"jencompgeek",1371533867000.0,1369924473000.0,43,1369924473000.0,-1.0,"Publish golo themed docs documentation to static.springsource.org as part of nightly build","The wiki repo contains a script, gen-docs.sh, that we are planning to use to generate a pretty HTML version of the Getting Started guide. 

We should consider using maven (or gradle, but there is currently an issue documented in build.gradle) to generate this and other reference docs and publish them automatically as part of a nightly build.",3.0,False,3.0,0,0.0,0,1,False
"XD-159","Story","Sprint 9","2013-05-30T11:46:58.000+0000","grussell","","Improve Module Parameter Parsing","Parameter parsing does not work if an argument contains '--'.

For example:

{code}
... | transform --expression=42 | transform --expression=--payload |...
{code}

Also, I was surprised that this worked..

{code}
| transform --expression=new StringBuilder(payload).reverse() |
{code}

... but this didn't...

{code}
| transform --expression='new StringBuilder(payload).reverse()' |
{code}

I think we need to tokenize the argument (with ' if contains spaces) and remove any surrounding '...' from the result. This means if someone wants a SpEL literal they would have to use something like 

{code}--expression=''Hello, world!''{code}

resulting in a SpEL literal 'Hello, world!'",1.0,-1.0,0.6677081594274431,0.1323771524299753,False,True,True,1369914418000.0,-1.0,-1.0,1392177715000.0,1392178615000.0,1369914418000.0,1384781657000.0,1372861689000.0,1372865289000.0,1384780404000.0,1384784004000.0,-1.0,-1.0,"grussell",1372865289000.0,1369914418000.0,43,1384781657000.0,-1.0,"Parameter parsing does not work if an argument contains '--'.","Parameter parsing does not work if an argument contains '--'.

For example:

{code}
... | transform --expression=42 | transform --expression=--payload |...
{code}

Also, I was surprised that this worked..

{code}
| transform --expression=new StringBuilder(payload).reverse() |
{code}

... but this didn't...

{code}
| transform --expression='new StringBuilder(payload).reverse()' |
{code}

I think we need to tokenize the argument (with ' if contains spaces) and remove any surrounding '...' from the result. This means if someone wants a SpEL literal they would have to use something like 

{code}--expression=''Hello, world!''{code}

resulting in a SpEL literal 'Hello, world!'",0.0,False,0.0,1,-1.0,0,2,True
"XD-158","Story","Sprint 9","2013-05-30T10:53:22.000+0000","mark.pollack","","Add jolokia configuration to export JMX managed shutdown bean over http","This will allow for a simple way to shutdown the server via an HTTP call.  Support for security is a separate story. The end goal is to have some shell scripts distributed that can issue HTTP requests to shutdown the xd-admin and xd-container servers.

The newest version of Jolokia has the ability to boostrap itself inside an application context vs. requiring a java agent.  I suspect using the application context approach will provide us with more flexibility (e.g. property replacement etc) but not sure.",4.0,0.915418764632696,0.9967196749952478,0.1169896410246731,True,True,True,1369911202000.0,1392998120000.0,1393001720000.0,1395130364000.0,1395131264000.0,1369911202000.0,-1.0,1372861688000.0,1372865288000.0,1395048534000.0,1395052134000.0,1392998120000.0,1393001720000.0,"mark.pollack",1372865288000.0,1369911202000.0,43,1369911202000.0,-1.0,"Expose shutdown operation over http","This will allow for a simple way to shutdown the server via an HTTP call.  Support for security is a separate story. The end goal is to have some shell scripts distributed that can issue HTTP requests to shutdown the xd-admin and xd-container servers.

The newest version of Jolokia has the ability to boostrap itself inside an application context vs. requiring a java agent.  I suspect using the application context approach will provide us with more flexibility (e.g. property replacement etc) but not sure.",4.0,False,4.0,0,0.0,0,0,True
"XD-157","Story","Sprint 9","2013-05-30T10:48:06.000+0000","mark.pollack","","Add a JMX managed bean to shutdown cleanly the xd-admin and xd-container servers","In both cases there are multiple application contexts that can be running in the process.  The JMX Managed bean should call 'close' on those application contexts.  The more detailed lifecycle of cleanly shutting down components within those application contexts is another story.",4.0,-1.0,0.9930828458139388,0.1135454234295212,False,True,True,1369910886000.0,-1.0,-1.0,1395897841000.0,1395898741000.0,1369910886000.0,-1.0,1372861688000.0,1372865288000.0,1395718979000.0,1395722579000.0,-1.0,-1.0,"mark.pollack",1372865288000.0,1369910886000.0,43,1369910886000.0,-1.0,"Verify use of JMX managed bean to shutdown cleanly the xd-admin and xd-container servers","In both cases there are multiple application contexts that can be running in the process.  The JMX Managed bean should call 'close' on those application contexts.  The more detailed lifecycle of cleanly shutting down components within those application contexts is another story.",4.0,False,4.0,0,0.0,0,0,True
"XD-155","Story","Sprint 5","2013-05-30T09:26:00.000+0000","dturanski","","Add a groovy script processor module","A processor module that accepts either the location of a groovy script resource or an inline script (string). Also some discussion about a default classpath location for scripts. ",5.0,0.2569066669443042,0.2568780355757768,0.0001223340291623,True,True,True,1369905960000.0,1370004662000.0,1370008262000.0,1370289254000.0,1370290154000.0,1369905960000.0,-1.0,1369906007000.0,1369909607000.0,1370004651000.0,1370008251000.0,1370004662000.0,1370008262000.0,"dturanski",1369909607000.0,1369905960000.0,43,1369905960000.0,-1.0,"Add a groovy script processor module","A processor module that accepts either the location of a groovy script resource or an inline script (string). Also some discussion about a default classpath location for scripts. ",5.0,False,5.0,0,0.0,0,0,False
"XD-153","Story","Sprint 5","2013-05-30T07:26:06.000+0000","dturanski","luke","create a gauge module","Spring config for simple gauge plus message handler to process message. There is some code common to RichGaugeHandler to coerce the payload to a double that should be refactored for reuse.",5.0,0.3389633278351447,0.0,0.0008473235872291,True,True,True,1369898766000.0,1369988775000.0,1369992375000.0,1370163408000.0,1370164308000.0,1369898766000.0,-1.0,1369898991000.0,1369902591000.0,1369898766000.0,1369902366000.0,1369988775000.0,1369992375000.0,"dturanski",1369902591000.0,1369898766000.0,43,1369898766000.0,-1.0,"create a gauge module","Spring config for simple gauge plus message handler to process message. There is some code common to RichGaugeHandler to coerce the payload to a double that should be refactored for reuse.",5.0,False,5.0,0,0.0,0,0,False
"XD-152","Story","Sprint 5","2013-05-30T07:23:50.000+0000","dturanski","dturanski","Create rich gauge module","Spring config for rich gauge plus message handler to coerce a numeric or string payload to a double.",5.0,-1.0,0.0,0.0889733840304182,False,True,True,1369898630000.0,-1.0,-1.0,1369901675000.0,1369902575000.0,1369898630000.0,-1.0,1369898981000.0,1369902575000.0,1369898630000.0,1369902230000.0,-1.0,-1.0,"dturanski",1369902575000.0,1369898630000.0,43,1369898630000.0,-1.0,"Create rich gauge module","Spring config for rich gauge plus message handler to coerce a numeric or string payload to a double.",5.0,False,5.0,0,0.0,-1,0,False
"XD-150","Story","Sprint 6","2013-05-29T23:06:37.000+0000","iperumal","iperumal","Publish Spring XD final distribution zip as part of Bamboo artifactory plugin","Currently, Bamboo's gradle artifactory plugin has the artifacts configured to projects target(build) directory 'archives'. We need to have a way to set the final distribution archive as one of the gradle 'configurations' in our build.gradle and refer it inside bamboo artifacts.",2.0,0.9806445591102868,0.9806323846008688,0.98074804244034,True,True,True,1369868797000.0,1370352091000.0,1370355691000.0,1370360730000.0,1370361630000.0,1369868797000.0,-1.0,1370352142000.0,1370355742000.0,1370352085000.0,1370355685000.0,1370352091000.0,1370355691000.0,"iperumal",1370355742000.0,1369868797000.0,43,1369868797000.0,-1.0,"Publish Spring XD final distribution zip as part of Bamboo artifactory plugin","Currently, Bamboo's gradle artifactory plugin has the artifacts configured to projects target(build) directory 'archives'. We need to have a way to set the final distribution archive as one of the gradle 'configurations' in our build.gradle and refer it inside bamboo artifacts.",2.0,False,2.0,0,0.0,0,0,False
"XD-149","Story","Sprint 5","2013-05-29T19:39:06.000+0000","mark.pollack","Mark Pollack","Create asciidoc toolchain script to create a 'toc2' style html output","Publishing an html version of the guide that uses the 'toc2' style format, table of contents on the left.

Looks like a 'stylesheet factory' http://asciidoctor.org/docs/produce-custom-themes-using-asciidoctor-stylesheet-factory/  needs to be installed.

From the theme showcase, http://themes.asciidoctor.org/preview/, the 'golo' theme has a toc2 style.

In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn't support using a single file with import as input. (See See https://github.com/asciidoctor/asciidoctor-gradle-plugin/issues/15 )

The mvn plugin does support this, so maybe using mvn is an option or just a bash script.",4.0,0.824455223552514,0.8247190023300459,0.0002637787775319,True,True,True,1369856346000.0,1369912606000.0,1369916206000.0,1369923685000.0,1369924585000.0,1369856346000.0,-1.0,1369856364000.0,1369859964000.0,1369912624000.0,1369916224000.0,1369912606000.0,1369916206000.0,"mark.pollack",1369859964000.0,1369856346000.0,43,1369856346000.0,-1.0,"Create asciidoc toolchain script to create a 'toc2' style html output","Publishing an html version of the guide that uses the 'toc2' style format, table of contents on the left.

Looks like a 'stylesheet factory' http://asciidoctor.org/docs/produce-custom-themes-using-asciidoctor-stylesheet-factory/  needs to be installed.

From the theme showcase, http://themes.asciidoctor.org/preview/, the 'golo' theme has a toc2 style.

In the root of the git repo for the wiki is a build.gradle file that uses the asciidoctor gradle plugin, but it doesn't support using a single file with import as input. (See See https://github.com/asciidoctor/asciidoctor-gradle-plugin/issues/15 )

The mvn plugin does support this, so maybe using mvn is an option or just a bash script.",4.0,False,4.0,0,0.0,0,0,False
"XD-148","Story","Sprint 5","2013-05-29T13:22:54.000+0000","hillert","jencompgeek","Improve User Experience when Redis is not running","Redis is not running we get a nasty stacktrace:

{code}
~/dev/git/spring-xd/dist/spring-xd/xd/bin (master)]  ./xd-container 
13/05/29 16:17:15 INFO support.ClassPathXmlApplicationContext: Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@851052d: startup date [Wed May 29 16:17:15 EDT 2013]; root of context hierarchy
13/05/29 16:17:16 INFO xml.XmlBeanDefinitionReader: Loading XML bean definitions from class path resource [META-INF/spring/launcher.xml]
13/05/29 16:17:16 INFO xml.XmlBeanDefinitionReader: Loading XML bean definitions from class path resource [META-INF/spring/redis.xml]
13/05/29 16:17:17 INFO support.DefaultListableBeanFactory: Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@4934ce4a: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,redisConnectionFactory,org.springframework.xd.dirt.launcher.RedisContainerLauncher#0]; root of factory hierarchy
13/05/29 16:17:17 INFO support.DefaultListableBeanFactory: Destroying singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@4934ce4a: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,redisConnectionFactory,org.springframework.xd.dirt.launcher.RedisContainerLauncher#0]; root of factory hierarchy
Exception in thread ""main"" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'redisConnectionFactory' defined in class path resource [META-INF/spring/redis.xml]: Invocation of init method failed; nested exception is com.lambdaworks.redis.RedisException: Unable to connect
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1488)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:524)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:461)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:295)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:292)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:626)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:932)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:479)
	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:139)
	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:83)
	at org.springframework.xd.dirt.launcher.RedisContainerLauncher.main(RedisContainerLauncher.java:68)
	at org.springframework.xd.ContainerMain.main(ContainerMain.java:68)
Caused by: com.lambdaworks.redis.RedisException: Unable to connect
	at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)
	at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.initConnection(LettuceConnectionFactory.java:108)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.afterPropertiesSet(LettuceConnectionFactory.java:86)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1547)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1485)
	... 13 more
Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:6379
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:150)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
{code}

I think it would be helpful to provide users with some helpful advice e.g.:

Redis does not seem to be running at 'localhost/127.0.0.1:6379'. Did you start or install Redis? Please see for further help http://foo/bar


",2.0,0.505342765725074,0.4268175556641,0.4234258461437141,True,True,True,1369833774000.0,1369925852000.0,1369929452000.0,1370015083000.0,1370015983000.0,1369833774000.0,-1.0,1369910926000.0,1369914526000.0,1369911544000.0,1369915144000.0,1369925852000.0,1369929452000.0,"hillert",1369914526000.0,1369833774000.0,43,1369833774000.0,-1.0,"Improve User Experience when Redis is not running","Redis is not running we get a nasty stacktrace:

{code}
~/dev/git/spring-xd/dist/spring-xd/xd/bin (master)]  ./xd-container 
13/05/29 16:17:15 INFO support.ClassPathXmlApplicationContext: Refreshing org.springframework.context.support.ClassPathXmlApplicationContext@851052d: startup date [Wed May 29 16:17:15 EDT 2013]; root of context hierarchy
13/05/29 16:17:16 INFO xml.XmlBeanDefinitionReader: Loading XML bean definitions from class path resource [META-INF/spring/launcher.xml]
13/05/29 16:17:16 INFO xml.XmlBeanDefinitionReader: Loading XML bean definitions from class path resource [META-INF/spring/redis.xml]
13/05/29 16:17:17 INFO support.DefaultListableBeanFactory: Pre-instantiating singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@4934ce4a: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,redisConnectionFactory,org.springframework.xd.dirt.launcher.RedisContainerLauncher#0]; root of factory hierarchy
13/05/29 16:17:17 INFO support.DefaultListableBeanFactory: Destroying singletons in org.springframework.beans.factory.support.DefaultListableBeanFactory@4934ce4a: defining beans [org.springframework.context.support.PropertySourcesPlaceholderConfigurer#0,redisConnectionFactory,org.springframework.xd.dirt.launcher.RedisContainerLauncher#0]; root of factory hierarchy
Exception in thread ""main"" org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'redisConnectionFactory' defined in class path resource [META-INF/spring/redis.xml]: Invocation of init method failed; nested exception is com.lambdaworks.redis.RedisException: Unable to connect
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1488)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:524)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:461)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:295)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:223)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:292)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:194)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:626)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:932)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:479)
	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:139)
	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:83)
	at org.springframework.xd.dirt.launcher.RedisContainerLauncher.main(RedisContainerLauncher.java:68)
	at org.springframework.xd.ContainerMain.main(ContainerMain.java:68)
Caused by: com.lambdaworks.redis.RedisException: Unable to connect
	at com.lambdaworks.redis.RedisClient.connect(RedisClient.java:176)
	at com.lambdaworks.redis.RedisClient.connectAsync(RedisClient.java:139)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.initConnection(LettuceConnectionFactory.java:108)
	at org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory.afterPropertiesSet(LettuceConnectionFactory.java:86)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1547)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1485)
	... 13 more
Caused by: java.net.ConnectException: Connection refused: localhost/127.0.0.1:6379
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:599)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:150)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
{code}

I think it would be helpful to provide users with some helpful advice e.g.:

Redis does not seem to be running at 'localhost/127.0.0.1:6379'. Did you start or install Redis? Please see for further help http://foo/bar


",2.0,False,2.0,0,0.0,0,0,False
"XD-147","Story","","2013-05-29T11:45:24.000+0000","iperumal","iperumal","Remove use of application plugin for redis project","Currently redis project uses application plugin to bundle distribution. This also includes 'java plugin' which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. ",2.0,0.1211044912923923,0.0,-1.0,True,False,True,1369827924000.0,1369830038000.0,1369833638000.0,1369844480000.0,1369845380000.0,1369827924000.0,-1.0,-1.0,-1.0,1369827924000.0,1369831524000.0,1369830038000.0,1369833638000.0,"iperumal",-1.0,-1.0,0,1369827924000.0,-1.0,"Remove use of application plugin for redis project","Currently redis project uses application plugin to bundle distribution. This also includes 'java plugin' which causes java specific build behavior on this project. We should try removing the use of application plugin and use something similar or custom tasks that does the bundling. ",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-143","Story","Sprint 5","2013-05-28T15:26:55.000+0000","iperumal","iperumal","Create externalized property file to support connectivity to redis","We need to have an externalized property file(under xd/conf/) for the xd-container & admin scripts to use as options. ",3.0,0.0034529424194875,0.0,0.227883708975702,True,True,True,1369754815000.0,1369757119000.0,1369760719000.0,1370421172000.0,1370422072000.0,1369754815000.0,-1.0,1369906872000.0,1369910472000.0,1369754815000.0,1369758415000.0,1369757119000.0,1369760719000.0,"iperumal",1369910472000.0,1369754815000.0,43,1369754815000.0,-1.0,"Create externalized property file to support connectivity to redis","We need to have an externalized property file(under xd/conf/) for the xd-container & admin scripts to use as options. ",3.0,False,3.0,0,0.0,0,1,False
"XD-142","Story","Sprint 7","2013-05-28T14:11:05.000+0000","grussell","jencompgeek","StreamServer Context Lifecycle Issues","The {{ModuleDeployer}} calls {{getBeansOfType}} before the context has had its {{PropertySourcesPlaceholderConfigurer}} attached. This can cause issues with {{FactoryBean}} s with placeholders in constructor args because the unresolved placeholder is used when the {{FactoryBean}} is pre-instantiated to determine the type of object it will serve up.",8.0,0.9095680634305996,0.9095717512561656,0.9095477803899876,True,True,True,1369750265000.0,1371723391000.0,1371726991000.0,1371918665000.0,1371919565000.0,1369750265000.0,1371723385000.0,1371723347000.0,1371726947000.0,1371723399000.0,1371726999000.0,1371723391000.0,1371726991000.0,"grussell",1371726947000.0,1371723385000.0,43,1371723385000.0,-1.0,"StreamServer Context Lifecycle Issues","The {{ModuleDeployer}} calls {{getBeansOfType}} before the context has had its {{PropertySourcesPlaceholderConfigurer}} attached. This can cause issues with {{FactoryBean}} s with placeholders in constructor args because the unresolved placeholder is used when the {{FactoryBean}} is pre-instantiated to determine the type of object it will serve up.",8.0,False,8.0,0,0.0,0,1,False
"XD-141","Story","Sprint 6","2013-05-28T12:18:17.000+0000","iperumal","iperumal","install-redis script should not use relative path to determine redis source dist","Currently, the install-redis script uses relative path to determine redis source  dist file. Since this is error prone, we need to fix it.",1.0,0.998948393424031,0.997957210651322,0.9979897465453852,True,True,True,1369743497000.0,1370603180000.0,1370604085000.0,1370603185000.0,1370604085000.0,1369743497000.0,-1.0,1370602355000.0,1370604085000.0,1370602327000.0,1370604085000.0,1370603180000.0,1370604085000.0,"iperumal",1370604085000.0,1369743497000.0,43,1369743497000.0,-1.0,"install-redis script should not use relative path to determine redis source dist","Currently, the install-redis script uses relative path to determine redis source  dist file. Since this is error prone, we need to fix it.",1.0,False,1.0,0,0.0,-1,0,False
"XD-140","Story","Sprint 5","2013-05-28T11:32:46.000+0000","grussell","grussell","Parameterize syslog Source; Add Support for TCP","The syslog source currently is hard-coded to use udp on port 11111.

Need to parameterize the port and provide an option to use TCP.",2.0,0.0014742014742014,0.0,0.0004914004914004,True,True,True,1369740766000.0,1369740787000.0,1369744387000.0,1369754111000.0,1369755011000.0,1369740766000.0,-1.0,1369740773000.0,1369744373000.0,1369740766000.0,1369744366000.0,1369740787000.0,1369744387000.0,"grussell",1369744373000.0,1369740766000.0,43,1369740766000.0,-1.0,"Parameterize syslog Source; Add Support for TCP","The syslog source currently is hard-coded to use udp on port 11111.

Need to parameterize the port and provide an option to use TCP.",2.0,False,2.0,0,0.0,-1,0,False
"XD-139","Story","Sprint 5","2013-05-28T10:33:26.000+0000","mark.pollack","iperumal","Update README.txt to include instructions on how to build","Building XD should not be part of the out first out of the box experience, but we should include some instructions on what targets are available, such as distXD.",2.0,0.0410458521580313,0.0,0.0041334230510429,True,True,True,1369737206000.0,1369737633000.0,1369741233000.0,1369746709000.0,1369747609000.0,1369737206000.0,-1.0,1369737249000.0,1369740849000.0,1369737206000.0,1369740806000.0,1369737633000.0,1369741233000.0,"mark.pollack",1369740849000.0,1369737206000.0,43,1369737206000.0,-1.0,"Update README.txt to include instructions on how to build","Building XD should not be part of the out first out of the box experience, but we should include some instructions on what targets are available, such as distXD.",2.0,False,2.0,0,0.0,0,0,False
"XD-138","Story","Sprint 5","2013-05-28T10:20:13.000+0000","mark.pollack","mark.pollack","Prepare blog post for M1","",5.0,0.6824675250074455,0.0,0.0002819109497217,True,True,True,1369736413000.0,1370508668000.0,1370512268000.0,1370867076000.0,1370867976000.0,1369736413000.0,-1.0,1369736732000.0,1369740332000.0,1369736413000.0,1369740013000.0,1370508668000.0,1370512268000.0,"mark.pollack",1369740332000.0,1369736413000.0,43,1369736413000.0,-1.0,"Prepare blog post for M1","",5.0,False,5.0,0,0.0,-1,0,False
"XD-137","Story","Sprint 5","2013-05-28T10:04:56.000+0000","mark.pollack","mark.pollack","Release Spring XD 1.0 M1","",5.0,0.658652280319738,0.0,0.0007188520700147,True,True,True,1369735496000.0,1370867988000.0,1370871588000.0,1371454004000.0,1371454904000.0,1369735496000.0,1370265326000.0,1369736732000.0,1369740332000.0,1369735496000.0,1369739096000.0,1370867988000.0,1370871588000.0,"mark.pollack",1369740332000.0,1369735559000.0,43,1370265326000.0,1370265326000.0,"Release Spring XD 1.0 M1","",10.0,True,10.0,1,-50.0,-1,0,False
"XD-136","Story","Sprint 5","2013-05-28T10:03:29.000+0000","mark.pollack","Mark Pollack","Documentation that points on how to install hadoop","Pointers to other documentation on how to install hadoop. ",3.0,0.9809459607395756,0.4625919716061973,0.0076227680500579,True,True,True,1369735409000.0,1369905661000.0,1369908968000.0,1369908068000.0,1369908968000.0,1369735409000.0,-1.0,1369736732000.0,1369740332000.0,1369815696000.0,1369819296000.0,1369905661000.0,1369908968000.0,"mark.pollack",1369740332000.0,1369735409000.0,43,1369735409000.0,-1.0,"Documentation that points on how to install hadoop","Pointers to other documentation on how to install hadoop. ",3.0,False,3.0,0,0.0,0,0,False
"XD-135","Story","Sprint 5","2013-05-28T10:02:55.000+0000","mark.pollack","Mark Pollack","Documentation on XD Architecture","Show overall flow of data in a stream, the server components 'admin' and 'container'.  How modules are deployed.",4.0,0.2180002382989785,0.2180083621277932,0.0018346313406483,True,True,True,1369735375000.0,1369896383000.0,1369899983000.0,1370473043000.0,1370473943000.0,1369735375000.0,-1.0,1369736730000.0,1369740330000.0,1369896389000.0,1369899989000.0,1369896383000.0,1369899983000.0,"mark.pollack",1369740330000.0,1369735375000.0,43,1369735375000.0,-1.0,"Documentation on XD Architecture","Show overall flow of data in a stream, the server components 'admin' and 'container'.  How modules are deployed.",4.0,False,4.0,0,0.0,-1,0,False
"XD-134","Story","Sprint 5","2013-05-28T09:59:31.000+0000","mark.pollack","Mark Pollack","Investigate link checking tool for user guide","Asciidoc/doctor might have one as part of it toolchain",2.0,0.4983690680964407,0.4651291226334229,0.0089801929504622,True,True,True,1369735171000.0,1369821801000.0,1369825401000.0,1369908098000.0,1369908998000.0,1369735171000.0,-1.0,1369736732000.0,1369740332000.0,1369816023000.0,1369819623000.0,1369821801000.0,1369825401000.0,"mark.pollack",1369740332000.0,1369735171000.0,43,1369735171000.0,-1.0,"Investigate link checking tool for user guide","Asciidoc/doctor might have one as part of it toolchain",2.0,False,2.0,0,0.0,0,0,False
"XD-133","Story","Sprint 5","2013-05-28T09:53:20.000+0000","mark.pollack","hillert","Fail Sonar CI build if there are any package tangles violated.","Similar to what would show up on structure101 reports.",1.0,0.9239277043431346,0.0,0.260453196654977,True,True,True,1369734800000.0,1369741650000.0,1369742214000.0,1369741314000.0,1369742214000.0,1369734800000.0,-1.0,1369736731000.0,1369740331000.0,1369734800000.0,1369738400000.0,1369741650000.0,1369742214000.0,"mark.pollack",1369740331000.0,1369734984000.0,43,1369734984000.0,-1.0,"Fail Sonar CI build if there are any package tangles violated.","Similar to what would show up on structure101 reports.",1.0,False,1.0,0,0.0,0,0,False
"XD-131","Story","Sprint 5","2013-05-28T09:42:13.000+0000","jencompgeek","jencompgeek","Upgrade Lettuce to 2.3.2","",2.0,0.5842763255423234,0.1135543805900084,0.1284281266986213,True,True,True,1369734133000.0,1369745957000.0,1369749557000.0,1369753470000.0,1369754370000.0,1369734133000.0,-1.0,1369736732000.0,1369740332000.0,1369736431000.0,1369740031000.0,1369745957000.0,1369749557000.0,"jencompgeek",1369740332000.0,1369734133000.0,43,1369734133000.0,-1.0,"Upgrade Lettuce to 2.3.2","",2.0,False,2.0,0,0.0,-1,0,False
"XD-130","Story","Sprint 5","2013-05-28T09:42:07.000+0000","mark.pollack","Mark Pollack","Remove container entry in Redis when the application context event to shutdown the container is fired","",3.0,0.5209385440136741,0.5058599236207814,0.0155627140321414,True,True,True,1369734127000.0,1369821292000.0,1369824892000.0,1369900550000.0,1369901450000.0,1369734127000.0,-1.0,1369736731000.0,1369740331000.0,1369818769000.0,1369822369000.0,1369821292000.0,1369824892000.0,"mark.pollack",1369740331000.0,1369734127000.0,43,1369734127000.0,-1.0,"Remove container entry in Redis when the application context event to shutdown the container is fired","",3.0,False,3.0,0,0.0,0,0,False
"XD-129","Story","Sprint 5","2013-05-28T09:28:09.000+0000","mark.pollack","Jennifer Hickey","Documenation for building/starting redis servers","",2.0,0.6084835917225471,0.6085694828739179,0.0211169530727559,True,True,True,1369733289000.0,1369832470000.0,1369836070000.0,1369895386000.0,1369896286000.0,1369733289000.0,-1.0,1369736731000.0,1369740331000.0,1369832484000.0,1369836084000.0,1369832470000.0,1369836070000.0,"mark.pollack",1369740331000.0,1369733289000.0,43,1369733289000.0,-1.0,"Documenation for building/starting redis servers","",2.0,False,2.0,0,0.0,-1,1,False
"XD-128","Story","Sprint 5","2013-05-28T07:36:05.000+0000","mark.pollack","grussell","Create TCP sink module","Based off SI tcp inbound adapter.  This will allow for event fowarding.",3.0,0.4955556994109123,0.0492682465304577,0.0506276796518257,True,True,True,1369726565000.0,1369826082000.0,1369829682000.0,1369926484000.0,1369927384000.0,1369726565000.0,-1.0,1369736732000.0,1369740332000.0,1369736459000.0,1369740059000.0,1369826082000.0,1369829682000.0,"mark.pollack",1369740332000.0,1369736075000.0,43,1369736075000.0,-1.0,"Create TCP sink module","Based off SI tcp inbound adapter.  This will allow for event fowarding.",3.0,False,3.0,0,0.0,-1,0,False
"XD-127","Story","Sprint 5","2013-05-28T07:35:42.000+0000","mark.pollack","grussell","Create TCP source module","Based off SI tcp inbound adapter.  This will allow for event forwarding that can select among the existing SI serialized/deserializer options.
",3.0,0.6675301428373924,0.072233662472986,0.0741473779187798,True,True,True,1369726542000.0,1369818280000.0,1369821880000.0,1369863071000.0,1369863971000.0,1369726542000.0,-1.0,1369736732000.0,1369740332000.0,1369736469000.0,1369740069000.0,1369818280000.0,1369821880000.0,"mark.pollack",1369740332000.0,1369736081000.0,43,1369736081000.0,-1.0,"Create TCP source module","Based off SI tcp inbound adapter.  This will allow for event forwarding that can select among the existing SI serialized/deserializer options.
",3.0,False,3.0,0,0.0,-1,1,False
"XD-126","Story","Sprint 5","2013-05-28T07:33:40.000+0000","mark.pollack","Mark Pollack","Documentation for sources, sinks, modules should define which attributes are required and which optional","This will eventually be supplied by the admin server, but for now write it up by hand in the documentation",2.0,0.8792661920073496,0.8793020785484612,0.0148010450160771,True,True,True,1369726420000.0,1370338952000.0,1370342552000.0,1370422160000.0,1370423060000.0,1369726420000.0,-1.0,1369736731000.0,1369740331000.0,1370338977000.0,1370342577000.0,1370338952000.0,1370342552000.0,"mark.pollack",1369740331000.0,1369726420000.0,43,1369726420000.0,-1.0,"Documentation for sources, sinks, modules should define which attributes are required and which optional","This will eventually be supplied by the admin server, but for now write it up by hand in the documentation",2.0,False,2.0,0,0.0,0,0,False
"XD-125","Story","Sprint 5","2013-05-28T07:30:30.000+0000","mark.pollack","mark.pollack","Stream documentation review","
",2.0,0.9739605326126736,0.0,0.073902823522788,True,True,True,1369726230000.0,1369864622000.0,1369868222000.0,1369867422000.0,1369868322000.0,1369726230000.0,1369868316000.0,1369736731000.0,1369740331000.0,1369726230000.0,1369729830000.0,1369864622000.0,1369868222000.0,"mark.pollack",1369740331000.0,1369726230000.0,43,1369868316000.0,1369868316000.0,"Stream documentation review","",1.0,True,1.0,1,100.0,-1,0,True
"XD-124","Story","Sprint 5","2013-05-28T06:58:40.000+0000","mark.pollack","jencompgeek","Clean shutdown of redis in xd-container","Need to shutdown cleanly, no exception messages are shown.  Order of components in the stream should be shut down from 'first to last'  (opposite of creation)",2.0,0.1507341830194774,0.0627051700240732,0.0646643809206207,True,True,True,1369724320000.0,1369753248000.0,1369756848000.0,1369915334000.0,1369916234000.0,1369724320000.0,-1.0,1369736730000.0,1369740330000.0,1369736354000.0,1369739954000.0,1369753248000.0,1369756848000.0,"mark.pollack",1369740330000.0,1369727926000.0,43,1369727926000.0,-1.0,"Clean shutdown of redis in xd-container","Need to shutdown cleanly, no exception messages are shown.  Order of components in the stream should be shut down from 'first to last'  (opposite of creation)",2.0,False,2.0,0,0.0,0,1,False
"XD-123","Bug","Sprint 5","2013-05-24T16:41:26.000+0000","iperumal","wkoh","XD scripts lib path needs to be dynamic","We currently have the manually created XD scripts. This makes it difficult to maintain as the lib path is error prone with the changes. We need to make sure that the properties such as lib path etc., are dynamically updated.",3.0,0.9453094228585992,0.9349412664210746,0.9355813118324412,True,True,True,1369413686000.0,1369740090000.0,1369743690000.0,1369758074000.0,1369758974000.0,1369413686000.0,-1.0,1369736731000.0,1369740331000.0,1369736510000.0,1369740110000.0,1369740090000.0,1369743690000.0,"iperumal",1369740331000.0,1369413686000.0,43,1369413686000.0,-1.0,"XD scripts lib path needs to be dynamic","We currently have the manually created XD scripts. This makes it difficult to maintain as the lib path is error prone with the changes. We need to make sure that the properties such as lib path etc., are dynamically updated.",3.0,False,3.0,0,0.0,0,0,False
"XD-122","Bug","","2013-05-24T16:19:59.000+0000","iperumal","Mark Pollack","XD scripts need to have spring-integration milestone versions updated","Spring-integration version is changed to 3.0.0.M2 and since we manually create the XD scripts, they still point to the 3.0.0.BUILD-SNAPSHOT version.

As discussed, we also need to have a better strategy on updating the lib directory inside the XD scripts.",2.0,-1.0,0.010126582278481,-1.0,False,False,False,1369412399000.0,-1.0,-1.0,1369412399000.0,1369413189000.0,1369412399000.0,-1.0,-1.0,-1.0,1369412407000.0,1369413189000.0,-1.0,-1.0,"iperumal",-1.0,-1.0,0,1369412399000.0,-1.0,"XD scripts need to have spring-integration milestone versions updated","Spring-integration version is changed to 3.0.0.M2 and since we manually create the XD scripts, they still point to the 3.0.0.BUILD-SNAPSHOT version.

As discussed, we also need to have a better strategy on updating the lib directory inside the XD scripts.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-120","Story","Sprint 5","2013-05-24T09:41:57.000+0000","mark.fisher","","Find and eliminate package-level cycles across XD projects","",3.0,0.7527864946227408,0.7512440562936803,0.7408256831938047,True,True,True,1369388517000.0,1369742353000.0,1369745953000.0,1369857652000.0,1369858552000.0,1369388517000.0,-1.0,1369736731000.0,1369740331000.0,1369741628000.0,1369745228000.0,1369742353000.0,1369745953000.0,"mark.fisher",1369740331000.0,1369388517000.0,43,1369388517000.0,-1.0,"Find and eliminate package-level cycles across XD projects","",3.0,False,3.0,0,0.0,0,1,False
"XD-119","Story","Sprint 9","2013-05-23T14:22:22.000+0000","thomas.risberg","thomas.risberg","HDFS sink should default to hdfs://localhost:8020","The current default is hdfs://localhost:9000 but most new distributions/installs use 8020",1.0,0.9856419667059916,0.0,0.7349316161142386,True,True,True,1369318942000.0,1374070236000.0,1374073836000.0,1374138549000.0,1374139449000.0,1369318942000.0,-1.0,1372861685000.0,1372865285000.0,1369318942000.0,1369322542000.0,1374070236000.0,1374073836000.0,"thomas.risberg",1372865285000.0,1369318942000.0,43,1369318942000.0,-1.0,"HDFS sink should default to hdfs://localhost:8020","The current default is hdfs://localhost:9000 but most new distributions/installs use 8020",1.0,False,1.0,0,0.0,0,0,False
"XD-118","Story","Sprint 4","2013-05-23T13:58:23.000+0000","mark.fisher","mark.fisher","replace testsource with time and testsink with log","This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing (should accept --interval for the seconds between time messages).",1.0,0.0207561156412157,0.0,0.0051890289103039,True,True,True,1369317503000.0,1369317531000.0,1369318852000.0,1369317952000.0,1369318852000.0,1369317503000.0,-1.0,1369317510000.0,1369318852000.0,1369317503000.0,1369318852000.0,1369317531000.0,1369318852000.0,"mark.fisher",1369318852000.0,1369317503000.0,43,1369317503000.0,-1.0,"replace testsource with time and testsink with log","This should facilitate testing while avoiding any class dependencies. Also, log is a generally useful sink by itself and time is a more interesting source for testing (should accept --interval for the seconds between time messages).",1.0,False,1.0,0,0.0,0,0,False
"XD-117","Story","Sprint 4","2013-05-23T12:36:27.000+0000","mark.fisher","mark.fisher","add spring-integration-groovy to container dependencies","This will enable the use of groovy scripts within modules.",1.0,0.019400352733686,0.0,0.0088183421516754,False,False,False,1369312587000.0,1369312598000.0,1369313154000.0,1369312587000.0,1369313154000.0,1369312587000.0,-1.0,1369312592000.0,1369313154000.0,1369312587000.0,1369313154000.0,1369312598000.0,1369313154000.0,"mark.fisher",1369313154000.0,1369312587000.0,43,1369312587000.0,-1.0,"add spring-integration-groovy to container dependencies","This will enable the use of groovy scripts within modules.",1.0,False,1.0,0,0.0,0,0,False
"XD-116","Story","Sprint 4","2013-05-23T12:17:39.000+0000","mark.fisher","mark.fisher","add SpEL 'filter' processor","It should provide an 'expression' param for SpEL and have a default value of true (accept everything).",1.0,0.5529531568228105,0.0,0.5458248472505092,True,True,True,1369311459000.0,1369312002000.0,1369312441000.0,1369311541000.0,1369312441000.0,1369311459000.0,-1.0,1369311995000.0,1369312441000.0,1369311459000.0,1369312441000.0,1369312002000.0,1369312441000.0,"mark.fisher",1369312441000.0,1369311459000.0,43,1369311459000.0,-1.0,"add SpEL 'filter' processor","It should provide an 'expression' param for SpEL and have a default value of true (accept everything).",1.0,False,1.0,0,0.0,0,0,False
"XD-115","Story","Sprint 4","2013-05-23T12:16:36.000+0000","mark.fisher","mark.fisher","add SpEL 'transform' processor","It should provide an 'expression' param for SpEL and have a default pass-thru of the payload.",1.0,0.5876089060987415,0.0,0.5711519845111326,True,True,True,1369311396000.0,1369312003000.0,1369312429000.0,1369311529000.0,1369312429000.0,1369311396000.0,-1.0,1369311986000.0,1369312429000.0,1369311396000.0,1369312429000.0,1369312003000.0,1369312429000.0,"mark.fisher",1369312429000.0,1369311396000.0,43,1369311396000.0,-1.0,"add SpEL 'transform' processor","It should provide an 'expression' param for SpEL and have a default pass-thru of the payload.",1.0,False,1.0,0,0.0,0,0,False
"XD-112","Story","Sprint 4","2013-05-23T10:15:09.000+0000","grussell","Gary Russell","Update XD to Use SI 3.0.0.M2","",1.0,0.0005891450033139,0.9885116724353782,0.0004628996454609,True,True,True,1369304109000.0,1369304165000.0,1369307765000.0,1369398262000.0,1369399162000.0,1369304109000.0,-1.0,1369304153000.0,1369307753000.0,1369398070000.0,1369399162000.0,1369304165000.0,1369307765000.0,"grussell",1369307753000.0,1369304109000.0,43,1369304109000.0,-1.0,"Update XD to Use SI 3.0.0.M2","",1.0,False,1.0,0,0.0,0,0,False
"XD-111","Story","Sprint 4","2013-05-23T08:39:54.000+0000","mark.pollack","David Turanski","Create final distribution zip across multiple projects","The final directory structure should look like

<install-dir>/xd
<install-dir>/redis
<install-dir>/gemfire

inside the XD directory 

/xd/bin - which has xd-container and xd-admin scripts
/xd/lib

inside the gemfire directory
/gemfire/bin - has the gemfire-server script
/gemfire/lib 

inside the redis directory is 

/redis/redis-latest-v.x.y.z.tar
/redis/README
/readis/install-redis  - script that does the basic 4 commands to install redis.


There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and 'redis-binary' directories and creates the final layout for the distribution.",5.0,0.2089580565572215,0.0712609004005553,0.0008690353707384,True,True,True,1369298394000.0,1369319313000.0,1369322913000.0,1369397605000.0,1369398505000.0,1369298394000.0,-1.0,1369298481000.0,1369302081000.0,1369305528000.0,1369309128000.0,1369319313000.0,1369322913000.0,"mark.pollack",1369302081000.0,1369298394000.0,43,1369298394000.0,-1.0,"Create final distribution zip across multiple projects","The final directory structure should look like

<install-dir>/xd
<install-dir>/redis
<install-dir>/gemfire

inside the XD directory 

/xd/bin - which has xd-container and xd-admin scripts
/xd/lib

inside the gemfire directory
/gemfire/bin - has the gemfire-server script
/gemfire/lib 

inside the redis directory is 

/redis/redis-latest-v.x.y.z.tar
/redis/README
/readis/install-redis  - script that does the basic 4 commands to install redis.


There should be a gradle task that runs after the distZip task, that will take the contents of different project directories, script diretories and 'redis-binary' directories and creates the final layout for the distribution.",5.0,False,5.0,0,0.0,0,0,False
"XD-110","Story","Sprint 4","2013-05-23T07:41:56.000+0000","mark.pollack","Mark Pollack","Remove use of system property xd.home to define location for install location, rely on environment variable XD_HOME","This is in the AdminMain and ContainerMain.  

Can get the environment property directly in java code unless provided explicitly on the command line using --xdHomeDir.",1.0,0.1768918534605341,0.1768837741410067,0.0002165257633341,True,True,True,1369294916000.0,1369404388000.0,1369407988000.0,1369912880000.0,1369913780000.0,1369294916000.0,-1.0,1369295050000.0,1369298650000.0,1369404383000.0,1369407983000.0,1369404388000.0,1369407988000.0,"mark.pollack",1369298650000.0,1369294916000.0,43,1369294916000.0,-1.0,"Remove use of system property xd.home to define location for install location, rely on environment variable XD_HOME","This is in the AdminMain and ContainerMain.  

Can get the environment property directly in java code unless provided explicitly on the command line using --xdHomeDir.",1.0,False,1.0,0,0.0,0,0,False
"XD-109","Story","Sprint 4","2013-05-23T01:06:00.000+0000","mark.pollack","mark.pollack","Documentation for starting redis and the Spring XD servers","",1.0,6.275616417012547e-05,0.0,1.7312045288310475e-05,True,True,True,1369271160000.0,1369271189000.0,1369274789000.0,1369732366000.0,1369733266000.0,1369271160000.0,-1.0,1369271168000.0,1369274768000.0,1369271160000.0,1369274760000.0,1369271189000.0,1369274789000.0,"mark.pollack",1369274768000.0,1369271160000.0,43,1369271160000.0,-1.0,"Documentation for starting Spring XD servers","",1.0,False,1.0,0,0.0,0,0,True
"XD-108","Story","Sprint 4","2013-05-23T00:27:49.000+0000","mark.pollack","iperumal","Build script should not package 'spring-xd-dirt' scripts ","We are packaging separate scripts to start XDAdmin and XDContainer.  The Gradle application plugin will generate an unwanted 'spring-xd-dirt' scripts, this should be removed from the bin directory when creating a distribution zip.",1.0,0.002405065236434,0.0,0.002366645663967,True,True,True,1369268869000.0,1369269182000.0,1369272782000.0,1369398111000.0,1369399011000.0,1369268869000.0,-1.0,1369269177000.0,1369272777000.0,1369268869000.0,1369272469000.0,1369269182000.0,1369272782000.0,"mark.pollack",1369272777000.0,1369268869000.0,43,1369268869000.0,-1.0,"Build script should not package 'spring-xd-dirt' scripts ","We are packaging separate scripts to start XDAdmin and XDContainer.  The Gradle application plugin will generate an unwanted 'spring-xd-dirt' scripts, this should be removed from the bin directory when creating a distribution zip.",1.0,False,1.0,0,0.0,0,0,False
"XD-106","Story","Sprint 4","2013-05-23T00:22:03.000+0000","mark.pollack","Mark Pollack","XDAdmin server does not log a message that it has started successfully","$ ./xd-admin --xdHomeDir /data/temp/spring-xd
processing module 'Module [name=file, type=sink]' from group 'tailtest' with index: 1
processing module 'Module [name=tail, type=source]' from group 'tailtest' with index: 0

Logging of 'processing module' should have log level, time..",1.0,0.6005530400884118,0.5948566339163726,2.797841931257024e-05,True,True,True,1369268523000.0,1369912469000.0,1369916069000.0,1370339878000.0,1370340778000.0,1369268523000.0,-1.0,1369268553000.0,1369272153000.0,1369906361000.0,1369909961000.0,1369912469000.0,1369916069000.0,"mark.pollack",1369272153000.0,1369268523000.0,43,1369268523000.0,-1.0,"Container server does not log a message that it has started or stopped successfully","$ ./xd-container 
processing module 'Module [name=file, type=sink]' from group 'tailtest' with index: 1
processing module 'Module [name=tail, type=source]' from group 'tailtest' with index: 0


Logging of 'processing module' should have log level, time..",1.0,False,1.0,0,0.0,0,0,True
"XD-105","Story","Sprint 4","2013-05-22T23:38:49.000+0000","mark.pollack","iperumal","Add LICENSE to be included in root directory of distribution","should contain apache licence",1.0,0.9999336650082918,0.0391671273263313,0.0200921319329279,True,True,True,1369265929000.0,1369401595000.0,1369401604000.0,1369400704000.0,1369401604000.0,1369265929000.0,-1.0,1369268655000.0,1369272255000.0,1369271243000.0,1369274843000.0,1369401595000.0,1369401604000.0,"mark.pollack",1369272255000.0,1369265929000.0,43,1369265929000.0,-1.0,"Add LICENSE to be included in root directory of distribution","should contain apache licence",1.0,False,1.0,0,0.0,0,0,False
"XD-104","Story","Sprint 4","2013-05-22T23:37:43.000+0000","mark.pollack","iperumal","Add README to be included in root directory of distribution","should explain basic layout of the distribution",1.0,0.9877839422942316,0.0389617087744892,0.0204976312047331,True,True,True,1369265863000.0,1369399928000.0,1369401586000.0,1369400686000.0,1369401586000.0,1369265863000.0,-1.0,1369268645000.0,1369272245000.0,1369271151000.0,1369274751000.0,1369399928000.0,1369401586000.0,"mark.pollack",1369272245000.0,1369265863000.0,43,1369265863000.0,-1.0,"Add README to be included in root directory of distribution","should explain basic layout of the distribution",1.0,False,1.0,0,0.0,0,0,False
"XD-103","Story","Sprint 4","2013-05-22T22:37:24.000+0000","mark.pollack","mark.pollack","Create XDAdmin server to start container launcher","This will launch the RedisContainerLauncher, in future will be able to select from a variety of middleware options.",1.0,0.9780477096443728,0.0,0.0998097468169179,True,True,True,1369262244000.0,1369268927000.0,1369269077000.0,1369268177000.0,1369269077000.0,1369262244000.0,-1.0,1369262926000.0,1369266526000.0,1369262244000.0,1369265844000.0,1369268927000.0,1369269077000.0,"mark.pollack",1369266526000.0,1369262244000.0,43,1369262244000.0,-1.0,"Create XDAdmin server to start container launcher","This will launch the RedisContainerLauncher, in future will be able to select from a variety of middleware options.",1.0,False,1.0,0,0.0,-1,0,False
"XD-102","Story","Sprint 4","2013-05-22T22:35:26.000+0000","mark.pollack","mark.pollack","Create XDContainer class to start stream server","Provide optional command line arg to embed the container launcher, aka - xd-admin server. 


XDContainer.sh --embeddAdmin",1.0,0.1278617710583153,0.0,0.1146148308135349,True,True,True,1369262126000.0,1369263014000.0,1369266614000.0,1369268171000.0,1369269071000.0,1369262126000.0,-1.0,1369262922000.0,1369266522000.0,1369262126000.0,1369265726000.0,1369263014000.0,1369266614000.0,"mark.pollack",1369266522000.0,1369262126000.0,43,1369262126000.0,-1.0,"Create XDContainer class to start stream server","Provide optional command line arg to embed the container launcher, aka - xd-admin server. 


XDContainer.sh --embeddAdmin",1.0,False,1.0,0,0.0,-1,2,False
"XD-99","Story","Sprint 4","2013-05-20T21:29:03.000+0000","mark.pollack","hillert","Sonar build is failing","https://build.springsource.org/browse/XD-SONAR-34

Caused by: java.lang.ClassNotFoundException: org.sonar.api.Plugin
        at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass(SelfFirstStrategy.java:50)
        at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:244)
        at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:230)
        ... 94 more",1.0,-1.0,0.0,0.999229711141678,False,True,True,1369085343000.0,-1.0,-1.0,1369120793000.0,1369121693000.0,1369085343000.0,-1.0,1369121665000.0,1369121693000.0,1369085343000.0,1369088943000.0,-1.0,-1.0,"mark.pollack",1369121693000.0,1369085343000.0,43,1369085343000.0,-1.0,"Sonar build is failing","https://build.springsource.org/browse/XD-SONAR-34

Caused by: java.lang.ClassNotFoundException: org.sonar.api.Plugin
        at org.codehaus.plexus.classworlds.strategy.SelfFirstStrategy.loadClass(SelfFirstStrategy.java:50)
        at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:244)
        at org.codehaus.plexus.classworlds.realm.ClassRealm.loadClass(ClassRealm.java:230)
        ... 94 more",1.0,False,1.0,0,0.0,0,0,False
"XD-98","Story","Sprint 4","2013-05-20T16:08:20.000+0000","mark.pollack","dturanski","Documentation on the module system and how to contribute new modules","For people who are familiar with Spring/Spring Integration provide documents that show how to add additional input sources/sinks.",3.0,-1.0,0.2225498477053418,0.2236828858344638,False,True,True,1369066100000.0,-1.0,-1.0,1369651235000.0,1369652135000.0,1369066100000.0,-1.0,1369197186000.0,1369200786000.0,1369196522000.0,1369200122000.0,-1.0,-1.0,"mark.pollack",1369200786000.0,1369066100000.0,43,1369066100000.0,-1.0,"Documentation on the module system and how to contribute new modules","For people who are familiar with Spring/Spring Integration provide documents that show how to add additional input sources/sinks.",3.0,False,3.0,0,0.0,0,0,False
"XD-91","Story","Sprint 4","2013-05-20T10:02:15.000+0000","mark.pollack","Mark Pollack","Documentation for field value taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,-1.0,0.2509354548489867,0.0003437539576935,False,True,True,1369044135000.0,-1.0,-1.0,1369651228000.0,1369652128000.0,1369044135000.0,-1.0,1369044344000.0,1369047944000.0,1369196702000.0,1369200302000.0,-1.0,-1.0,"mark.pollack",1369047944000.0,1369044135000.0,43,1369044135000.0,-1.0,"Documentation for field value taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,-1,0,False
"XD-90","Story","Sprint 4","2013-05-20T10:02:06.000+0000","mark.pollack","Mark Pollack","Documentation for rich gauge taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,0.900477343333808,0.7218401717381099,0.0002299687221439,True,True,True,1369044126000.0,1369897738000.0,1369901338000.0,1369991181000.0,1369992081000.0,1369044126000.0,-1.0,1369044344000.0,1369047944000.0,1369728398000.0,1369731998000.0,1369897738000.0,1369901338000.0,"mark.pollack",1369047944000.0,1369044126000.0,43,1369044126000.0,-1.0,"Documentation for rich gauge taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,-1,0,False
"XD-89","Story","Sprint 4","2013-05-20T10:01:54.000+0000","mark.pollack","Mark Pollack","Documentation for gauge taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,0.5842711625888605,0.6486114286261857,0.0001574266357654,True,True,True,1369044114000.0,1369897733000.0,1369901333000.0,1370504212000.0,1370505112000.0,1369044114000.0,-1.0,1369044344000.0,1369047944000.0,1369991734000.0,1369995334000.0,1369897733000.0,1369901333000.0,"mark.pollack",1369047944000.0,1369044114000.0,43,1369044114000.0,-1.0,"Documentation for gauge taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,0,0,False
"XD-88","Story","Sprint 4","2013-05-20T10:01:44.000+0000","mark.pollack","Mark Pollack","Documentation for counter taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,0.9900781007944536,0.989823805543052,0.0003507520708986,True,True,True,1369044104000.0,1369721559000.0,1369725159000.0,1369727448000.0,1369728348000.0,1369044104000.0,-1.0,1369044344000.0,1369047944000.0,1369721385000.0,1369724985000.0,1369721559000.0,1369725159000.0,"mark.pollack",1369047944000.0,1369044104000.0,43,1369044104000.0,-1.0,"Documentation for counter taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,-1,0,False
"XD-87","Story","Sprint 4","2013-05-20T10:01:20.000+0000","mark.pollack","Mark Pollack","End user guide for data taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,0.8215864385423692,0.9994110859643296,0.0001800981295131,True,True,True,1369044080000.0,1370243855000.0,1370247455000.0,1370503495000.0,1370504395000.0,1369044080000.0,-1.0,1369044343000.0,1369047943000.0,1370503535000.0,1370504395000.0,1370243855000.0,1370247455000.0,"mark.pollack",1369047943000.0,1369044080000.0,43,1369044080000.0,-1.0,"Documentation that introduces taps","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,0,0,True
"XD-86","Story","Sprint 4","2013-05-20T10:00:53.000+0000","mark.pollack","Mark Pollack","Documentation for ""twittersearch | file"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,0.9962127857467078,0.7637335249218191,0.0008075677451873,True,True,True,1369044053000.0,1369401796000.0,1369403156000.0,1369402256000.0,1369403156000.0,1369044053000.0,-1.0,1369044343000.0,1369047943000.0,1369318312000.0,1369321912000.0,1369401796000.0,1369403156000.0,"mark.pollack",1369047943000.0,1369044053000.0,43,1369044053000.0,-1.0,"Documentation for ""twittersearch | file"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,-1,0,False
"XD-85","Story","Sprint 4","2013-05-20T10:00:37.000+0000","mark.pollack","Mark Pollack","Documentation for ""gemfirecq | file"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,-1.0,0.2509172478781191,0.0005032323634324,False,True,True,1369044037000.0,-1.0,-1.0,1369651206000.0,1369652106000.0,1369044037000.0,-1.0,1369044343000.0,1369047943000.0,1369196612000.0,1369200212000.0,-1.0,-1.0,"mark.pollack",1369047943000.0,1369044037000.0,43,1369044037000.0,-1.0,"Documentation for ""gemfirecq | file"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,-1,0,False
"XD-84","Story","Sprint 4","2013-05-20T10:00:22.000+0000","mark.pollack","Mark Pollack","Documentation for ""http | gemfire"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,-1.0,0.2509118895931114,0.0005278928032259,False,True,True,1369044022000.0,-1.0,-1.0,1369651200000.0,1369652100000.0,1369044022000.0,-1.0,1369044343000.0,1369047943000.0,1369196596000.0,1369200196000.0,-1.0,-1.0,"mark.pollack",1369047943000.0,1369044022000.0,43,1369044022000.0,-1.0,"Documentation for ""http | gemfire"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,0,0,False
"XD-82","Story","Sprint 4","2013-05-20T09:59:52.000+0000","mark.pollack","Mark Pollack","Documentation for ""tail | file"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,-1.0,0.2217183720013215,0.0018703441220039,False,True,True,1369043992000.0,-1.0,-1.0,1369230758000.0,1369231658000.0,1369043992000.0,-1.0,1369044343000.0,1369047943000.0,1369085601000.0,1369089201000.0,-1.0,-1.0,"mark.pollack",1369047943000.0,1369043992000.0,43,1369043992000.0,-1.0,"Documentation for ""tail | file"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,0,0,False
"XD-81","Story","Sprint 4","2013-05-20T09:59:25.000+0000","mark.pollack","","Documentation for ""syslog | file"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,0.3586253645995944,0.3701167562519865,0.0005316051945421,True,True,True,1369043965000.0,1369298967000.0,1369302567000.0,1369754119000.0,1369755019000.0,1369043965000.0,-1.0,1369044343000.0,1369047943000.0,1369307138000.0,1369310738000.0,1369298967000.0,1369302567000.0,"mark.pollack",1369047943000.0,1369043965000.0,43,1369043965000.0,-1.0,"Documentation for ""syslog | file"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,False,3.0,0,0.0,-1,1,False
"XD-80","Story","Sprint 4","2013-05-20T09:58:57.000+0000","mark.pollack","luke","Documentation for ""http | hdfs"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",1.0,0.9914602118532784,0.0,0.0005909130701216,True,True,True,1369043937000.0,1369723464000.0,1369727064000.0,1369728417000.0,1369729317000.0,1369043937000.0,1369299002000.0,1369044342000.0,1369047942000.0,1369043937000.0,1369047537000.0,1369723464000.0,1369727064000.0,"mark.pollack",1369047942000.0,1369043937000.0,43,1369299002000.0,1369299002000.0,"Documentation for ""http | hdfs"" processing","Put on the guide as a section in an 'input-stream' wiki page.


",3.0,True,3.0,1,-66.66666666666666,0,1,False
"XD-79","Story","Sprint 4","2013-05-20T09:58:24.000+0000","mark.pollack","luke","End user guide for data streams","Put on the guide as a section in an 'streams' wiki page.

End user focused, no need to mention spring underpinning, impl details.

",5.0,0.3895886217984092,0.0,0.000647492071901,True,True,True,1369043904000.0,1369308647000.0,1369312247000.0,1369722549000.0,1369723449000.0,1369043904000.0,-1.0,1369044344000.0,1369047944000.0,1369043904000.0,1369047504000.0,1369308647000.0,1369312247000.0,"mark.pollack",1369047944000.0,1369043904000.0,43,1369043904000.0,-1.0,"End user guide for data streams","Put on the guide as a section in an 'streams' wiki page.

End user focused, no need to mention spring underpinning, impl details.

",5.0,False,5.0,0,0.0,-1,0,False
"XD-78","Story","Sprint 4","2013-05-20T09:56:33.000+0000","mark.pollack","Mark Pollack","Documentation for ""http | file"" processing","Put on the guide as a section in an 'input sources' wiki page.

https://github.com/springsource/spring-xd/wiki/GuideGettingStarted

",1.0,0.8682436475328095,0.1667717100801787,0.0021859667318201,True,True,True,1369043793000.0,1369261453000.0,1369265053000.0,1369293583000.0,1369294483000.0,1369043793000.0,1369294477000.0,1369044341000.0,1369047941000.0,1369085601000.0,1369089201000.0,1369261453000.0,1369265053000.0,"mark.pollack",1369047941000.0,1369043793000.0,43,1369294477000.0,1369294477000.0,"Documentation for ""http | file"" processing","Put on the guide as a section in an 'input sources' wiki page.

https://github.com/springsource/spring-xd/wiki/GuideGettingStarted

",3.0,True,3.0,1,-66.66666666666666,-1,1,False
"XD-76","Story","Sprint 4","2013-05-20T09:45:00.000+0000","mark.pollack","Winston Koh","Add gemfire-server application to the distribution zip","",1.0,0.9837859843888233,0.987430419090418,0.0048696850362877,True,True,True,1369043100000.0,1369294416000.0,1369298016000.0,1369297658000.0,1369298558000.0,1369043100000.0,1369294427000.0,1369044344000.0,1369047944000.0,1369295347000.0,1369298558000.0,1369294416000.0,1369298016000.0,"mark.pollack",1369047944000.0,1369043100000.0,43,1369294427000.0,1369294427000.0,"Add gemfire-server application to the distribution zip of the project spring-xd-gemfire-server","",2.0,True,2.0,1,-50.0,0,0,True
"XD-74","Story","Sprint 4","2013-05-20T09:16:40.000+0000","mark.pollack","grussell","Create XD module for tail file adapter","",1.0,0.9221322868996192,0.0,0.0157339292587998,True,True,True,1369041400000.0,1369213883000.0,1369217483000.0,1369227548000.0,1369228448000.0,1369041400000.0,-1.0,1369044343000.0,1369047943000.0,1369041400000.0,1369045000000.0,1369213883000.0,1369217483000.0,"mark.pollack",1369047943000.0,1369041400000.0,43,1369041400000.0,-1.0,"Create XD module for tail file adapter","",1.0,False,1.0,0,0.0,0,0,False
"XD-73","Story","","2013-05-17T10:03:29.000+0000","wkoh","wkoh","add test to start/stop stream server","in additional to existing tests that check for redis connection, we need to add tests that start/stop stream server. ",2.0,-1.0,0.0,-1.0,False,False,True,1368785009000.0,-1.0,-1.0,1368803652000.0,1368804552000.0,1368785009000.0,-1.0,-1.0,-1.0,1368785009000.0,1368788609000.0,-1.0,-1.0,"wkoh",-1.0,-1.0,0,1368785009000.0,-1.0,"add test to start/stop stream server","in additional to existing tests that check for redis connection, we need to add tests that start/stop stream server. ",-1.0,False,-1.0,0,-1.0,-1,0,False
"XD-72","Story","Sprint 4","2013-05-17T08:24:13.000+0000","mark.pollack","mark.fisher","Provide a http source","stream should be able to ingest data from http ",5.0,0.755663869830749,0.0,0.7351060174460492,True,True,True,1368779053000.0,1369051760000.0,1369055360000.0,1369139037000.0,1369139937000.0,1368779053000.0,-1.0,1369044341000.0,1369047941000.0,1368779053000.0,1368782653000.0,1369051760000.0,1369055360000.0,"mark.pollack",1369047941000.0,1368779053000.0,43,1368779053000.0,-1.0,"Provide a http source","stream should be able to ingest data from http ",5.0,False,5.0,0,0.0,0,0,False
"XD-71","Story","Sprint 9","2013-05-17T00:34:15.000+0000","mark.pollack","Mark Pollack","Remove UUID from Tuple class or replace with more efficient implementation","The Java UUID class is known not to be the fasted implementation available. 

See https://github.com/stephenc/eaio-uuid and http://mvnrepository.com/artifact/com.eaio.uuid/uuid for high perf impls.  ",1.0,0.9781968675928292,0.9780463983331336,0.2553898840547999,True,True,True,1368750855000.0,1384496216000.0,1384499816000.0,1384846266000.0,1384847166000.0,1368750855000.0,-1.0,1372861690000.0,1372865290000.0,1384493794000.0,1384497394000.0,1384496216000.0,1384499816000.0,"mark.pollack",1372865290000.0,1368750855000.0,43,1368750855000.0,-1.0,"Remove UUID from Tuple class or replace with more efficient implementation","The Java UUID class is known not to be the fasted implementation available. 

See https://github.com/stephenc/eaio-uuid and http://mvnrepository.com/artifact/com.eaio.uuid/uuid for high perf impls.  ",1.0,False,1.0,0,0.0,0,0,False
"XD-70","Story","Sprint 3","2013-05-16T23:25:41.000+0000","mark.pollack","mark.pollack","Create general structure for AsciiDoc based wiki and Spring XD guide.","Adopt Asciidoc as the markdown syntax, useful for generating pdf and more feature rich than standard github flavored markdown.  Loosely following the conventions of https://community.jboss.org/wiki/TheHolyGrailAsciiDocOnGitHubToDocBookTrain that have generate docbook/pdf docs from the Asciidoc wiki.

The asciidoctor project is a key element in the adoption of AsciiDoc for use as the format in github, it is the rendering engine used by github for AsciiDoc.  See http://asciidoctor.org/docs/asciidoc-writers-guide/ for guidance.

",2.0,0.1038374717832957,0.0,0.0835214446952596,False,False,False,1368746741000.0,1368746787000.0,1368747184000.0,1368746741000.0,1368747184000.0,1368746741000.0,-1.0,1368746778000.0,1368747184000.0,1368746741000.0,1368747184000.0,1368746787000.0,1368747184000.0,"mark.pollack",1368747184000.0,1368746741000.0,43,1368746741000.0,-1.0,"Create general structure for AsciiDoc based wiki and Spring XD guide.","Adopt Asciidoc as the markdown syntax, useful for generating pdf and more feature rich than standard github flavored markdown.  Loosely following the conventions of https://community.jboss.org/wiki/TheHolyGrailAsciiDocOnGitHubToDocBookTrain that have generate docbook/pdf docs from the Asciidoc wiki.

The asciidoctor project is a key element in the adoption of AsciiDoc for use as the format in github, it is the rendering engine used by github for AsciiDoc.  See http://asciidoctor.org/docs/asciidoc-writers-guide/ for guidance.

",2.0,False,2.0,0,0.0,-1,0,False
"XD-69","Story","Sprint 9","2013-05-16T22:47:47.000+0000","mark.pollack","mark.pollack","Export of data from HDFS to MongoDB","Based on a single process Spring Batch job, ETL of data from HDFS to MongoDB.",5.0,-1.0,0.0,0.1945130266118337,False,True,True,1368744467000.0,-1.0,-1.0,1389910391000.0,1389911291000.0,1368744467000.0,-1.0,1372861690000.0,1372865290000.0,1368744467000.0,1368748067000.0,-1.0,-1.0,"mark.pollack",1372865290000.0,1368744467000.0,43,1368744467000.0,-1.0,"Export of data from HDFS to MongoDB","Based on a single process Spring Batch job, ETL of data from HDFS to MongoDB.",5.0,False,5.0,0,0.0,-1,1,False
"XD-67","Story","Sprint 3","2013-05-14T12:06:23.000+0000","wkoh","wkoh","Create brew-based install for Spring XD","- Host the Spring XD distributable zip somewhere that is accessible by external http request.
- Create brew formula for Spring XD install while specifying redis as dependency. 
- starting up stream server upon successful brew install

couple of questions:
- should we name the brew task springxd? (name not taken yet)
- should we start the stream server as part of the brew install process?
- should we specify redis as a recommended dependency? user can pass in 'brew install springxd --without-redis' to skip redis installation. by default, 'brew install springxd' will install redis as well.",8.0,0.0216451606429732,0.0168389155345066,0.0398968513575886,True,True,True,1368533183000.0,1368543969000.0,1368547569000.0,1369030593000.0,1369031493000.0,1368533183000.0,-1.0,1368553064000.0,1368556664000.0,1368541574000.0,1368545174000.0,1368543969000.0,1368547569000.0,"wkoh",1368556664000.0,1368533183000.0,43,1368533183000.0,-1.0,"Submit a brew-based install for Spring XD","- Host the Spring XD distributable zip somewhere that is accessible by external http request.
- Create brew formula for Spring XD install while specifying redis as dependency. 
- starting up stream server upon successful brew install

couple of questions:
- should we name the brew task springxd? (name not taken yet)
- should we start the stream server as part of the brew install process?
- should we specify redis as a recommended dependency? user can pass in 'brew install springxd --without-redis' to skip redis installation. by default, 'brew install springxd' will install redis as well.",8.0,False,8.0,0,0.0,0,1,True
"XD-66","Bug","","2013-05-13T11:25:31.000+0000","luke","luke","Gauge and Counter hash and equals should not depend on values","The value is mutable so this causes problems if storing metrics in a HashSet, for example.",1.0,-1.0,0.0,-1.0,False,False,True,1368444331000.0,-1.0,-1.0,1368448221000.0,1368449121000.0,1368444331000.0,-1.0,-1.0,-1.0,1368444331000.0,1368447931000.0,-1.0,-1.0,"luke",-1.0,-1.0,0,1368444331000.0,-1.0,"Gauge and Counter hash and equals should not depend on values","The value is mutable so this causes problems if storing metrics in a HashSet, for example.",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-65","Story","Sprint 3","2013-05-13T09:26:16.000+0000","mark.pollack","Mark Pollack","Gemfire Sink to update a gemfire cache.","Update a gemfire region.",2.0,-1.0,0.2103028297203376,0.1917739533344365,False,True,True,1368437176000.0,-1.0,-1.0,1369040576000.0,1369041476000.0,1368437176000.0,-1.0,1368553065000.0,1368556665000.0,1368564262000.0,1368567862000.0,-1.0,-1.0,"mark.pollack",1368556665000.0,1368437176000.0,43,1368437176000.0,-1.0,"Gemfire Sink to update a gemfire cache.","Update a gemfire region.",2.0,False,2.0,0,0.0,-1,0,False
"XD-63","Story","Sprint 3","2013-05-13T09:02:52.000+0000","mark.pollack","Mark Pollack","Document tuple data structure on XD wiki","",2.0,0.9521473130116792,2.055663111009882,0.3710378468663402,True,True,True,1368435772000.0,1368736761000.0,1368740361000.0,1368750988000.0,1368751888000.0,1368435772000.0,-1.0,1368553063000.0,1368556663000.0,1369085600000.0,1368751888000.0,1368736761000.0,1368740361000.0,"mark.pollack",1368556663000.0,1368435772000.0,43,1368435772000.0,-1.0,"Document tuple data structure on XD wiki","",2.0,False,2.0,0,0.0,-1,0,False
"XD-62","Story","Sprint 3","2013-05-13T09:02:00.000+0000","mark.pollack","mminella","Use the tuple data structure to process data in a spring batch step ","Do not require a POJO in order to do end-to-end processing in a batch step.",5.0,0.3075834272569684,4.4490142961659386e-05,0.1933558090944443,True,True,True,1368435720000.0,1368622385000.0,1368625985000.0,1369041696000.0,1369042596000.0,1368435720000.0,-1.0,1368553063000.0,1368556663000.0,1368435747000.0,1368439347000.0,1368622385000.0,1368625985000.0,"mark.pollack",1368556663000.0,1368435720000.0,43,1368435720000.0,-1.0,"Use the tuple data structure to process data in a spring batch step ","Do not require a POJO in order to do end-to-end processing in a batch step.",5.0,False,5.0,0,0.0,-1,0,False
"XD-61","Story","Sprint 3","2013-05-13T07:47:41.000+0000","mark.pollack","wkoh","Create distributable artifact that contains server application and start/stop scripts","The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat/jetty etc.

Now there is a launch task

task(launch, dependsOn: 'classes', type: JavaExec) {
		main = 'org.springframework.xd.dirt.stream.StreamServer'
		classpath = sourceSets.test.runtimeClasspath
	}


The same main should be referenced in the application plugin, a task to create a .zip distributable is needed.

Ideally would be nice to 
1. download .zip
2. unzip
3. cd spring-xd/bin
4. xdserver start


and gracefully shutdown later with 

5. xdserver stop

I don't know if we can/should bundle redis, I think we should bundle it.

The scripts can be for unix/linux and for windows.  

Discuss a brew based install as well.
",8.0,0.1649053327294162,0.0154979944829886,0.1991667252597034,True,True,True,1368431261000.0,1368532111000.0,1368535711000.0,1369041924000.0,1369042824000.0,1368431261000.0,-1.0,1368553064000.0,1368556664000.0,1368440739000.0,1368444339000.0,1368532111000.0,1368535711000.0,"mark.pollack",1368556664000.0,1368431261000.0,43,1368431261000.0,-1.0,"Create distributable artifact that contains server application and start/stop scripts","The gradle application task should get us most of the way to create a distributable artifact akin to what you see when downloading tomcat/jetty etc.

Now there is a launch task

task(launch, dependsOn: 'classes', type: JavaExec) {
		main = 'org.springframework.xd.dirt.stream.StreamServer'
		classpath = sourceSets.test.runtimeClasspath
	}


The same main should be referenced in the application plugin, a task to create a .zip distributable is needed.

Ideally would be nice to 
1. download .zip
2. unzip
3. cd spring-xd/bin
4. xdserver start


and gracefully shutdown later with 

5. xdserver stop

I don't know if we can/should bundle redis, I think we should bundle it.

The scripts can be for unix/linux and for windows.  

Discuss a brew based install as well.
",8.0,False,8.0,0,0.0,-1,4,False
"XD-59","Story","Sprint 3","2013-05-10T09:27:04.000+0000","mark.pollack","mark.pollack","Tuple should support storing nested tuples","Nested tuple structures shoudl be supported,  getTuple(int index), getTuple(String name)",1.0,0.9934369311633028,0.0,0.4105081343305637,True,True,True,1368178024000.0,1369085625000.0,1369089225000.0,1369090721000.0,1369091621000.0,1368178024000.0,-1.0,1368553063000.0,1368556663000.0,1368178024000.0,1368181624000.0,1369085625000.0,1369089225000.0,"mark.pollack",1368556663000.0,1368178024000.0,43,1368178024000.0,-1.0,"Tuple should support storing nested tuples","Nested tuple structures shoudl be supported,  getTuple(int index), getTuple(String name)",1.0,False,1.0,0,0.0,-1,0,False
"XD-58","Bug","Sprint 5","2013-05-09T13:02:50.000+0000","gregturn","mark.pollack","build.gradle doesn't handle a small handful of libraries","Trying to build spring-xd for the first resulted in lots of errors inside STS (I had an empty .m2 repo).",1.0,-1.0,0.0,0.9996453816006337,False,True,True,1368104570000.0,-1.0,-1.0,1369736411000.0,1369737311000.0,1368104570000.0,-1.0,1369736732000.0,1369737311000.0,1368104570000.0,1368108170000.0,-1.0,-1.0,"gregturn",1369737311000.0,1368104570000.0,43,1368104570000.0,-1.0,"build.gradle doesn't handle a small handful of libraries","Trying to build spring-xd for the first resulted in lots of errors inside STS (I had an empty .m2 repo).",1.0,False,1.0,0,0.0,0,0,False
"XD-57","Story","Sprint 2","2013-05-08T14:21:40.000+0000","mark.fisher","mark.fisher","add counter module","",1.0,0.5763888888888888,0.0,0.0347222222222222,False,False,False,1368022900000.0,1368022983000.0,1368023044000.0,1368022900000.0,1368023044000.0,1368022900000.0,-1.0,1368022905000.0,1368023044000.0,1368022900000.0,1368023044000.0,1368022983000.0,1368023044000.0,"mark.fisher",1368023044000.0,1368022900000.0,43,1368022900000.0,-1.0,"add counter module","",1.0,False,1.0,0,0.0,0,0,False
"XD-56","Story","Sprint 3","2013-05-08T13:35:44.000+0000","mark.pollack","mark.pollack","Switch to use Lettuce driver for Redis","Replace the use of Jedis with Lettuce as it has higher performance",2.0,0.8566563174733258,0.8566121889331926,0.8566374052418402,True,True,True,1368020144000.0,1368563701000.0,1368567301000.0,1368653754000.0,1368654654000.0,1368020144000.0,-1.0,1368563689000.0,1368567289000.0,1368563673000.0,1368567273000.0,1368563701000.0,1368567301000.0,"mark.pollack",1368567289000.0,1368020144000.0,43,1368020144000.0,-1.0,"Switch to use Lettuce driver for Redis","Replace the use of Jedis with Lettuce as it has higher performance",2.0,False,2.0,0,0.0,-1,0,False
"XD-55","Story","Sprint 2","2013-05-08T13:18:45.000+0000","mark.pollack","mark.pollack","XD Metrics backed Field Counter","A Spring Integration based @ServiceActivator that counts the occurrence of field names, from either a tuple data structure or a POJO, using the Spring XD metrics support.",3.0,0.8894216236187149,0.0,0.0012609299253238,True,True,True,1368019125000.0,1368654663000.0,1368658263000.0,1368732777000.0,1368733677000.0,1368019125000.0,1368656623000.0,1368020026000.0,1368023626000.0,1368019125000.0,1368022725000.0,1368654663000.0,1368658263000.0,"mark.pollack",1368023626000.0,1368019125000.0,43,1368656623000.0,1368656623000.0,"SI ServiceActivator for an XD Metrics backed Field Value Counter","A Spring Integration based @ServiceActivator that counts the occurrence of field names, from either a tuple data structure or a POJO, using the Spring XD metrics support.",1.0,True,1.0,1,200.0,-1,0,True
"XD-54","Story","Sprint 2","2013-05-08T13:17:42.000+0000","mark.pollack","mark.pollack","XD Metrics backed Message Counter","A Spring Integration based @ServiceActivator that counts the number of messages using the Spring XD metrics support",1.0,0.0056151855638891,0.0,0.0054438167267409,True,True,True,1368019062000.0,1368020045000.0,1368023645000.0,1368193223000.0,1368194123000.0,1368019062000.0,-1.0,1368020015000.0,1368023615000.0,1368019062000.0,1368022662000.0,1368020045000.0,1368023645000.0,"mark.pollack",1368023615000.0,1368019062000.0,43,1368019062000.0,-1.0,"XD Metrics backed Message Counter","A Spring Integration based @ServiceActivator that counts the number of messages using the Spring XD metrics support",1.0,False,1.0,0,0.0,-1,0,False
"XD-53","Story","Sprint 3","2013-05-08T13:07:47.000+0000","mark.pollack","mark.pollack","Design and document desired high level DSL for configuring data processing in XD","Start to explore how the DSL can cover both advanced (non-linear) spring integration flows as well as spring batch jobs.",13.0,0.9655953299587128,0.0,0.0424231497226206,True,True,True,1368018467000.0,1380186432000.0,1380190032000.0,1380619083000.0,1380619983000.0,1368018467000.0,-1.0,1368553063000.0,1368556663000.0,1368018467000.0,1368022067000.0,1380186432000.0,1380190032000.0,"mark.pollack",1368556663000.0,1368436324000.0,43,1368436324000.0,-1.0,"Design and document desired high level DSL for configuring data processing in XD","Start to explore how the DSL can cover both advanced (non-linear) spring integration flows as well as spring batch jobs.",13.0,False,13.0,0,0.0,-1,0,False
"XD-52","Story","Sprint 2","2013-05-07T14:36:00.000+0000","mark.fisher","mark.fisher","add twitter search source module","",1.0,0.0265832681782642,0.0,0.0086004691164972,True,True,True,1367937360000.0,1367937394000.0,1367938639000.0,1367937739000.0,1367938639000.0,1367937360000.0,-1.0,1367937371000.0,1367938639000.0,1367937360000.0,1367938639000.0,1367937394000.0,1367938639000.0,"mark.fisher",1367938639000.0,1367937360000.0,43,1367937360000.0,-1.0,"add twitter search source module","",1.0,False,1.0,0,0.0,0,0,False
"XD-51","Story","Sprint 2","2013-05-07T14:08:54.000+0000","mark.fisher","mark.fisher","Add xd.stream.name property in StreamPlugin","",1.0,0.0341005967604433,0.0,0.0076726342710997,True,True,True,1367935734000.0,1367935774000.0,1367936907000.0,1367936007000.0,1367936907000.0,1367935734000.0,-1.0,1367935743000.0,1367936907000.0,1367935734000.0,1367936907000.0,1367935774000.0,1367936907000.0,"mark.fisher",1367936907000.0,1367935734000.0,43,1367935734000.0,-1.0,"Add xd.stream.name property in StreamPlugin","",1.0,False,1.0,0,0.0,0,0,False
"XD-50","Story","Sprint 2","2013-05-07T07:37:05.000+0000","mark.fisher","mark.fisher","Add tap support to DIRT","syntax:

{code}
tap @ somechannel --key=value | somecounter
{code}
",2.0,0.7652796561498416,9.263204698297422e-05,0.0002964225503455,True,True,True,1367912225000.0,1367994840000.0,1367998440000.0,1368019279000.0,1368020179000.0,1367912225000.0,-1.0,1367912257000.0,1367915857000.0,1367912235000.0,1367915835000.0,1367994840000.0,1367998440000.0,"mark.fisher",1367915857000.0,1367912225000.0,43,1367912225000.0,-1.0,"Add tap support to DIRT","syntax:

{code}
tap @ somechannel --key=value | somecounter
{code}
",2.0,False,2.0,0,0.0,0,0,False
"XD-49","Story","Sprint 2","2013-05-07T07:26:46.000+0000","mark.fisher","Mark Fisher","Move Redis Queue Channel Adapters into spring-integration-redis","Currently these implementations are in the spring-xd-dirt module, but they should be moved into spring-integration-redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone, so this should be okay for the Redis Channel Adapters also - until Spring Integration M2 is released.",3.0,0.0326007770403764,0.0239083122719933,3.181871839747844e-07,True,True,True,1367911606000.0,1368628811000.0,1368632411000.0,1389910335000.0,1389911235000.0,1367911606000.0,1369294894000.0,1367911613000.0,1367915213000.0,1368437580000.0,1368441180000.0,1368628811000.0,1368632411000.0,"mark.fisher",1367915213000.0,1367911606000.0,43,1369294894000.0,1369294894000.0,"Move Redis Queue Channel Adapters into spring-integration-redis","Currently these implementations are in the spring-xd-dirt module, but they should be moved into spring-integration-redis. We are already depending upon Spring Integration 3.0 snapshots since the ChannelRegistry implementation is not yet at a milestone, so this should be okay for the Redis Channel Adapters also - until Spring Integration M2 is released.",1.0,True,1.0,1,200.0,0,0,False
"XD-48","Story","Sprint 2","2013-05-06T12:50:05.000+0000","mark.fisher","mark.fisher","Parameterizable streams","",2.0,0.020242914979757,0.0128205128205128,0.0138326585695006,True,True,True,1367844605000.0,1367844665000.0,1367847569000.0,1367846669000.0,1367847569000.0,1367844605000.0,-1.0,1367844646000.0,1367847569000.0,1367844643000.0,1367847569000.0,1367844665000.0,1367847569000.0,"mark.fisher",1367847569000.0,1367844639000.0,43,1367844639000.0,-1.0,"Parameterizable streams","",2.0,False,2.0,0,0.0,0,1,False
"XD-47","Story","Sprint 2","2013-05-06T09:59:43.000+0000","mark.fisher","Mark Pollack","HDFS sink module","",1.0,0.974145891043398,0.1270967990150815,0.0304997691597414,True,True,True,1367834383000.0,1367935663000.0,1367938351000.0,1367937451000.0,1367938351000.0,1367834383000.0,-1.0,1367837554000.0,1367841154000.0,1367847597000.0,1367851197000.0,1367935663000.0,1367938351000.0,"mark.fisher",1367841154000.0,1367834383000.0,43,1367834383000.0,-1.0,"HDFS sink module","",1.0,False,1.0,0,0.0,0,0,False
"XD-45","Story","Sprint 2","2013-05-06T06:44:41.000+0000","mark.pollack","luke","Redis based repositories should use a ExpiryStrategy class to encapsulate key expiry functionality","There is duplicated code in Redis based repositories that related to expiry behavior, move into a common shared helper class and/or base class.",1.0,0.9772325696837556,0.0001340762862145,0.0237394833913,True,True,True,1367822681000.0,1368434926000.0,1368438526000.0,1368448290000.0,1368449190000.0,1367822681000.0,-1.0,1367837554000.0,1367841154000.0,1367822765000.0,1367826365000.0,1368434926000.0,1368438526000.0,"mark.pollack",1367841154000.0,1367822681000.0,43,1367822681000.0,-1.0,"Remove the expiry of keys in Redis based repositories","There is duplicated code in Redis based repositories that related to expiry behavior, move into a common shared helper class and/or base class.",1.0,False,1.0,0,0.0,0,0,True
"XD-44","Story","Sprint 2","2013-05-06T06:39:10.000+0000","mark.pollack","luke","Redis based repositories should use a NamingStrategy class to calculate the name of the key to use for persistence","RedisCounterRepository and RedisGaugeRepository have duplicated code that needs to be factored out into a one place.  One such duplication is the determination of the key name to use for persistence.  This should be abstracted out into a strategy helper class.",1.0,-1.0,0.0008498946582895,0.0286514376599441,False,True,True,1367822350000.0,-1.0,-1.0,1368352104000.0,1368353004000.0,1367822350000.0,-1.0,1367837554000.0,1367841154000.0,1367822801000.0,1367826401000.0,-1.0,-1.0,"mark.pollack",1367841154000.0,1367822350000.0,43,1367822350000.0,-1.0,"Redis based repositories should use a NamingStrategy class to calculate the name of the key to use for persistence","RedisCounterRepository and RedisGaugeRepository have duplicated code that needs to be factored out into a one place.  One such duplication is the determination of the key name to use for persistence.  This should be abstracted out into a strategy helper class.",1.0,False,1.0,0,0.0,0,0,False
"XD-43","Story","Sprint 2","2013-05-06T06:35:36.000+0000","mark.pollack","Luke Taylor","Metric repositories should support Spring Data CrudRepository interface","This provides common CRUD behavior and a shared interface that can be useful in testing scenarios.  ",5.0,0.692485662222202,0.9916873875329012,0.0175374738666842,True,True,True,1367822136000.0,1368430932000.0,1368434532000.0,1368700382000.0,1368701282000.0,1367822136000.0,1368436316000.0,1367837554000.0,1367841154000.0,1368693974000.0,1368697574000.0,1368430932000.0,1368434532000.0,"mark.pollack",1367841154000.0,1367822497000.0,43,1368436316000.0,1368436316000.0,"Metric repositories should support Spring Data CrudRepository interface","This provides common CRUD behavior and a shared interface that can be useful in testing scenarios.  ",2.0,True,2.0,1,150.0,-1,0,False
"XD-41","Bug","Sprint 1","2013-05-02T10:18:13.000+0000","grussell","grussell","Add JUnit @Rule so Tests Fail Fast with Clear Messaging if Redis Not Available","",1.0,-1.0,0.0,0.7333097010516365,False,True,True,1367489893000.0,-1.0,-1.0,1367497456000.0,1367498356000.0,1367489893000.0,-1.0,1367496099000.0,1367498356000.0,1367489893000.0,1367493493000.0,-1.0,-1.0,"grussell",1367498356000.0,1367489893000.0,43,1367489893000.0,-1.0,"Add JUnit @Rule so Tests Fail Fast with Clear Messaging if Redis Not Available","",1.0,False,1.0,0,0.0,0,2,False
"XD-40","Story","Sprint 1","2013-04-29T08:45:01.000+0000","mark.pollack","Mark Pollack","Build script that creates an executable server as an artifact","Gradle application plugin is a good starting point.  this should be the main server that would host SI based modules to do syslog->file ingestion (as an example)",2.0,0.9993028334260176,0.9217111777232222,2.170168323680605e-05,True,True,True,1367225101000.0,1367593479000.0,1367593736000.0,1367592836000.0,1367593736000.0,1367225101000.0,-1.0,1367225109000.0,1367228709000.0,1367564876000.0,1367568476000.0,1367593479000.0,1367593736000.0,"mark.pollack",1367228709000.0,1367225101000.0,43,1367225101000.0,-1.0,"Build script that creates an executable server as an artifact","Gradle application plugin is a good starting point.  this should be the main server that would host SI based modules to do syslog->file ingestion (as an example)",2.0,False,2.0,0,0.0,0,0,False
"XD-37","Story","Sprint 1","2013-04-22T08:48:16.000+0000","mark.pollack","Mark Pollack","Gradle based multi-project build","",2.0,0.0226682381101568,0.0077561013614203,5.304450599817328e-05,True,True,True,1366620496000.0,1366634171000.0,1366637771000.0,1367222863000.0,1367223763000.0,1366620496000.0,-1.0,1366620528000.0,1366624128000.0,1366625175000.0,1366628775000.0,1366634171000.0,1366637771000.0,"mark.pollack",1366624128000.0,1366620496000.0,43,1366620496000.0,-1.0,"Gradle based multi-project build","multi project build. - look to Spring Framework for source of starting point.",2.0,False,2.0,0,0.0,0,0,True
"XD-36","Story","Sprint 1","2013-04-22T08:47:07.000+0000","mark.pollack","Mark Pollack","SI Outbound HDFS Channel Adapter","",3.0,0.6058466118962946,0.4462247608889906,4.059570506666692e-05,True,True,True,1366620427000.0,1367933731000.0,1367937331000.0,1368787244000.0,1368788144000.0,1366620427000.0,1367847683000.0,1366620515000.0,1366624115000.0,1367587716000.0,1367591316000.0,1367933731000.0,1367937331000.0,"mark.pollack",1366624115000.0,1366620427000.0,43,1367935790000.0,1367847683000.0,"SI Outbound HDFS Channel Adapter","",1.0,True,1.0,2,200.0,0,0,False
"XD-35","Story","Sprint 1","2013-04-22T08:46:13.000+0000","mark.pollack","Mark Pollack","Create CI process for XD build","bamboo based",1.0,0.2008961702723517,0.2006067819552381,0.0006674601507619,True,True,True,1366620373000.0,1366663414000.0,1366667014000.0,1366833718000.0,1366834618000.0,1366620373000.0,-1.0,1366620516000.0,1366624116000.0,1366663352000.0,1366666952000.0,1366663414000.0,1366667014000.0,"mark.pollack",1366624116000.0,1366620373000.0,43,1366620373000.0,-1.0,"Create CI process for XD build","bamboo based",1.0,False,1.0,0,0.0,0,0,False
"XD-31","Story","Sprint 1","2013-04-17T07:51:18.000+0000","mark.pollack","mark.pollack","Create field-value counters","A field-value counter is useful for bar chart graphs, Strings on x-axis and count on y-axis.  Maps well to zset in redis. Implementations for in-memory and redis.",5.0,0.8135522323512869,0.0,2.2271074611744308e-05,True,True,True,1366185078000.0,1368194203000.0,1368197803000.0,1368653749000.0,1368654649000.0,1366185078000.0,1366626126000.0,1366185133000.0,1366188733000.0,1366185078000.0,1366188678000.0,1368194203000.0,1368197803000.0,"mark.pollack",1366188733000.0,1366185078000.0,43,1368436405000.0,1366626126000.0,"Create field-value counters","A field-value counter is useful for bar chart graphs, Strings on x-axis and count on y-axis.  Maps well to zset in redis. Implementations for in-memory and redis.",3.0,True,3.0,2,66.66666666666666,-1,1,False
"XD-30","Story","Sprint 1","2013-04-17T07:50:13.000+0000","mark.pollack","mark.pollack","Create a simple counter","A simple counters can increment/decrement a number.  Implementations for in-memory and redis.",1.0,0.9390481014712888,0.0,7.7660882213629e-05,True,True,True,1366185013000.0,1367527186000.0,1367530786000.0,1367613404000.0,1367614304000.0,1366185013000.0,1366626115000.0,1366185124000.0,1366188724000.0,1366185013000.0,1366188613000.0,1367527186000.0,1367530786000.0,"mark.pollack",1366188724000.0,1366185013000.0,43,1366626115000.0,1366626115000.0,"Create a simple counter service","A simple counters can increment/decrement a number.  Implementations for in-memory and redis.",3.0,True,3.0,1,-66.66666666666666,-1,1,True
"XD-29","Story","Sprint 1","2013-04-17T07:49:15.000+0000","mark.pollack","Mark Pollack","Create rich gague service","A rich gague stores a number and also rmd, min, max. Implementations for in-memory and redis.",5.0,0.8675002921043894,0.6343346633241441,6.516916411099427e-05,True,True,True,1366184955000.0,1368434601000.0,1368438201000.0,1368777306000.0,1368778206000.0,1366184955000.0,1366626106000.0,1366185124000.0,1366188724000.0,1367829944000.0,1367833544000.0,1368434601000.0,1368438201000.0,"mark.pollack",1366188724000.0,1366184955000.0,43,1368436291000.0,1366626106000.0,"Create rich gauge service","A rich gauge stores a number and also rmd, min, max. Implementations for in-memory and redis.",3.0,True,3.0,2,66.66666666666666,0,1,True
"XD-28","Story","Sprint 1","2013-04-17T07:48:08.000+0000","mark.pollack","mark.pollack","Create simple gague service","A gauge just stores a number.  Implementations for in-memory and redis.",1.0,0.9215281684591689,0.0,0.0001441576155841,True,True,True,1366184888000.0,1367693519000.0,1367697119000.0,1367821085000.0,1367821985000.0,1366184888000.0,1366626088000.0,1366185124000.0,1366188724000.0,1366184888000.0,1366188488000.0,1367693519000.0,1367697119000.0,"mark.pollack",1366188724000.0,1366184888000.0,43,1366626088000.0,1366626088000.0,"Create simple gague service","A gauge just stores a number.  Implementations for in-memory and redis.",3.0,True,3.0,1,-66.66666666666666,0,1,False
"XD-27","Story","Sprint 1","2013-04-16T10:33:07.000+0000","mark.pollack","","Gemfire CQ module for ingestion","",1.0,0.8514225065818588,0.793530106080267,3.7503469070889058e-06,True,False,True,1366108387000.0,1368605662000.0,1368609262000.0,1369040549000.0,1369041449000.0,1366621179000.0,-1.0,1366108398000.0,1366111998000.0,1368435860000.0,1368439460000.0,1368605662000.0,1368609262000.0,"mark.pollack",1366111998000.0,1366108387000.0,43,1367830155000.0,-1.0,"Gemfire CQ module for ingestion","",0.0,False,0.0,1,-1.0,-1,0,False
"XD-26","Story","","2013-04-16T10:30:12.000+0000","mark.pollack","","Basic Performance Test For syslog injestion","",20.0,0.9271604999528628,0.9271601691649424,-1.0,False,False,False,1366108212000.0,1371713982000.0,1371717582000.0,1372153482000.0,1372154382000.0,1372154268000.0,-1.0,-1.0,-1.0,1371713980000.0,1371717580000.0,1371713982000.0,1371717582000.0,"mark.pollack",-1.0,-1.0,0,1372154268000.0,-1.0,"Basic Performance Test For syslog injestion","",-1.0,False,-1.0,0,-1.0,0,0,False
"XD-24","Story","Sprint 1","2013-04-16T10:23:51.000+0000","mark.pollack","","Create pipes and filters DSL for ingestion","",2.0,-1.0,0.9965357314555584,1.8179834927098863e-05,False,False,True,1366107831000.0,-1.0,-1.0,1367592093000.0,1367592993000.0,1366625001000.0,-1.0,1366107858000.0,1366111458000.0,1367587848000.0,1367591448000.0,-1.0,-1.0,"mark.pollack",1366111458000.0,1366107831000.0,43,1366625001000.0,-1.0,"Create pipes and filters DSL for ingestion","Initial simple handcoded implementation for straight through pipe and filter model, e.g. a | b | c",2.0,False,2.0,0,0.0,0,0,True
"XD-23","Story","Sprint 2","2013-04-16T10:21:12.000+0000","mark.pollack","","Create File module","",1.0,0.9998373030302038,0.9998340272522884,0.9444515419632612,True,True,True,1366107672000.0,1367939000000.0,1367939298000.0,1367938398000.0,1367939298000.0,1366622211000.0,-1.0,1367837554000.0,1367841154000.0,1367938994000.0,1367939298000.0,1367939000000.0,1367939298000.0,"mark.pollack",1367841154000.0,1367830163000.0,43,1367830163000.0,-1.0,"add file source and sink modules","",1.0,False,1.0,0,0.0,0,0,True
"XD-20","Story","Sprint 1","2013-04-16T09:55:10.000+0000","mark.pollack","","DIRT Runtime","",3.0,0.9984871672493648,0.3485773229003454,7.425774322617491e-06,True,False,True,1366106110000.0,1367585196000.0,1367587437000.0,1367586537000.0,1367587437000.0,1366622467000.0,-1.0,1366106121000.0,1366109721000.0,1366622467000.0,1366626067000.0,1367585196000.0,1367587437000.0,"mark.pollack",1366109721000.0,1366106110000.0,43,1366622467000.0,-1.0,"DIRT Runtime that deploys an application context across multiple nodes using redis.","",3.0,False,3.0,0,0.0,-1,0,True
"XD-14","Story","Sprint 9","2013-04-12T05:48:58.000+0000","mark.pollack","Mark Pollack","DIRT Runtime on EC2","",3.0,-1.0,0.3125384701651119,0.2944755249674837,False,False,True,1365745738000.0,-1.0,-1.0,1389909668000.0,1389910568000.0,1373298177000.0,-1.0,1372861689000.0,1372865289000.0,1373298177000.0,1373301777000.0,-1.0,-1.0,"mark.pollack",1372865289000.0,1365745738000.0,43,1373298177000.0,-1.0,"Design for deploying XD on EC2","create enough of a design to develop additional stories.",3.0,False,3.0,0,0.0,0,1,True
"XD-13","Story","Sprint 2","2013-04-12T05:48:36.000+0000","mark.pollack","grussell","Tail file channel adapters","",8.0,0.691750908035301,0.6323714357301502,0.6345293656219989,True,True,True,1365745716000.0,1368026196000.0,1368029796000.0,1369041494000.0,1369042394000.0,1367830487000.0,1368436187000.0,1367837555000.0,1367841155000.0,1367830441000.0,1367834041000.0,1368026196000.0,1368029796000.0,"mark.pollack",1367841155000.0,1367830487000.0,43,1368436233000.0,1368436187000.0,"Tail file channel adapters","",2.0,True,2.0,2,300.0,-1,0,False
"XD-9","Story","Sprint 3","2013-04-12T05:44:44.000+0000","mark.pollack","grussell","Reactor based tcp ingestion","",13.0,0.8674486084854801,0.8159598870640816,0.8515321536469572,True,True,True,1365745484000.0,1368605541000.0,1368609141000.0,1369041675000.0,1369042575000.0,1368435902000.0,-1.0,1368553063000.0,1368556663000.0,1368435778000.0,1368439378000.0,1368605541000.0,1368609141000.0,"mark.pollack",1368556663000.0,1368435906000.0,43,1368435906000.0,-1.0,"Basic implementation of a reactor based tcp server","",13.0,False,13.0,0,0.0,-1,0,True
"XD-8","Story","Sprint 1","2013-04-12T05:44:27.000+0000","mark.pollack","Mark Pollack","Syslog Ingestion","",1.0,0.999993233029479,0.4577459949945239,0.1883695557119477,True,False,True,1365745467000.0,1367666550000.0,1367666563000.0,1367665663000.0,1367666563000.0,1366624841000.0,-1.0,1366107343000.0,1366110943000.0,1366624841000.0,1366628441000.0,1367666550000.0,1367666563000.0,"mark.pollack",1366110943000.0,1365745467000.0,43,1366624841000.0,-1.0,"Syslog Ingestion","Have a syslog.xml config file that can be added to a module and registered with a module registry.",1.0,False,1.0,0,0.0,-1,0,True
"XD-7","Story","Sprint 1","2013-04-12T05:43:52.000+0000","mark.pollack","mark.pollack","Tuple data structure","",1.0,0.2562170143587948,0.0,0.2027152117017678,False,False,False,1365745432000.0,1366200755000.0,1366204355000.0,1367521631000.0,1367522531000.0,1366621009000.0,-1.0,1366105677000.0,1366109277000.0,1365745432000.0,1365749032000.0,1366200755000.0,1366204355000.0,"mark.pollack",1366109277000.0,1365745432000.0,43,1366621009000.0,-1.0,"Tuple data structure","The tuple data structure should be backward compatible in functionality for use in spring batch.  Porting over FieldSet tests in spring batch to use the tuple data structure is one way help ensure that compatibility.",1.0,False,1.0,0,0.0,0,0,True
"XD-6","Story","Sprint 1","2013-04-12T05:43:17.000+0000","mark.pollack","Mark Pollack","Channel Registry","",3.0,0.6232747639339326,0.5266149404547095,0.1720067826423626,True,False,True,1365745397000.0,1366788562000.0,1366792162000.0,1367418181000.0,1367419081000.0,1366619641000.0,-1.0,1366033282000.0,1366036882000.0,1366626784000.0,1366630384000.0,1366788562000.0,1366792162000.0,"mark.pollack",1366036882000.0,1365745397000.0,43,1367418959000.0,1366619713000.0,"Channel Registry","",0.0,True,0.0,4,-1.0,0,0,False
"XD-2","Story","Sprint 1","2013-04-12T05:38:02.000+0000","mark.pollack","mark.pollack","HDFS Channel Adapter","",1.0,-1.0,0.0,0.0381555031369728,False,False,False,1365745082000.0,-1.0,-1.0,1373297483000.0,1373298383000.0,1366620303000.0,-1.0,1366033282000.0,1366036882000.0,1365745082000.0,1365748682000.0,-1.0,-1.0,"mark.pollack",1366036882000.0,1365745082000.0,43,1366620303000.0,-1.0,"HDFS Core writing helper classes","Simple file writer that has existed in the spring hadoop samples.",1.0,False,1.0,0,0.0,-1,0,True
"XD-1","Story","Sprint 1","2013-04-12T05:37:16.000+0000","mark.pollack","Mark Pollack","HDFS ItemWriter","Base integration of core HDFS writer functionality with Spring Batch.",1.0,0.9999853652306262,0.7387015145655869,0.3883083270950781,True,True,True,1365745036000.0,1367999907000.0,1367999940000.0,1367999040000.0,1367999940000.0,1366620665000.0,-1.0,1366620634000.0,1366624234000.0,1367410737000.0,1367414337000.0,1367999907000.0,1367999940000.0,"mark.pollack",1366624234000.0,1366620665000.0,43,1366620665000.0,-1.0,"HDFS ItemWriter","Base integration of core HDFS writer functionality with Spring Batch.",1.0,False,1.0,0,0.0,0,2,False
